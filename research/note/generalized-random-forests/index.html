
<!doctype html>
<html lang="ja" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Statistics, Tomoshige Nakamura, Machine Learning, Juntendo University">
      
      
        <meta name="author" content="Tomoshige Nakamura">
      
      
        <link rel="canonical" href="https://tomoshige.github.io/website/research/note/generalized-random-forests/">
      
      
        <link rel="prev" href="../../">
      
      
        <link rel="next" href="../variable-importance/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.50">
    
    
      
        <title>Generalized random forests - Statistical Learning Laboratory</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.a40c8224.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          コンテンツにスキップ
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="ヘッダー">
    <a href="../../.." title="Statistical Learning Laboratory" class="md-header__button md-logo" aria-label="Statistical Learning Laboratory" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Statistical Learning Laboratory
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Generalized random forests
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="検索" placeholder="検索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="検索">
        
        <button type="reset" class="md-search__icon md-icon" title="クリア" aria-label="クリア" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            検索を初期化
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://tomoshige.github.io/website" title="リポジトリへ" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    tomoshige/website
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="タブ" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../lectures/" class="md-tabs__link">
          
  
    
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
    
  
  Research

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="ナビゲーション" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Statistical Learning Laboratory" class="md-nav__button md-logo" aria-label="Statistical Learning Laboratory" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Statistical Learning Laboratory
  </label>
  
    <div class="md-nav__source">
      <a href="https://tomoshige.github.io/website" title="リポジトリへ" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    tomoshige/website
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../lectures/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Lectures
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Lectures
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../lectures/LA/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Linear algebra
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Linear algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/01-introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/02-vector-and-matrix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. Vector and Matrix1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/03-vector-and-matrix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. Vector and Matrix2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/04-vector-and-matrix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. Vector and Matrix3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/05-vector-and-matrix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Vector and Matrix4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/06-vector-and-matrix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Vector and Matrix5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/07-vector-and-matrix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Vector and Matrix6
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/08-vector-and-matrix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Vector and Matrix7
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/09-exercise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. Exersice
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/10-system-of-linear-equation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. System of Linear Equation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/11-system-of-linear-equation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. System of Linear Equation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/12-system-of-linear-equation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. System of Linear Equation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/13-system-of-linear-equation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. System of Linear Equation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/14-system-of-linear-equation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. System of Linear Equation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/15-system-of-linear-equation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. System of Linear Equation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/16-system-of-linear-equation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. System of Linear Equation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/17-system-of-linear-equation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. System of Linear Equation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/18-system-of-linear-equation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. System of Linear Equation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/19-exercise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    19. Exercise
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/20-midterm-exam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    20. Midterm Exam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/21-determinant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    21. Determinant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/22-determinant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    22. Determinant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/23-determinant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    23. Determinant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/24-determinant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    24. Determinant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/25-exercise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    25. Exercise
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/26-vector-space-and-inner-product/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    26. Vector space
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/27-vector-space-and-inner-product/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    27. Vector space
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/28-vector-space-and-inner-product/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    28. Vector space
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/29-exercise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    29. Exercise
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/30-report/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    30. Report Exam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/31-exam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    31. Semester Exam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/32-eigen-value-vector/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    32. Eigen value and vector
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/33-eigen-value-vector/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    33. Eigen value and vector
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/34-eigen-value-vector/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    34. Eigen value and vector
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/35-eigen-value-vector/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    35. Eigen value and vector
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/36-eigen-value-vector/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    36. Eigen value and vector
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/37-exercise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    37. Exercise
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/38-midterm-exam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    38. Midterm Exam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/39-singular-value-decomposition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    39. Singular value decomposition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/40-singular-value-decomposition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    40. Singular value decomposition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/41-principal-component-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    41. Principal component analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/42-principal-component-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    42. Principal component analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/43-principal-component-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    43. Principal component analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/44-factor-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    44. Factor analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/45-factor-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    45. Factor analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/46-nonlinear-dimension-reduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    46. Nonlinear dimension reduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/47-wrapup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    47. Wrap up
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/LA/48-final-report/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    48. Final Report
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../lectures/SIWS/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Data Science without syntax
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Data Science without syntax
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/01-getting-started/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Getting-Started
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/02-visualization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. Data Visualizaion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/03-wrangling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. Data Wrangling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/04-data-import-and-tidy-data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. Data Import and Tidy Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/05-simple-linear-regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Simple Linear Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/06-multiple-regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Multiple Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/07-sampling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Sampling Method
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/08-Estimation-CI-Bootstrapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Estimation, Confidence Interval and Bootstrapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/09-hypothesis-testing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. Hypothesis Testing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/10-inference-for-regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Inference for Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../lectures/SIWS/11-tell-your-story-with-data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Tell Your Story with Data
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../lectures/SP/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Statistics and Probability
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Statistics and Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Research
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Research
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Generalized random forests
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Generalized random forests
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目次">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目次
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. ランダムフォレストの概観
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. ランダムフォレストの概観">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-leo-breiman-2012" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 Leo Breiman によるランダムフォレストの提案から、2012年までのランダムフォレストの理論解析の歩み
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-2012" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 2012年以降のランダムフォレストの理論解析の歩み：一致性と、漸近正規性の証明
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.2 2012年以降のランダムフォレストの理論解析の歩み：一致性と、漸近正規性の証明">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scornet-et-al-2015" class="md-nav__link">
    <span class="md-ellipsis">
      Scornet et al. 2015の貢献
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wager-et-al-2014-2018" class="md-nav__link">
    <span class="md-ellipsis">
      Wager et al., 2014-2018の貢献
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#guess-and-check-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Guess-and-Check Forest
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#input" class="md-nav__link">
    <span class="md-ellipsis">
      Input
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      アルゴリズム
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#guess-and-check-forest_1" class="md-nav__link">
    <span class="md-ellipsis">
      Guess-and-Check Forest
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの一致性・漸近正規性
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一般化ランダムフォレストの一致性・漸近正規性">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストと局所推定方程式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      仮定と理論的な背景
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの一致性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの漸近正規性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの漸近分散推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      推定方程式に対応した勾配ベースの回帰木
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの応用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一般化ランダムフォレストの応用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quantile-regression-forest" class="md-nav__link">
    <span class="md-ellipsis">
      quantile regression forest と、一般化ランダムフォレストによる分位点推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストによる条件付き因果効果の推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-centering-r-learner-by-nie-and-wager-2021" class="md-nav__link">
    <span class="md-ellipsis">
      Local Centering (R-Learner by Nie and Wager., 2021)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instrumental-variable" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストによるInstrumental variableを用いた因果効果の推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-survival-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Random Survival Forest と 一般化ランダムフォレストによる生存関数推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#causal-survival-effect" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストによる 条件付き causal survival effectの推定
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの推定精度の改善
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一般化ランダムフォレストの推定精度の改善">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#locally-linear-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Locally Linear Forest
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストのハイパーパラメータ選択
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレスト予測の解釈
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの応用事例
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一般化ランダムフォレストの応用事例">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      医療分野
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      経済分野
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      金融分野
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      マーケティング分野
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      因果効果推定のいくつかの重要なトピックス
    </span>
  </a>
  
    <nav class="md-nav" aria-label="因果効果推定のいくつかの重要なトピックス">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#overlap" class="md-nav__link">
    <span class="md-ellipsis">
      因果推論における特徴量空間の外れ値とoverlap仮定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    <span class="md-ellipsis">
      因果推論における結果変数の外れ値の問題
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conformal-predictions" class="md-nav__link">
    <span class="md-ellipsis">
      Conformal Predictions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    <span class="md-ellipsis">
      まとめ
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../variable-importance/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Variable Importance Measures
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../forest-kernel-and-its-asymptotics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Random forest kernels
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../consistency-of-soft-decision-trees/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Consistency of SRT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sparseBART/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sparse Causal BART
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../causal-mediation-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Causal mediation analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../causal-data-repository/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Causal data repository
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../causal-brain-analysis.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Causal Brain Analysis
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_10" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../factor-analysis/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Factor analysis
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_10" id="__nav_3_10_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_10">
            <span class="md-nav__icon md-icon"></span>
            Factor analysis
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/01-introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. はじめに
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/02-factor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. 因子とは
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/03-factor-loading-matrix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. 因子負荷行列
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/04-latent-factor-estimation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. 潜在因子推定法
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/05-rotation-and-interpretation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. 回転基準と結果の解釈
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/06-sensitivity-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. 感度分析
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/07-analysis-step/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. 因子分析の手順
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/08-simulation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. シミュレーション
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/09-pima-indians-diabetes-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. 糖尿病潜在原因分析
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/10-ordered-categorical-factor-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. 順序ありカテゴリカル変数の扱い
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/11-airline-passenger-satisfaction-analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. 飛行機乗客満足度分析
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../factor-analysis/12-technical-note-matrix-factorization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. 行列分解と因子分析(Technical)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目次">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目次
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. ランダムフォレストの概観
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. ランダムフォレストの概観">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-leo-breiman-2012" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 Leo Breiman によるランダムフォレストの提案から、2012年までのランダムフォレストの理論解析の歩み
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-2012" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 2012年以降のランダムフォレストの理論解析の歩み：一致性と、漸近正規性の証明
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.2 2012年以降のランダムフォレストの理論解析の歩み：一致性と、漸近正規性の証明">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scornet-et-al-2015" class="md-nav__link">
    <span class="md-ellipsis">
      Scornet et al. 2015の貢献
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wager-et-al-2014-2018" class="md-nav__link">
    <span class="md-ellipsis">
      Wager et al., 2014-2018の貢献
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#guess-and-check-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Guess-and-Check Forest
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#input" class="md-nav__link">
    <span class="md-ellipsis">
      Input
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      アルゴリズム
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#guess-and-check-forest_1" class="md-nav__link">
    <span class="md-ellipsis">
      Guess-and-Check Forest
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの一致性・漸近正規性
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一般化ランダムフォレストの一致性・漸近正規性">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストと局所推定方程式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      仮定と理論的な背景
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの一致性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの漸近正規性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの漸近分散推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      推定方程式に対応した勾配ベースの回帰木
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの応用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一般化ランダムフォレストの応用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quantile-regression-forest" class="md-nav__link">
    <span class="md-ellipsis">
      quantile regression forest と、一般化ランダムフォレストによる分位点推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストによる条件付き因果効果の推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-centering-r-learner-by-nie-and-wager-2021" class="md-nav__link">
    <span class="md-ellipsis">
      Local Centering (R-Learner by Nie and Wager., 2021)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instrumental-variable" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストによるInstrumental variableを用いた因果効果の推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-survival-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Random Survival Forest と 一般化ランダムフォレストによる生存関数推定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#causal-survival-effect" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストによる 条件付き causal survival effectの推定
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの推定精度の改善
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一般化ランダムフォレストの推定精度の改善">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#locally-linear-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Locally Linear Forest
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストのハイパーパラメータ選択
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレスト予測の解釈
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      一般化ランダムフォレストの応用事例
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一般化ランダムフォレストの応用事例">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      医療分野
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      経済分野
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      金融分野
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      マーケティング分野
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      因果効果推定のいくつかの重要なトピックス
    </span>
  </a>
  
    <nav class="md-nav" aria-label="因果効果推定のいくつかの重要なトピックス">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#overlap" class="md-nav__link">
    <span class="md-ellipsis">
      因果推論における特徴量空間の外れ値とoverlap仮定
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    <span class="md-ellipsis">
      因果推論における結果変数の外れ値の問題
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conformal-predictions" class="md-nav__link">
    <span class="md-ellipsis">
      Conformal Predictions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    <span class="md-ellipsis">
      まとめ
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="_1">一般化ランダムフォレストと因果推論への応用</h1>
<h2 id="1">1. ランダムフォレストの概観</h2>
<h3 id="11-leo-breiman-2012">1.1 Leo Breiman によるランダムフォレストの提案から、2012年までのランダムフォレストの理論解析の歩み</h3>
<p><a href="https://link.springer.com/content/pdf/10.1023/a:1010933404324.pdf">Breiman (2001)</a>年に提案したランダムフォレスト（Random Forest）は、複数の決定木をランダムに生成し、それらの予測を平均する強力なアンサンブル学習法である。提案当初は、クラス分類問題へのランダムフォレストの応用がメインであったため、決定木がアンサンブルの弱学習器となっていたが、現在では回帰の文脈でも用いられるため、この論文では回帰木と呼ぶことにする。ランダムフォレストの基本的な構成要素は2つのランダム化メカニズムによる。1つ目はデータの<strong>サブサンプリング</strong>である。各回帰木の訓練において、訓練データセットからランダムに抽出されたサブサンプルを用いる。このサブサンプリングは、ブートストラップサンプリングを用いる場合もあれば、各サンプルは1回しかとらないという方法を用いることもある。もう1つは<strong>特徴量のランダムな選択</strong>である。回帰木を学習する際に、各ノードの分割で用いることのできる変数をランダムに選択するというメカニズムである（この点に日本語の文献の説明では誤りがある場合がある。各決定木の学習の際に用いる変数をランダムに選択すると書かれているが、この表現は誤解を招くので適切ではない）。2001年に提案されて以来、ランダムフォレストは応用の文脈では高次元データへの適応力や汎用性の高さから様々な応用が行われている。（<a href="https://www.sciencedirect.com/science/article/abs/pii/S0924271616000265">Belgiu and Dragut,2016</a>, <a href="https://link.springer.com/chapter/10.1007/978-3-030-03146-6_86">Parmar et al.2018</a>, <a href="https://arxiv.org/abs/2208.04112">Hu and Szymczak, 2022</a>, <a href="https://link.springer.com/article/10.1007/s41060-024-00509-w">Iranzad and Liu</a>, <a href="https://www.mdpi.com/2073-4441/11/5/910">Tyralis et al.,2019</a>, <a href="https://link.springer.com/article/10.1007/s11749-016-0481-7">Biau and Scornet, 2016</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0888754312000626">Chen and Ishwaran, 2012</a>, <a href="https://www.sciencedirect.com/science/article/pii/S2001037022002513">Walker et al.,2022</a>）</p>
<p>しかし、木の構造がデータに強く依存し、さらにランダム化要素も含むため、その数学的解析は困難であり、広く使われているにもかかわらず理論的性質の多くがわかっていないと指摘されてきた。例えば、<a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=2a42f39add8332a7139d44a6e77496c0571e4f24">Breiman (2004)</a> においても理論的な解析を試みているが、シンプルなランダムフォレストに対する性質の解析の留まり、一般的なランダムフォレストの理論解析には至っていない。ランダムフォレストの理論研究が大きく進んだのは2010年以降であるが、その理論解析に大きな貢献を果たしているのは、<a href="https://www.tandfonline.com/doi/abs/10.1198/016214505000001230?casa_token=tUe3DH9TNEEAAAAA:sQZepLDXzrVjTx9kd14uU0BhkQKRTBrQLncg2fs-mm63ya_auCXQvNYscDwWd4Swep1SBDE5tWB0qA">Lin and Jean (2006)</a>の研究である。Lin and Jean (2006) では、回帰木によって生成される木構造によって、点<span class="arithmatex">\(x\)</span>における回帰木の推定値の計算が、k-近傍法と見なすことができることを示した。この結果は、<a href="https://www.sciencedirect.com/science/article/pii/S0047259X10001387">Biau and Devroye (2010)</a>によってランダムフォレストの理論解析へと拡張されている。そして、2010年以前の結果は、2014年以降に続くランダムフォレストの理論的な結果の大きな礎となっている。また、同時期にはランダムフォレストを用いて、条件付き平均関数以外を推定することに取り組む研究も発展している。例えば、<a href="https://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf">Meinshausen (2006)</a> では、ランダムフォレストによる条件付き分位点関数の推定法である quantile regression forest を提案し、その一致性に言及されている。また、<a href="https://arxiv.org/pdf/0811.1645">Ishwaran et al.,2008</a> では、累積ハザード関数が共変量によってどのように変動するのかを明らかにするための random survival forest を提案している。</p>
<p>また、ランダムフォレストが注目された1つの理由に変数重要度の概念がある。変数重要度は、Breiman (2001)が木構造の特徴を活かして提案したランダムフォレストにおいて、どの特徴量が予測寄与度が大きいかを定量化するための方法である。ランダムフォレストには、Mean Decrease Accuracy (MDA) と Mean Decrease Impurity (MDI) の2つが変数重要度として利用される。MDAは、ランダムフォレストを一度学習させた後で、ランダムフォレストの予測値の<span class="arithmatex">\(R^2\)</span>決定係数と、特定の変数をランダムにシャッフルした場合のランダムフォレストの予測値の<span class="arithmatex">\(R^{2}\)</span>の決定係数を計算し、その差によって変数の重要度を定量化する。一方で、MDIは各回帰木の学習において変数による分割で不純度が改善する量のノード内サンプルの重み付け和として定量化し、それを全ての回帰木で平均かすることで変数の重要度を計算する。これら2つの変数重要度についても、Breiman (2001) において提案されて以来、予測に関係する変数の発見で用いられてきた。しかしながら、ランダムフォレストの理論基盤が確立していないこともあり、変数重要度に対する理論もまた確立されていなかった。よって、近年までの変数重要度の結果からは統計的な推測の妥当性などは述べることができていない。しかし、2020年から変数重要度に関する理論的な解析結果の多くが示されており、従来経験的にしかわかっていなかった変数重要度の問題点の解消に新たな光が当てられている。</p>
<h3 id="12-2012">1.2 2012年以降のランダムフォレストの理論解析の歩み：一致性と、漸近正規性の証明</h3>
<p>Biau (2012)​ はBreimanの提案したアルゴリズムに近いランダムフォレストモデル（例えば各木でランダムに特徴次元を選ぶ方法など簡略化した設定）について初めて厳密な解析を行いました​。この研究では、「ランダムフォレストは一貫して（consistent）動作し、またスパース性（不要なノイズ特徴が多数存在する状況）に適応する」ことが示されています​。スパース性への適応とは、真に有効な特徴変数がごく一部であっても、その収束レート（予測誤差の減少速度）が有効特徴の次元数にのみ依存し、無関係な特徴の数には依存しないことを意味します​。これは、ランダムフォレストが高次元でも不要な変数を無視し、本質的な変数に集中できることを直感的に裏付ける結果です。</p>
<h4 id="scornet-et-al-2015">Scornet et al. 2015の貢献</h4>
<p>続いて、Erwan Scornet と Gérard Biau らによるさらなる発展があります。<a href="https://projecteuclid.org/journals/annals-of-statistics/volume-43/issue-4/Consistency-of-random-forests/10.1214/15-AOS1321.full">Scornet・Biau・Vert (2015)</a>​ では、Breiman (2001) のオリジナルのランダムフォレストアルゴリズムにおける（非ブートストラップサンプリングと、<span class="arithmatex">\(m_{try}\)</span> パラメータによるランダムな特徴選択による分割）ランダムフォレスト推定量の一致性が、推定対象となる真の関数が特徴量に対して加法的である場合に、一致性が成り立つことを示しました。</p>
<ul>
<li>加法性の仮定
$$
    Y = \sum_{j=1}^{p}m_{j}(X^{(j)})+\varepsilon \quad \mathrm{where} \quad X = (X^{(1)},...,X^{(p)}) \sim U([0,1]^{p}),\quad \varepsilon \sim N(0,\sigma^2) 
$$
ここで、<a href="https://projecteuclid.org/journals/annals-of-statistics/volume-43/issue-4/Consistency-of-random-forests/10.1214/15-AOS1321.full">Scornet・Biau・Vert (2015)</a>​の一致性の証明に限らず、一致性や漸近正規性の証明では、ランダムフォレストを構成する木のパラメータをどのように制御するかが本質的であり、損失関数をどのように設定しているかは一致性や漸近正規性に対して影響を与えていません。この点には注意が必要です。</li>
</ul>
<p><a href="https://projecteuclid.org/journals/annals-of-statistics/volume-43/issue-4/Consistency-of-random-forests/10.1214/15-AOS1321.full">Scornet・Biau・Vert (2015)</a>においては、木に対して次の仮定が置かれます。まず、木を構成する際のサブサンプルサイズ<span class="arithmatex">\(a_n\)</span>は、観測されたサンプル<span class="arithmatex">\(n\)</span> に対して、無限大に発散します<span class="arithmatex">\(a_n \rightarrow \infty\)</span>. また、木を構成する葉の数<span class="arithmatex">\(t_n\)</span>も、<span class="arithmatex">\(t_n \rightarrow \infty\)</span>を満たしますが、この発散速度には制約があり、<span class="arithmatex">\(t_n (\log a_n)^9/a_n \rightarrow 0\)</span> となることが条件として与えられます。つまり、サンプルの発散速度に対して、木の葉を増やす速度はそれよりも遅い速度が要求されます。</p>
<p>また、<span class="arithmatex">\(m_{try} = p\)</span>の状況下において、真の関数が、観測された変数<span class="arithmatex">\(p\)</span>個のうち、モデルに含まれる変数が<span class="arithmatex">\(s\)</span>個である状況を考えます。
$$
    Y = \sum_{j=1}^{s} m_j (X^{(j)}) + \varepsilon
$$
このとき、任意の<span class="arithmatex">\(m_j\)</span> が 任意の<span class="arithmatex">\(Y\)</span>の区間<span class="arithmatex">\([a,b]\)</span>において定数関数でないならば、十分高い確率で木の分割変数は<span class="arithmatex">\(\{1,2,...,s\}\)</span>から選ばれることを示しました。これは、ランダムフォレストが高次元の観測のもとで有効に動作することを示す根拠の1つと捉えることができます。</p>
<h4 id="wager-et-al-2014-2018">Wager et al., 2014-2018の貢献</h4>
<p>次に、Stefan Wager が Stanfordの研究グループで取り組んだランダムフォレストの一致性と漸近正規性、および漸近分散の導出、さらにオリジナルのcausal forestの提案までの流れを説明します。まず、ランダムフォレストの一致性について<a href="https://arxiv.org/abs/1503.06388">Wager and Walther (2014)</a> では、Scornet et al.,(2015)とは異なる仮定をおいて一致性を示しています。まず、木の葉の数<span class="arithmatex">\(k_n\)</span>に対して、
$$
    \lim_{n\rightarrow \infty}\frac{\log(n)\max \left(\log(d), \log\log(n)\right)}{k} = 0
$$
の仮定および Sparse signal の仮定をおく。すなわち、<span class="arithmatex">\(p\)</span>次元の特徴量<span class="arithmatex">\(X\)</span>に対して、有効な次元の集合<span class="arithmatex">\(\mathcal{Q} =\{1,2,...,s\}\)</span>次元で、<span class="arithmatex">\((Y,X_{\mathcal{Q}}) \perp \!\!\! \perp (X_{-\mathcal{Q}})\)</span> が成り立つ。
さらに、Monotone signal の仮定をおく。この仮定は、<span class="arithmatex">\(j \in \mathcal{Q}\)</span>（有効な次元）に対して、<span class="arithmatex">\(x_{(-j)} \in [0,1]^{d-1}\)</span> を固定した時に、以下の式を満たすような最小の効果<span class="arithmatex">\(\beta &gt; 0\)</span>が存在することである。
<div class="highlight"><pre><span></span><code>    \left|\mathbb{E} \left[ Y_i \mid (X_i)_{-j} = x_{-j}, (X_i)_{j} &gt; \frac{1}{2} \right] - \mathbb{E}\left[ Y_i \mid (X_i)_{-j} = x_{-j}, (X_i)_{j} \leq \frac{1}{2} \right] \right| \geq \beta
</code></pre></div>
さらに、<span class="arithmatex">\(E[Y|X=x]\)</span> に対するLipchitz連続性を仮定したもとで、以下で定義されるGuess-and-Check forestは、一様一致性を持つ。Guess-and-check forest は次のように定義される。</p>
<hr />
<h3 id="guess-and-check-forest">Guess-and-Check Forest</h3>
<h3 id="input">Input</h3>
<ul>
<li>$ n $ 個の訓練データ <span class="arithmatex">\((X_i, Y_i)\)</span></li>
<li>最小葉ノードサイズ $ k $</li>
<li>バランスパラメータ $ 0 &lt; \alpha &lt; 1/2 $</li>
</ul>
<h3 id="_2">アルゴリズム</h3>
<p>Guess-and-Check Tree は、以下の分割手順を再帰的に適用し、分割が不可能になるまで処理を行う。すなわち、全ての終端ノードの訓練データ数が $ 2k $ 未満になるか、そもそも (12) の条件を満たす分割が存在しない場合まで繰り返す。</p>
<ol>
<li>ノード $ \nu $ を選択し、そこに少なくとも $ 2k $ 個の訓練データが含まれることを確認する。</li>
<li><strong>候補となる分割変数</strong> $ j \in {1, \dots, d} $ を一様ランダムに選択する。</li>
<li><strong>最小二乗誤差の分割点</strong> $ \hat{\theta} $ を選択する。より具体的には、
    $$
    \hat{\theta} = \arg\max_{\theta} \ell (\theta)
    $$
    ここで、目的関数 $ \ell(\theta) $ は次のように定義される。
    $$
    \ell (\theta) := \frac{4 N^-(\theta) N^+(\theta)}{(N^-(\theta) + N^+(\theta))^2} \Delta^2(\theta)
    $$
    分割点 $ \theta $ は、ノード $ \nu $ に属するサンプル $ X_i $ の成分 $ (X_i)<em>j $ のいずれかに対応する値とする。
    $$
    \alpha \times | { i : X_i \in \nu } |,  k \, \leq\, N^-(\theta), \quad N^+(\theta)
    $$
    また、以下の定義を用いる。
    $$
    \Delta(\theta) = \frac{1}{N^+}\sum</em>{{ i : X_i \in \nu(x), (X_i)<em>j &gt; \theta }} Y_i - \frac{1}{N^-}\sum</em>{{ i : X_i \in \nu(x), (X_i)_j \leq \theta }} Y_i
    $$
    ただし、
    $$
    N^-(\theta) = | { i : X_i \in \nu, (X_i)_j \leq \theta } |, \, N^+(\theta) = | { i : X_i \in \nu, (X_i)_j &gt; \theta } |
    $$</li>
<li><strong>分割条件の判定:</strong><ul>
<li>変数 $ j $ に対して、すでに成功した分割が存在する場合。</li>
<li>または、以下の条件が満たされる場合。
$$
\ell (\hat{\theta}) \geq \left( 2 \times 9M \sqrt{\frac{\log(n) \log(d)}{k \log((1 - \alpha)^{-1})}} \right)^2
$$
この場合、$ j $ 番目の変数に対してノード $ \nu $ を $ \hat{\theta} $ で分割する。そうでない場合は、ノード $ \nu $ はこの時点では分割されない。</li>
</ul>
</li>
</ol>
<h3 id="guess-and-check-forest_1"><strong>Guess-and-Check Forest</strong></h3>
<p>Guess-and-Check Forest は、上記のアルゴリズムに従って <strong>独立に生成された <span class="arithmatex">\( B \)</span> 本の Guess-and-Check 木の平均</strong> を取ることで構築される。</p>
<hr />
<p>次は、Random forestの漸近分散の推定法に関する結果である。まず、漸近正規性の結果に先立って、<a href="https://jmlr.org/papers/volume15/wager14a/wager14a.pdf">Wager, Hastie and Efron (2014)</a> では、<a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2013.823775">Efron (2014)</a> においてモデル選択によるデータ駆動型のアプローチがもたらす予測誤差の過小評価を補正する方法として提案した Infinitesimal Jacknifeによる方法を、ランダムフォレストに拡張し、ランダムフォレストによる推定量のばらつきを評価する方法を提案した。
<span class="arithmatex">\(b=1,2,...,B\)</span>番目の回帰木による推定値を<span class="arithmatex">\(\hat{f}_{b}(x)\)</span>とし、 <span class="arithmatex">\(N_{ib} \in \{0,1\}\)</span> が <span class="arithmatex">\(b\)</span>番目の回帰木（Double Sample Treeの場合には、いずれか一方のグループに含まれる場合に<span class="arithmatex">\(N_{bi}=1\)</span>と定義する）に含まれるかどうかを表す指示関数とする。
$$
    \hat{V}<em>{IJ}(x) = \frac{n-1}{n} \left(\frac{n}{n-s}\right)^2 \sum</em>{i=1}^{n}\mathrm{Cov}^{<em>}\left[\hat{f}_{b}^{</em>}(x), N_{ib}^{<em>}\right]^2
$$
これに対して、有限標本化での推定量は
$$
    \hat{V}<em>{IJ}^{B}(x) = \frac{n-1}{n} \left(\frac{n}{n-s}\right)^2 \sum</em>{i=1}^{n}\frac{1}{B}\sum_{b=1}^{B}\left(N_{bi}^{</em>}-\frac{s}{n}\right)\left(t_{b}^{<em>}(x)-\bar{t}^{</em>}(x)\right)
$$
となる。ただし、この結果はEmpiricalな評価にとどまっており、実際に漸近分散と一致することを示したのは、このあとの<a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1319839">Wager and Athey (2019)</a>である。漸近正規性の議論に移る前にもう1つの重要な論文は、<a href="https://www.pnas.org/doi/10.1073/pnas.1510489113">Athey and Imbens (2016)</a> において指摘された木構造モデルの学習におけるバイアスの問題である。木構造モデルを学習する際には、テストデータへの当てはまりがよくなるように学習を行う必要がある。しかし、一般的なCARTのような学習では訓練データを用いて木構造の学習と木による予測値の両方を学習するため、実はテストデータに対しての当てはまりを最適化しているのではないということを示している。また、この論文では、ランダム化比較試験に対する介入効果の異質性を発見する回帰木としてCausal Treeを提案している。従来のCART的なアプローチでは、共変量<span class="arithmatex">\(X\)</span>の分割と分割によって生成された葉の値には相関があるため、相関に依存した因果効果が検出されてしまう。そこで、<strong>Honest（誠実性）</strong> と呼ばれる概念を導入することで、この問題を解消する。誠実性とは、データを2つに分け、一方で木構造を学習させ、もう一方で予測値の計算と検定を行えば、分割と予測値は独立になるため、効果の検証ができるということである。ここで、導入された<strong>誠実性</strong>が次に示す漸近正規性を示すための1つの鍵となる。</p>
<p><a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1319839">Wager and Athey (2019)</a> では、次の性質を満たすbagging treeに対して、有限標本化でのバイアスを評価し、漸近正規性を示した。
- サブサンプルを2つに分割し、サブサンプルの片方を木構造の学習に、もう一方を葉の推定値の計算に用いる Double Sample Tree を 弱学習器として採用する
- 任意の特徴量<span class="arithmatex">\(X^{(j)}, \, j=1,2,...,d\)</span>が、ノードの分割で選択される確率が<span class="arithmatex">\(0\)</span>ではない。
- subsample size <span class="arithmatex">\(s_n\)</span> は、サンプルサイズ<span class="arithmatex">\(n\)</span>、次元<span class="arithmatex">\(d\)</span>、および分割を制御するパラメータ<span class="arithmatex">\(\alpha \leq 0.2\)</span> に対して、次の関係式を満たす。
    $$
        s_n \approx n^{\beta}, \qquad 1-\left(1+\frac{d}{\pi}\frac{\log(\alpha^{-1})}{\log\left((1-\alpha)^{-1}\right)}\right)^{-1} &lt; \beta &lt; 1
    $$
- <span class="arithmatex">\(\mu(x) := E[Y|X]\)</span> が リプシッツ連続 である。</p>
<p>また、この論文では上記に述べた<span class="arithmatex">\(\hat{V}_{IJ}\)</span>がランダムフォレストの漸近分散に対する一致推定量であることが示されており、ランダムフォレストによる推定量の分散を与えた点で画期的な論文であった。</p>
<p>これに加えて<a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1319839">Wager and Athey (2019)</a>では、因果効果を推定する方法として、causal forestを提案している。このcausal forestは現在はほとんど採用されないが、概念だけ紹介しておく。causal forest は、<a href="https://www.pnas.org/doi/10.1073/pnas.1510489113">Athey and Imbens (2016)</a> で提案された causal tree を弱学習器として用いるランダムフォレストである。causal tree は、回帰におけるノードの分割基準を、処置効果推定の文脈へと拡張したものである。ただし、単純に拡張したものではなく、テストデータへの当てはまりを最適化するような基準を用いている。
$$
\begin{align<em>}
    -\widehat{\text{EMSE}}<em>{r} \left( \mathcal{S}^{\text{tr}}, N^{\text{est}}, \Pi \right) 
    &amp;\equiv \frac{1}{N^{\text{tr}}} \sum</em>{i \in \mathcal{S}^{\text{tr}}} \hat{\tau}^2 \left( X_i, \mathcal{S}^{\text{tr}}, \Pi \right) \
    &amp;\quad - \left( \frac{1}{N^{\text{tr}}} + \frac{1}{N^{\text{est}}} \right) \cdot \sum_{\ell \in \Pi} 
    \left( \frac{S^2_{\mathcal{S}^{\text{tr}}<em>{\text{treat}}} (\ell)}{p} 
    + \frac{S^2</em>{\mathcal{S}^{\text{tr}}_{\text{control}}} (\ell)}{1 - p} \right).
\end{align</em>}
$$
この基準は、第1項が葉の推定値間の分散を最大化させる項であり、異質性を捉えるための項となっている。一方で、第2項は葉内の処置群および対照群の分散の和であり、当てはまりに対する罰則項である。つまり、予測の異質性を高めつつも、葉の中のサンプルが極端に少なくなったりすることで予測全体の分散が大きくなりすぎないように制御していると捉えることができる。</p>
<h2 id="_3">一般化ランダムフォレストの一致性・漸近正規性</h2>
<p>一般化ランダムフォレスト (Athey, Tibshirani and Wager, 2020) は、ランダムフォレストが回帰と分類の問題にしか対象としないのに対して、統計モデルなどで定義されるパラメータの推定を対象するランダムフォレストの拡張の1つである。例えば、後にも扱う因果効果は、データから直接推定することはできないため、通常のランダムフォレストでは因果効果を直接的に推定することはできない。これに対して、一般化ランダムフォレストは、推定方程式をランダムフォレストで直接取り扱う方法を提供する。</p>
<p>この枠組みを提供するために、まずはランダムフォレストに対する視点転換を行い、ランダムフォレストをカーネル重み付け関数の推定法として捉える。いくつかの記号の導入を行い、その後でランダムフォレストのカーネル重み付け推定としての側面を説明する。</p>
<p>問題設定として、<span class="arithmatex">\(p\)</span>次元の共変量 <span class="arithmatex">\(X\)</span> と 1次元の結果変数 <span class="arithmatex">\(Y\)</span> を考え、<span class="arithmatex">\((X,Y)\)</span>の従う確率分布を<span class="arithmatex">\(P_{XY}\)</span>とする。次に、<span class="arithmatex">\(P_{XY}\)</span>からの独立同一な観測の組を <span class="arithmatex">\(D_{n} = (X_{i}, Y_{i}), i=1,2,...,n\)</span>とする。いま、観測データ<span class="arithmatex">\(D_n\)</span>上で学習された木による点<span class="arithmatex">\(x\)</span>の予測を<span class="arithmatex">\(T(x;D_n)\)</span>とし、木の葉のうち点<span class="arithmatex">\(x\)</span>を含むものを<span class="arithmatex">\(L(x)\)</span>とする。すると、
$$
    T(x;D_n) = \sum_{i=1}^{n} \frac{1{X_i \in L(x) }}{|L(x)|}\cdot Y_i
$$
ただし、<span class="arithmatex">\(|L(x)|\)</span>は点<span class="arithmatex">\(x\)</span>を含む葉に含まれる観測データのサンプルサイズである。この式は、点<span class="arithmatex">\(x\)</span>に対する回帰木による予測とは、点<span class="arithmatex">\(x\)</span>を含む葉に存在する観測データに対して、<span class="arithmatex">\(1/|L(x)|\)</span>の重み付けて和を取ることで得られることを意味している。</p>
<p>同様にして、ランダムフォレストを考えてみる。いま、サイズ <span class="arithmatex">\(n\)</span> のサンプルから、サイズ <span class="arithmatex">\(s_n\)</span>のサブサンプル を <span class="arithmatex">\(B\)</span>回とり、それぞれを <span class="arithmatex">\(D_{n}^{(b)}, b=1,2,...,B\)</span> をとする。ランダムフォレストは、サブサンプル <span class="arithmatex">\(D_{n}^{(b)}\)</span> 上に、回帰木を学習させ、各回帰木の出力の平均値を テスト点<span class="arithmatex">\(x\)</span>の推定値として返す。各回帰木を<span class="arithmatex">\(T_{b}\)</span>とし、点<span class="arithmatex">\(x\)</span>に対する回帰木<span class="arithmatex">\(T_{b}\)</span>による予測を<span class="arithmatex">\(T_{b}(x;D_n, \xi)\)</span>する。ここで、<span class="arithmatex">\(\xi\)</span> は回帰木を学習させる際のランダムネスに対応する。例えば、splitの候補となる変数の集合をランダムに選ぶことなどに対応する確率変数である。いま、回帰木<span class="arithmatex">\(T_b\)</span>において 点<span class="arithmatex">\(x\)</span>を含む葉を<span class="arithmatex">\(L_{b}(x)\)</span>とする。すると、先ほどと同様の議論から、
$$
    \alpha_{bi} = \frac{1{X_i \in L_{b}(x) }}{|L_{b}(x)|}, \qquad T(x;D_n) = \sum_{i=1}^{n} \frac{1{X_i \in L_{b}(x) }}{|L_{b}(x)|}\cdot Y_i = \sum_{i=1}^{n}\alpha_{bi}(x)Y_i.
$$
となる。ランダムフォレストによる点<span class="arithmatex">\(x\)</span>の予測値 <span class="arithmatex">\(RF_{B}(x; D_n)\)</span>は、回帰木の平均であるから、
$$
    RF_{B}(x;D_n) = \frac{1}{B}\sum_{b=1}^{B}\sum_{i=1}^{n}\alpha_{bi}(x)Y_i = \sum_{i=1}^{n}\left{\frac{1}{B}\sum_{b=1}^{B}\alpha_{bi}(x)\right}Y_i = \sum_{i=1}^{n}\alpha_{i}(x)Y_i 
$$
ここで、
$$
    \alpha_{i}(x) = \left{\frac{1}{B}\sum_{b=1}^{B}\alpha_{bi}(x)\right}
$$
とおいた。<span class="arithmatex">\(\alpha_{i}(x)\)</span>は、回帰木それぞれが点<span class="arithmatex">\(i\)</span>を含むかどうかを確率変数とみなしたとき、<span class="arithmatex">\(B\)</span>個の確率変数の平均と解釈できる。つまり、点<span class="arithmatex">\(x\)</span>を同じ葉に含まれやすいサンプル<span class="arithmatex">\(i\)</span>に対しては、大きな重みがかかり、同じ葉にほとんど含まれないサンプル<span class="arithmatex">\(i\)</span>に対しては小さな重みがかかる。</p>
<p>一方で、<span class="arithmatex">\(\hat{\theta}_{B}(x) = RF_{B}(x;D_n)\)</span> とおくと、これは以下の推定方程式の解として解釈することができる。
$$
    \hat{\theta}<em>{B}(x) = \arg\min</em>{\theta}\sum_{i=1}^{n}\alpha_{i}(x)(Y_i - \theta)^2
$$
よって、ランダムフォレストは回帰木のアンサンブルと捉えることもできるが、回帰木によってテスト点<span class="arithmatex">\(x\)</span>と観測データの関係の強さを<span class="arithmatex">\(\alpha_{i}(x)\)</span>として定量化し、それによって推定方程式を解く方法と捉え直すことができる。この視点の切り替えによって、ランダムフォレストが<span class="arithmatex">\(\alpha_{i}(x)\)</span>をカーネル重み付け関数としたカーネル重み付け推定量であることが理解できる。この性質を用いて、ランダムフォレストでパラメータの推定を行うというのが一般化ランダムフォレストである。</p>
<h3 id="_4">一般化ランダムフォレストと局所推定方程式</h3>
<p>確率変数の組 <span class="arithmatex">\(Z = (X, O)\)</span> と考え、これらが従う分布を<span class="arithmatex">\(P\)</span>とおく。いま、<span class="arithmatex">\(P\)</span>からの独立同一なサンプルを<span class="arithmatex">\(Z_i, (i=1,2,...,n)\)</span> とする。ただし、<span class="arithmatex">\(O\)</span>は回帰であれば <span class="arithmatex">\(Y\)</span> であるが、因果推論などにおいては処置変数<span class="arithmatex">\(T\)</span>と結果変数<span class="arithmatex">\(Y\)</span>の組 <span class="arithmatex">\(O=\{T, Y\}\)</span> となる場合もあり、問題によって<span class="arithmatex">\(O\)</span>は読み替えるものとする。いま、スコア関数 <span class="arithmatex">\(\psi(\cdot)\)</span> によって特徴づけられる<strong>局所推定方程式</strong>
$$
    E[\psi_{\theta(x),\nu(x)}(O_i)\mid X_i = x] = 0
$$
を考え、このスコア方程式の解として定義されるパラメータ <span class="arithmatex">\(\theta(x), \nu(x)\)</span> に興味があるとする。例えば、<span class="arithmatex">\(\theta(x) = E[Y\mid X=x]\)</span> に興味がある場合には、<span class="arithmatex">\(\psi(O) = \psi_{\theta(x)}(O) = Y-\theta(x)\)</span>とする。また、統計的因果推論の文脈において、観測される結果変数<span class="arithmatex">\(Y\)</span>が、共変量<span class="arithmatex">\(X\)</span>と処置変数<span class="arithmatex">\(T \in \{0,1\}\)</span>によって、<span class="arithmatex">\(Y = \mu(X) + T \cdot \tau(X) + \varepsilon\)</span> とモデル化され、<span class="arithmatex">\(E[\varepsilon \mid X] = 0\)</span> および <span class="arithmatex">\(\varepsilon \perp\!\!\!\perp T \mid X\)</span> を満たすとする。このとき、<span class="arithmatex">\(E[\varepsilon \mid X] = 0\)</span> および、<span class="arithmatex">\(E[\varepsilon \cdot T \mid X] = 0\)</span> であるから、以下のようになる。
$$
    \psi_{\theta(x),\nu(x)}(O) = \psi_{\mu(x),\tau(x)}(T,Y) = \left(Y - \mu(x) - T\cdot \tau(x)\right)\begin{pmatrix}1 \ T \end{pmatrix}
$$</p>
<p>次に、局所推定方程式によって特徴づけられるパラメータの推定について考える。いま、分布<span class="arithmatex">\(P\)</span>からの独立で同一なサンプルを<span class="arithmatex">\((X_i, O_i), i=1,2,...,n\)</span>とする。このとき、パラメータ <span class="arithmatex">\(\theta(x)\)</span> の推定量は、一般的には点<span class="arithmatex">\(x\)</span>と<span class="arithmatex">\(z\)</span>の距離 <span class="arithmatex">\(u = |x-z|\)</span>に依存する、バンド幅を<span class="arithmatex">\(h\)</span>とするカーネル重み付け関数 <span class="arithmatex">\(K_{h}(u)\)</span> に基づいて重み付け推定方程式を解くことによって構成される。
$$
    \hat\theta(x) = \arg\min_{\theta,\nu}\left|\sum_{i=1}^{n}K_{h}(X_i - x)\psi_{\theta,\nu}(O_i)\right|_{2}
$$
この重み付け推定方程式の解 <span class="arithmatex">\(\hat\theta(x)\)</span> は、適当な正則条件のもとで一致性と漸近正規性を満たすことはよく知られている。</p>
<p>ランダムフォレストによって構成される <span class="arithmatex">\(\alpha_{i}(x)\)</span> は、点<span class="arithmatex">\(x\)</span> とサンプル<span class="arithmatex">\(i\)</span>の関連性の強さの指標であり、これはカーネル重み付け関数 <span class="arithmatex">\(K_{h}(X_i - x)\)</span> に概念的に対応づけることができる。よって、直感的には <span class="arithmatex">\(\alpha_{i}(x)\)</span> がカーネル重み付け関数と同様の性質を満たすのであれば、ランダムフォレストによって得られる推定方程式の解もまた、一致性と漸近正規性を満たすということがわかる。</p>
<h3 id="_5">仮定と理論的な背景</h3>
<p>ここで、理論的結果を述べる上で重要な指摘を行なっておく。現状、Wager and Athey (2019) や、Athey et al. (2020) などで述べられている理論的な結果では、回帰木を構成する際の損失関数に対しての制約はなく、これから述べるような仮定を満たす回帰木を用いていればよい。よって、あるパラメータに対する推定法を構成する上で最適な損失関数の選択などについては、ここでは全く議論されていないことに注意して欲しい。これらの推定における細やかな指針については本節の後半で紹介する。</p>
<p>まずは回帰木の構成に関する仮定をおく。
- (A1) 回帰木の構成はサンプルの順序に依存しない（訓練データの添字には依存しない, i.i.d. であれば関係はない）
- (A2) ノードを分割する際には、分割によって生成される子ノードが、分割元のノードに含まれるサンプルの<span class="arithmatex">\(0 &lt; \omega &lt; 0.5\)</span> 以上の割合を含む（極端な分割を抑制する）
- (A3) ノードの分割において、任意の次元 <span class="arithmatex">\(j\)</span> が分割される確率 <span class="arithmatex">\(\pi &gt; 0\)</span> 以上である（つまり、どの次元<span class="arithmatex">\(j\)</span>も分割される可能性がある）。
- (A4) 任意の回帰木の葉は<span class="arithmatex">\(k\)</span>以上、<span class="arithmatex">\(2k-1\)</span>以下のサンプルを含む <span class="arithmatex">\((k \geq 1)\)</span>。
- (A5) Double sample trees の回帰木の構成方法を利用する。ただし、random forest を構成する際のサブサンプルのサイズ <span class="arithmatex">\(s_n\)</span> は、<span class="arithmatex">\(s_n \rightarrow \infty\)</span> かつ <span class="arithmatex">\(s/n \rightarrow 0\)</span> を満たす。</p>
<p>これらの仮定は、Wager and Athey (2019) で用いられた漸近正規性を満たすランダムフォレスト推定量のための仮定と同じである。ここで、(A3) の実装においては、<code>min(max{Poisson(m),1},p)</code> を <code>grf</code> のパッケージでは用いることで、どの変数にも分割される確率が存在することを保証している。</p>
<p>さらに、局所推定方程式に対して以下の仮定を記述するために、
$$
    M_{\theta,\nu}(x) := E[\psi_{\theta,\nu}(O) \mid X=x]
$$
とおく。</p>
<ul>
<li>(B1) 固定された<span class="arithmatex">\((\theta,\nu)\)</span>に対して、<span class="arithmatex">\(M_{\theta,\nu}(x)\)</span> は <span class="arithmatex">\(x\)</span> に対して連続である。</li>
<li>(B2) <span class="arithmatex">\(x\)</span>を固定した時、<span class="arithmatex">\(M_{\theta,\nu}\)</span>は、<span class="arithmatex">\((\theta,\nu)\)</span> について2階連続微分可能であり、<span class="arithmatex">\(V_{\theta,\nu}(x) := \partial/\partial(\theta,\nu)M_{\theta,\nu}(x)\mid_{\theta(x),\nu(x)}\)</span> が、任意の<span class="arithmatex">\(x \in \mathcal{X}\)</span> に対して正則である。</li>
<li>(B3) <span class="arithmatex">\(\gamma\)</span> を以下で定義されるバリオグラムとする
    $$
        \gamma\left(\begin{pmatrix}\theta \ \nu\end{pmatrix}, \begin{pmatrix}\theta' \ \nu'\end{pmatrix}\right) := \sup_{x\in\mathcal{X}}\left{| Var[\psi_{\theta,\nu}(O_i) - \psi_{\theta',\nu'}(O_i)]\mid X=x|_{F}\right}
    $$
    このとき、<span class="arithmatex">\((\theta,\nu)\)</span>について、<span class="arithmatex">\(\gamma\)</span> はリプシッツ連続である。</li>
<li>(B4) <span class="arithmatex">\(\psi\)</span>が、<span class="arithmatex">\(\psi_{\theta,\nu}(O) = \lambda(\theta,\nu ; O_i) + \zeta_{\theta,\nu}(g(O_i))\)</span> の形状に分解される。ただし、<span class="arithmatex">\(\lambda\)</span> は <span class="arithmatex">\((\theta,\nu)\)</span>についてリプシッツ連続かつ、<span class="arithmatex">\(g : \{O_i\} \rightarrow \mathbb{R}\)</span>、および <span class="arithmatex">\(\zeta_{\theta,\nu} : \mathbf{R} \rightarrow \mathbf{R}\)</span> の単調かつ有界な関数である。</li>
<li>(B5) <span class="arithmatex">\(\sum_{i}\alpha_i = 1\)</span> を満たす重み<span class="arithmatex">\(\alpha_i\)</span> に対して、重み付け推定方程式の解<span class="arithmatex">\(\hat\theta, \hat\nu\)</span>が、ある定数<span class="arithmatex">\(C\)</span>に対して、<span class="arithmatex">\(\|\sum_{i=1}^{n}\alpha_{i}\psi_{\hat\theta,\hat\nu}(O_i)\|_{2} \leq C \max\{\alpha_i\}\)</span> を満たす。</li>
<li>(B6) スコア関数 <span class="arithmatex">\(\psi_{\theta,\nu}(O_i)\)</span> は、凸関数の負の劣導関数となっており、<span class="arithmatex">\(M_{\theta,\nu}\)</span>は、狭義凸関数の負の導関数となっている。</li>
</ul>
<p>これらの仮定は証明において本質的であるが、回帰の問題、分位点推定の問題、2値の処置変数に対する因果パラメータの推定、2値の処置変数と2値の操作変数に対する操作変数法による因果パラメータの推定などのさまざまな設定に対して満たされる。また、これらの仮定の中で大きな特徴なのは、スコア関数の期待値である<span class="arithmatex">\(M\)</span>が微分可能であれば、<span class="arithmatex">\(\psi\)</span>は微分可能ではなくてよいというところである。これによって、スコア関数として不連続な関数となる<strong>分位点回帰</strong>についても一般化ランダムフォレストは理論的な保証を与えることができる。</p>
<h3 id="_6">一般化ランダムフォレストの一致性</h3>
<p>以上の仮定のもとで構成された回帰木のアンサンブルによって生成される重みを <span class="arithmatex">\(\alpha_{i}(x)\)</span>とするとき、ランダムフォレストによる重みづけ推定方程式
$$
    (\hat\theta(x),\hat\nu(x)) = \arg\min_{\theta,\nu}\left|\sum_{i=1}^{n}\alpha_{i}(x)\psi_{\theta,\nu}(O_i)\right|<em>{2}
$$
の解 <span class="arithmatex">\((\hat\theta(x),\hat\nu(x))\)</span> は、<span class="arithmatex">\((\theta(x),\nu(x))\)</span> に対する一致推定量となる。この証明の中で最も重要なのは、ランダムフォレストの生成する重みの性質である。Wager and Athey (2018) においては、<span class="arithmatex">\(\alpha_{i}(x)\)</span>の局所化が、上記の仮定を満たす回帰木では次のレートで起こることを示した。
$$
    E[\sup{|X_i - x|</em>{2} : \alpha_{i}(x) &gt; 0}] = O\left(s^{-\frac{\pi}{2}\cdot \frac{\log((1-\omega)^{-1})}{\log(\omega^{-1})}}\right)
$$
この局所化は、点<span class="arithmatex">\(x\)</span>を推定する際に、L2距離でどの程度離れたサンプルまで重みがかかるのかを示している。<span class="arithmatex">\(\omega\)</span> は分割を制御するパラメータで、<span class="arithmatex">\(\omega = 0.5\)</span> の場合はサンプルを<span class="arithmatex">\(1:1\)</span>に分割する点のみを分割点として採用することになり、<span class="arithmatex">\(\omega\)</span>の値が<span class="arithmatex">\(0\)</span>に近づくほどより柔軟なノードの分割が可能になる。このとき、<span class="arithmatex">\(0 &lt; \omega &lt; 0.5 <span class="arithmatex">\(に対して、\)</span>\frac{\log((1-\omega)^{-1})}{\log(\omega^{-1})}\)</span>は単調増加で、<span class="arithmatex">\([0,1]\)</span>の値を取るので、極端な分割を許容するとランダムフォレストの生成する重みの裾が<span class="arithmatex">\(x\)</span>の周辺への収束するのが遅くなることがわかる。</p>
<p>しかし、いずれにせよ<span class="arithmatex">\(\alpha_{i}(x)\)</span>が<span class="arithmatex">\(x\)</span>の周辺に収束するということは、重みづけ推定方程式が<span class="arithmatex">\(\sum_{i=1}^{n}\alpha_{i}(x)\psi_{\theta,\nu}(O_i)\)</span> が <span class="arithmatex">\(E[\psi_{\theta,\nu}(O)\mid X=x]\)</span> に収束することに対応するため、任意の <span class="arithmatex">\(x\in \mathcal{X}\)</span>得られる推定方程式の解も <span class="arithmatex">\((\hat\theta,\hat\nu) \rightarrow (\theta(x),\nu(x))\)</span> の確率収束が成り立つことを意味している。これらは直感的な説明であるが、詳細な証明の過程は経験過程を用いた議論となる。</p>
<h3 id="_7">一般化ランダムフォレストの漸近正規性</h3>
<p>一般化ランダムフォレストの漸近正規性についても、ここでは証明の直感的なプロセスを紹介する。いま、パラメータ<span class="arithmatex">\(\theta(x)\)</span>に対する <span class="arithmatex">\(i\)</span>番目のサンプルに対する影響関数を
$$
    \rho_{i}^{<em>}(x) := - \xi^{T}V(x)^{-1}\psi_{\theta(x),\nu(x)}(O_i)
$$
とおく。次に、擬似的なランダムフォレストの推定量を
$$
    \tilde{\theta}^{</em>}(x) := \theta(x) + \sum_{i=1}^{n}\alpha_{i}(x)\rho_{i}^{*}(x)
$$
と構成する。この擬似的なランダムフォレスト推定量は、<span class="arithmatex">\(\theta(x) + \rho_{i}^{*}(x)\)</span>を結果変数とする重み<span class="arithmatex">\(\alpha_{i}(x)\)</span>による回帰フォレストによる推定量となっている。この<span class="arithmatex">\(\tilde{\theta}^{*}(x)\)</span>を用いて次の2つを示す。</p>
<ol>
<li><span class="arithmatex">\((\tilde{\theta}^{*}(x)-\theta(x))\)</span> が適当な収束レートのもとで漸近的に正規分布に収束すること</li>
<li><span class="arithmatex">\((\tilde{\theta}^{*}(x) - \hat\theta)\)</span> が <span class="arithmatex">\((\tilde{\theta}^{*}(x)-\theta(x))\)</span> の収束レートよりも速いレートで<span class="arithmatex">\(0\)</span>に確率収束すること</li>
</ol>
<p>この2つの結果が得られれば、ランダムフォレストによる重みづけ推定方程式によって得られる <span class="arithmatex">\(\hat\theta\)</span> が <span class="arithmatex">\(\theta(x)\)</span> に対して漸近正規性を持つことを示すことができる。まず、1つ目の結果は infinite-order U統計量の議論となる。これは、Mentch and Hooker (2016) の結果から示すことができる。2つ目の結果が、Athey et al.(2020) の結果の1つで Lemma 4として記されているものである。このとき、収束のレートをコントロールするために必要なのが、ランダムフォレストを構成する際のサブサンプルサイズ <span class="arithmatex">\(s_n\)</span>である。Athey et al. (2020) の結果では、<span class="arithmatex">\(s_n = n^{\beta}\)</span>の指数部について、以下のレートを要求している
$$
    \beta_{\min} = 1 - \left(1 + \frac{\pi^{-1}\log(\omega^{-1})}{\log((1-\omega)^{-1})}\right)^{-1} &lt; \beta &lt; 1
$$
このレートは先ほどの重み<span class="arithmatex">\(\alpha_{i}(x)\)</span>でみた対数の逆数に似た形状の項が左片にあり、<span class="arithmatex">\(\omega\)</span>が<span class="arithmatex">\(0\)</span>に近いほど、<span class="arithmatex">\(\beta\)</span>の下限が<span class="arithmatex">\(1\)</span>に近づくようになっている。このサブサンプルレートのもとで、以下の結果が得られる。</p>
<p>$$
    \frac{\hat\theta(x) - \theta(x)}{\sigma_{n}(x)} \rightarrow N(0,1), \quad \sigma_{n}^{2}(x) = (s_n/n)\cdot\mathrm{polylog}(s_n/n)^{-1}
$$
ただし、<span class="arithmatex">\(\mathrm{polylog}(u) = a_{k}(\log(u))^{k} + \cdots + a_{1}(\log(u))^{1} + a_{0}\)</span>を意味する。よって、この結果から一般化ランダムフォレストに基づく推定量が漸近正規性を持つことがわかる。ここで重要なのは、<span class="arithmatex">\(\sigma_{n}^{2}(x)\)</span>の収束速度である。</p>
<p>簡単な例でランダムフォレストの収束レートを考える。いま、共変量を<span class="arithmatex">\(1\)</span>次元であると仮定する。すなわち<span class="arithmatex">\(\pi = 1\)</span>である。このとき、<span class="arithmatex">\(\beta\)</span>の下限<span class="arithmatex">\(\beta_{\min}\)</span>は、<span class="arithmatex">\(0 &lt; w &lt; 0.5\)</span> に対して、<span class="arithmatex">\((1, 0.5]\)</span>の区間での単調減少関数となる。一方で、共変量の次元を<span class="arithmatex">\(2,3,4\)</span>と増やしていくと、<span class="arithmatex">\((1,0.66]\)</span>、<span class="arithmatex">\((1,0.75]\)</span>、<span class="arithmatex">\((1,0.8]\)</span>と下限の<span class="arithmatex">\(\beta\)</span>の値が大きくなる。ここで、ランダムフォレストの収束レートは<span class="arithmatex">\(n^{1-\beta}\)</span>に依存するので、この結果から共変量の次元に対して収束のレートが悪くなることがわかる。</p>
<p>実際、ランダムフォレストは探索する次元の大きさに対して、分割可能な回数がサンプルサイズに依存するため、サンプルサイズが小さい場合には点<span class="arithmatex">\(x\)</span>を含む葉が大きくなり、それだけバイアスも大きくなる点には注意が必要である。以上の点から、ランダムフォレストを利用する場合には、次元の大きさとサンプルサイズには十分に注意が必要である。</p>
<h3 id="_8">一般化ランダムフォレストの漸近分散推定</h3>
<p>次に、一般化ランダムフォレストに対する漸近分散の推定を考える。Wager and Athey (2018) での議論では、Infinitesimal Jacknifeを用いてランダムフォレスト推定量の漸近分散を推定したが、ここでは一般化ランダムフォレストの推定量が推定方程式の解として得られることを用いてデルタ法を応用することで漸近分散の推定を考える。</p>
<p>まず、上記の漸近正規性の議論から、漸近分散に寄与する項が<span class="arithmatex">\((\tilde{\theta}^{*}(x)-\theta(x))\)</span>の部分であるから、
$$
    \frac{\mathrm{Var}[\tilde{\theta}^{<em>}(x)]}{\sigma_{n}^{2}(x)} \rightarrow 1
$$
であるから、<span class="arithmatex">\(\tilde{\theta}^{*}(x)\)</span> の分散について考えれば良いということになる。いま、
$$
    H_{n}(x; \theta,\nu) = \mathrm{Var}\left[ \sum_{i=1}^{n}\alpha_{i}(x)\psi_{\theta,\nu}(O_i) \right]
$$
とおくと、<span class="arithmatex">\(\tilde{\theta}^{*}(x)\)</span>の定義から、
$$
    \mathrm{Var}[\tilde{\theta}^{</em>}(x)] = \xi^{T}V(x)^{-1}H_{n}(x; \theta,\nu)V(x)^{-T}\xi
$$
となる。各項の計算について見ていくと、まず<span class="arithmatex">\(V(x)^{-1}\)</span>については、<span class="arithmatex">\(V(x)\)</span>がスコア関数の条件付き期待値の導関数なので、これを推定して逆行列をとる。例えば、<span class="arithmatex">\(\psi_{\theta}(O) = Y - \theta\)</span> のようなシンプルな回帰モデルの場合には、<span class="arithmatex">\(V(x) = -1\)</span> となり、<span class="arithmatex">\(x\)</span>には依存しない。一方で、因果推論の場合には <span class="arithmatex">\(V(x) = - \begin{pmatrix}1 &amp; E[T|X=x] \\ E[T|X=x] &amp; E[T|X=x]\end{pmatrix}\)</span>となる。よって、傾向スコアの推定が必要である（現在の <code>grf</code> では推定方法が異なるので必要はない）。</p>
<p>よって、分散の推定に必要なのは <span class="arithmatex">\(H_{n}(x; \theta,\nu)\)</span> に対する推定量の構成ということになる。<span class="arithmatex">\(H_{n}(x; \theta,\nu)\)</span>の推定のための基本的な方針は、ブートストラップを用いることである。記号を簡単にするために、
$$
    \Psi(\hat\theta(x),\hat\nu(x)) = \sum_{i=1}^{n}\alpha_{i}(x)\psi_{\theta,\nu}(O_i)
$$
とおく。次のような Half サンプルに基づく推定量を考える。
$$
    \widehat{H}<em>{n}^{HS} = \binom{n}{\lfloor n/2 \rfloor} \sum</em>{\mathcal{H}:|\mathcal{H}|= \lfloor n/2 \rfloor} \left(\Psi_{\mathcal{H}}(\hat\theta(x),\hat\nu(x)) - \Psi(\hat\theta(x),\hat\nu(x)) \right)^2
$$
ここで、<span class="arithmatex">\(\Psi_{\mathcal{H}}\)</span> は Half サンプル <span class="arithmatex">\(\mathcal{H} \in \{1, 2, ..., n\}\)</span> を回帰木の<span class="arithmatex">\(\mathcal{I}\)</span>-サブサンプルが含むような回帰木によって推定される<span class="arithmatex">\(\Psi\)</span>によって計算される。ここで、<span class="arithmatex">\(\mathcal{I} \subseteq \mathcal{H}\)</span>であることを満たしているようなすべての回帰木を考えれば良いので、サブサンプルの<span class="arithmatex">\(\mathcal{J}\)</span>の選び方については <span class="arithmatex">\(\{1,2,...,n\}\backslash \mathcal{I}\)</span> の自由度があり、<span class="arithmatex">\(\mathcal{I} \subseteq \mathcal{H}\)</span>であればよいから、<span class="arithmatex">\(\mathcal{I}\)</span>についてはこれを含む形でより大きな集合をとっても良いので、ランダムフォレストを構成できる。このようなランダムフォレストの構成をすべての<span class="arithmatex">\(\mathcal{H}\)</span>の全てのパターンについて考えるというものである。実際、この推定量は<span class="arithmatex">\(H_{n}(x; \theta,\nu)\)</span>に対する一致性を持つことは、Athey et al.(2020) の Theorem 6からわかるが、すべての <span class="arithmatex">\(\mathcal{H}\)</span> について計算するのは実質的に不可能である。</p>
<p>そこで、Sexton and Laake (2009) で提案された方法を拡張することで、<span class="arithmatex">\(\widehat{H}_{n}^{HS}\)</span> に対する推定量を構成する。いま、<span class="arithmatex">\(\ell \geq 2\)</span> を <code>little bag size</code> とし、<span class="arithmatex">\(B\)</span> を<span class="arithmatex">\(\ell\)</span>に乗じる定数とする。いま、<span class="arithmatex">\(g = 1, ..., B/\ell\)</span> 回 <span class="arithmatex">\(|\mathcal{H}_{g}| = n/2\)</span> となる Halfサンプル <span class="arithmatex">\(\mathcal{H}_{g} \in \{1,2,...,n\}\)</span> をとる。
次に、回帰木を構成する際の予測に用いるサンプル <span class="arithmatex">\(\mathcal{I}_{b}\)</span> を、$\mathcal{I}<em>{b} \subseteq \mathcal{H}</em>{\lceil b/\ell \rceil} $ となるように <span class="arithmatex">\(b=1,2,...,B\)</span> に対して取る。</p>
<p>例えば、<span class="arithmatex">\(\ell = 2\)</span>, <span class="arithmatex">\(B=10\)</span> の場合を考えると、<span class="arithmatex">\(\mathcal{H}_{g}, (g=1,2,3,4,5)\)</span> が Halfサンプルとして生成される。次に、ランダムフォレストの木を <span class="arithmatex">\(B\)</span> 回生成するときに、それぞれの木を構成する際のDouble Sampleで予測に用いるサンプル $\mathcal{I}<em>{b} = \mathcal{H}</em>{\lceil b/\ell \rceil} $ となるようにするので、<span class="arithmatex">\(\mathcal{I}_{1} = \mathcal{H}_{\lceil 1/2 \rceil} = \mathcal{H}_{1}\)</span>, <span class="arithmatex">\(\mathcal{I}_{2} = \mathcal{H}_{\lceil 2/2 \rceil} = \mathcal{H}_{1}\)</span>, <span class="arithmatex">\(\mathcal{I}_{3} = \mathcal{H}_{\lceil 3/2 \rceil} = \mathcal{H}_{2}...\)</span>  のようになる。</p>
<p>ここで、データが固定されたもとでのサブサンプリングメカニズムについての期待値を<span class="arithmatex">\(E_{ss}\)</span>で表すことにすると、以下の形に分割できる。
$$
    E_{ss}\left[\left(\frac{1}{\ell}\sum_{b=1}^{\ell}\Psi_{b} - \Psi\right)^{2}\right] = \widehat{H}<em>{n}^{HS} + \frac{1}{\ell-1}E</em>{ss}\left[\frac{1}{\ell}\sum_{b=1}^{\ell}\left(\Psi_{b} - \frac{1}{\ell}\sum_{b=1}^{\ell}\Psi_{b}\right)^{2}\right]
$$
この展開は、左辺の展開において<span class="arithmatex">\(\Psi_{\mathcal{H}}\)</span>を挟んで分解する。また、<span class="arithmatex">\(E_{ss}[\Psi_{\mathcal{H}} - \Psi]\)</span>は、すべての Half サンプル<span class="arithmatex">\(\mathcal{H}\)</span>について平均を取る操作を表すから、これは<span class="arithmatex">\(\widehat{H}_{n}^{HS}\)</span>　そのものであることに注意する。左辺は、little in Bagsによって構成されるフォレストの分散（全分散)が、これが <span class="arithmatex">\(\widehat{H}_{n}^{HS}\)</span> と、little in Bags内の分散に分解されるという意味になる。</p>
<p>以上の議論から、十分大きな<span class="arithmatex">\(B\)</span>に対しては上記の関係性が成り立つから、漸近分散に対する一致推定量が計算可能となる。この方法による漸近分散の計算は、回帰木が最初に与えた仮定を満たしていれば妥当となるので、以降に説明する様々な一般化ランダムフォレストによる推定に対して漸近分散を与えることができる。</p>
<h3 id="_9">推定方程式に対応した勾配ベースの回帰木</h3>
<p>ここまでの議論では、推定方程式によって定義されるパラメータを扱っているにもかかわらず回帰木の構成法に触れていない。これは、回帰木の構成法にランダムフォレストによって生成された重みに基づく推定方程式の解が一致性や漸近正規性を有しており、漸近分散の導出結果を用いてよいということを表している。しかし、一般的にランダムフォレストを構成する際に、回帰木の学習は損失関数の最小化することによって行われる。これは、実際その方が効率的な学習ができるということと、有効な変数以外でのノードの分割を避けることは、予測における実質的な次元削減の効果があることから有効である。ここでは、Athey et al.(2020) で提案された推定方程式によって定義されるパラメータの推定において有効かつ計算効率の良いノードの繰り返し分割法である<strong>勾配ベースの回帰木</strong>について導入を行う。</p>
<p>推定方程式によって定義されるパラメータは一般的に観測されず推定方程式を解くことによって得られる場合がほとんどである。そのため、予測の対象となる結果変数<span class="arithmatex">\(Y\)</span>が観測されたもとでの、条件付き平均 <span class="arithmatex">\(\mu(x)=E[Y|X=x]\)</span>の場合とは異なり、損失関数を工夫する必要がある。条件付き平均を予測する問題では、ノードを2つに分割する際には、分割された後のノード内での結果変数の分散の重み付け和が最小になるようにするのが一般的である。勾配ベースの回帰木においてもこの考え方を継承し、ノードを分割した後でパラメータ <span class="arithmatex">\(\theta(x)\)</span>のノード内での分散の重み付け和が最小になるように分割を考える。</p>
<p>いま、ノード<span class="arithmatex">\(P\)</span>を2つの子ノード<span class="arithmatex">\(C_1, C_2\)</span>に分割する場合を考え、ノード<span class="arithmatex">\(P, C_1, C_2\)</span>における推定方程式の解を
$$
    (\hat\theta_{P}, \hat\nu_{P}) \in \argmin_{\theta,\nu}\left{\left|\sum_{{i\in \mathcal{J}:X_i \in P}}\psi_{\theta,\nu}(O_i)\right|<em>{2}\right}
$$
$$
    (\hat\theta</em>{C_{j}}, \hat\nu_{C_{j}}) \in \argmin_{\theta,\nu}\left{\left|\sum_{{i\in \mathcal{J}:X_i \in C_{j}}}\psi_{\theta,\nu}(O_i)\right|<em>{2}\right}, \qquad j=1,2
$$
とする。このとき、損失関数 <span class="arithmatex">\(\mathrm{err}(C_1, C_2)\)</span> は
$$
    \mathrm{err}(C_1, C_2) = \sum</em>{j=1,2}\Pr(X\in C_j \mid X\in P) E[(\hat\theta_{C_{j}} - \theta(X))^2 \mid X\in C_j]
$$
とかける。この損失関数を用いて回帰木を学習させるには2つの問題がある。1つ目は<span class="arithmatex">\(\theta(X)\)</span>が観測できないことである。もう1つは、候補となる分割を試すたびに推定方程式を解く必要があるということである。特に、大量の回帰木を作成するランダムフォレストの場合、2つ目の問題はクリティカルな課題となる。そこで、漸近的には同値な言い換え (Proposition 1. of Athey et al.(2020))を用いて、
$$
    \widetilde{\mathrm{err}}(C_1, C_2) = \frac{n_{C_1}n_{C_2}}{n_{P}^{2}}(\hat\theta_{C_1}-\hat\theta_{C_2})^{2}
$$
とする。ここで、<span class="arithmatex">\(n_{P}, n_{C_1}, n_{C_2}\)</span>はそれぞれノード<span class="arithmatex">\(P, C_1, C_2\)</span>のノードに含まれるサンプルサイズである。この言い換えは、ノード内の重み付け分散の最小化問題を、ノード間のパラメータ推定量の差の大きさに言い換えたものであるから自然である。</p>
<p>次に2つ目の問題であった推定方程式の繰り返し計算の問題を、推定方程式の解を漸近展開することで親ノードの推定方程式の解と影響関数を用いて近似する。具体的には、
$$
    \hat\theta_{C_j} \approx \hat\theta_{P} - \frac{1}{|{i:X_i\in C}|}\sum_{{i\in X_i \in C_j }}\xi^{T}A_{P}^{-1}\psi_{\hat\theta_{P},\hat\nu_{P}}(O_i)
$$
とする。ここで、 <span class="arithmatex">\(A_{P}\)</span>は<span class="arithmatex">\(\nabla E[\psi_{\hat\theta_{P},\hat\nu_{P}}(O_i)\mid X\in P]\)</span> に対する一致推定量であればよく、スコア関数が微分可能な場合には、
$$
    A_{P} = \frac{1}{|{i:X_i\in P}|}\sum_{{i\in X_i \in P }}\nabla\psi_{\hat\theta_{P},\hat\nu_{P}}(O_i)
$$
によって推定する。この計算の中には、分割後のノード<span class="arithmatex">\(C_j\)</span>における推定方程式の計算が行われていないことに注意する。よって、
$$
    \rho_{i} = -\xi^{T}A_{P}^{-1}\psi_{\hat\theta_{P},\hat\nu_{P}}(O_i)
$$
とおいて、 <span class="arithmatex">\(\widetilde{\mathrm{err}}(C_1, C_2)\)</span> に代入したものを計算すると、
$$
    \widehat{\mathrm{err}}(C_1, C_2) := \left(\sqrt{\frac{n_{C_2}}{n_{C_1}}}\sum_{{i\in X_i \in C_1 }}\rho_{i} - \sqrt{\frac{n_{C_1}}{n_{C_2}}}\sum_{{i\in X_i \in C_2 }}\rho_{i}\right)^{2}
$$
となる。よって、一般化ランダムフォレストにおける推定では、近似的な損失関数を定義した回帰木をもとにして学習を行うことで、計算量を減らしつつランダムフォレストの重み <span class="arithmatex">\(\alpha_{i}(x)\)</span> を推定する。何度も指摘しておくが、この方式で学習しないと一致性がないということではなく、パラメータ<span class="arithmatex">\(\theta(x)\)</span>に対する回帰木を効率的に学習し、<span class="arithmatex">\(\alpha_{i}(x)\)</span>がパラメータ<span class="arithmatex">\(\theta(x)\)</span>の変動をうまく反映するように構成するためのアプローチの1つであると認識してもらいたい。</p>
<h2 id="_10">一般化ランダムフォレストの応用</h2>
<p>一般化ランダムフォレストは、その一般的な理論的枠組みによって様々な回帰の問題を扱うことができる。ここでは、一般化ランダムフォレストで扱える代表的な回帰モデルについて紹介する。これらの方法の大枠では、1) 興味のあるパラメータを規定する局所推定方程式 <span class="arithmatex">\(\psi_{\theta,\nu}(O_i)\)</span>を構成し、次に局所推定方程式を用いて Double sample gradient tree （以降、勾配回帰木） を学習させる。パラメータの値を推定したい点<span class="arithmatex">\(x\)</span>に対して、学習済み勾配回帰木を用いて推定方程式に観測データ<span class="arithmatex">\(X_i\)</span>の重み <span class="arithmatex">\(\alpha_{i}(x)\)</span> を計算する。最後に、点<span class="arithmatex">\(x\)</span>におけるパラメータの推定量 <span class="arithmatex">\(\hat\theta(x), \hat\nu(x)\)</span>を<span class="arithmatex">\(\alpha_{i}(x)\)</span>で重み付けた推定方程式を解くことで推定する。
$$
    \hat\theta(x),\hat\nu(x) = \operatorname{argmin}<em>{\theta,\nu} \left{\left| \sum</em>{i=1}^{n}\alpha_{i}(x)\psi_{\theta,\nu}(O_i)\right|_{2} \right}
$$
よって、<span class="arithmatex">\(\hat\theta(x), \hat\nu(x)\)</span> は正規方程式の解となる。この方針に従うことで、様々な推定方程式の解を求めることができる。</p>
<h3 id="quantile-regression-forest">quantile regression forest と、一般化ランダムフォレストによる分位点推定</h3>
<p>ここでは、確率変数の組 <span class="arithmatex">\((X_i, Y_i) \in \mathbb{R}^{p}\times \mathbb{R}\)</span> (<span class="arithmatex">\(i=1,2,...,n\)</span>) が観測されたもとでの、条件付きq-分位点関数 <span class="arithmatex">\(\theta_{Y|X}(q) = \inf\{y : F_{Y|X}(y) \geq q\}\)</span> を推定するための方法である。この問題は、スコア関数として
$$
    \psi_{\theta}(Y_i) = q - \mathbb{1}({Y_i \leq \theta})
$$
を設定したときの解である。条件付き分位点に対するスコア関数自体は微分できないが、この期待値をとった関数が微分可能であればGRFを用いた際の理論的妥当性は保証される。しかしながら、この方法に基づいた条件付き分位点関数の推定量は、GRFによる <code>quantile_regression</code> 関数では、分位点関数が滑らかではない上に、また2つの分位点<span class="arithmatex">\(q_1 &lt; q_2\)</span>に対する条件付き分位点関数が交差しないなどの条件が満たされないため、ノンパラメトリックに分位点を推定するというタスクにおいては、qgam (<a href="https://arxiv.org/pdf/1707.03307">Fasiolo and Wood</a>) や、GP(<a href="https://pure.aston.ac.uk/ws/files/25959086/1206.6391.pdf">Boukouvalas et al</a>) などを用いることが望ましいと言える。</p>
<h3 id="_11">一般化ランダムフォレストによる条件付き因果効果の推定</h3>
<p>一般化ランダムフォレストが最も注目を集めたのは、条件付き因果効果の推定をノンパラメトリックに行えるという点である。以下では、処置変数が2値の場合を扱うが、処置変数は連続変数でも仮定の拡張によって同様の議論が成り立つため、推定の枠組みは同じである。</p>
<p>確率変数の組 <span class="arithmatex">\((X, T, Y(0), Y(1)) \in \mathbb{R}^{p}\times \{0,1\} \times \mathbb{R} \times \mathbb{R}\)</span>が、<span class="arithmatex">\(Y(0), Y(1) \perp\!\!\!\perp T \mid X\)</span> を満たし、任意の<span class="arithmatex">\(x \in [0,1]^{p}\)</span>に対して<span class="arithmatex">\(0 &lt; \Pr(T \mid X = x) &lt; 1\)</span>、確率変数<span class="arithmatex">\(Y = TY(1)+ (1-T)Y(0)\)</span>であるとする。確率変数の組 <span class="arithmatex">\((X_i, T_i, Y_i), i=1,2,...,n\)</span> が、<span class="arithmatex">\((X, T, Y(0), Y(1))\)</span>の従う分布から観測されたとき、<span class="arithmatex">\(\tau(x) = E[Y(1)-Y(0)\mid X=x]\)</span>を推定するというのが因果効果推定の一般的な枠組みである。<span class="arithmatex">\(\mu(x) = E[Y(0)\mid X=x]\)</span>とおくと、強く無視可能な割り付けから<span class="arithmatex">\(\mu(x), \tau(x)\)</span>に対するスコア関数は次のように導かれる。
$$
    \psi_{\theta(x),\nu(x)}(O) = \psi_{\mu(x),\tau(x)}(T,Y) = \left(Y - \mu(x) - T\cdot \tau(x)\right)\begin{pmatrix}1 \ T \end{pmatrix}
$$
推定方程式 <span class="arithmatex">\(E[\psi_{\mu(x),\tau(x)}(O_i)\mid X_{i}=x] = 0\)</span> に対しては、正規方程式と解くことで解が求められるので、
$$
    \theta(x) = \xi^{T}Var(W_i \mid X_i = x)^{-1}Cov[W_i, Y_i \mid X_i = x]
$$
となる。よって、ランダムフォレストによる重み <span class="arithmatex">\(\alpha_{i}\)</span> を用いて、それぞれの期待値に対するカーネル重み付け推定量を計算することで
$$
    \hat\theta(x) = \xi^{T}\left(\sum_{i=1}^{n}\alpha_{i}(x)(W_i - \bar{W}<em>{\alpha})(W_i - \bar{W}</em>{\alpha})^{T}\right)^{-1} \sum_{i=1}^{n}\alpha_{i}(x)(W_i - \bar{W}<em>{\alpha})(Y_i - \bar{Y}</em>{\alpha})
$$
ただし、<span class="arithmatex">\(\bar{W}_{\alpha} = \sum_{i=1}^{n}\alpha_{i}(x)W_{i}\)</span> および <span class="arithmatex">\(\bar{Y}_{\alpha} = \sum_{i=1}^{n}\alpha_{i}(x)Y_{i}\)</span> とかける。</p>
<h3 id="local-centering-r-learner-by-nie-and-wager-2021">Local Centering (R-Learner by Nie and Wager., 2021)</h3>
<p>条件付き因果効果の推定と同様の設定で、異なるアプローチを取ることもできる。Nie and Wager (2021) では、Partial linear model (Robinson 1988) の考え方を用いて、<span class="arithmatex">\(\tau(x)\)</span>を推定する方法を提案しており、この考え方を取り入れて<span class="arithmatex">\(\tau(x)\)</span>を推定する方法について説明する。いま、<span class="arithmatex">\(m(x) = E[Y_{i} \mid X=x]\)</span> および、<span class="arithmatex">\(e(x) = E[W_{i} \mid X=x]\)</span>とおくと、
$$
    m(x) = E[(1-T)Y(0) + TY(1) \mid X=x] = (1-e(x))\mu(x) + e(x)(\mu(x)+\tau(x)) 
$$
であるから、
$$
    Y - m(X) = (T - e(X)) \tau(X) + \varepsilon
$$
が成立する。ただし、<span class="arithmatex">\(T \perp\!\!\! \perp \varepsilon \mid X\)</span> および、<span class="arithmatex">\(E[\varepsilon \mid X] = 0\)</span> である。</p>
<p>よって、<span class="arithmatex">\(m(X)\)</span>と<span class="arithmatex">\(e(X)\)</span>が既知であるときは、スコア関数を
$$
    \psi_{\tau}(Y,T) = (Y - m(X)) - (T - e(X)) \tau(X)
$$
とすることで、<span class="arithmatex">\(\tau(x)\)</span>を推定できることがわかる。すなわち、正規方程式を考えて<span class="arithmatex">\(\tau(x)\)</span>について解くと、任意の<span class="arithmatex">\(x\)</span>に対する局所推定方程式の解は、
$$
\begin{align}
    \theta(x) &amp;= \xi^{T}Var[(W_i - E[W_i \mid X_i = x]) \mid X_i = x]^{-1} \ &amp;\times Cov[(W_i - E[W_i \mid X_i = x]),(Y_i - E[Y_i \mid X_i]) \mid X_i = x]
\end{align}
$$
となる。よって、条件付き因果効果の推定と同様に、ランダムフォレストの重み <span class="arithmatex">\(\alpha_{i}(x)\)</span> で重み付けた empirical version を推定量として用いる。</p>
<p>しかし、<span class="arithmatex">\(m(x)\)</span>と<span class="arithmatex">\(e(x)\)</span>は一般的には未知であるため、推定する必要がある。ただし、<span class="arithmatex">\(m(x)\)</span>と<span class="arithmatex">\(e(x)\)</span>をランダムフォレストや、ブースティングなどを用いて推定する場合には、真の関数への収束レートが<span class="arithmatex">\(\sqrt{n}\)</span>オーダーではないためバイアスを生じるため、推定にはcross-fitting (Chernozhukov et al., 2018) を用いることが推奨されている。</p>
<p>cross-fitting は、サンプル<span class="arithmatex">\(i\)</span>に対する<span class="arithmatex">\(m(x)\)</span>と<span class="arithmatex">\(e(x)\)</span>の推定値<span class="arithmatex">\(\hat{m}(X_i)\)</span>を計算する際に用いるモデル<span class="arithmatex">\(\hat{m}\)</span>の推定に、<span class="arithmatex">\(i\)</span>サンプルを用いないという方法である。例えば、Kennedy et al., (2020) では、観測データを3つのグループ <span class="arithmatex">\(\mathcal{I}, \mathcal{J}, \mathcal{K}\)</span>に分け、<span class="arithmatex">\(\mathcal{I}\)</span>に対するスコア関数を計算する際には、<span class="arithmatex">\(m\)</span>の推定量<span class="arithmatex">\(\hat{m}\)</span>を<span class="arithmatex">\(\mathcal{J}\)</span>を用いて構成し、<span class="arithmatex">\(e\)</span>の推定量<span class="arithmatex">\(\hat{e}\)</span>を<span class="arithmatex">\(\mathcal{K}\)</span>を用いて構成し、<span class="arithmatex">\(i \in \mathcal{I}\)</span>に属するサンプルに対する<span class="arithmatex">\(m(X_i)\)</span>と<span class="arithmatex">\(e(X_i)\)</span>の推定量を<span class="arithmatex">\(\hat{m}(X_i)\)</span>と<span class="arithmatex">\(\hat{e}(X_i)\)</span>として構成する。</p>
<p>この方法については、推定モデルが<span class="arithmatex">\(i\)</span>サンプルを用いていなければよいため、ここでは明示的に<span class="arithmatex">\(m\)</span>と<span class="arithmatex">\(e\)</span>の推定量はそれぞれ<span class="arithmatex">\(i\)</span>サンプルを用いていないことを明示するため、<span class="arithmatex">\(\hat{m}^{(-i)}\)</span>および、<span class="arithmatex">\(\hat{e}^{(-i)}\)</span> と書くことにする。新たにスコア関数として、
$$
    \tilde{\psi}_{\tau}(Y,T) = (Y - \hat{m}^{(-i)}(X)) - (T - \hat{e}^{(-i)}(X)) \tau(X)
$$
を定義し、このもとで勾配回帰木を実行する。ここで、注意として、<span class="arithmatex">\(\hat\tau(x)\)</span>の一致性は、ここでの <span class="arithmatex">\(\hat{m}, \hat{e}\)</span> の一致性が直接影響するわけではなく、 <span class="arithmatex">\(\alpha_{i}(x)\)</span> で重み付けたあとのスコア方程式の設計に依存することを述べておく。</p>
<h3 id="instrumental-variable">一般化ランダムフォレストによるInstrumental variableを用いた因果効果の推定</h3>
<p>処置変数に対する因果効果の推定において、<span class="arithmatex">\(T_i\)</span>と誤差<span class="arithmatex">\(\varepsilon\)</span>が条件付き独立ではない場合に、因果効果を推定する手法として操作変数による因果効果の推定がある。独立同一な確率変数の組 <span class="arithmatex">\((X_i, Y_i, T_i, Z_i) \in \mathcal{X}\times \mathbb{R} \times \{0,1\}\times \{0,1\}\)</span> (<span class="arithmatex">\(i=1,2,...,n\)</span>) が観測されたもとで、以下のモデルを考える。
$$
    Y_i = \mu(X_i) + T_i \tau(X_i) + \varepsilon
$$
ただし<span class="arithmatex">\(T_i\)</span> と <span class="arithmatex">\(\varepsilon\)</span> は<span class="arithmatex">\(X_i\)</span>が与えられたもとで独立ではなく、任意の<span class="arithmatex">\(x\in \mathcal{X}\)</span>において <span class="arithmatex">\(Cov[Z_i, W_i \mid X=x] \neq 0\)</span>かつ、<span class="arithmatex">\(Z_i \perp\!\!\!\perp \varepsilon \mid X=x\)</span> が成り立つと仮定する。このとき、<span class="arithmatex">\(\tau(x)\)</span> は識別可能で、
$$
    \tau(x) = \frac{Cov[Y_i, Z_i \mid X_i = x]}{Cov[T_i, Z_i \mid X_i = x]}
$$
となる。このとき、<span class="arithmatex">\(\tau(x)\)</span> はモデルに基づいたスコア関数
$$
    \psi_{\theta(x),\nu(x)}(O) = \psi_{\mu(x),\tau(x)}(T,Z,Y) = \left(Y - \mu(x) - T\cdot \tau(x)\right)\begin{pmatrix}1 \ Z \end{pmatrix}
$$
を用いて推定可能である。また、この推定方程式もやはり Local centering を行うことができて、<span class="arithmatex">\(E[Y_{i}|X=x], E[T_{i}|X=x], E[Z_{i}|X=x]\)</span> に対する推定量 <span class="arithmatex">\(m^{(-i)}(X_i)\)</span>, <span class="arithmatex">\(e^{(-i)}(X_i)\)</span>, <span class="arithmatex">\(\eta^{(-i)}(X_i)\)</span>を用いて、
$$
    \tau(x) = \frac{Cov[Y_i - m^{(-i)}(X_i), Z_i -\eta^{(-i)}(X_i) \mid X_i = x]}{Cov[T_i - e^{(-i)}(X_i), Z_i - \eta^{(-i)}(X_i) \mid X_i = x]}
$$
とできるので、ランダムフォレストによって構成された重み <span class="arithmatex">\(\alpha_{i}\)</span> による重み付けで推定量を計算することができる。</p>
<h3 id="random-survival-forest">Random Survival Forest と 一般化ランダムフォレストによる生存関数推定</h3>
<p>Random survival forest (SRF, <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Random-survival-forests/10.1214/08-AOAS169.full">Ishwaran et al.,2008</a>) は、ランダムフォレスト手法を生存分析の設定に拡張し、制限的な仮定を課すことなく、共変量と生存結果間の複雑な関係を捉えることができる柔軟なノンパラメトリックな手法である。SRFはランダムフォレストを、共変量条件付きの生存関数および累積ハザード関数へ拡張するため、回帰木におけるノード分割において、ログランク統計量を用いて検定統計量が最も大きくなるように分割を行っている。回帰木を生存時間へと拡張した survival trees (生存木) については、<a href="https://projecteuclid.org/journals/statistics-surveys/volume-5/issue-none/A-review-of-survival-trees/10.1214/09-SS047.full">Hamad et al.,(2011)</a> が詳しい。また、<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4409541/">Zhou and McArdle (2015)</a> では、　生存木 について応用事例などをまとめている。</p>
<p>いま、<span class="arithmatex">\(T^{*}\)</span>を生存時間（イベントまでの時間）とし、<span class="arithmatex">\(C\)</span>を右側打ち切り時間とする。観測されるイベント発生時刻を<span class="arithmatex">\(T\)</span>とすると、
<span class="arithmatex">\(<span class="arithmatex">\(T = \min(T^{*}, C)\)</span>\)</span>
と表すことができる。また、右側打ち切りに対する指示変数を、
<span class="arithmatex">\(<span class="arithmatex">\(\Delta = \mathbf{1}\{T^{*} \leq C\}\)</span>\)</span>
とする。共変量ベクトルを<span class="arithmatex">\(X \in \mathbb{R}^{p}\)</span>とする。ただし、共変量ベクトルは時間変化しないものとする。また、生存時間における一般的な仮定である、打ち切りに対する条件付き独立性 <span class="arithmatex">\(<span class="arithmatex">\(T \perp C | X\)</span>\)</span>を仮定する。いま、<span class="arithmatex">\((T^{*},C,X)\)</span> が従う分布<span class="arithmatex">\(P\)</span>からの独立同一なサイズ<span class="arithmatex">\(n\)</span>の確率変数 <span class="arithmatex">\((T^{*}_{i}, C_{i}, X_i)\)</span> に基づいて、<span class="arithmatex">\((T_{i}, \Delta_{i}, X_i)\)</span>が観測されたとする。このとき、SRFに基づく生存時間解析では、条件付き生存関数 <span class="arithmatex">\(S(t|x) = P(T &gt; t|X = x)\)</span> や、累積ハザード関数 <span class="arithmatex">\(\Lambda(t|x) = -\log S(t|x)\)</span> を推定することが目標である。SRFでは、通常のランダムフォレスト同様に、サイズ<span class="arithmatex">\(n\)</span>の訓練データからサイズ<span class="arithmatex">\(s\)</span>のサブサンプリングを<span class="arithmatex">\(B\)</span>回行い、それぞれに対して 生存木 を当てはめた結果を、平均することで条件付き生存関数および、累積ハザード関数を推定する。</p>
<p>SRFを構成する 生存木 では、通常の回帰木とは異なりログランク統計量を用いる。ログランク統計量は、2つの生存曲線を比較し、差があるかどうかを検定するための用いる統計量である。ここでは、親ノードを2つの子ノードに分割するための 生存木 の損失関数について説明する。いま、親ノード<span class="arithmatex">\(L\)</span>内のイベント発生時間を<span class="arithmatex">\(\{t_1, t_2, \ldots, t_M\}\)</span>とし、各時点<span class="arithmatex">\(t_j\)</span>について、</p>
<ul>
<li><span class="arithmatex">\(d_j = \sum_{i \in L}\mathbb{1}\{T_i = t_j , \Delta_{i} = 1\}\)</span>：親ノードにおける時間<span class="arithmatex">\(t_j\)</span>でのイベント数</li>
<li><span class="arithmatex">\(d_{j,L}\)</span>：左子ノードにおける時間<span class="arithmatex">\(t_j\)</span>でのイベント数</li>
<li><span class="arithmatex">\(N_j = \sum_{i \in L}\mathbb{1}\{T_i \geq t_j\}\)</span>：親ノードにおける時間<span class="arithmatex">\(t_j\)</span>でリスク下にある対象者数</li>
<li><span class="arithmatex">\(N_{j,L}\)</span>：左子ノードにおける時間<span class="arithmatex">\(t_j\)</span>でリスク下にある対象者数</li>
</ul>
<p>と定義する。このとき、ログランク統計量は次のように与えられる。</p>
<div class="arithmatex">\[L = \frac{\left(\sum_{j=1}^M \left(d_{j,L} - N_{j,L} \cdot \frac{d_j}{N_j}\right)\right)^2}{\sum_{j=1}^M \frac{N_{j,L}}{N_j} \cdot \left(1 - \frac{N_{j,L}}{N_j}\right) \cdot \frac{N_j - d_j}{N_j - 1} \cdot d_j}\]</div>
<p>この統計量は、二つの子ノードにおけるハザード関数が同じであるという帰無仮説の下で、漸近的に自由度1の<span class="arithmatex">\(\chi^2\)</span>分布に従い、この値が大きほど生存時間関数に差があることを意味する。よって生存木では、生存時間関数のノード間での異質性を捉えるために、ログランク統計量を最大にする分割を各ノードで選択し、再起的分割を行い木構造を推定する。木構造が推定されたあとで、生存木の葉それぞれにおいてカプラン・マイヤー推定量（KM推定量）と、ネルソン・アーレン推定量（AL推定量）を計算し、条件付き生存関数と、条件付き累積ハザード関数を推定する。生存関数に対するKM推定量は、生存木の各葉内のサンプルに対して、
$$
    \hat{S}<em>{\text{KM}}(t) = \prod</em>{j: t_j \leq t} \left(1 - \frac{d_j}{N_j}\right)
$$
によって計算される。また、累積ハザード関数に対するAL推定量を、各葉に対して、
$$
    \hat{\Lambda}<em>{\text{NA}}(t) = \sum</em>{j: t_j \leq t} \frac{d_j}{N_j}
$$
によって推定し、AL推定量に基づいて生存関数の推定を行う。
$$
    \hat{S}<em>{\text{NA}}(t) = \exp(-\hat{\Lambda}</em>{\text{NA}}(t))
$$
サバイバルフォレストでは、点<span class="arithmatex">\(x\)</span>における生存関数または累積ハザード関数を、生存木の各葉で推定されるKM推定量または、AL推定量の平均によって推定する。</p>
<p>生存木に用いる分割基準の損失関数は、ログランク統計量に限る必要はなく、2つの生存関数の非類似度を測る指標であればよい。すなわち、損失関数の設計は研究者や実務家が、何を使うかを研究の文脈において決定することが望ましい。例えば、<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0116774">Li et al. (2015)</a> において報告されているように、ログランク検定は比例ハザード性が成り立つ場合において検出力が高いが、生存時間の交差が認められる場合などでは検出力が低下するため、代替的な手法を用いることが推奨されている。この論文では、<a href="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00622.x">Qui-Shengの2段階手順</a>や、適応的ネイマン平滑化検定などが、生存関数の交差においてその差を検出できたという報告がなされている。また、<a href="https://academic.oup.com/biostatistics/article-abstract/15/4/757/266340?redirectedFrom=fulltext">Ishwaran et al.(2014)</a> によってGray検定などを用いることで条件付き累積発生関数を推定することで、競合リスクがある場合へ対応した SRF も提案されている。</p>
<p>一方で、一般化ランダムフォレストでは、生存木ではKM推定量や、AL推定量を計算せずに重みを計算し、その重み付けによってカーネル重み付け生存関数の推定を行う。いま、<span class="arithmatex">\(T_{b}, b=1,2,...,B\)</span>を、生存木であるとし、点<span class="arithmatex">\(x\)</span>を含む葉を<span class="arithmatex">\(L_{b}(x)\)</span>とすると、点<span class="arithmatex">\(x\)</span>を含むランダムフォレストの重みは、回帰木の場合同様に
$$
    \alpha_i(x) = \frac{1}{B} \sum_{b=1}^B \frac{\mathbf{1}{X_i \in L_b(x)}}{|L_b(x)|}
$$
となる。よって、サバイバルランダムフォレストにおけるKM推定量は<span class="arithmatex">\(\alpha_{i}(x)\)</span>による重み付け関数として得られる。
$$
    \hat{S}<em>{KM}^{(GRF)}(t|x) = \prod</em>{j: t_j \leq t} \left(1 - \frac{\sum_{i=1}^n \alpha_i(x) \cdot \mathbf{1}{T_i = t_j, \Delta_i = 1}}{\sum_{i=1}^n \alpha_i(x) \cdot \mathbf{1}{T_i \geq t_j}}\right)
$$
また、AL推定量も同様に得られる。
$$
    \hat{S}^{(GRF))}<em>{\text{NA}}(t|x) = \exp\left(-\sum</em>{j: t_j \leq t} \frac{\sum_{i=1}^n \alpha_i(x) \cdot \mathbf{1}{T_i = t_j, \Delta_i = 1}}{\sum_{i=1}^n \alpha_i(x) \cdot \mathbf{1}{T_i \geq t_j}}\right)
$$
GRFによる生存時間の推定量に対する一致性については結果が証明されているわけではなく、今後の課題である。関連する文献をいくつか挙げると、<a href="https://projecteuclid.org/journals/annals-of-statistics/volume-17/issue-3/Uniform-Consistency-of-the-Kernel-Conditional-Kaplan-Meier-Estimate/10.1214/aos/1176347261.full">Dabrowska (1989)</a> において、カーネル重み付け条件付きKM推定量については一様一致性の結果が得られている。また、<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/sjos.12774">Bladt and Furrer (2025)</a>競合リスク下における 条件付きAalen-Johansen推定量に対する一致性と漸近正規性の結果も示されている。</p>
<h3 id="causal-survival-effect">一般化ランダムフォレストによる 条件付き causal survival effectの推定</h3>
<p>処置を受けたグループ（処置群）と、処置を受けなかったグループ（対照群）を比較して、その生存関数が個体の特性によってどう異なるのかに着目するのが、Conditional causal survival effect の推定である。ここでは、<a href="https://academic.oup.com/jrsssb/article/85/2/179/7058918">Cui et al. (2023)</a> を中心に説明を行う。</p>
<p>確率変数の組 <span class="arithmatex">\(\{X, T(0), T(1), C, W\} \in \mathcal{X} \times \mathbb{R}_{+} \times \mathbb{R}_{+} \times \mathbb{R}_{+} \times \{0,1\}\)</span> が従う分布を<span class="arithmatex">\(P\)</span>とする。 <span class="arithmatex">\(X\)</span> は特徴量であり、<span class="arithmatex">\(T(0)\)</span> と <span class="arithmatex">\(T(1)\)</span> は潜在的な生存時間、 <span class="arithmatex">\(C\)</span> は打ち切り時間、 <span class="arithmatex">\(W\)</span> はバイナリーの処置変数である。</p>
<p>確率分布<span class="arithmatex">\(P\)</span>に独立同一に従うサイズ<span class="arithmatex">\(n\)</span>の確率変数の組を <span class="arithmatex">\(\{X_{i}, T_{i}(0), T_{i}(0), C_{i}, W_{i}\}_{i=1}^{n}\)</span> とし、ここから観測データ <span class="arithmatex">\(\{X_{i}, T_{i}, C_{i}, W_{i}\}_{i=1}^{n}\)</span> が得られるとする。ただし、<span class="arithmatex">\(T_i = W_i T_{i}(1) + (1-W_i) T_{i}(0)\)</span> である。Cui et al.,(2023)では、生存時間<span class="arithmatex">\(T\)</span>に対する変換 <span class="arithmatex">\(y\)</span>を用いて生存時間における条件付き因果効果 <span class="arithmatex">\(\tau(x)\)</span>を定義する。
$$
    \tau(x) = E[y(T_{i}(1)) - y(T_{i}(0)) \mid X_{i} = x]
$$
ここで、Cui et al.,(2023) では、<span class="arithmatex">\(y\)</span>に対して 定数 <span class="arithmatex">\(0 &lt; h &lt; \infty\)</span> に対して、<span class="arithmatex">\(y(t) = y(h), \; (t \geq h)\)</span>を仮定して、制限付き平均生存時間を考える。打ち切りが存在しない理想的な状況においては、標準的な因果推論の仮定のもとで、<span class="arithmatex">\(\tau(x)\)</span>の推定は Local centering を行なった causal forest と同様の枠組みで推定が可能であり、スコア関数 <span class="arithmatex">\(\psi^{(c)}\)</span>を
$$
    \psi_{\tau}^{(c)}\bigl(X_i, y(T_i), W_i; \hat{e}, \hat{m}\bigr) = \bigl[W_i - \hat{e}^{(-i)}(X_i)\bigr]\bigl[y(T_i) - \hat{m}^{(-i)}(X_i) - \tau(W_i - \hat{e}(X_i))\bigr]
$$
とした一般化ランダムフォレストによって<span class="arithmatex">\(\tau(x)\)</span>が推定可能である。ただし、<span class="arithmatex">\(\hat{m}^{(-i)}\)</span>および
<span class="arithmatex">\(\hat{e}^{(-i)}\)</span> は <span class="arithmatex">\(e(x) = E(W_i = 1 \mid X = x)\)</span>および、<span class="arithmatex">\(m(x) = E[y(T_i)\mid X_i = x]\)</span> に対するcross-fitting推定量である。しかし、生存時間の打ち切りの影響を含んで因果効果を推定する場合には、打ち切りの影響を制御するための拡張が必要である。ここでは、識別可能性を保証するために、一般的な因果推論の仮定のほかに打ち切りに対して2つの仮定を置く。
- 処置変数と共変量を条件づけたもとでの打ち切り変数と生存時間は独立性： 
    <span class="arithmatex">\(<span class="arithmatex">\(T_i \perp\!\!\!\perp C_i \mid X_i , W_i\)</span>\)</span>
- 任意の処置変数と共変量を条件づけたもとでの打ち切り発生は確率的である：ある<span class="arithmatex">\(0 &lt; \eta_{C} \leq 1\)</span>
    <span class="arithmatex">\(<span class="arithmatex">\(\Pr[C_{i} &lt; h \mid X_i, W_i] \leq 1-\eta_{C}\)</span>\)</span> </p>
<p>これらの仮定のもとで、Cui et al.,(2023) では、<span class="arithmatex">\(\tau(x)\)</span>に対する推定量として、打ち切り逆確率重み付け（Inverse Probability censoring weighting）推定量を提案している。打ち切り過程に対する確率に対するモデル <span class="arithmatex">\(S_{w}^{C}(s\mid x)\)</span> を
$$
    S_{w}^{C}(s\mid x) = \Pr[C_i \geq s \mid W_i = w, X_i=x]
$$
で定義する。非打ち切りに対応する確率変数を、<span class="arithmatex">\(\Delta_{i}^{h} = 1\{(T_{i} \wedge h) \leq C_i\}\)</span>とすると、非打ち切りデータに打ち切り過程の確率の推定値の逆数で重み付けた推定方程式
$$
    \sum_{{i : \Delta_{i}^{h}=1}}\frac{\alpha_{i}(x)}{\hat{S}<em>{w}^{C}(s\mid x)}\psi</em>{\tau}^{(c)}\bigl(X_i, y(T_i), W_i; \hat{e}, \hat{m}\bigr) = 0
$$
を解くことで、<span class="arithmatex">\(\tau(x)\)</span>が推定できる。ここで、<span class="arithmatex">\(\hat{S}_{w}^{C}(s\mid x)\)</span>は、survival forest を用いて推定することができる。 Cui et al., (2023) では、IPCW推定量を用いる場合の問題点として、<span class="arithmatex">\(\hat{S}_{w}^{C}(s\mid x)\)</span> の推定精度が、 <span class="arithmatex">\(\tau(x)\)</span> の推定精度に大きな影響を与えることと、非打ち切りデータのみに絞った解析の問題として、観測データを捨てることによる情報量ロスの問題を挙げている。そこで、<a href="https://link.springer.com/book/10.1007/0-387-37345-4">Tsiatis (2007)</a> において提案された打ち切りに対して2重に頑健な推定量の構成の考え方を用いて、<a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476818">Robins et al.(1994)</a> を拡張した観測データに基づく新たなスコア関数を提案した。</p>
<p><span class="arithmatex">\(U_i = T_i \wedge C_i\; (i=1,2,...,n)\)</span>とし、スコア関数<span class="arithmatex">\(\psi_{\tau}(X_i, y(U_i), U_i \wedge h, W_i, \Delta_{i}^{h})\)</span>を以下で定義する。</p>
<p>$$
    \begin{aligned}
    \psi_{\tau}(X_i, y(U_i), U_i \wedge h, W_i, \Delta_i^{h})
    &amp;= \frac{\Delta_i^{h}\psi_{\tau}^{(c)}(X_i, y(U_i), W_i)}{S_{W_i}^{C}(U_i \wedge h|X_i)} \
    &amp;\quad + \frac{(1 - \Delta_i^{h})\mathbb{E}\left[\psi_{\tau}^{(c)}(X_i, y(T_i), W_i)|T_i \wedge h &gt; U_i \wedge h, W_i, X_i\right]}{S_{W_i}^{C}(U_i \wedge h|X_i)} \
    &amp;\quad - \int_{0}^{U_i \wedge h}\frac{\lambda_{W_i}^{C}(s|X_i)}{S_{W_i}^{C}(s|X_i)}\mathbb{E}\left[\psi_{\tau}^{(c)}(X_i, y(T_i), W_i)|T_i \wedge h &gt; s, W_i, X_i\right]ds,
    \end{aligned}
$$
ここで、<span class="arithmatex">\(\lambda_{w}^{C}(s\mid x) = -\frac{d}{ds}\log S_{w}^{C}(s\mid x)\)</span> である。第1項は打ち切られなかったデータに対するIPCWに対応するスコア関数である。第2項は、打ち切りデータに対して、<span class="arithmatex">\(E[\cdot \mid T_i \wedge h &gt; U_i \wedge h, W_i, X_i]\)</span> の条件は観測データに基づいた条件付きスコア関数の推定量である。これは Robins et al.(1994) でも述べられている Augmented IPW の補完項に対応する。最後の項は、Tsiatis (2006) の理論に基づく補正項で、<span class="arithmatex">\(\frac{\lambda_{W_i}^{C}(s|X_i)}{S_{W_i}^{C}(s|X_i)}\)</span> は時間<span class="arithmatex">\(s\)</span>における条件付きの打ち切りハザードであり、特定時点での打ち切りが発生する条件付き確率に対応する。</p>
<p>実際の因果効果推定においては、<span class="arithmatex">\(\mathbb{E}\left[\psi_{\tau}^{(c)}(X_i, y(T_i), W_i)|T_i \wedge h &gt; U_i \wedge h, W_i, X_i\right]\)</span> および、<span class="arithmatex">\(\lambda_{W_i}^{C}(s|X_i) <span class="arithmatex">\(、\)</span> S_{W_i}^{C}(s|X_i)\)</span>をデータから推定したものをプラグインしたスコア関数を用いる。ここで、推定量の構成は cross-fitting を用いて行う。
$$
    \begin{aligned}
    &amp;\psi_{\tau}(X_i, y(U_i), U_i \wedge h, W_i, \Delta_i^{h}; \hat{e}, \hat{m}, \hat{\lambda}^{C}<em>{w}, \hat{S}^{C}</em>{w}, \hat{Q}<em>{w}) \
    &amp;= \left(\frac{\hat{Q}</em>{W_i}(U_i \wedge h|X_i) + \Delta_i^{h}[y(U_i) - \hat{Q}<em>{W_i}(U_i \wedge h|X_i)] - \hat{m}(X_i) - \tau(W_i - \hat{e}(X_i))}{\hat{S}^{C}</em>{W_i}(U_i \wedge h|X_i)}\right. \
    &amp;\quad - \left.\int_{0}^{U_i \wedge h}\frac{\hat{\lambda}^{C}<em>{W_i}(s|X_i)}{\hat{S}^{C}</em>{W_i}(s|X_i)}[\hat{Q}_{W_i}(s|X_i) - \hat{m}(X_i) - \tau(W_i - \hat{e}(X_i))]\,ds\right)(W_i - \hat{e}(X_i)),
    \end{aligned}
$$
ここで、<span class="arithmatex">\(\hat{Q}_{W_i}(U_i \wedge h|X_i)\)</span>は、<span class="arithmatex">\(\mathbb{E}\left[\psi_{\tau}^{(c)}(X_i, y(T_i), W_i)|T_i \wedge h &gt; U_i \wedge h, W_i, X_i\right]\)</span>に対する推定量である。</p>
<p>Cui et al.(2023) では、
$$
    \sum_{i=1}^{n}\alpha_{i}(x)\psi_{\tau}(X_i, y(U_i), U_i \wedge h, W_i, \Delta_i^{h}; \hat{e}, \hat{m}, \hat{\lambda}^{C}<em>{w}, \hat{S}^{C}</em>{w}, \hat{Q}_{w}) = 0
$$
の解として推定される条件付き因果効果の推定量 <span class="arithmatex">\(\hat\tau(x)\)</span> は、cross-fitting 推定量が <span class="arithmatex">\(\hat{e}\)</span>,<span class="arithmatex">\(\hat{m}\)</span>,<span class="arithmatex">\(\hat{Q}\)</span>,<span class="arithmatex">\(\hat{\lambda}_{w}^{C}\)</span>,<span class="arithmatex">\(\hat{S}_{w}^{C}\)</span> が一様一致性を満たす場合には、<span class="arithmatex">\(\hat{\tau}(x)\)</span>が一致性を持つことを示している。また、条件付き因果効果の推定と同様に、cross-fitting推定量の収束速度が適当な正則条件を満たす場合には、causal forest の推定量と同様に漸近正規性が成り立つ。また、漸近分散の推定方法についても一般化ランダムフォレストの枠組みで推定することがで可能である。ただし、Cui et al.(2023) では、推定された<span class="arithmatex">\(\hat\tau(x)\)</span>は、しばしば複雑な関数となることと、漸近分散は非常に大きくなるため、解釈を行う際には適当な特徴量の部分集合に対して線形射影した結果に対して解釈を行うことを推奨している。</p>
<p>Causal survival forest を応用した事例としては、Cui et al.,(2023) においては、AIDS臨床試験グループプロトコル175（ACTG175）[Hammer et al., 1996]データへの応用を行なっており、このデータでは4つの治療群へ患者はランダムに割り付けられており、そのうち2つの治療群間の間での条件付き生存因果効果の推定を行っている。また、<a href="https://arxiv.org/abs/2312.02482">Sverdrup and Wager (2024)</a> では、National Job Training Partnership Act（JTPA）に基づくランダム化比較試験（RCT）のデータを用いて、職業訓練プログラムへの参加資格が失業期間に与える影響を、打ち切り時間（2年）を考慮してRMSTベースの因果効果を推定している。</p>
<h2 id="_12">一般化ランダムフォレストの推定精度の改善</h2>
<p>一般化ランダムフォレストによる解は、漸近的な性能は良いが、実際のデータに当て嵌めを行った場合、ランダムフォレスト同様の問題が生じる場合が多い。ここでは、ランダムフォレスト特有の問題を解消するための方法についてLocal Linear forest について説明する。</p>
<h3 id="locally-linear-forest">Locally Linear Forest</h3>
<p>ランダムフォレストは、点<span class="arithmatex">\(x\)</span>に対する推定量をその周辺のサンプルに重み付けして推定するという性質上、特徴空間<span class="arithmatex">\(\mathcal{X}\)</span>の裾などの境界上において推定精度が低下するという問題がある。Local regression forest (LLF, <a href="https://arxiv.org/pdf/1807.11408">Friedberg et al.2020</a>) は、この問題に対処するために局所回帰の考え方を用いる。いま、確率変数の組 <span class="arithmatex">\((X_1, Y_1), ..., (X_n, Y_n) \in [0,1]^{p} \times \mathbb{R}\)</span> が観測されたと仮定し、<span class="arithmatex">\(Y_i = \mu(X_i) + \varepsilon\)</span> を満たすとする。このとき、点 <span class="arithmatex">\(x \in \mathcal{X}\)</span> に対する <span class="arithmatex">\(\mu(x) = E[Y \mid X=x]\)</span>に対するLLF推定量を、
$$
    \begin{pmatrix}\hat\mu(x_0) \ \hat\theta(x_0)\end{pmatrix} = \argmin_{\mu,\theta}\left{\sum_{i=1}^{n}\alpha_{i}(x_0)(Y_i - \mu(x_0) - (X_i - x_0)\theta(x_0))^{2} + \lambda|\theta(x_0)|_{2}^{2} \right}
$$
によって定義する。<span class="arithmatex">\(\hat\mu(x_0)\)</span> は <span class="arithmatex">\(\mu(x_0)\)</span>に対する推定量であり、<span class="arithmatex">\(\theta(x_0)\)</span>は、<span class="arithmatex">\(X_i - x_0\)</span> による局所的な変動を補正するための項である。また、リッジ罰則 <span class="arithmatex">\(\lambda\|\theta(x_0)\|_{2}^{2}\)</span> は局所変動への過剰な当てはまりを抑制する項である。</p>
<p>いま、<span class="arithmatex">\(A\)</span>を<span class="arithmatex">\(A_{i,i} = \alpha_{i}(x_0)\)</span> である対角行列とし、<span class="arithmatex">\(J\)</span>を <span class="arithmatex">\((d+1)\times (d+1)\)</span> の対角行列とし、<span class="arithmatex">\(J_{1,1} = 0\)</span> かつ <span class="arithmatex">\(J_{i,i} = 1 (i=2,3,...,d+1)\)</span> とする（この行列は、リッジ回帰の罰則項を<span class="arithmatex">\(\mu(x_0)\)</span>にのみかけないことに対応する）。さらに、<span class="arithmatex">\(\Delta\)</span>を切片と<span class="arithmatex">\(x_0 = (x_{0,1}, x_{0,2},...,x_{0,p})\)</span>に中心化した特徴量からなる行列で、<span class="arithmatex">\(\Delta_{i,1} = 1\)</span> かつ <span class="arithmatex">\(\Delta_{i,j+1} = X_{i,j} - x_{0,j}\)</span> (<span class="arithmatex">\(j=1,2,...,p\)</span>) とすると、LLF推定量は
$$
    \begin{pmatrix}\hat\mu(x_0) \ \hat\theta(x_0)\end{pmatrix} = (\Delta^{T}A\Delta + \lambda J)^{-1} \Delta^{T}AY
$$
と陽に表すことができる。</p>
<p>次に、LLFにおける分割基準について考える。LLFの目標は、局所的な変動を除いて<span class="arithmatex">\(\mu(x)\)</span>を推定することである。そこで、分割においても局所的な変動を除いた上で、<span class="arithmatex">\(\mu(x)\)</span>の変動を捉えるような分割を考えるのは自然である。親ノード <span class="arithmatex">\(P\)</span> において、サイズ<span class="arithmatex">\(n_P\)</span> の観測データ <span class="arithmatex">\((X_1,Y_1),...,(X_n,Y_n)\)</span> が含まれるとする。2つのノード<span class="arithmatex">\(C_1, C_2\)</span> への分割は、分割後のノードの <span class="arithmatex">\(Y\)</span> の平均を、<span class="arithmatex">\(\bar{Y_1}, \bar{Y_2}\)</span> とおくと、2乗損失の和
$$
    \sum_{i : X_i \in C_1}\frac{#{i : X_i \in C_1}}{n_{P}}(Y_i - \bar{Y}<em>1)^{2} + \sum</em>{i : X_i \in C_2}\frac{#{i : X_i \in C_2}}{n_{P}}(Y_i - \bar{Y}<em>{2})^{2}
$$
を最小にするのが一般的である。これに対して、LLFでは局所的な変動を除いた<span class="arithmatex">\(\mu(x)\)</span>への当てはまりをよくする。親ノードで、<span class="arithmatex">\(Y_i\)</span> に対してリッジ回帰モデルを当てはめた推定値を
$$
    \hat{Y}</em>{i} = \hat{\alpha}<em>{P} + X</em>{i}^{T}\hat\beta_{P}, \quad \hat\beta_{P} = (X_{P}^{T}X_{P}+ \lambda J)^{-1}X_{P}^{T}Y_{P}
$$
とする。ここで、<span class="arithmatex">\(X_{P}\)</span> と <span class="arithmatex">\(Y_{P}\)</span> はそれぞれ、親ノードに含まれる <span class="arithmatex">\(X_{i}\)</span>と<span class="arithmatex">\(Y_{i}\)</span>からなる特徴量の行列、および結果変数のベクトルである。LLFにおけるノードの分割では局所変動を除いた斬差
$$
    Y-\hat{Y}_{i}
$$
に対してCARTによる分割を行う。</p>
<p>ここで、リッジ回帰における罰則<span class="arithmatex">\(\lambda\)</span>は、大きい値を用いると<span class="arithmatex">\(\mu(x)\)</span>の変動の学習も抑制するため、回帰木の学習時には小さい値を使うことが推奨されている。一方で、推定方程式を解く場合には、回帰木の学習時よりも大きい罰則の値を用いる。</p>
<p>この方法は、条件付き因果効果の推定にも応用することができる。因果効果の推定の場合には、回帰の問題の場合に加えて、因果効果の局所変動も捉えたい。そこで、次のL2罰則付きのリッジ回帰を用いる。
$$
    \left{\hat\tau(x_0), \hat\theta_{\tau}(x_0), \hat{a}(x_0), \hat\theta_{a}(x_0) \right} = \argmin_{\tau,\theta}\left{
        \sum_{i=1}^{n}\alpha_{i}(x_0)\left(
            Y_i - \hat{m}^{(-i)}(X_i) - a - (X_i - x_0)\theta_{a} - (\tau + \theta_{\tau}(X_i - x_0))(W_i - \hat{e}^{(-i)}(X_i))
        \right)^2 + \lambda_{\tau}|\theta_{\tau}|<em>{2}^{2} + \lambda</em>{a}|\theta_{a}|_{2}^{2}
    \right}
$$</p>
<h2 id="_13">一般化ランダムフォレストのハイパーパラメータ選択</h2>
<p>一般的な機械学習と同様に、ランダムフォレストの予測精度は、ハイパーパラメータの選択に依存している。ここでは、ランダムフォレストのパラメータチューニングの方法と、それによる予測性能の改善について説明する。まず、一般的な結果として、<a href="https://jmlr.org/papers/volume20/18-444/18-444.pdf">Probst et al.,(2019)</a> において、ランダムフォレストのデフォルトのパラメータと、ハイパーパラメータチューニングを行った場合の最小2乗誤差やAUCを比較し、ハイパーパラメータのチューニングがパフォーマンス改善に寄与するかを、SVMやXgBoostなどと比較をすることで、ランダムフォレストの tunability を評価している。その結果、k-近傍法についでランダムフォレストのパラメータはデフォルトと、CV等でチューニングした結果が変わらないという結果が得られている。よって、他のブースティングやNN等と比較して、ハイパーパラメータの選択が予測結果に大きな影響を与えないという点は、評価できる手法であることがわかる。</p>
<p>しかし、ランダムフォレストにおいてパラメータチューニングが全く必要ないというわけではなく、いくつかのパラメータのチューニングは重要であることがわかっている。<a href="https://arxiv.org/abs/2001.04295">Scornet (2020)</a>の指摘にもある通り、ランダムフォレストにおいて推定量に影響を与えるのは、<code>min-node-size</code> と <code>subsample size</code> そして、<code>mtry</code> である。</p>
<p><code>min-node-size</code> は、ランダムフォレストの生成するカーネル <span class="arithmatex">\(\alpha_{i}(x)\)</span> がどの範囲に重みを与えるのかに関わっている。また、<code>min-node-size</code> が小さいと、狭い範囲に重みがかかることになるので、データのノイズが大きい場合などは、推定値が不安定になる可能性がある。一方で広すぎると、関数の変動を捉えきれなくなる。</p>
<p><code>subsample size</code> は、データを2つに分割する回帰木のアルゴリズム（Double sample）を用いている限りは、木構造の学習と予測に使われるサンプルがランダムに分割されるため、異なる回帰木間の相関は小さくなるため、<span class="arithmatex">\(n \times  [0.9,1.0)\)</span> 程度に大きくとっても原理上は問題にはならない。</p>
<p><code>mtry</code>は、回帰木1本を構築する際に用いる特徴量の数で、package{ranger} などを見る限り一般的には <span class="arithmatex">\(\sqrt{p}\)</span> が用いられる。しかし、一般化ランダムフォレストにおいては <code>mtry = min{p, 20 + $\sqrt{p}$}</code> が用いられる。木における各ノードの分割で使用する変数の選択を <span class="arithmatex">\(\min\{1+\mathrm{Poisson}(mtry),p\}\)</span> で行い、ランダム性を持たせているため、他のランダムフォレストとは扱いが異なるが、基本的にデフォルトで問題はない。</p>
<p>また、ランダムフォレストを構成する回帰木の本数 <span class="arithmatex">\(B\)</span> は、サンプルと同じオーダー以上が望ましいとされることと、<span class="arithmatex">\(B\)</span>は大きい方が回帰木のランダムネスが平均化されるため結果が安定する。<a href="https://www.jmlr.org/papers/volume17/14-168/14-168.pdf">Mentch and Hooker (2016)</a> では、<span class="arithmatex">\(B=500\)</span>以上を選択しており、この他 package{grf} などでも <span class="arithmatex">\(B=2000\)</span>をデフォルトにしていることを指摘しておく。</p>
<p>以上の議論から、一般化ランダムフォレストを用いる場合には<code>subsample size</code> がチューニングにおいて最も重要になるため、この変数についてはクロスバリデーションなどを通して、慎重に選択することが望ましい。</p>
<p>最後にチューニングとは直接的に関係はないが、一般化ランダムフォレストにおける Double sampleのメカニズムによって、多くの論文でのパフォーマンスの評価は、<span class="arithmatex">\(n=1000\)</span>以上が多く比較的大きな規模のデータセットを対象としていることには注意していただきたい。特に因果推論などの文脈において、<span class="arithmatex">\(n=400\)</span>程度で一般化ランダムフォレストを使っているケースが見受けられるが、提案時において<span class="arithmatex">\(n\)</span>が小さい状況はあまり想定されているものではないと著者は考えている。</p>
<h2 id="_14">一般化ランダムフォレスト予測の解釈</h2>
<p>一般化ランダムフォレストに対する予測の解釈については、実は理論が大きく深まっているわけではなく現在発展途上である。従来のランダムフォレストでは、主に Mead Decrease Impurity (MDI;平均不純度低下)または、Mean Decrease Accuracy (MDA; 平均精度低下) のいずれかを用いて、ランダムフォレストの変数の重要度が測られてきた。</p>
<p>MDIは、各回帰木における分割で低下した損失に、元のノードに含まれるサンプルサイズ（の全体に対する）比を乗じた値が、選択された変数の重要度となる。これを全ての木で平均することでMDIが計算される。一方のMDAは、観測された変数のパーミュテーションを用いて計算される統計量で、ある変数 <span class="arithmatex">\(j \in \{1,2,...,p\}\)</span>をモデルから除いた場合の推定制度の低下を、観測データのうち変数<span class="arithmatex">\(j\)</span>をパーミュテーションすることで代替的に観測することで、推定誤差の上昇量によって変数<span class="arithmatex">\(j\)</span>の重要度を捉える方法である。しかし、一般化ランダムフォレストでの関心は、パラメータ<span class="arithmatex">\(\theta(x)\)</span>に対する変数重要度となるため、従来のランダムフォレストの枠組みで推定することが難しい。ここでは、<a href="https://arxiv.org/pdf/2308.03369">Benard and Josse (2023)</a> による条件付き因果効果に対する変数重要度の推定法を紹介する。</p>
<p>先ほどの条件付き因果効果の例において、<span class="arithmatex">\(\tau(x)\)</span>に対する変数重要度について考える。いま、条件付き因果効果が観測変数<span class="arithmatex">\(X = (X_1,...,X_p)\)</span>のうち、<span class="arithmatex">\(\mathcal{S} \subset \{1,2,...,p\}\)</span>に依存する場合を考える。結果変数<span class="arithmatex">\(Y\)</span>は、
$$
    Y = \mu(X) + T \cdot \tau(X^{\mathcal{S}}) + \varepsilon
$$
を満たすとし、前に指摘したモデルの仮定が成り立つと仮定する。このとき、条件付き因果効果に対する 変数<span class="arithmatex">\(j\)</span>の変数重要度 <span class="arithmatex">\(I^{(j)}\)</span> を以下のように定義する。
$$
    I^{(j)} = \frac{Var[\tau(X^{\mathcal{S}})] - Var[\tau(X^{\mathcal{S}})\mid X^{(-j)}]}{Var[\tau(X^{\mathcal{S}})]} = \frac{E[(\tau(X^{\mathcal{S}}) - E[\tau(X^{\mathcal{S}})\mid X^{(-j)}])^{2}]}{Var[\tau(X^{\mathcal{S}})]}
$$
<span class="arithmatex">\(I^{(j)}\)</span> は <span class="arithmatex">\(j \not\in \mathcal{S}\)</span>の場合には、<span class="arithmatex">\(I^{(j)}=0\)</span> となり、変数<span class="arithmatex">\(X_{j}\)</span> が<span class="arithmatex">\(\tau(X^{\mathcal{S}})\)</span> に対して大きな影響を持つならば、<span class="arithmatex">\(I^{(j)}\)</span>が大きくなることがわかる。ここで、定義から <span class="arithmatex">\(j \in \mathcal{S}\)</span>ならば、<span class="arithmatex">\(0 &lt; I^{(j)} \leq 1\)</span>である。</p>
<p>ここで、<span class="arithmatex">\(\tau(X^{\mathcal{S}})\)</span>は Local centering による推定方程式の解であるから、以下の方程式を満たす。
$$
    \tau(X^{\mathcal{S}}) \times \operatorname{Var}[(T-e(X))\mid X^{\mathcal{S}}] -\operatorname{Cov}[T-e(X),Y-m(X) \mid X^{\mathcal{S}}] = 0
$$
一方で、<span class="arithmatex">\(X^{(-j)} = (X_1, ..., X_{j-1}, X_{j+1},...,X_{p})\)</span>を条件づけた場合には、以下の関係式が成り立つ。
$$
    \begin{align}
    &amp;E[\tau(X^{\mathcal{S}})\mid X^{(-j)}] \times \operatorname{Var}[(T-e(X))\mid X^{(-j)}] - \operatorname{Cov}[T-e(X),Y-m(X) \mid X^{(-j)}]\
    &amp; \phantom{====} + \operatorname{Cov}[\tau(X^{\mathcal{S}}),e(X)(1-e(X)) \mid X^{(-j)}] = 0
    \end{align}
$$
この結果から、<span class="arithmatex">\(E[\tau(X^{\mathcal{S}})\mid X^{(-j)}]\)</span>に対して、以下の関係式が得られる。
$$
    E[\tau(X^{\mathcal{S}})\mid X^{(-j)}] = \frac{\operatorname{Cov}[T-e(X),Y-m(X) \mid X^{(-j)}]}{\operatorname{Var}[(T-e(X))\mid X^{(-j)}]} - \frac{\operatorname{Cov}[\tau(X^{\mathcal{S}}),e(X)(1-e(X)) \mid X^{(-j)}]}{\operatorname{Var}[(T-e(X))\mid X^{(-j)}]}
$$
この第1項は、<span class="arithmatex">\(\tilde{Y}_{i} = Y_{i} - m(X_{i})\)</span> および <span class="arithmatex">\(\tilde{T}_{i} = T_{i} - e(X_{i})\)</span> と定義した時に、<span class="arithmatex">\((X_{i}^{(-j)}, \tilde{T}_{i}, \tilde{Y}_{i})\)</span> (<span class="arithmatex">\(i=1,2,...,n\)</span>) に対して、causal forest を学習させて得られる条件付き因果効果の推定値である。第2項は <span class="arithmatex">\(X_{j}\)</span> を条件から外すことで生じるバイアスに対応する。</p>
<p>よって、このバイアス項に対応する推定量が得られれば、causal forest の推定量に対する変数重要度を推定することができる。<span class="arithmatex">\((X_{i}, \tilde{T}_{i}, \tilde{Y}_{i}), i=1,2,...,n\)</span> 上で学習された causal forest の点 <span class="arithmatex">\(x\)</span> に対する推定量を<span class="arithmatex">\(\hat{\tau}(x)\)</span> とする。一方で、特徴量の集合から特徴量<span class="arithmatex">\(X_{j}\)</span>を除いたデータ <span class="arithmatex">\((X_{i}^{(-j)}, \tilde{T}_{i}, \tilde{Y}_{i}), i=1,2,...,n\)</span> 上で学習された causal forest が生成するフォレストの重みを <span class="arithmatex">\(\alpha'_{i}(x^{(-j)})\)</span>とし、条件付き因果効果の推定量を<span class="arithmatex">\(\hat{\tau}^{(-j)}(x)\)</span> とする。このとき、<span class="arithmatex">\(E[\tau(X^{\mathcal{S}})\mid X^{(-j)}]\)</span>に対するバイアス修正済みの条件付き因果効果の推定量を、
$$
    \hat{\theta}^{(-j)}(x) = \hat{\tau}^{(-j)}(x) - \frac{\sum_{i=1}^{n}\alpha'<em>{i}(x^{(-j)})\tilde{T</em>{i}}^{2}\hat{\tau}(X_i) - \bar{T}<em>{\alpha'}^{2}\bar{\tau}</em>{\alpha'}}{\bar{T}<em>{\alpha'}^{2} - (\bar{T}</em>{\alpha'})^{2}}
$$
で定義する。ただし、<span class="arithmatex">\(\bar{T}_{\alpha'}^{2} = \sum_{i=1}^{n}\alpha'_{i}(x^{(-j)})\tilde{T}_{i}^{2}\)</span>, <span class="arithmatex">\(\bar{T}_{\alpha'} = \sum_{i=1}^{n}\alpha'_{i}(x^{(-j)})\tilde{T}_{i}\)</span> および、<span class="arithmatex">\(\bar{\tau}_{\alpha'} = \sum_{i=1}^{n}\alpha'_{i}(x^{(-j)}) \hat\tau(X_i)\)</span> である。それぞれ、<span class="arithmatex">\(X^{(-j)}\)</span>を除いた causal forest によって得られた重みを用いて構成されている。</p>
<p>変数重要度は、得られた推定量で<span class="arithmatex">\(I^{(j)}\)</span>の各項を置き換えることで得られる。いま、観測データの組 <span class="arithmatex">\(D= \{(X_i, T_i, Y_i)\}_{i=1}^{n}\)</span> が従う分布<span class="arithmatex">\(P\)</span> から、新たにサイズ<span class="arithmatex">\(n\)</span>の観測データとは独立に i.i.d. なサンプル <span class="arithmatex">\(D' = \{(X'_i, T'_i, Y'_i)\}_{i=1}^{n}\)</span> が得られたする。変数重要度の推定量は、<span class="arithmatex">\(D'\)</span>に基づいて以下のように構成する。
$$
    \hat{I}^{(j)} = \frac{\sum_{i=1}^{n}[\hat\tau(X'<em>i) - \hat{\theta}^{(-j)}(X'_i)]^{2}}{\sum</em>{i=1}^{n}[\hat\tau(X'<em>i) - \bar{\tau}]} - \hat{I}^{(0)}
$$
ここで、<span class="arithmatex">\(I^{(0)}\)</span> は、<span class="arithmatex">\(\hat{I}^{(j)}\)</span> に対するバイアスの修正項である。一般的に、ランダムフォレストでは木の構成にランダムネスが存在するため、変数重要度の推定ではその影響を取り除いている。
$$
    \hat{I}^{(j)} = \frac{\sum</em>{i=1}^{n}[\hat\tau(X'<em>i) - \hat{\theta}^{(-j)}(X'_i)]^{2}}{\sum</em>{i=1}^{n}[\hat\tau(X'_i) - \bar{\tau}]} - \hat{I}^{(0)}
$$</p>
<p><a href="https://arxiv.org/pdf/2308.03369">Benard and Josse (2023)</a> の結果は、一般化ランダムフォレストによる推定量から、変数重要度を計算する場合には、従来の回帰の文脈では生じないバイアスが生じる可能性がある。バイアスの生じ方は、推定方程式の形状に依存するため、例えば生存データの文脈では再解析を行う必要がある。また、変数重要度の推定精度は、サンプルサイズに依存する。<a href="https://arxiv.org/pdf/2308.03369">Benard and Josse (2023)</a> では、 <span class="arithmatex">\(n=3000\)</span> を用いてシミュレーションを行っている。変数重要度は結果の解釈においては有効であるが、その一方でランダムフォレストの推定精度に依存することには注意が必要である。</p>
<p>また、変数重要度の推定で注意が必要なのは、<span class="arithmatex">\(X_{j}\)</span>の変数重要度を評価する際に、相関の強い変数<span class="arithmatex">\(X_{k}\)</span>が存在すると、<span class="arithmatex">\(X_{j}\)</span>を取り除いても代替する情報を<span class="arithmatex">\(X_{k}\)</span>が与えるので、変数重要度が小さくなるという問題がある。これに対する対処としては、ドメイン知識などで変数をグループ化することで、変数単一の重要度ではなく、グループ単位の変数重要度を評価するなどの対処を行うことが望ましい。</p>
<p>一般化ランダムフォレストに対する変数重要度の今後の方針として、特に重要なアプリケーションの1つである生存時間解析への拡張は今後行われていくと予想される。また近年、変数重要度に関する研究は、セミパラメトリック推論からの理論解析、Model-X knockoffs などの概念、Shapley Value などの分野で発展している。一般化ランダムフォレストの推定量を、これらの枠組みの中で解釈する取り組みが現在も行われている。</p>
<h2 id="_15">一般化ランダムフォレストの応用事例</h2>
<h3 id="_16">医療分野</h3>
<ul>
<li>Suicide Risk Among Hospitalized Versus Discharged Deliberate Self-Harm Patients: Generalized Random Forest Analysis Using a Large Claims Data Set</li>
</ul>
<p>自傷行為を行って救急部に受診した患者は、自殺リスクが非常に高いため治療（入院）を行うことが推奨されている。この論文では、治療が自殺リスクに与える影響が明らかにすることを目的としている。このデータでは、57,312人の患者データに対して、causal forestを当てはめることで、交絡要因を取り除いて治療の因果効果を推定している。また、E-valueを用いて未観測交絡に対する影響を評価し、結果に対しては慎重であるべきであることを付記している。</p>
<h3 id="_17">経済分野</h3>
<ul>
<li>Heterogeneous effects of Medicaid coverage on cardiovascular risk factors: secondary analysis of randomized controlled trial</li>
</ul>
<p>この論文では、オレゴン健康保険実験のデータを用いて、Medicaid（低所得者向け公的医療保険）の心血管リスク因子（主に収縮期血圧とヘモグロビンA1c）の改善効果における個人差（ヘテロジニアス効果）を機械学習の手法で評価している。12,134人の参加者を対象として、抽選でMedicaid申請可能か（実際に加入したかどうかは、別の変数）どうかを振り分けており、ランダム化比較試験となっている。収縮機血圧と、ヘモグロビンA1cを目的変数として、Medicate加入によりこれらの健康指標が改善するかを確かめている。共変量としては、年齢、性別、人種・民族、教育水準、既往症（高血圧、糖尿病など）および過去の医療費（総医療費や救急部利用費用）が観測されている。このモデルを、抽選を操作変数<span class="arithmatex">\(Z\)</span>、Medicaidへの加入を処置 <span class="arithmatex">\(T\)</span>として、instrumental variable causal forest を当てはめて、条件付き局所平均処置効果（Conditional LATE）効果を推定している。この分析の結果では、全体としての効果は小さいが、一部のサブグループで改善が見られることが、CLATEの推定によって明らかになったと結果を報告している。</p>
<h3 id="_18">金融分野</h3>
<ul>
<li>Predicting Value at Risk for Cryptocurrencies With Generalized Random Forests</li>
</ul>
<p>この論文は、暗号資産（クリプトカレンシー）のリスク管理において重要な指標であるVaR（Value at Risk）の予測に焦点を当てています。従来の手法（例えば、分位点回帰、GARCH系モデル、CAViaRモデルなど）は、暗号資産特有の極端な変動性やスパイクに対応するには柔軟性が不足していることが指摘されている。そこで、著者ら Generalized Random Forests (GRF)による分位点推定を活用している。その結果、局所的な大きな変動に対応は、GRFによる条件付き分位点の推定が優れていることを報告している。この結果は、<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/jtsa.12731">Shiraishi, Nakamura and Shibuki (2024)</a>によって報告されたGRFによる時系列分位点推定の一致性の論文でも同様の結果が報告されている。</p>
<h3 id="_19">マーケティング分野</h3>
<ul>
<li>GCF: Generalized Causal Forest for Heterogeneous Treatment Effects Estimation in Online Marketplace</li>
</ul>
<p>この論文では、オンラインマーケットプレイス（特にライドシェアリングサービス）の文脈において、連続的な治療（例：価格や割引）の異質的な介入効果（HTE）を非パラメトリックに推定するための手法として、Generalized Causal Forest を用いることを提案している。ライドシェアリングなどのプラットフォームでは、需要と供給のバランスを取るために、各地域・時間帯で価格や割引といった連続的な介入が重要である。例えば、ある地域で割引を実施することで利用者のリクエストを促進し、供給過多の状況を解消しようとする一方、過度な割引は混雑やサービス品質の低下を招く可能性があるため、データから適切な割引率などを推定したいという需要がある。特に、混雑時・平常時や、時間帯で提供するディスカウントを使い分けたいという需要も存在する。そこで、<a href="https://arxiv.org/abs/2004.03036">Colangelo and Lee (2020)</a> によって提案された連続処置変数に対する用量反応関数の DML推定を GRFに組み合わせることで推定を実現している。実際のデータで、学習したアルゴリズムと別のアルゴリズムの比較も行なっており、その結果として提案した手法による方法がA/Bテストの結果、改善をもたらすことを報告している。</p>
<h2 id="_20">因果効果推定のいくつかの重要なトピックス</h2>
<p>一般化ランダムフォレストを用いた因果効果の推定においては、モデルの性質や推定の精度に影響を与える重要な課題が複数存在する。ここでは、現在の因果推論研究において特に重要な「共変量の外れ値と重複の仮定」、「結果変数の外れ値とロバスト推定」、および「共形予測」について論じる。</p>
<h3 id="overlap">因果推論における特徴量空間の外れ値とoverlap仮定</h3>
<p>因果効果の推定において最も重要な仮定の一つが "Overlap Assumption" または "Common Support Assumption" である。この仮定は、任意の共変量 <span class="arithmatex">\(X = x\)</span> に対して、処置群と対照群の両方に十分なサンプルが存在することを意味している。
$$
    0 &lt; \Pr(T=1 \mid X=x) &lt; 1 \quad (x \in \mathcal{X})
$$
この仮定が満たされない場合、つまり特定の <span class="arithmatex">\(x\)</span> の値に対して処置群または対照群のサンプルが極端に少ないもしくは存在しない場合、その領域での因果効果の推定は外挿に依存することになり、不安定かつ信頼性の低いものとなる。特に、機械学習の文脈における性能比較などでこの仮定が満たされないベンチマークデータなどで比較が行われているケースが存在する。特に、一般化ランダムフォレストによる推定においても、共変量空間の端や<span class="arithmatex">\(\mathcal{X}\)</span>の外れ値領域（X-outlier）は問題となる。</p>
<h3 id="_21">因果推論における結果変数の外れ値の問題</h3>
<p>機械学習を用いた因果推論においては、多くの場合<span class="arithmatex">\(Y\)</span>の分布に対して意識されないことが多い。特に、一般化ランダムフォレストで因果推論を行うケースにおいては、観測データをそのまま訓練データとして用いている場合が多い。一方で、<span class="arithmatex">\(Y\)</span>の分布の一部のデータが大きな値を取るなどの分布の形状は、回帰木の学習時の2乗損失の最小化を通して、分割に影響を与える。そのため、<span class="arithmatex">\(Y\)</span>のスケーリングは機械学習を用いた因果推論においては本質的な問題となる。医療データにおける医療費の分布や、マーケティングのデータにおける購入頻度の分布などは、ロングテールな分布であり、これらを直接機械学習に入力する場合には問題が生じる。そのためには、結果変数<span class="arithmatex">\(Y\)</span>に対して変数変換を施して、機械学習に適した分布形状に変換し、損失関数が外れ値の影響を大きく受けすぎないような工夫をすることが重要となる。例えば、Bayesian additive regression trees (Chipman et al.,2010) においては、最大最小スケーリングを用いることを推奨している。ランダムフォレストにおいても、遺伝子データの解析などではBox-Cox変換が利用されることもあるため、一般化ランダムフォレストにおいても同様に変数変換については今後考えていく必要がある。または、<span class="arithmatex">\(Y\)</span> の分布に関する問題については、この他の対処として損失関数をパラメータの2乗損失から、Huber損失や、擬似Huber損失などを採用することにより、より頑健な推定を行うという方針も考えられる。</p>
<h3 id="conformal-predictions">Conformal Predictions</h3>
<p>因果効果の推定において、点推定値だけでなく予測区間を提供することは、不確実性の評価と意思決定の観点から極めて重要である。特に、漸近論に基づかず予測区間が求めるカバレッジを達成することに関心が高まっている。
<a href="https://dl.acm.org/doi/10.5555/2074094.2074112">Gammerman et al.,1998</a>によって提案されたConformal Prediction (CP) は、モデルに依存しない（model-agnostic）予測区間を構築するための手法であり、限られた仮定のもとで有限サンプルにおける正確な被覆確率（coverage probability）の下限を保証することができる。<a href="https://link.springer.com/chapter/10.1007/3-540-36755-1_29">Papadopoulos et al.,2002</a> によって、CPの効率的な計算方法である Split Conformal Prediction が導入され、その後 <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2012.751873">Lei et al.,(2013)</a>、 <a href="https://academic.oup.com/jrsssb/article-abstract/76/1/71/7075937?redirectedFrom=fulltext">Lei and Wasserman (2014)</a> および <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1307116">Lei et al (2018)</a> によって体系的に整理された。Conformal prediction についての理論的な解説は <a href="https://arxiv.org/abs/2411.11824">Angelopoulos et al.(2024)</a> が詳しい。その後、<a href="https://papers.neurips.cc/paper_files/paper/2019/hash/8fb21ee7a2207526da55a679f0332de2-Abstract.html">Tibshirani et al.,(2019)</a> によって共変量シフトのもとでのConformal prediction の理論が提案されている。<a href="https://arxiv.org/abs/2006.06138">Lei and Candes (2021)</a> は、共変量シフトのもとでのCPの枠組みを因果推論へと拡張し、Causal Conformal Inference を提案している。また、<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/94ab02a30b0e4a692a42ccd0b4c55399-Paper-Conference.pdf">Alaa et al.,(2023)</a> は、2値の処置における Meta-learner に対するCPの枠組みを提案している。<a href="https://arxiv.org/abs/2407.03094">Schröder et al.,(2024)</a> では、連続処置に対するCPの枠組みを提案している。平均処置効果に着目したConformal Predictionとしては、<a href="https://arxiv.org/abs/2401.01977">Wang et al.,(2024)</a> がある。</p>
<p>因果推論におけるCPの応用は、特に政策決定や臨床意思決定など、不確実性の適切な評価が重要な文脈で価値が高い。例えば、医療介入の効果に関する予測区間を提供することで、患者ごとの治療の不確実性を定量化し、より情報に基づいた意思決定が可能になるなどの利点がある。現在の一般化ランダムフォレストの推定は漸近論に基づいた結果となっているが、今後は conformal prediction等を通した、有限標本下でのカバレッジ保証を行なっていくことも求められている。</p>
<h2 id="_22">まとめ</h2>
<p>本レビュー論文では、一般化ランダムフォレストとその因果推論への応用について体系的に概観した。Breiman (2001) によって提案された当初のランダムフォレストは、その予測精度と汎用性の高さから多くの分野で広く利用されてきたが、理論的な解析は困難とされてきた。2010年代以降、Lin and Jean (2006) のカーネル重み付け推定量としての解釈を基礎として、Biau (2012) や Scornet et al. (2015) らによる一致性の証明、Wager and Athey (2019) による漸近正規性の証明と漸近分散の推定方法の確立など、理論的な基盤が徐々に整備されてきた。</p>
<p>Athey, Tibshirani, and Wager (2020) によって提案された一般化ランダムフォレストは、ランダムフォレストをカーネル重み付け関数の推定法として捉え直し、局所推定方程式の枠組みに拡張することで、条件付き平均以外の多様なパラメータの推定を可能にした画期的な手法である。本手法の最大の利点は、条件付き因果効果、分位点推定、操作変数法による因果パラメータの推定など、従来のランダムフォレストでは困難だった統計量の柔軟な推定を理論的保証付きで行える点にある。</p>
<p>特に因果推論の分野では、Wager and Athey (2019) によるcausal forestの提案以降、様々な拡張が行われている。Local centering (Nie and Wager, 2021) によるアプローチは推定の効率性を高め、Local Linear Forest (Friedberg et al., 2020) は境界付近での推定精度の問題を改善し、推定の局所的な変動への適応力を向上させた。これらの技術的進展は、医療、経済、金融、マーケティングなど様々な分野での実証研究を可能にしている。</p>
<p>しかしながら、一般化ランダムフォレストを用いた推定にはいくつかの重要な課題も残されている。特徴空間における重複（overlap）の問題、結果変数の外れ値の扱い、サンプルサイズの小さい状況での適用限界などは、実践上の重要な課題である。また、理論的には共変量の次元が大きくなるにつれて収束のレートが悪化するという「次元の呪い」の問題も完全には解決されていない。</p>
<p>さらに、推定結果の解釈においても課題が残されている。Benard and Josse (2023) による条件付き因果効果に対する変数重要度の推定法の提案など進展はあるものの、一般化ランダムフォレストの枠組みにおける変数重要度の理論は発展途上である。また、予測の不確実性の定量化において、有限標本下でのカバレッジを保証するConformal Predictionの枠組みとの統合も今後の重要な研究方向である。</p>
<p>近年では、因果推論とランダムフォレストの両方の分野で理論的な発展が著しく、一般化ランダムフォレストはその二つの領域を橋渡しする重要な手法となっている。今後は、高次元データへの適応、モデル解釈性の向上、有限標本での性能保証、計算効率の改善など多くの方向性が考えられる。特に、複雑な因果構造を持つデータに対する拡張や、時系列データや空間データなど特殊な構造を持つデータへの応用は重要な研究課題である。</p>
<p>一般化ランダムフォレストは、理論と実践の両面で重要な貢献をもたらした手法であり、統計的機械学習と因果推論の融合を促進している。今後もデータ駆動型の意思決定が重要性を増す様々な分野において、その応用範囲はさらに拡大していくと考えられる。理論的基盤の深化と実践的な適用範囲の拡大という両面からの発展を通じて、一般化ランダムフォレストは因果推論の方法論的ツールキットの中で今後も重要な役割を果たすと考えられる。本レビュー論文が理論研究者と実務家の双方にとって一般化ランダムフォレストについて有益な橋渡しとなることを願っています。</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  ページトップへ戻る
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 - 2025 Tomoshige Nakamura
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.expand", "navigation.path", "navigation.indexes", "navigation.sections", "navigation.top", "navigation.tracking", "toc.follow"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "\u30b3\u30d4\u30fc\u3057\u307e\u3057\u305f", "clipboard.copy": "\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u3078\u30b3\u30d4\u30fc", "search.result.more.one": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3082\u30461\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.more.other": "\u3053\u306e\u30da\u30fc\u30b8\u5185\u306b\u3042\u3068#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.none": "\u4f55\u3082\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f", "search.result.one": "1\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.other": "#\u4ef6\u898b\u3064\u304b\u308a\u307e\u3057\u305f", "search.result.placeholder": "\u691c\u7d22\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044", "search.result.term.missing": "\u691c\u7d22\u306b\u542b\u307e\u308c\u306a\u3044", "select.version": "\u30d0\u30fc\u30b8\u30e7\u30f3\u5207\u308a\u66ff\u3048"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.60a45f97.min.js"></script>
      
        <script src="../../../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>