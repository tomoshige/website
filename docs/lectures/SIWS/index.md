# 数理情報リテラシー

## Introduction

この授業は、[Statistical Inference via Data Science: A ModernDive into R and the Tidyverse](https://moderndive.com/v2/) (Chester Ismay, Albert Y. Kim, and Arturo Valdivia, 2025) にインスピレーション受けて、Python と Google Colaboratory を用いた講義として再開発したものです。この授業は、線形代数、微積分、プログラミング/コーディングの経験を前提とせず、とにかく手を動かしながらデータサイエンスで行われる1つ1つの事柄について理解を深めていきます。以下の図が、この授業の大まかな流れです。

![ModernDive フローチャート](https://raw.githubusercontent.com/moderndive/ModernDive_book/master/images/flowcharts/flowchart/flowchart.002.png)

まず、第2回でデータの基礎から始め、PythonとGoogle Colabの違いを学び、コーディングを開始し、最初のPythonライブラリをインストールして読み込み、2023年のニューヨーク市空港からの国内出発便データを探索します。その後、本書の以下の3つの部分に進みます：

1. **Pandasとデータサイエンス**：Pandasなどのデータサイエンスライブラリを使ってツールボックスを構築します。特に：
   + 第3回：Matplotlib/Seabornを使ったデータの可視化
   + 第4回：Pandasを使ったデータの整形
   + 第5回：「整形された」データの概念と標準化されたデータ入出力形式の学習

2. **統計/データモデリング**：データサイエンスツールとStatsmodelsを使って、最初のデータモデルを作成します。特に：
   + 第6回：1つの説明変数だけを持つ基本的な回帰モデルの探索
   + 第7回：複数の説明変数を持つ重回帰モデルの検討

3. **統計的推論**：データサイエンスツールを用いて、統計的推論を理解します。
   + 第8回：サンプリングと推定量のばらつき
   + 第9-10回：ブートストラップ法を用いた信頼区間の構築
   + 第10-11回：ブートストラップ法による仮説検定
   + 第12-13回：回帰分析における信頼区間と仮説検定の解釈

最後に、第14回では「[データで意味のあるストーリーを語る](https://www.thinkwithgoogle.com/marketing-resources/data-measurement/tell-meaningful-stories-with-data/)」とはどういう意味かを事例研究を通して説明します。

### この授業を通して学んで欲しいこと

本書の終わりまでに、以下のことを学ぶことを目標にします

1. データサイエンスのためのPandasなどの*ライブラリ*を使う
2. *線形回帰*として知られる方法を用いて、データに*モデル*を当てはめる
3. *サンプリング*、*信頼区間*、*仮説検定*を用いた*統計的推論*を行う
4. これらのツールを使って*データでストーリーを語る*

特にこの授業は、データの可視化に重点をおきました。また、統計学的な理解を数学の公式の使用を最小限にして伝えるように努力をしていますが、どうしても一部は数式が用いられています。ただし、多くの概念については、データの可視化とコンピュータシミュレーションを用いて理解できるように工夫しています。

また、この本ではPython + Google Colabを用いているため、コードの書き方が分からず困ることもあるでしょう。その場合は、ChatGPT や Claude、または Gemini (Google AI Studio)等を用いて問題を解決することができます。ぜひ、分からないところは「**まずは生成AIに尋ねる**」という姿勢を持って、AIと一緒に学習してください。


### データサイエンスパイプライン

統計とは数字の集まりではありませんし、データ分析は野球の打率などの事象を把握するための数字を提供するにとどまらず、すべての科学的な営みにおいて重要な役割を果たします。例えば、この学部でデータ分析に携われば、「統計的に有意」とか「p < 0.05」のような数字、「データ分析から、チョコレートが体に良いことが示された」といった話題を目にすることになります。みなさんはデータサイエンス学部で学ぶ意味は、目の前にある主張が信頼できるものかどうか、あるいは信頼には値しないものかということについて、理由をもって説明できるようになることです。例えば、データ分析の中には、様々な要素技術があり、これらが一体となってデータ分析を支えています。

- データ収集
- データ整形
- データ可視化
- 統計モデリング
- 推論
- 相関と回帰
- 結果の解釈
- データコミュニケーション/ストーリーテリング

これらの要素技術は、Garrett GrolemundとHadley Wickhamが以前「データサイエンスパイプライン」と呼ぶフローにまとめらています。

![データ/サイエンスパイプライン](https://raw.githubusercontent.com/moderndive/ModernDive_book/master/images/r4ds/data_science_pipeline.png)

まずは、サイクルのグレーの**理解**部分をデータ可視化から深め、次に整形されたデータとデータ整形について議論し、最後にモデルの結果を解釈し議論する**コミュニケーション**について話します。これらのステップはあらゆる統計分析に不可欠です。

### 授業の評価
この授業の評価は、授業終了後のレポート課題（100％）によって行います。授業の内容を十分に理解して、レポート課題に取り組むようにしてください。