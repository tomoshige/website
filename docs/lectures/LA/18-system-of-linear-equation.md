# 講義17: 連立1次方程式と2つの説明変数がある場合の線形回帰モデル

## 1. 講義概要
- **講義時間:** 60分　/　**演習時間:** 30分
- **目的:**  
  2つの説明変数（独立変数）を持つ線形回帰モデルの数学的表現（ベクトル・行列による表現）と、最小二乗法によるパラメータ推定（正規方程式）の考え方を理解する。これにより、平面（3次元空間における回帰面）の推定とその解釈ができるようになる。
- **今日の目標:**  
  ・2つの説明変数がある場合の線形回帰モデルの定式化と、そのベクトル・行列表示を明確にする。  
  ・最小二乗法に基づく正規方程式の導出と、実際に数値例を通してパラメータ推定の過程を理解する。  
  ・Google Colabを用いて、実データにモデルを当てはめ、推定結果を解釈できるようになる。

## 2. 理論的背景と内容の説明
### 2.1 定義・基本概念
- **統計モデルと線形回帰モデル:**  
  統計モデルは、観測データに基づく現象の関係性を数理的に表現する枠組みです。  
  単回帰モデルでは1つの説明変数を用いますが、本講義では2つの説明変数 \( x_1 \) と \( x_2 \) を用いるため、モデルは  
  $$
  y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon
  $$
  と表され、これは3次元空間内の「平面」を推定する問題と捉えられます。
- **説明変数と反応変数:**  
  説明変数（独立変数）は \( x_1, x_2 \)、反応変数（従属変数）は \( y \) です。  
  切片 \( \beta_0 \) は、説明変数がゼロのときの \( y \) の予測値、回帰係数 \( \beta_1, \beta_2 \) は、それぞれの説明変数が \( y \) に与える影響を示します。
- **ベクトルと行列による表現:**  
  \( n \) 個の観測があるとき、  
  - 説明変数行列 \( X \) は、  
    $$
    X = \begin{pmatrix}
    1 & x_{11} & x_{12} \\
    1 & x_{21} & x_{22} \\
    \vdots & \vdots & \vdots \\
    1 & x_{n1} & x_{n2}
    \end{pmatrix} \in \mathbb{R}^{n \times 3},
    $$
  - 反応変数ベクトル \( \mathbf{y} \) は、  
    $$
    \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix} \in \mathbb{R}^{n},
    $$
  - パラメータベクトルは、  
    $$
    \boldsymbol{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{pmatrix}.
    $$
  モデル全体は  
  $$
  \mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}
  $$
  と表現されます。

### 2.2 定理・命題

- **定理1: 最小二乗推定解の一意性と正則性**  
  観測データを表す設計行列 \( X \) の列が線形独立（すなわち、\( X \) の列空間の次元が説明変数の数と等しい）であるならば、  
  - 行列 \( X^T X \) は正則（逆行列が存在）となる。  
  - このとき、正規方程式  
    $$
    X^T X\,\hat{\boldsymbol{\beta}} = X^T \mathbf{y}
    $$
    は一意な解 \( \hat{\boldsymbol{\beta}} = (X^T X)^{-1}X^T \mathbf{y} \) を持つ。  
  ※ この定理は、最小二乗法の解が唯一であるための必要十分条件として重要であり、実際のデータ解析では設計行列のランクが十分であるかを確認することが求められる。

- **定理2: 残差の直交性**  
  最小二乗推定において、推定されたパラメータ \( \hat{\boldsymbol{\beta}} \) により得られる予測値 \( \hat{\mathbf{y}} = X\hat{\boldsymbol{\beta}} \) と、残差ベクトル  
  $$
  \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}}
  $$
  は、設計行列 \( X \) の各列に対して直交する（内積が0になる）。すなわち、  
  $$
  X^T \mathbf{r} = \mathbf{0}.
  $$
  これは、\( \hat{\mathbf{y}} \) が \( \mathbf{y} \) の \( X \) の列空間への正射影であることを意味し、残差がその直交補空間に属することを示す。

- **命題: 目的関数の凸性とヘッセ行列**  
  目的関数  
  $$
  S(\boldsymbol{\beta}) = \|\mathbf{y} - X\boldsymbol{\beta}\|^2 = (\mathbf{y} - X\boldsymbol{\beta})^T (\mathbf{y} - X\boldsymbol{\beta})
  $$
  は、二次形式で表されるため、\( \boldsymbol{\beta} \) に関して凸関数となる。  
  そのヘッセ行列は  
  $$
  H = 2X^T X,
  $$
  となり、\( X \) の列が線形独立ならば \( H \) は正定値となり、局所最小値は一意な大域的最小値であることが保証される。

---

### 2.3 数式・証明の詳細

- **目的関数の設定と微分**  
  最小二乗法では、残差の二乗和を最小にするパラメータ \( \boldsymbol{\beta} \) を求めるため、以下の目的関数を定義します。  
  $$
  S(\boldsymbol{\beta}) = \|\mathbf{y} - X\boldsymbol{\beta}\|^2 = (\mathbf{y} - X\boldsymbol{\beta})^T (\mathbf{y} - X\boldsymbol{\beta}).
  $$
  この関数は \( \boldsymbol{\beta} \) の二次形式であり、凸関数です。  
  \( S(\boldsymbol{\beta}) \) を \( \boldsymbol{\beta} \) で偏微分すると、
  $$
  \frac{\partial S}{\partial \boldsymbol{\beta}} = -2X^T(\mathbf{y} - X\boldsymbol{\beta}).
  $$
  これをゼロに等しくする条件
  $$
  -2X^T(\mathbf{y} - X\boldsymbol{\beta}) = \mathbf{0}
  $$
  を考えると、正規方程式
  $$
  X^T X\,\boldsymbol{\beta} = X^T \mathbf{y}
  $$
  が得られます。

- **正規方程式の解法**  
  もし \( X^T X \) が正則であれば、両辺に \( (X^T X)^{-1} \) を作用させることで、  
  $$
  \hat{\boldsymbol{\beta}} = (X^T X)^{-1}X^T \mathbf{y}
  $$
  が唯一の最小二乗解として得られます。  
  この解は、目的関数 \( S(\boldsymbol{\beta}) \) のグローバルミニマムに対応し、推定された \( \hat{\mathbf{y}} = X\hat{\boldsymbol{\beta}} \) は、観測値 \( \mathbf{y} \) の \( X \) の列空間への直交射影となります。

- **幾何学的解釈**  
  ここで、\( \hat{\mathbf{y}} \) は \( X \) の列空間に属し、残差 \( \mathbf{r} = \mathbf{y} - \hat{\mathbf{y}} \) はその直交補空間に属することが先述の定理2により保証されます。  
  つまり、  
  $$
  X^T(\mathbf{y} - \hat{\mathbf{y}}) = \mathbf{0},
  $$
  であり、これは \( \hat{\mathbf{y}} \) が観測データ \( \mathbf{y} \) に最も近い（ユークリッド距離が最小）点であることを意味します。

- **凸最適化の観点**  
  目的関数 \( S(\boldsymbol{\beta}) \) のヘッセ行列は \( 2X^T X \) であり、これは \( X \) の列が線形独立なら正定値です。  
  このため、最小化問題は凸最適化問題となり、局所最小解が大域的最小解であることが保証されます。  
  これが、正規方程式を解くことで得られる \( \hat{\boldsymbol{\beta}} \) が、実際に「残差の二乗和」を最小にするパラメータである理由です。

- **平面の推定:**  
  2つの説明変数の場合、得られる回帰モデルは \( \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 \) であり、これは3次元空間における平面としてデータにフィットします。

## 3. 扱う内容の実例（GPT, Colab等は使用せず）
- **実例1: 行列の乗算を用いた予測値の生成**  
  例として、5人の学生のデータを考えます。  
  各学生のデータ（例：勉強時間 \( x_1 \) と睡眠時間 \( x_2 \) ）と、テスト得点 \( y \) を以下のように与えたとします。  
  | 学生 | 勉強時間 (\( x_1 \)) | 睡眠時間 (\( x_2 \)) | 得点 (\( y \)) |
  |------|--------------------|-------------------|-------------|
  | 1    | 2                  | 7                 | 65          |
  | 2    | 3                  | 6                 | 70          |
  | 3    | 4                  | 8                 | 75          |
  | 4    | 5                  | 5                 | 80          |
  | 5    | 3                  | 7                 | 68          |
  
  説明変数行列 \( X \) は  
  $$
  X = \begin{pmatrix}
  1 & 2 & 7 \\
  1 & 3 & 6 \\
  1 & 4 & 8 \\
  1 & 5 & 5 \\
  1 & 3 & 7
  \end{pmatrix},
  $$
  パラメータベクトル \(\boldsymbol{\beta} = (\beta_0, \beta_1, \beta_2)^T\) として、  
  モデル \( \mathbf{y} = X\boldsymbol{\beta} \) により各学生の予測得点が計算される仕組みを、行列の乗算で示します。

- **実例2: 正規方程式による最小二乗推定の数値例**  
  上記データを用いて、正規方程式  
  $$
  X^T X\,\hat{\boldsymbol{\beta}} = X^T \mathbf{y}
  $$
  を構築し、\( X^T X \) と \( X^T \mathbf{y} \) を計算する過程をホワイトボードで示します。  
  具体的な数値計算を通して、どのようにして最小二乗解が得られるか、その手順と意味を確認します。

- **実例3: 実世界のデータ例—広告費と商品の売上**  
  実際のビジネス例として、ある企業の広告費を2種類（オンライン広告費 \( x_1 \) とテレビ広告費 \( x_2 \) ）と、それに伴う商品の売上 \( y \) の関係を解析するケースを考えます。  
  仮想データとして以下のような表を示します。  
  | 企業 | オンライン広告費 (\( x_1 \)) | テレビ広告費 (\( x_2 \)) | 売上 (\( y \)) |
  |------|---------------------------|------------------------|-------------|
  | A    | 10                        | 20                     | 150         |
  | B    | 15                        | 25                     | 180         |
  | C    | 12                        | 22                     | 160         |
  | D    | 20                        | 30                     | 210         |
  | E    | 18                        | 28                     | 200         |
  
  このデータを用いて、設計行列 \( X \) を作成し、正規方程式を解くことで、広告費が売上に与える影響（各回帰係数）の推定方法と、その解釈（例えば、オンライン広告費が1単位増加すると売上が何単位増えるか等）を具体的に説明します。

## 4. ChatGPTによる解説＋Colabでの実行例
- **ChatGPT解説:**  
  2つの説明変数がある場合の線形回帰モデルは、\( y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \) という形で表現され、これは3次元空間における「平面」をデータにフィットさせる問題です。  
  モデルをベクトル・行列形式で表すと、  
  $$
  \mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon},
  $$
  となり、最小二乗法により、正規方程式  
  $$
  X^T X\,\hat{\boldsymbol{\beta}} = X^T \mathbf{y}
  $$
  を解くことで、最も誤差が小さい回帰面が求められます。  
  Google Colab では、Python の NumPy ライブラリを用いてこれらの計算が簡単に実装でき、さらに3次元プロットを使って推定された平面とデータ点を視覚的に確認することが可能です。

- **Colab実行例:** 以下は、仮想データを用いて2つの説明変数がある場合の単回帰モデル（平面）のパラメータを推定し、結果をプロットするサンプルコードです。
  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  from mpl_toolkits.mplot3d import Axes3D

  # サンプルデータ（例：学生の勉強時間、睡眠時間、テスト得点）
  # 各行: [1, x1 (勉強時間), x2 (睡眠時間)]
  X = np.array([
      [1, 2, 7],
      [1, 3, 6],
      [1, 4, 8],
      [1, 5, 5],
      [1, 3, 7]
  ])
  y = np.array([65, 70, 75, 80, 68])

  # 正規方程式により最小二乗解を求める
  beta_hat = np.linalg.inv(X.T @ X) @ (X.T @ y)
  print("推定されたパラメータ (切片, β1, β2):", beta_hat)

  # 予測値の計算
  y_pred = X @ beta_hat

  # 3次元プロット
  fig = plt.figure(figsize=(10, 8))
  ax = fig.add_subplot(111, projection='3d')
  # データ点のプロット
  ax.scatter(X[:,1], X[:,2], y, color='blue', label='観測値')
  # 推定された回帰平面のプロット用グリッド
  x1_range = np.linspace(X[:,1].min()-1, X[:,1].max()+1, 20)
  x2_range = np.linspace(X[:,2].min()-1, X[:,2].max()+1, 20)
  x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
  # 回帰平面の計算
  y_grid = beta_hat[0] + beta_hat[1]*x1_grid + beta_hat[2]*x2_grid
  ax.plot_surface(x1_grid, x2_grid, y_grid, alpha=0.5, color='red', label='回帰平面')
  ax.set_xlabel('勉強時間 (x1)')
  ax.set_ylabel('睡眠時間 (x2)')
  ax.set_zlabel('テスト得点 (y)')
  ax.set_title('2変数線形回帰モデルによる回帰平面の推定')
  plt.legend()
  plt.show()
  ```

## 5. 学んだ内容の応用例（optional)
- **応用分野:**  
  - **マーケティング:** 複数の広告媒体（例：オンライン広告費、テレビ広告費）を説明変数として、商品の売上を予測する。  
  - **経済学:** 経済指標（例：雇用率、消費者信頼感指数）を用いて、経済成長率を予測する。  
  - **医療・健康科学:** 患者の年齢、体重、血圧など複数の変数から治療効果や疾患リスクを推定する。  
  - **環境科学:** 気温、湿度、風速などのデータから、大気汚染レベルや気候変動の影響を解析する。  
- **具体例:**  
  単回帰モデルの拡張として、2変数回帰モデルは多変量解析の基礎となり、PCAや機械学習の回帰モデルの発展にも直結するため、幅広い分野で実世界の問題解決に活用されています。

## 6. 演習問題
- **問題1:**  
  2つの説明変数 \( x_1, x_2 \) と反応変数 \( y \) を用いた線形回帰モデルにおいて、各パラメータ（切片 \( \beta_0 \)、回帰係数 \( \beta_1, \beta_2 \) ）の意味を自分の言葉で説明せよ。  
  *【ヒント】* 実際の例（例えば、勉強時間と睡眠時間がテスト得点に与える影響）を考えてみる。

- **問題2:**  
  以下のデータを用いて、説明変数行列 \( X \) と観測値ベクトル \( \mathbf{y} \) を作成し、正規方程式  
  $$
  X^T X\,\hat{\boldsymbol{\beta}} = X^T \mathbf{y}
  $$
  を構築せよ。  
  データ例（各行： [1, \( x_1 \), \( x_2 \)]）：  
  $$
  X = \begin{pmatrix}
  1 & 2 & 7 \\
  1 & 3 & 6 \\
  1 & 4 & 8 \\
  1 & 5 & 5 \\
  1 & 3 & 7
  \end{pmatrix}, \quad
  \mathbf{y} = \begin{pmatrix}
  65 \\
  70 \\
  75 \\
  80 \\
  68
  \end{pmatrix}.
  $$
  *【ヒント】* \( X^T X \) と \( X^T \mathbf{y} \) をそれぞれ計算しなさい。

- **問題3:**  
  正規方程式の解 \( \hat{\boldsymbol{\beta}} = (X^T X)^{-1}X^T \mathbf{y} \) の導出過程を説明し、なぜこの解が残差の二乗和を最小にするのかを論じよ。  
  *【ヒント】* 目的関数 \( S(\boldsymbol{\beta}) = \|\mathbf{y} - X\boldsymbol{\beta}\|^2 \) の最小化から導かれる条件に注目する。

- **問題4:**  
  Google Colab を用いて、実際のデータ（例：広告費2種類と売上、または複数の生活習慣指標と健康指標）に2変数線形回帰モデルを適用し、得られた回帰平面のパラメータとその解釈についてレポートを作成せよ。  
  *【ヒント】* データの前処理、設計行列の作成、正規方程式の解法、3次元プロットによる視覚化を含めること。

- **問題5 (応用):**  
  2つの説明変数がある場合の線形回帰モデルで、もし \( X^T X \) が特異またはほぼ特異な場合、どのような問題が発生するか、またそれをどのように解決するか（例：リッジ回帰などの正則化手法）を考察せよ。

## 7. 付録・参考資料
- **参考文献:**  
  - 『多変量解析入門』、『線形代数学入門』、および統計学・回帰分析の教科書  
  - ウェブリンク: Khan Academy, MIT OpenCourseWare, Coursera など
- **補足資料:**  
  - 詳細な解説スライド  
  - 追加例題とその解答例  
  - FAQ: 正規方程式や多変量回帰モデルに関するよくある質問