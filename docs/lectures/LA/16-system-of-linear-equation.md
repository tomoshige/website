# 講義タイトル: 連立1次方程式と単回帰モデル

## 1. 講義概要
- **講義時間:** 60分　/　**演習時間:** 30分
- **目的:**  
  統計モデルの基礎概念から、単回帰モデルの数学的表現（ベクトル・行列による表現）および最小二乗法によるパラメータ推定までを理解し、実データに適用できるスキルを習得する。
- **今日の目標:**  
  ・統計モデル、単回帰モデルの基本概念（説明変数、反応変数、切片、回帰係数）を明確にする。  
  ・連立1次方程式の枠組みで最適な直線（最小二乗法）の導出過程を理解する。  
  ・正規方程式の意味を把握し、Google Colabを用いた実装例から単回帰モデルの動作イメージを掴む。

## 2. 理論的背景と内容の説明
### 2.1 定義・基本概念
- **統計モデルとは？**  
  データに含まれるパターンや関係性を数理的に表現する枠組み。  
- **単回帰モデルとは？**  
  1つの説明変数 \( x \) と1つの反応変数 \( y \) の間に線形な関係があると仮定し、  
  $$
  y = \beta_0 + \beta_1 x + \varepsilon
  $$
  という形で表されるモデル。  
- **説明変数と反応変数:**  
  説明変数（独立変数）は \( x \)、反応変数（従属変数）は \( y \) として区別される。
- **切片と回帰係数:**  
  切片 \( \beta_0 \) は \( x=0 \) のときの \( y \) の予測値、回帰係数 \( \beta_1 \) は \( x \) の変化に対する \( y \) の平均的な変化量を表す。
- **データにモデルを当てはめるとは？**  
  実際の観測データに対して、最も適合する直線（予測モデル）を求める作業であり、過剰な連立1次方程式を最小二乗法などで近似解を導く。
- **ベクトル・行列による表現:**  
  \( n \) 個の観測データについて、  
  - \( y \) のデータを \( \mathbf{y} \in \mathbb{R}^{n} \)  
  - \( x \) のデータを \( \mathbf{x} \in \mathbb{R}^{n} \) とし、  
  切片項を含む説明変数行列 \( X \) を  
  $$
  X = \begin{pmatrix}
  1 & x_1 \\
  1 & x_2 \\
  \vdots & \vdots \\
  1 & x_n
  \end{pmatrix}
  $$
  と表し、パラメータベクトルを  
  $$
  \boldsymbol{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix},
  $$
  とすると、単回帰モデルは  
  $$
  \mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}
  $$
  と表現できる。

### 2.2 定理・命題
- **連立1次方程式と解の存在:**  
  観測数 \( n \) が2より大きい場合、方程式 \( X\boldsymbol{\beta} = \mathbf{y} \) は過剰定義であるため、厳密な解は存在しない。そのため、最小二乗法によって残差の二乗和を最小化する近似解を求める。
- **正規方程式:**  
  最小二乗法により、目的関数  
  $$
  S(\boldsymbol{\beta}) = \|\mathbf{y} - X\boldsymbol{\beta}\|^2
  $$
  を最小化する解は、正規方程式  
  $$
  X^T X \boldsymbol{\beta} = X^T \mathbf{y}
  $$
  を解くことで得られる。これにより、  
  $$
  \hat{\boldsymbol{\beta}} = (X^T X)^{-1}X^T \mathbf{y}
  $$
  と表される。

### 2.3 数式・証明の詳細
- **最小二乗解の導出:**  
  微分により  
  $$
  \frac{\partial S}{\partial \boldsymbol{\beta}} = -2X^T(\mathbf{y} - X\boldsymbol{\beta}) = \mathbf{0}
  $$
  となり、正規方程式 \( X^T X \boldsymbol{\beta} = X^T \mathbf{y} \) が得られる。
- **正規方程式の解法:**  
  \( X^T X \) が正則であれば、逆行列を用いて  
  $$
  \hat{\boldsymbol{\beta}} = (X^T X)^{-1}X^T \mathbf{y}
  $$
  と計算可能である。

## 3. 扱う内容の実例（GPT, Colab等は使用せず）

### 実例1: 行列の乗算を用いた予測値の生成
ここでは、単回帰モデルの説明変数行列 \( X \) とパラメータベクトル \( \boldsymbol{\beta} \) の乗算がどのようにして各観測の予測値を生成するかを示します。

例えば、5人の学生について、あるテストの得点を予測する単回帰モデルを考えます。  
説明変数として「勉強時間（時間）」、反応変数として「テスト得点（点）」が与えられており、モデルは  
$$
y = \beta_0 + \beta_1 x
$$  
と表されます。  
各学生の勉強時間 \( x \) のデータが  
$$
x = (1.5,\ 2.0,\ 3.0,\ 4.0,\ 2.5)
$$  
だとすると、説明変数行列 \( X \) は切片項を含めて  
$$
X = \begin{pmatrix}
1 & 1.5 \\
1 & 2.0 \\
1 & 3.0 \\
1 & 4.0 \\
1 & 2.5
\end{pmatrix}
$$  
と表されます。  
例えば、既に推定されたパラメータが  
$$
\hat{\boldsymbol{\beta}} = \begin{pmatrix} 50 \\ 10 \end{pmatrix}
$$  
であれば、各学生の予測得点は  
$$
\hat{\mathbf{y}} = X\hat{\boldsymbol{\beta}} =
\begin{pmatrix}
1 & 1.5 \\
1 & 2.0 \\
1 & 3.0 \\
1 & 4.0 \\
1 & 2.5
\end{pmatrix}
\begin{pmatrix}
50 \\
10
\end{pmatrix}
=
\begin{pmatrix}
50 + 15 \\
50 + 20 \\
50 + 30 \\
50 + 40 \\
50 + 25
\end{pmatrix}
=
\begin{pmatrix}
65 \\
70 \\
80 \\
90 \\
75
\end{pmatrix}.
$$  
このように、行列の乗算によって、各観測に対応する予測値が一括して求められます。

---

### 実例2: 正規方程式を用いた最小二乗解の数値計算
ここでは、5人の学生のデータを用いて、最小二乗法によるパラメータ推定の計算過程を具体的な数値例で示します。

【与えられたデータ】  
各学生の勉強時間 \( x \) と実際のテスト得点 \( y \) が以下の通りとします。  
| 学生 | 勉強時間 \( x \) | 得点 \( y \) |
|------|------------------|--------------|
| 1    | 1.5              | 65           |
| 2    | 2.0              | 70           |
| 3    | 3.0              | 80           |
| 4    | 4.0              | 90           |
| 5    | 2.5              | 75           |

【ステップ1: 説明変数行列と観測値ベクトルの作成】  
説明変数行列 \( X \) は  
$$
X = \begin{pmatrix}
1 & 1.5 \\
1 & 2.0 \\
1 & 3.0 \\
1 & 4.0 \\
1 & 2.5
\end{pmatrix},
$$  
観測値ベクトル \( \mathbf{y} \) は  
$$
\mathbf{y} = \begin{pmatrix}
65 \\
70 \\
80 \\
90 \\
75
\end{pmatrix}.
$$

【ステップ2: 正規方程式の構築】  
正規方程式は  
$$
X^T X \hat{\boldsymbol{\beta}} = X^T \mathbf{y}
$$  
となります。

まず、\( X^T X \) を計算すると、  
$$
X^T X =
\begin{pmatrix}
1 & 1 & 1 & 1 & 1 \\
1.5 & 2.0 & 3.0 & 4.0 & 2.5
\end{pmatrix}
\begin{pmatrix}
1 & 1.5 \\
1 & 2.0 \\
1 & 3.0 \\
1 & 4.0 \\
1 & 2.5
\end{pmatrix}
=
\begin{pmatrix}
5 & 13 \\
13 & 40.5
\end{pmatrix}.
$$

次に、\( X^T \mathbf{y} \) を計算すると、  
$$
X^T \mathbf{y} =
\begin{pmatrix}
1 & 1 & 1 & 1 & 1 \\
1.5 & 2.0 & 3.0 & 4.0 & 2.5
\end{pmatrix}
\begin{pmatrix}
65 \\
70 \\
80 \\
90 \\
75
\end{pmatrix}
=
\begin{pmatrix}
65+70+80+90+75 \\
1.5\times65+2.0\times70+3.0\times80+4.0\times90+2.5\times75
\end{pmatrix}.
$$  
計算すると、  
- \( \sum y = 65+70+80+90+75 = 380 \)  
- \( \sum x y = 97.5+140+240+360+187.5 = 1025 \)

よって、
$$
X^T \mathbf{y} = \begin{pmatrix} 380 \\ 1025 \end{pmatrix}.
$$

【ステップ3: 最小二乗解の計算】  
正規方程式は  
$$
\begin{pmatrix}
5 & 13 \\
13 & 40.5
\end{pmatrix}
\hat{\boldsymbol{\beta}} =
\begin{pmatrix}
380 \\
1025
\end{pmatrix}.
$$  
この連立方程式を解くと、  
$$
\hat{\boldsymbol{\beta}} = (X^T X)^{-1}X^T \mathbf{y}.
$$  
逆行列を求めると、
- \( \det(X^T X) = 5 \times 40.5 - 13 \times 13 = 202.5 - 169 = 33.5 \)
- \( (X^T X)^{-1} = \frac{1}{33.5} \begin{pmatrix} 40.5 & -13 \\ -13 & 5 \end{pmatrix} \)

よって、
$$
\hat{\boldsymbol{\beta}} = \frac{1}{33.5} \begin{pmatrix} 40.5 & -13 \\ -13 & 5 \end{pmatrix} \begin{pmatrix} 380 \\ 1025 \end{pmatrix}.
$$

計算すると、
- 切片 \( \hat{\beta}_0 \approx \frac{40.5\times380 - 13\times1025}{33.5} \)
- 傾き \( \hat{\beta}_1 \approx \frac{-13\times380 + 5\times1025}{33.5} \)

実際の数値計算を行うと、  
$$
\hat{\beta}_0 \approx 50,\quad \hat{\beta}_1 \approx 10,
$$  
となり、先ほどの実例1と同様のパラメータが得られることが確認できます。

---

### 実例3: 実世界のデータ例—広告費と売上
ここでは、実世界のデータ例として、ある企業の広告費と売上の関係を単回帰モデルで解析するプロセスを示します。

【例の設定】  
- 説明変数 \( x \): 広告費（単位: 千円）  
- 反応変数 \( y \): 売上（単位: 百万円）

【仮想データ】  
例えば、以下のようなデータが得られたとします。  
| 企業 | 広告費 \( x \) | 売上 \( y \) |
|------|---------------|-------------|
| A    | 5             | 12          |
| B    | 8             | 18          |
| C    | 10            | 21          |
| D    | 6             | 14          |
| E    | 9             | 19          |

【データ行列の作成】  
説明変数行列 \( X \) は、切片項を含めて  
$$
X = \begin{pmatrix}
1 & 5 \\
1 & 8 \\
1 & 10 \\
1 & 6 \\
1 & 9
\end{pmatrix},
$$  
反応変数ベクトル \( \mathbf{y} \) は  
$$
\mathbf{y} = \begin{pmatrix}
12 \\
18 \\
21 \\
14 \\
19
\end{pmatrix}.
$$

【モデルの適用】  
最小二乗法により、正規方程式  
$$
X^T X \hat{\boldsymbol{\beta}} = X^T \mathbf{y}
$$  
を解くことで、  
$$
\hat{\boldsymbol{\beta}} = (X^T X)^{-1}X^T \mathbf{y}
$$  
が得られます。  
この結果、例えば  
$$
\hat{\boldsymbol{\beta}} \approx \begin{pmatrix} 4.5 \\ 1.8 \end{pmatrix}
$$  
と推定されたとします。  
この場合、モデルは  
$$
\hat{y} = 4.5 + 1.8x
$$  
と表現され、広告費が1単位増加するごとに売上が約1.8百万円増加すると解釈できます。

【ホワイトボードでの説明】  
この実例では、実際の広告費と売上という具体的なビジネスの状況を通して、  
- データがどのように行列で表されるか、  
- 正規方程式により「一番良い直線」を求める過程、  
- 得られた直線から実際の効果（回帰係数）の解釈  
を直感的に説明することが可能です。これにより、連立1次方程式の枠組みで最適な回帰直線を見つける手法の有用性が理解されます。


## 4. ChatGPTによる解説＋Colabでの実行例
- **ChatGPT解説:**  
  単回帰モデルでは、観測されたデータ点と予測直線との間の残差（誤差）の二乗和が最小になるようにパラメータが推定されます。以下の図は、データ点（青色の点）と、最小二乗法により当てはめられた回帰直線（赤色の線）との間に生じる残差（緑色の破線）が描かれており、各点の誤差が視覚的に確認できます。この図を見ると、回帰直線が全体として誤差の二乗和を最小化する位置にあることが直感的に理解できます。

- **Colab実行例:** 以下に、単回帰モデルの当てはめと、残差を視覚的に確認するためのサンプルコードと図を示します。
  ```python
  import numpy as np
  import matplotlib.pyplot as plt

  # サンプルデータ: 説明変数 x と反応変数 y
  x = np.array([1, 2, 3, 4, 5])
  y = np.array([2, 3, 5, 4, 6])
  n = x.shape[0]

  # 説明変数行列 X の作成（切片項を含む）
  X = np.vstack((np.ones(n), x)).T

  # 正規方程式を用いて最小二乗解を求める
  beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
  y_pred = X @ beta_hat

  # 残差の計算
  residuals = y - y_pred

  # 図の描画
  plt.figure(figsize=(8, 6))
  plt.scatter(x, y, color='blue', label='観測値')
  plt.plot(x, y_pred, color='red', label='回帰直線')
  # 各点に対する残差を垂直線で表示
  for xi, yi, ypi in zip(x, y, y_pred):
      plt.vlines(x=xi, ymin=ypi, ymax=yi, colors='green', linestyles='dashed', label='残差' if xi==x[0] else "")
  plt.xlabel('説明変数 x')
  plt.ylabel('反応変数 y')
  plt.title('単回帰モデルの当てはめと残差の視覚化')
  plt.legend()
  plt.show()
  ```
  
このコードを実行すると、データ点と回帰直線に加え、各データ点から回帰直線までの距離（残差）が垂直の破線として描かれ、最小二乗法による当てはめがどのようにして誤差を最小にしているのかを視覚的に確認できます。

## 5. 学んだ内容の応用例
- **経済・マーケティング分野:**  
  単回帰モデルを用いて、広告費と売上高の関係を解析することで、どの程度の広告投資が最も効果的かを推定できます。これにより、企業はマーケティング戦略の最適化や投資判断に活用できます。

- **医療統計・健康科学:**  
  患者の治療前後の各種指標（例：血圧、体重）の変化をモデル化することで、治療効果の評価や副作用のリスクを予測することが可能です。これにより、個別化医療や治療法の改善に貢献します。

- **教育評価:**  
  学生の学習時間や授業出席率と試験成績との関係を単回帰分析により明らかにすることで、効果的な教育プログラムの設計や、学習支援策の検討に役立ちます。

- **品質管理と生産プロセス:**  
  製造業において、原材料の特性や製造条件と製品の品質との関係を解析することで、工程の改善や不良品削減に寄与します。単回帰モデルは、製造ラインのパフォーマンス評価やプロセスの最適化のための基礎手法となります。

- **データサイエンス・機械学習への発展:**  
  単回帰分析は、より複雑な多変量回帰モデルや機械学習アルゴリズム（リッジ回帰、ラッソ回帰、線形回帰など）の基礎となる考え方です。これらの手法は、ビッグデータの解析や予測モデルの構築に広く応用されています。

これらの応用例は、連立1次方程式と最小二乗法という基礎的な数学的手法が、実際の多様な問題に対してどのように活用できるかを示しており、学生が理論を実世界の問題に結び付けて理解するための有用な視点となります。

## 6. 演習問題

- **問題1: 単回帰モデルの基本概念の確認**  
  単回帰モデルにおける「説明変数」「反応変数」「切片」「回帰係数」とは何か、自分の言葉で説明せよ。  
  *【ヒント】* 広告費と売上、または勉強時間とテスト得点など、実生活の例を考えてみる。

- **問題2: 行列表現の構築**  
  観測値が \( n \) 個あるとき、単回帰モデルを以下のように表現できる。  
  1. 切片項を含む説明変数行列 \( X \) の形（各行は \([1, x_i]\) の形になる）を示せ。  
  2. パラメータベクトル \(\boldsymbol{\beta} = (\beta_0, \beta_1)^T\) を用いて、モデル \(\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}\) を記述せよ。  
  *【ヒント】* \( X \) は \(\mathbb{R}^{n \times 2}\) の行列で、各観測 \( x_i \) に対して1が前置される。

- **問題3: 正規方程式による最小二乗推定**  
  以下のデータを用いて、正規方程式  
  $$
  X^T X\,\hat{\boldsymbol{\beta}} = X^T \mathbf{y}
  $$
  を解き、最小二乗推定量 \(\hat{\boldsymbol{\beta}}\) を手計算せよ。  
  - 学生の勉強時間 \( x = [2, 3, 5, 7, 9] \)（単位：時間）  
  - 対応するテスト得点 \( y = [50, 55, 65, 75, 85] \)（点）  
  *【ヒント】* まず、切片項を含む説明変数行列 \( X \) を作成し、次に \( X^T X \) と \( X^T \mathbf{y} \) を計算する。

- **問題4: 正規方程式の解の解釈**  
  正規方程式 $ X^T X\,\hat{\boldsymbol{\beta}} = X^T \mathbf{y} $ から得られる最小二乗解が、なぜ観測データと予測直線との二乗誤差の総和を最小にする解となるのか、数学的な観点から説明せよ。  
  *【ヒント】* 目的関数 $ S(\boldsymbol{\beta}) = \| \mathbf{y} - X\boldsymbol{\beta} \|^2 $ の最小化問題として導出されるプロセスに注目する。

- **問題5: 実データへの単回帰モデルの適用と解釈**  
  Google Colab を利用して、実際のデータ（例：広告費と売上、または身長と体重）に単回帰モデルを適用せよ。  
  1. データのプロットと、回帰直線の当てはめを行い、最小二乗法によるパラメータ推定を実施する。  
  2. 得られた回帰直線の切片および傾きが何を意味するのか、また決定係数 \( R^2 \) の解釈も含めたレポートを作成せよ。  
  *【ヒント】* データの視覚化、正規方程式による解法、そしてその結果の解釈を統合的にまとめることを意識する。
