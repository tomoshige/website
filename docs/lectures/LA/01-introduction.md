# 講義1: イントロダクション

## 1. 本日扱う内容について（概要）

  - 線形代数学I/基礎/IIの概要説明  
  - 評価方法の説明
  - 使用ツール（ChatGPT, Claude, Google Colab）の紹介  
  - 線形代数学の意義と、統計学・機械学習、医療データ、画像処理への応用事例の提示

## 2. 本日の内容の説明（概要・定義・定理など）

### 2.1 線形代数学とは
  線形代数学は、ベクトル、行列、線形写像などの基本概念を用いて、データやシステムの構造を解析する数学の分野です。
  - **基本概念:**  
    - **ベクトル:** 大きさと方向を持つ量。  
    - **行列:** 数値や関数を格子状に並べたもので、複数のデータを一括して扱うためのツール。  
    - **線形写像:** ベクトル空間間の変換で、加法性とスカラー倍の性質を保つ写像。
  
  この講義は、前期の「線形代数学I」「線形代数学（基礎）」、後期の「線形代数学II」の3つの授業で完結するように設計されています。


### 2.2 線形代数とデータサイエンスの関係

データサイエンスで解きたい課題は、大きく教師あり学習と教師なし学習に大別されます。

**教師あり学習**とは、例えば、医療データにおいて、過去の診断結果や治療記録を基に患者の病状を予測することや、スポーツデータにおいて、試合結果や選手の成績を元に次のパフォーマンスを予測するなどの正解があるデータを用いてモデルを学習し、未知のデータに対する予測を行うことです。

**教師なし学習**とは、医療データで疾患の共通パターンを抽出したり、スポーツデータで選手のプレースタイルやチームの戦略的特徴を明らかにするために、データ自体の構造や特徴を解析する手法を指します。

教師あり学習の基本手法として最も広く知られているのが**線形回帰モデル**であり、その基礎となるのは線形代数学で学ぶ**連立1次方程式**です。また、教師なし学習の代表的手法である**主成分分析**は、**固有値・固有ベクトルを用いた行列の固有値分解**に基づいて行われるものです。つまり、データ分析の基礎となる2つの手法は線形代数学に基づいています。

この他にも多くのデータサイエンスで扱う方法は、線形代数学をもとに発展しています。この授業で皆さんに求めることは、まずは`線形代数学の基本的な計算ができるようになること`です。理屈を知るのは計算ができるようになってからで構いません。まずは理屈は度外視で`計算`ができるようになりましょう。

大学で学ぶ数学の理論を1度聞いただけで理解するのはそれほど簡単なことではありませんし、論理を研究で用いるのは研究者だけです。それよりも皆さんはまずは`計算`ができるようになり、その上で2年生・3年生と学年が上がる中で`何度も同じ概念に触れることで、徐々に理論について目を向ける余裕が出てきます`。必要なものを必要なだけ学んでいきましょう。

### 2.3 成績の評価
前期の`線形代数学I`と`線形代数学（基礎）`は、合わせて評価を行います。3つの試験によって評価を行います。

- **中間試験:** 持ち込み不可の90分の筆記試験です。基本的な行列の計算や、性質が理解できているかを確認します（40％）。
- **期末試験:** 持ち込み不可の90分の筆記試験です。基本的な行列の計算や、性質が理解できているかを確認します（60％）。
- **生成AIの活用レポート:** 最終授業後にこの授業の学習にどのように生成AIを用いたか（質問内容・プロンプト）などをレポートで提出してもらう課題です（20%）。ただしこの課題はOptionalなもので、全ての人が取り組む必要はありません。

中間試験と期末試験は、線形代数で学ぶ基本計算の理屈がきちんと理解できているかを確かめるためのものです。基本的に、難しい問題ではなく`計算問題`を適切に処理して、正しい答えを導くことができるかを確かめます。

#### 成績評定

- **S** : 90%以上
- **A** : 80%以上〜89.9%以下
- **B** : 70%以上〜79.9%以下
- **C** : 60%以上〜69.9%以下
- **D** : 59.9%以下

#### 再試験・追試験

学習要覧通りです。

### 2.4 生成AIの活用

- この授業では、生成AIを活用します。この授業には、**必ずPCを持ち込み**ましょう。
- わからないことは**ChatGPT**や**Claude**および**Gemini**（そのほかの生成AIでもok）を用いて解決するというのを実践してください。
- 計算結果や図の表示は、**Google Colaboratory**を利用します。

## 3. 生成AIの活用

### 3.1 概要
生成AIは、ウェブをメインとした文章を学習した深層学習モデルで、一般的には大規模言語モデル（Large Language Model; LLM）といいます。有名なものとしては、OpenAIのChatGPT, AnthropicのClaude、GoogleのGemini があります。
- [ChatGPT](https://chatgpt.com/)
- [Claude](https://claude.ai/)
- [Gemini](https://aistudio.google.com/app/prompts/new_chat)

ChatGPTの無料版は、GPT-4o mini へのアクセスが基本ですが、数学的な能力はそれほど高くないので線形代数の質問への回答は不正確な場合が多いです。Claude の無料版は最新のモデルへのアクセイスが可能で、回数制限はありますが説明をさせるなら非常に有用です([リンク](https://www.anthropic.com/pricing))。Gemini は google AI studio を経由して利用するとほとんど利用制限がなく、最新のモデル Gemini 2.5 へのアクセスが可能です。基本的には、皆さんの好みで使ってもらえれば良いですが、どのモデルが良かったなどの情報交換は積極的に行ってください。App storeや、Android store には、それぞれの生成AIのアプリもリリースされています。


## 4. Google Colaboratory の使い方

### 4.1 概要
**Google Colaboratory** (通称：コラボ) は、ブラウザ上でPython, Rのコードを実行できるオンライン環境です。講義で紹介するコード例や演習問題を、実際に動かして確認するために使用できます。また、AIが搭載されているので「日本語で〇〇をするコードを書いて」と書けば、コーディングを補助してもらうことができます。

この授業では、例えば線形代数の問題をプログラムによって解かせる実演を行います。そのプログラムを皆さんの手元のPC上で動かす手段として、Colabを利用します。皆さんにプログラムを書いてもらうことはありませんが、それをコピペして問題に応じて数値を変えることで、解を求めるということを体験してもらう予定です。


### 4.2 利用方法の手順

1. **Google Colabへのアクセス:**  

   - ウェブブラウザで [Google Colab](https://colab.research.google.com/) にアクセスします。  
   - Googleアカウントでログインします。

2. **新しいノートブックの作成:**  

   - 「新しいノートブック」をクリックして、空のノートブックを作成します。  
   - ノートブックの名前を「線形代数学_第1回」などに変更して、管理しやすくします。

3. **セルの使い方の理解:**  

   - Colabには「コードセル」と「テキストセル」があり、コードの実行や解説の記入に利用します。  
   - コードセルには講義のコード例を貼り付け、テキストセルにはメモや解説、質問などを記入してください。

4. **コードの実行:**  

   - コードセルを選択し、左側の再生ボタンをクリックするか、Shift+Enterキーを押してコードを実行します。
   - 実行結果はセルの下に表示され、グラフや数値出力も確認できます。

5. **演習問題への応用:**  

   - 講義で出された演習問題のコードをColabに入力し、実際に実行して動作を確認します。  
   - 分からない点はセル内にコメントを追加するなどして、後で見返しやすくします。

6. **ファイルの保存と共有:**  

   - ノートブックは自動的にGoogleドライブに保存されます。  
   - 「共有」ボタンを使って、他の学生や講師とノートブックを共有することも可能です。


## 5. いくつかの線形代数の応用例
線形代数を活用しているデータサイエンスの手法の具体的な例を紹介します。
以下のリンクから、今回使用するコードを実行した結果が確認できます。
https://colab.research.google.com/drive/1iW2o1gJHAYwW2xxGiC1j41CtdumtcgRo?usp=sharing


### 5.1 最小二乗法による線形回帰

#### 最小二乗法とは
最小二乗法は、データポイントと予測モデル間の誤差の二乗和を最小化する手法です。線形回帰の文脈では、この方法は以下の最適化問題を解きます：

$$\min_{\beta_0, \beta_1} \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2$$

ここで：
- $y_i$ は観測値（実際のデータ点）
- $\beta_0$ は切片
- $\beta_1$ は傾き
- $x_i$ は説明変数

例えば、血糖値の値を $y_i$ にして、年齢を $x_i$ とすると、年齢が血糖値の値とどのような直線関係を持っているのかを明らかにできます。また、説明変数 $x_i$ に年齢だけではなく、性別や食事習慣、睡眠時間などのさまざまな情報を追加することで、より複雑な関係を表現することができます。


### 5.2 主成分分析（PCA）による次元削減

- **概要:**  
  線形代数学の基礎概念（固有値・固有ベクトルや行列分解）を用いて、多次元データから次元の削減を行い、重要な情報を抽出する方法です。医療分野では、多数の測定値から病気の状態を評価する際に、データの次元削減が有効ですし、スポーツ分野でもセンター技術の発展で、さまざまなデータがとられるようになった今、その情報の中から有用な知見を発見するためには、データの次元削減の技術は非常に重要視されています。
    
- **2つの具体例:**  
  - Irisデータセットを用いて、4次元のデータを2次元に縮約し、データのクラスタリング傾向を視覚化します。
  - 乳がんデータセットを用いてPCAによりデータの低次元表現を取得し、良性と悪性のサンプルを視覚化します。


### 5.3 画像処理への応用例: SVDによる画像の低ランク近似

- **概要:**  
  画像処理では、特に画像圧縮やノイズ除去の分野で線形代数学が活用されます。ここでは、特異値分解（SVD）を用いて画像の低ランク近似を行い、元の画像と近似画像の比較を行います。  
  
  特異値分解（Singular Value Decomposition, SVD）は線形代数学における強力な行列分解手法であり、画像処理において圧縮やノイズ除去、特徴抽出など様々なタスクに応用できます。本解説では、SVDの理論的背景から具体的な実装、そして画像処理への応用とその解釈までを体系的に説明します。

#### 数学的定義

任意の $m \times n$ 行列 $A$ に対して、SVDは以下のように行列を分解します：

$$A = U\Sigma V^T$$

ここで：
- $U$ は $m \times m$ の直交行列で、$A$ の**左特異ベクトル**を列として持ちます
- $\Sigma$ は $m \times n$ の対角行列で、対角成分に**特異値**を持ちます（通常は降順）
- $V^T$ は $n \times n$ の直交行列の転置で、$A$ の**右特異ベクトル**を行として持ちます

特異値は $A$ の表現において各「成分」の重要度を表しており、多くの場合、少数の大きな特異値だけでデータの本質的な部分を捉えることができます。

#### 画像処理における意味

画像は2次元の行列（グレースケール）または3次元のテンソル（カラー画像）として表現できます。カラー画像の場合、各チャンネル（RGB）を独立した行列として扱い、それぞれにSVDを適用します。

SVDを画像に適用すると：
1. 左特異ベクトル $U$ は画像の「縦方向」のパターン
2. 右特異ベクトル $V$ は画像の「横方向」のパターン
3. 特異値 $\Sigma$ はそれらのパターンの重要度

を表します。特異値の大きい上位の成分だけを使って画像を再構成することで、本質的な情報を保持しながらデータ量を削減できます。

#### 圧縮のアプローチ

画像圧縮においては、ランク $r$ の行列 $A$ を、より少ないランク $k$ (ただし $k < r$)の行列 $A_k$ で近似します：

$$A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T$$

ここで $\sigma_i$ は $i$ 番目の特異値、$u_i$ と $v_i$ はそれぞれ $i$ 番目の左右特異ベクトルです。

#### 圧縮率の計算

元の $m \times n$ のカラー画像（3チャンネル）を保存するには $3 \times m \times n$ の要素が必要です。一方、SVDで圧縮した場合は：

- 左特異ベクトル: $3 \times m \times k$ 要素
- 特異値: $3 \times k$ 要素
- 右特異ベクトル: $3 \times k \times n$ 要素

合計で $3 \times (m \times k + k + k \times n)$ 要素となります。圧縮率は：

$$\text{圧縮率} = \frac{3 \times (m \times k + k + k \times n)}{3 \times m \times n}$$

$k$ が小さいほど、圧縮率は高くなります（値は小さくなる）。

#### 特異値の分布と累積寄与率

コードの実行結果から、特異値は急速に減衰していることがわかります。最初の数十の特異値だけで画像の情報の大部分（95〜99%）を表現できています。このことは、画像データに高い冗長性があり、効率的に圧縮できることを示しています。

各チャンネルについて：
- 95%のエネルギーを保持するには約50-100の特異値が必要
- 99%のエネルギーを保持するには約150-200の特異値が必要

これは元の次元数（画像の幅や高さは数百ピクセル）と比較して非常に少ない数です。


## 6. 次回の内容
以上で今回の講義を終わります。今回は第1回目ということで、インパクトのある応用例を中心に紹介しましたが、実際の線形代数は、ベクトルや行列の計算といった手を動かしながら計算手順を追っていくというものになるので、応用までの距離が長くてツラいと感じると思います。ただ、データサイエンスを学ぶ上で絶対に避けては通れないので、1つ1つ確実に勉強しましょう。まずはベクトルの基礎から復習します。ベクトルは習っていないという方も、高校までの内容を忘れてしまったという方も、最初から丁寧に勉強しますので、ついてきてください。