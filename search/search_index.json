{"config":{"lang":["ja"],"separator":"[\\s\\-\u3000\u3001\u3002\uff0c\uff0e]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Welcome to Statistical Learning Laboratory","text":"<p>My name is Tomoshige Nakamura. I am an Assistant Professor in the Department of Health Data Science at Juntendo University. I received my Ph.D. in Engineering from the Keio University in Feb, 2021. I am very fortunate to be supervised by Professor Mihoko Minami. I am also very fortunate to work with Professor Hiroshi Shiraishi on random forest research.</p>"},{"location":"#contact","title":"Contact","text":"<ul> <li>Address : 6-8-1, Hinode, Urayasu Shi, Chiba Prefecture, 279-0013, Japan</li> <li>Email : t.nakamura.gs[at]juntendo.ac.jp</li> </ul>"},{"location":"#research-interest","title":"Research Interest","text":"<p>My research focuses on elucidating the properties of tree-structured models, such as decision trees and regression trees, and developing novel variants of these models. Tree-structured models have a unique advantage in that a single tree can effectively compress and visualize data, while an ensemble of multiple trees (e.g., random forests) can attain high predictive accuracy. Moreover, these models accommodate continuous, categorical, and discrete variables in a unified manner, making them both versatile and practical. It is no surprise, therefore, that tree-based methods consistently appear among the top-performing approaches in competitive data analytics venues such as Kaggle.</p> <p>Recent theoretical advances have shed light on the statistical properties of tree-structured models, pointing to continued improvements in their performance and interpretability. For example, new modeling techniques, such as recursive partitioning models, Isolation Forests for anomaly detection, and causal trees and forests for causal inference, highlight the growing versatility of tree-structured approaches. These models also provide a straightforward measure of variable importance, enabling analysts and domain experts to interpret how different predictors contribute to the final outcome.</p> <p>In my work, I aim to develop new tree architectures that not only preserve high predictive accuracy but also support more complex background information. By integrating hierarchical Bayesian models, it becomes possible to combine rich contextual details\u2014such as multi-level factors or nested data structures\u2014while maintaining strong predictive capabilities. This fusion of tree-based learning and hierarchical modeling is especially promising for tackling challenging research questions that require both flexibility and interpretability.</p> <p>Beyond conventional domains, tree-structured models show great potential in health and medical applications. For instance, the interpretability offered by these models can help medical professionals identify critical risk factors and intervene more effectively. Their ability to deal with heterogeneous data types\u2014ranging from imaging features to genomic markers\u2014makes them well suited for integrative analyses in personalized medicine and other emerging areas. Although this is a challenging field due to the complexity and sensitivity of medical data, the ongoing convergence of statistical theory, machine learning, and healthcare innovation offers immense opportunities to push these models into clinical practice.</p> <p>By advancing the theoretical and practical foundations of tree-structured models, my research strives to illuminate new frontiers in data-driven decision making. From competitive data analytics to healthcare, these models hold promise for more interpretable, accurate, and robust solutions in a wide range of applications. It is my goal to continue exploring these possibilities, driving tree-based methods into even more challenging and impactful domains.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>Department of Health Data Science, Juntendo University</li> <li>Stat Lab, Department of Mathematics, Keio Universiry</li> </ul>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> \\[ f(x) = \\int_{-\\infty}^\\infty     \\hat f(\\xi)\\,e^{2 \\pi i \\xi x}     \\,d\\xi \\] <p>inline tex codes \\(E=mc^2\\).</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]"},{"location":"00-prefare/","title":"Prefare","text":"<p>These are exciting times in statistics and data science education. (I am predicting this statement will continue to be true regardless of whether you are reading this foreword in 2020 or 2050.) But (isn't there always a but?), as a statistics educator, it can also feel a bit overwhelming to stay on top of all the new statistical, technological, and pedagogical innovations. I find myself constantly asking, \"Am I teaching my students the correct content, with the relevant software, and in the most effective way?\" Before I make all of us feel lost at sea, let me point out how great a life raft I have found in ModernDive. In a sea of intro stats textbooks, ModernDive floats to the top of my list, and let me tell you why. (Note my use of ModernDive here refers to the book in its shortened title version. This also matches up nicely with the neat hex sticker Drs. Ismay and Kim created for the cover of ModernDive, too.)</p> <pre><code>print('hello world')\n</code></pre> <p>My favorite aspect of ModernDive, if I must pick a favorite, is that students gain experience with the whole data analysis pipeline (see Figure \\@ref(fig:pipeline-figure)). In particular, ModernDive is one of the few intro stats textbooks that teaches students how to wrangle data. And, while data cleaning may not be as groovy as model building, it's often a prerequisite step! The world is full of messy data and ModernDive equips students to transform their data via the <code>dplyr</code> package.</p> <p>Speaking of <code>dplyr</code>, students of ModernDive are exposed to the <code>tidyverse</code> suite of R packages. Designed with a common structure, <code>tidyverse</code> functions are written to be easy to learn and use. And, since most intro stats students are programming newbies, ModernDive carefully walks the students through each new function it presents and provides frequent reinforcement through the many Learning checks dispersed throughout the chapters.</p> <p>Overall, ModernDive includes wise choices for the placement of topics. Starting with data visualization, ModernDive gets students building <code>ggplot2</code> graphs early on and then continues to reinforce important concepts graphically throughout the book. After moving through data wrangling and data importing, modeling plays a prominent role, with two chapters devoted to building regression models and a later chapter on inference for regression. Lastly, statistical inference is presented through a computational lens with calculations done via the <code>infer</code> package.</p> <p>I first met Drs. Ismay and Kim while attending their workshop at the 2017 US Conference on Teaching Statistics. They pushed us as participants to put data first and to use computers, instead of math, as the engine for statistical inference. That experience helped me modernize my own intro stats course and introduced me to two really forward-thinking statistics and data science educators. It has been exciting to see ModernDive develop and grow into such a wonderful, timely textbook. I hope you have decided to dive on in!</p> <pre><code>import numpy as np\n</code></pre>"},{"location":"lectures/linear-algebra/","title":"Linear algebra","text":""},{"location":"lectures/linear-algebra/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> \\[ f(x) = \\int_{-\\infty}^\\infty     \\hat f(\\xi)\\,e^{2 \\pi i \\xi x}     \\,d\\xi \\] <p>inline tex codes \\(E=mc^2\\).</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]"},{"location":"lectures/statistical-inference-without-syntax/","title":"Data Science without syntax","text":""},{"location":"lectures/statistical-inference-without-syntax/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> \\[ f(x) = \\int_{-\\infty}^\\infty     \\hat f(\\xi)\\,e^{2 \\pi i \\xi x}     \\,d\\xi \\] <p>inline tex codes \\(E=mc^2\\).</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]"},{"location":"lectures/statistics-and-probability/","title":"Statistics and probability","text":""},{"location":"lectures/statistics-and-probability/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> \\[ f(x) = \\int_{-\\infty}^\\infty     \\hat f(\\xi)\\,e^{2 \\pi i \\xi x}     \\,d\\xi \\] <p>inline tex codes \\(E=mc^2\\).</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]"},{"location":"research/","title":"Index","text":""},{"location":"research/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> \\[ f(x) = \\int_{-\\infty}^\\infty     \\hat f(\\xi)\\,e^{2 \\pi i \\xi x}     \\,d\\xi \\] <p>inline tex codes \\(E=mc^2\\).</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]"}]}