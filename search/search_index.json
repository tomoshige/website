{"config":{"lang":["ja"],"separator":"[\\s\\-\u3000\u3001\u3002\uff0c\uff0e]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Welcome to Statistical Learning Laboratory","text":"<p>My name is Tomoshige Nakamura. I am an Assistant Professor in the Department of Health Data Science at Juntendo University. I received my Ph.D. in Engineering from the Keio University in Feb, 2021. I am very fortunate to be supervised by Professor Mihoko Minami. I am also very fortunate to work with Professor Hiroshi Shiraishi on random forest research.</p>"},{"location":"#contact","title":"Contact","text":"<ul> <li>Address : 6-8-1, Hinode, Urayasu Shi, Chiba Prefecture, 279-0013, Japan</li> <li>Email : t.nakamura.gs[at]juntendo.ac.jp</li> </ul>"},{"location":"#research-interest","title":"Research Interest","text":"<p>My research focuses on elucidating the properties of tree-structured models, such as decision trees and regression trees, and developing novel variants of these models. Tree-structured models have a unique advantage in that a single tree can effectively compress and visualize data, while an ensemble of multiple trees (e.g., random forests) can attain high predictive accuracy. Moreover, these models accommodate continuous, categorical, and discrete variables in a unified manner, making them both versatile and practical. It is no surprise, therefore, that tree-based methods consistently appear among the top-performing approaches in competitive data analytics venues such as Kaggle.</p> <p>Recent theoretical advances have shed light on the statistical properties of tree-structured models, pointing to continued improvements in their performance and interpretability. For example, new modeling techniques, such as recursive partitioning models, Isolation Forests for anomaly detection, and causal trees and forests for causal inference, highlight the growing versatility of tree-structured approaches. These models also provide a straightforward measure of variable importance, enabling analysts and domain experts to interpret how different predictors contribute to the final outcome.</p> <p>In my work, I aim to develop new tree architectures that not only preserve high predictive accuracy but also support more complex background information. By integrating hierarchical Bayesian models, it becomes possible to combine rich contextual details\u2014such as multi-level factors or nested data structures\u2014while maintaining strong predictive capabilities. This fusion of tree-based learning and hierarchical modeling is especially promising for tackling challenging research questions that require both flexibility and interpretability.</p> <p>Beyond conventional domains, tree-structured models show great potential in health and medical applications. For instance, the interpretability offered by these models can help medical professionals identify critical risk factors and intervene more effectively. Their ability to deal with heterogeneous data types\u2014ranging from imaging features to genomic markers\u2014makes them well suited for integrative analyses in personalized medicine and other emerging areas. Although this is a challenging field due to the complexity and sensitivity of medical data, the ongoing convergence of statistical theory, machine learning, and healthcare innovation offers immense opportunities to push these models into clinical practice.</p> <p>By advancing the theoretical and practical foundations of tree-structured models, my research strives to illuminate new frontiers in data-driven decision making. From competitive data analytics to healthcare, these models hold promise for more interpretable, accurate, and robust solutions in a wide range of applications. It is my goal to continue exploring these possibilities, driving tree-based methods into even more challenging and impactful domains.</p>"},{"location":"#lectures","title":"Lectures","text":"<p>I teach courses in Linear Algebra, Probability and Statistics, Practical Machine Learning, and Practical Artificial Intelligence. Each course is designed to build a solid foundation for understanding and applying data science concepts. For further details, please visit the Lectures page.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>Department of Health Data Science, Juntendo University</li> <li>Stat Lab, Department of Mathematics, Keio Universiry</li> </ul>"},{"location":"lectures/","title":"Lecture","text":"<p>\u62c5\u5f53\u3057\u3066\u3044\u308b\u8b1b\u7fa9\u306e\u30da\u30fc\u30b8\u3067\u3059\u3002</p>"},{"location":"lectures/#i-ii","title":"\u7dda\u5f62\u4ee3\u6570\u5b66I / \u57fa\u790e / II","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u57fa\u672c\u6982\u5ff5\u3068\u305d\u306e\u5fdc\u7528\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u5b66\u306f\u3001\u6570\u5b66\u3001\u7269\u7406\u5b66\u3001\u5de5\u5b66\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306a\u3069\u591a\u304f\u306e\u5206\u91ce\u3067\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002\u672c\u8b1b\u7fa9\u3092\u901a\u3058\u3066\u3001\u30d9\u30af\u30c8\u30eb\u3001\u884c\u5217\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u3001\u884c\u5217\u5f0f\u3001\u7dda\u5f62\u7a7a\u9593\u3001\u7dda\u5f62\u5199\u50cf\u3001\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306a\u3069\u306e\u57fa\u672c\u7684\u306a\u6982\u5ff5\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\u3002Lecture page</p>"},{"location":"lectures/#_1","title":"\u6570\u7406\u60c5\u5831\u30ea\u30c6\u30e9\u30b7\u30fc","text":"<p>\u672c\u8b1b\u7fa9\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u790e\u3092\u5b66\u3076\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3001\u6570\u5b66\uff08\u4ee3\u6570\u3084\u5fae\u7a4d\u5206\uff09\u3084\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u4e8b\u524d\u77e5\u8b58\u3092\u5fc5\u8981\u3068\u3057\u307e\u305b\u3093\u3002\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6790\u3084\u53ef\u8996\u5316\u3001\u7d71\u8a08\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\u3001\u7d71\u8a08\u7684\u63a8\u8ad6\u3092\u5b9f\u8df5\u3057\u306a\u304c\u3089\u5b66\u3073\u307e\u3059\u3002\u4f7f\u7528\u3059\u308b\u74b0\u5883\u306f\u3001Google Colab + \u5185\u8535\u3055\u308c\u3066\u3044\u308b\u751f\u6210AI\u3092\u7528\u3044\u307e\u3059\u3002\u4f7f\u7528\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306f Python \u3067\u3042\u308a\u3001\u7279\u306b pandas \u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u6d3b\u7528\u3057\u3066\u30c7\u30fc\u30bf\u5206\u6790\u3092\u884c\u3044\u307e\u3059\u3002Lecture page</p>"},{"location":"lectures/#i-ii_1","title":"\u78ba\u7387\u3068\u7d71\u8a08I / II","text":"<p>\u672c\u8b1b\u7fa9\u306f\u3001\u78ba\u7387\u3068\u7d71\u8a08 I \u306e\u7d9a\u7de8\u3068\u3057\u3066\u3001\u78ba\u7387\u3068\u7d71\u8a08 II \u3067\u306f\u3001\u6570\u5b66\u7d71\u8a08\u5b66\u306e\u91cd\u8981\u306a\u6982\u5ff5\u3092\u5b66\u3076\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u307e\u3059\u3002\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u78ba\u7387\u5909\u6570\u306e\u95a2\u6570\u3068\u3057\u3066\u306e\u63a8\u5b9a\u91cf\u3001\u6a19\u672c\u5206\u5e03\u3001\u63a8\u5b9a\u7406\u8ad6\u3001\u304a\u3088\u3073\u63a8\u5b9a\u91cf\u306e\u6027\u8cea\u306b\u95a2\u3059\u308b\u7406\u8ad6\u3068\u5fdc\u7528\u3092\u53d6\u308a\u6271\u3044\u307e\u3059\u3002\u8b1b\u7fa9\u3067\u306f\u7dda\u5f62\u4ee3\u6570\u304a\u3088\u3073\u5fae\u7a4d\u5206\u3092\u6d3b\u7528\u3057\u3066\u7406\u8ad6\u7684\u306a\u7d50\u679c\u3092\u5c0e\u304d\u51fa\u3057\u3001\u305d\u308c\u3089\u3092\u5b9f\u969b\u306e\u554f\u984c\u306b\u9069\u7528\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002Lecture page</p>"},{"location":"lectures/#_2","title":"\u30aa\u30fc\u30d7\u30f3\u30ad\u30e3\u30f3\u30d1\u30b9\u30fb\u5b66\u5185\u767a\u8868","text":"<p>\u30aa\u30fc\u30d7\u30f3\u30ad\u30e3\u30f3\u30d1\u30b9\u3084\u5b66\u5185\u3067\u767a\u8868\u3057\u305f\u5185\u5bb9\u3092\u8a18\u8f09\u3057\u3066\u3044\u307e\u3059\u3002Lecture page</p>"},{"location":"lectures/LA/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II","text":"<p>\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u6388\u696d\u3067\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u790e\u3067\u3042\u308b\u884c\u5217\u306e\u53d6\u308a\u6271\u3044\u3084\u3001\u305d\u306e\u8a08\u7b97\u65b9\u6cd5\u306b\u3064\u3044\u3066\u53d6\u308a\u6271\u3044\u307e\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u304c\u3001\u672c\u6765\u6271\u3046\u7bc4\u56f2\u306f\u975e\u5e38\u306b\u5e83\u7bc4\u56f2\u3067\u3042\u308a\u7406\u8ad6\u3082\u8907\u96d1\u3067\u3059\u304c\u3001\u3053\u306e\u6388\u696d\u306e\u4e3b\u773c\u306f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u5fc5\u8981\u3068\u306a\u308b\u7dda\u5f62\u4ee3\u6570\u5b66\u3092\u8eab\u306b\u3064\u3051\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u7528\u3044\u3089\u308c\u308b\u624b\u6cd5\u3068\u306e\u7e4b\u304c\u308a\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p> <p>\u3053\u306e\u6388\u696d\u3067\u306f\u3001\u307e\u305a\u306f\u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u306e\u6982\u5ff5\u3092\u5b66\u3073\u3001\u305d\u306e\u3042\u3068\u3067\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u3092\u6271\u3044\u307e\u3059\u3002\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306f\u3001\u7d71\u8a08\u5b66\u30fb\u6a5f\u68b0\u5b66\u7fd2\u306b\u304a\u3044\u3066\u3001\u4e88\u6e2c\u3084\u3042\u308b\u7d50\u679c\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u539f\u56e0\u306e\u5927\u304d\u3055\u3092\u6e2c\u5b9a\u3059\u308b\u305f\u3081\u306b\u7528\u3044\u3089\u308c\u308b\u57fa\u672c\u7684\u306a\u30c4\u30fc\u30eb\u3067\u3042\u308b\u300c\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u300d\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u305d\u306e\u3042\u3068\u3067\u3001\u884c\u5217\u5f0f\u3068\u3044\u3046\u6982\u5ff5\u3068\u3001\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u76f4\u4ea4\u3068\u3044\u3046\u6982\u5ff5\u3092\u6271\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306f\u7406\u8ad6\u7684\u306a\u6982\u5ff5\u3067\u3059\u3002\u305d\u306e\u3042\u3068\u3001\u884c\u5217\u5f0f\u3068\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u76f4\u4ea4\u3092\u751f\u304b\u3059\u3053\u3068\u3067\u3001\u884c\u5217\u306b\u5bfe\u3059\u308b\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u5b66\u3073\u307e\u3059\u3002\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3001\u7d71\u8a08\u5b66\u30fb\u6a5f\u68b0\u5b66\u7fd2\u306b\u304a\u3051\u308b\u300c\u4e3b\u6210\u5206\u5206\u6790\u300d\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002\u4e3b\u6210\u5206\u5206\u6790\u306f\u3001\u4e0e\u3048\u3089\u308c\u305f\u30c7\u30fc\u30bf\u304b\u3089\u96a0\u308c\u305f\u69cb\u9020\u3092\u898b\u3064\u3051\u51fa\u3059\u305f\u3081\u306e\u624b\u6cd5\u3067\u3059\u3002\u307e\u305f\u3001\u3053\u306e\u6388\u696d\u306e\u5f8c\u534a\u306b\u304a\u3044\u3066\u306f\u3001\u884c\u5217\u5206\u89e3\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u304b\u3089\u96a0\u3055\u308c\u305f\u69cb\u9020\u3092\u767a\u898b\u3059\u308b\u30a2\u30d7\u30ed\u30fc\u30c1\u306a\u3069\u3082\u7d39\u4ecb\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306f\u3001\u7d39\u4ecb\u7a0b\u5ea6\u306b\u3068\u3069\u3081\u307e\u3059\u304c\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306f\u9762\u767d\u3044\u306a\u3068\u611f\u3058\u308b\u304d\u3063\u304b\u3051\u306b\u3057\u3066\u3082\u3089\u3048\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002</p> <p>\u3053\u306e\u6388\u696d\u3092\u901a\u3057\u3066\u3001\u7686\u3055\u3093\u306b\u671f\u5f85\u3059\u308b\u3053\u3068\u306f\u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u306e\u8a08\u7b97\u306b\u3064\u3044\u3066\u7406\u89e3\u3057\u3001\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3068\u4e3b\u6210\u5206\u5206\u6790\u306b\u3064\u3044\u3066\u6df1\u3044\u7406\u89e3\u3092\u5f97\u308b\u3053\u3068\u3067\u3059\u3002</p>"},{"location":"lectures/LA/#_1","title":"\u30b7\u30e9\u30d0\u30b9","text":"<p>\u4ee5\u4e0b\u306f\u3001\u8b1b\u7fa945\u56de+2\u56de\u5206\u306e\u8b1b\u7fa9\u30b7\u30e9\u30d0\u30b9\u3067\u3059\uff082\u56de\u5206\u306f\u671f\u672b\u8a66\u9a13\u3067\u3059\uff09\u3002</p>"},{"location":"lectures/LA/#1","title":"\u7b2c1\u90e8 : \u57fa\u790e\u6982\u5ff5\u306e\u5c0e\u5165","text":"<ol> <li> <p>\u30b3\u30fc\u30b9\u30a4\u30f3\u30c8\u30ed\u30c0\u30af\u30b7\u30e7\u30f3</p> <ul> <li>\u8b1b\u7fa9\u5168\u4f53\u306e\u6982\u8981\u3001\u8a55\u4fa1\u65b9\u6cd5\u3001\u4f7f\u7528\u30c4\u30fc\u30eb\uff08ChatGPT, Claude, Google Colab\uff09\u306e\u7d39\u4ecb  </li> <li>\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u610f\u7fa9\u3068\u73fe\u4ee3\u5fdc\u7528\u4e8b\u4f8b\u306e\u63d0\u793a</li> </ul> </li> <li> <p>\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\u3068\u57fa\u672c\u64cd\u4f5c\u2460</p> <ul> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u8868\u3057\u65b9  </li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u548c</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u6f14\u7b97</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u548c\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473  </li> <li>\u30d9\u30af\u30c8\u30eb\u306e\u548c\u30fb\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u53ef\u8996\u5316 (Google Colab)</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u64cd\u4f5c\u2461\u3068\u5185\u7a4d\u306e\u5c0e\u5165</p> <ul> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\u306e\u5b9a\u7fa9</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u306e\u5b9a\u7fa9</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u3068cos\u306e\u95a2\u4fc2</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u306e\u6027\u8cea</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u5c04\u5f71\u306e\u5b9a\u7fa9</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u5c04\u5f71\u306e\u6027\u8cea</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\uff08\u89d2\u5ea6\u3001\u5c04\u5f71\uff09  </li> <li>\u30d9\u30af\u30c8\u30eb\u5185\u7a4d\u3001\u5c04\u5f71\u306e\u53ef\u8996\u5316\u5b9f\u7fd2</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u884c\u5217\u306e\u5b9a\u7fa9\u30fb\u884c\u5217\u306e\u548c\u30fb\u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d\uff09</p> <ul> <li>\u884c\u5217\u306e\u5b9a\u7fa9</li> <li>\u884c\u5217\u306e\u8868\u8a18</li> <li>\u884c\u5217\u306e\u548c\u306e\u8a08\u7b97</li> <li>\u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d</li> <li>\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u95a2\u4fc2\u6027\u306e\u78ba\u8a8d\uff08\u30d9\u30af\u30c8\u30eb\u3092\u5217\u306b\u4e26\u3079\u308b\u3068\u884c\u5217\u306b\u306a\u308b\uff09</li> <li>Colab \u3092\u7528\u3044\u305f\u884c\u5217\u306e\u8a08\u7b97</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u884c\u5217\u306e\u7a4d  </p> <ul> <li>\u884c\u5217\u7a4d\u306e\u5b9a\u7fa9</li> <li>\u884c\u5217\u306e\u7a4d\u306e\u8a08\u7b97\u65b9\u6cd5</li> <li>\u884c\u5217\u306e\u7a4d\u306e\u6ce8\u610f\u70b9 (AB \u3068 BA\u306f\u7570\u306a\u308b\u306a\u3069)</li> <li>\u9006\u884c\u5217\u306e\u5b9a\u7fa9</li> <li>2\u6b21\u306e\u884c\u5217\u306e\u9006\u884c\u5217\u306e\u6c42\u3081\u65b9</li> <li>Google colab \u306b\u3088\u308b\u884c\u5217\u306e\u7a4d\u306e\u8a08\u7b97</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u5fc5\u8981\u306a\u884c\u5217</p> <ul> <li>\u5358\u4f4d\u884c\u5217\u3068\u305d\u306e\u6027\u8cea</li> <li>\u8ee2\u7f6e\u884c\u5217\u3068\u305d\u306e\u6027\u8cea</li> <li>\u5bfe\u79f0\u884c\u5217\u3068\u305d\u306e\u6027\u8cea</li> <li>\u5b9f\u4f8b\u3092\u901a\u3057\u3066\u3001\u4e0a\u306e4\u3064\u306e\u6027\u8cea\u3092\u78ba\u8a8d</li> <li>Google Colab\u3067\u306e\u8a08\u7b97</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>1\u6b21\u5143\u30c7\u30fc\u30bf\u3068\u30d9\u30af\u30c8\u30eb\u306e\u548c\u3068\u7a4d</p> <ul> <li>\u30c7\u30fc\u30bf\u306e\u5e73\u5747</li> <li>\u30c7\u30fc\u30bf\u306e\u504f\u5dee</li> <li>\u30c7\u30fc\u30bf\u306e\u5206\u6563</li> <li>1\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u30d9\u30af\u30c8\u30eb\u3068\u898b\u3066\u3001\u5e73\u5747\u3068\u5206\u6563\u8a08\u7b97\u3092\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u884c\u3046</li> <li>\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u306e\u5e73\u5747</li> <li>\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u306e\u504f\u5dee</li> <li>\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u306e\u5206\u6563</li> <li>\u8a08\u7b97\u306e\u5b9f\u4f8b</li> <li>Google colab\u3067\u306e\u8a08\u7b97</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>2\u6b21\u5143\u30c7\u30fc\u30bf\u3068\u884c\u5217\u306e\u7a4d</p> <ul> <li>\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563</li> <li>\u30c7\u30fc\u30bf\u306e\u76f8\u95a2\u4fc2\u6570</li> <li>2\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u884c\u5217\u3068\u898b\u3066\u3001\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97\u3092\u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u3092\u7528\u3044\u3066\u884c\u3046</li> <li>\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u5171\u5206\u6563</li> <li>\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u76f8\u95a2\u4fc2\u6570</li> <li>\u8a08\u7b97\u306e\u5b9f\u4f8b</li> <li>Google colab\u3067\u306e\u8a08\u7b97</li> <li>\u6f14\u7fd2\u8ab2\u984c</li> </ul> </li> <li> <p>\u7dcf\u5408\u6f14\u7fd2</p> <ul> <li>\u7b2c1\u56de\u301c\u7b2c8\u56de\u3067\u6271\u3063\u305f\u5185\u5bb9\u306e\u7406\u89e3\u5ea6\u3092\u78ba\u304b\u3081\u308b\u30c1\u30a7\u30c3\u30af\u554f\u984c\u3092\u51fa\u984c</li> <li>\u57fa\u790e\u7684\u306a\u5185\u5bb9\u306e\u7406\u89e3\u5ea6\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002</li> <li>\u554f\u984c\u6570\u306f6\u554f\u7a0b\u5ea6</li> <li>\u7b54\u3048\u3082\u63d0\u4f9b\u3057\u3066\u52c9\u5f37\u306b\u5f79\u7acb\u3066\u3066\u3082\u3089\u3046\u3002</li> </ul> </li> </ol>"},{"location":"lectures/LA/#2","title":"\u7b2c2\u90e8 : \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3068\u305d\u306e\u6027\u8cea","text":"<ol> <li> <p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u8868\u73fe\u3068\u89e3\u6cd5\u306e\u57fa\u790e </p> <ul> <li>\u9023\u7acb\u65b9\u7a0b\u5f0f\u3068\u306f\uff1f\uff08\u9ad8\u6821\u306e\u5fa9\u7fd2\uff09</li> <li>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe\u3068\u3001\u62e1\u5927\u4fc2\u6570\u884c\u5217</li> <li>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5</li> <li>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u5b9f\u4f8b</li> <li>2\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u30013\u51431\u6b21\u65b9\u7a0b\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8</li> <li>Google colab\u306b\u3088\u308b\u8a08\u7b97\u65b9\u6cd5</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3068\u89e3\u306e\u63a2\u7d22</p> <ul> <li>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe\u3068\u3001\u62e1\u5927\u4fc2\u6570\u884c\u5217\uff08\u5fa9\u7fd2\uff09</li> <li>\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u3088\u308b\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u6d88\u53bb\u6cd5\u306e\u5bfe\u5fdc\u3065\u3051</li> <li>\u5b9f\u4f8b\u3092\u901a\u3057\u305f\u8a08\u7b97\u65b9\u6cd5\u306e\u5b66\u7fd2</li> <li>Google Colab\u3067\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u30e9\u30f3\u30af\u306e\u6982\u5ff5\u3068\u305d\u306e\u8a08\u7b97</p> <ul> <li>\u968e\u6bb5\u884c\u5217\u306e\u5b9a\u7fa9</li> <li>\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u5b9a\u7fa9</li> <li>\u968e\u6bb5\u884c\u5217\u3068\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u4f8b</li> <li>\u884c\u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u5c0e\u51fa\u65b9\u6cd5</li> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\u306e\u5b9a\u7fa9\uff08\u968e\u6bb5\u884c\u5217\u3092\u7528\u3044\u305f\u5b9a\u7fa9\uff09</li> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\u306e\u8a08\u7b97</li> <li>\u5b9f\u4f8b\u901a\u3057\u305f\u884c\u5217\u306e\u30e9\u30f3\u30af\u8a08\u7b97</li> <li>Google colab \u3067\u306e\u884c\u5217\u306e\u30e9\u30f3\u30af\u8a08\u7b97</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e</p> <ul> <li>\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e</li> <li>\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u305f\u30601\u3064\u306b\u5b9a\u307e\u308b\u5834\u5408\u306e\u4f8b</li> <li>\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u7121\u6570\u306b\u5b58\u5728\u3059\u308b\u5834\u5408\u306e\u4f8b</li> <li>\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u7121\u6570\u306b\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306e\u4f8b</li> <li>\u5b9f\u4f8b\u3092\u901a\u3057\u305f\u89e3\u306e\u30d1\u30bf\u30fc\u30f3\u5206\u3051</li> <li>\u5b9f\u4f8b\u3092Google Colab\u3067\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3057\u56f3\u793a\u3059\u308b\u3002</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6</p> <ul> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u95a2\u4fc2</li> <li>\u5b9f\u4f8b\u3092\u901a\u3057\u305f\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5224\u5225\u3068\u89e3\u306e\u6c42\u3081\u65b9</li> <li>\u5b9f\u4f8b\u3092Google Colab\u3067\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u9006\u884c\u5217\u306e\u6982\u5ff5\u3068\u5b58\u5728\u6761\u4ef6</p> <ul> <li>\u9006\u884c\u5217\u306e\u5b9a\u7fa9</li> <li>\u9006\u884c\u5217\u306e\u6027\u8cea</li> <li>\u9006\u884c\u5217\u3092\u6c42\u3081\u308b\u3053\u3068\u306f\u3001\u8907\u6570\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u540c\u6642\u306b\u89e3\u304f\u3068\u3044\u3046\u3053\u3068\u306b\u5bfe\u5fdc\u3059\u308b\u3002</li> <li>\u9006\u884c\u5217\u3068\u884c\u5217\u306e\u30e9\u30f3\u30af\u306e\u95a2\u4fc2</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u3088\u308b\u5177\u4f53\u7684\u306a\u8a08\u7b97\u904e\u7a0b</li> <li>\u9006\u884c\u5217\u8a08\u7b97\u3092step by step \u3067 Google colab\u3067\u5b9f\u884c\u3059\u308b</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u3068\u5358\u56de\u5e30\u30e2\u30c7\u30eb</p> <ul> <li>\u7d71\u8a08\u30e2\u30c7\u30eb\u3068\u306f\u4f55\u304b\uff1f</li> <li>\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3068\u306f\uff1f</li> <li>\u8aac\u660e\u5909\u6570\u3068\u53cd\u5fdc\u5909\u6570\u3068\u306f\uff1f</li> <li>\u5207\u7247\u3068\u56de\u5e30\u4fc2\u6570\u3068\u306f\uff1f</li> <li>\u30c7\u30fc\u30bf\u3078\u30e2\u30c7\u30eb\u3092\u5f53\u3066\u306f\u3081\u308b\u3068\u306f\uff1f</li> <li>\u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u3092\u4f7f\u3063\u3066\u3001\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u8868\u305d\u3046\u3002</li> <li>\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u304b\u3089\u3001\u8fd1\u4f3c\u7684\u306b\u89e3\u3044\u3066\u4e00\u756a\u3044\u3044\u76f4\u7dda\u3092\u63a2\u3059\u3002</li> <li>\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u5207\u7247\u3068\u50be\u304d\u306e\u63a8\u5b9a</li> <li>\u6b63\u898f\u65b9\u7a0b\u5f0f\u3068\u306f\u4f55\u304b\uff1f</li> <li>google colab \u3067\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u304c\u4f55\u3092\u3057\u3066\u3044\u308b\u306e\u304b\u30a4\u30e1\u30fc\u30b8\u3059\u308b</li> <li>google colab \u3067\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u89e3\u304f</li> <li>google colab \u3067\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u306b\u5f53\u3066\u306f\u3081\u308b</li> <li>google colab \u3067\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u89e3\u91c8\u3059\u308b</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u3068\u9006\u884c\u5217</p> <ul> <li>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u3068\u306f</li> <li>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u3068\u306f</li> <li>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u306e\u5224\u5b9a\uff08\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\uff09</li> <li>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u3068\u9006\u884c\u5217\u306e\u5b58\u5728</li> <li>1\u6b21\u72ec\u7acb\u3067\u306f\u306a\u3044\u30d9\u30af\u30c8\u30eb\u304c\u3042\u308b\u5834\u5408\u306e\u9006\u884c\u5217\u306e\u4e0d\u5b89\u5b9a\u6027</li> <li>Google Colab \u30671\u6b21\u72ec\u7acb\u3092\u8996\u899a\u7684\u306b\u7406\u89e3\u3059\u308b</li> <li>\u6f14\u7fd2\u554f\u984c(1\u6b21\u72ec\u7acb\u3068\u9006\u884c\u5217)</li> </ul> </li> <li> <p>\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u30682\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb</p> <ul> <li>2\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb</li> <li>\u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u3092\u4f7f\u3063\u3066\u30012\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u8868\u305d\u3046\u3002</li> <li>2\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u3001\u5e73\u9762\u306e\u63a8\u5b9a\u3092\u3057\u3066\u3044\u308b\u3002</li> <li>2\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u89e3\u91c8</li> <li>\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u63a8\u5b9a</li> <li>2\u3064\u306e\u5909\u6570\u3068\u5207\u7247\u304c\u3042\u308b\u5834\u5408\u306e\u6b63\u898f\u65b9\u7a0b\u5f0f</li> <li>google colab \u30672\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u304c\u4f55\u3092\u3057\u3066\u3044\u308b\u306e\u304b\u30a4\u30e1\u30fc\u30b8\u3059\u308b</li> <li>google colab \u30672\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u89e3\u304f</li> <li>google colab \u30672\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u306b\u5f53\u3066\u306f\u3081\u308b</li> <li>google colab \u30672\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u89e3\u91c8\u3059\u308b</li> <li>\u6f14\u7fd2\u554f\u984c(2\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb)</li> </ul> </li> <li> <p>\u7dcf\u5408\u6f14\u7fd2</p> <ul> <li>\u7b2c10\u56de\u301c\u7b2c19\u56de\u3067\u6271\u3063\u305f\u5185\u5bb9\u306e\u7406\u89e3\u5ea6\u3092\u78ba\u304b\u3081\u308b\u30c1\u30a7\u30c3\u30af\u554f\u984c\u3092\u51fa\u984c</li> <li>\u57fa\u790e\u7684\u306a\u5185\u5bb9\u306e\u7406\u89e3\u5ea6\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002</li> <li>\u554f\u984c\u6570\u306f6\u554f\u7a0b\u5ea6</li> <li>\u7b54\u3048\u3082\u63d0\u4f9b\u3057\u3066\u52c9\u5f37\u306b\u5f79\u7acb\u3066\u3066\u3082\u3089\u3046\u3002</li> </ul> </li> <li> <p>\u4e2d\u9593\u8a66\u9a13\uff08Midterm Exam\uff09</p> <ul> <li>\u7b2c1\u56de\u301c\u7b2c18\u56de\u307e\u3067\u306e\u5185\u5bb9\u304b\u30894\u554f\u51fa\u984c\u3059\u308b</li> <li>\u6642\u9593\u306f60\u5206</li> </ul> </li> </ol>"},{"location":"lectures/LA/#3","title":"\u7b2c3\u90e8 : \u884c\u5217\u5f0f","text":"<ol> <li> <p>\u884c\u5217\u5f0f\u306e\u5b9a\u7fa9\u3068\u57fa\u672c\u6027\u8cea</p> <ul> <li>\u884c\u5217\u5f0f\u3068\u306f\uff1f</li> <li>2\u6b21\u30fb3\u6b21\u306e\u884c\u5217\u5f0f\u306e\u5b9a\u7fa9</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u3067\u306e\u5358\u4f4d\u884c\u5217\u306b\u5bfe\u3059\u308b\u884c\u5217\u5f0f</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u3067\u306e\u884c\u5217\u5f0f\u306e\u591a\u91cd\u7dda\u5f62\u6027</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u3067\u306e\u884c\u5217\u5f0f\u306e\u4ea4\u4ee3\u6027</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u3067\u306e\u884c\u5217\u306e\u7a4d\u306e\u884c\u5217\u5f0f\u306e\u6027\u8cea</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u3067\u306e\u9006\u884c\u5217\u306e\u5b58\u5728\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2</li> <li>google colab \u3067 1\u6b21\u30012\u6b21\u30013\u6b21\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u57fa\u672c\u5909\u5f62\u3068\u884c\u5217\u5f0f\u306e\u8a08\u7b97</p> <ul> <li>\u57fa\u672c\u5909\u5f62\u3068\u306f\uff08\u5fa9\u7fd2\uff09</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u306b\u5bfe\u3059\u308b\u57fa\u672c\u5909\u5f62\u3068\u3001\u884c\u5217\u5f0f\u306e\u5024\u306e\u5909\u5316</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u306e\u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\u3068\u884c\u5217\u5f0f\u306e\u5024\u306e\u95a2\u4fc2</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3068\u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6\u306e\u95a2\u4fc2\u3092\u3001\u57fa\u672c\u5909\u5f62\u3068\u30e9\u30f3\u30af\u304b\u3089\u8003\u5bdf\u3059\u308b</li> <li>\u5b9f\u4f8b\u3092\u901a\u3057\u305f\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u305f\u884c\u5217\u5f0f\u306e\u8a08\u7b97</li> <li>google colab \u3067 2\u6b21\u30fb3\u6b21\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u884c\u5217\u5f0f\u306e\u5024\u306e\u5909\u5316\u3092\u5b9f\u6f14</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u306b\u3088\u308b\u884c\u5217\u5f0f\u306e\u8a08\u7b97</p> <ul> <li>\u9084\u5143\u5b9a\u7406\u306e\u5b9a\u7fa9</li> <li>\u4f59\u56e0\u5b50\u306e\u5b9a\u7fa9</li> <li>\u4f59\u56e0\u5b50\u5c55\u958b\u306e\u5b9a\u7fa9</li> <li>\u9084\u5143\u5b9a\u7406\u3001\u4f59\u56e0\u5b50\u5c55\u958b\u3067\u884c\u5217\u5f0f\u306e\u6b21\u6570\u3092\u4e0b\u3052\u308b</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u306b\u5bfe\u3059\u308b\u9084\u5143\u5b9a\u7406\u306b\u3088\u308b\u884c\u5217\u5f0f\u306e\u8a08\u7b97</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u306b\u5bfe\u3059\u308b\u4f59\u56e0\u5b50\u5c55\u958b\u306b\u3088\u308b\u884c\u5217\u5f0f\u306e\u8a08\u7b97</li> <li>\u57fa\u672c\u5909\u5f62\u304b\u3089\u9084\u5143\u5b9a\u7406\u304c\u4f7f\u3048\u308b\u5f62\u306b\u884c\u5217\u5f0f\u3092\u5909\u5f62\u3059\u308b</li> <li>google colab \u3067 2\u6b21\u30013\u6b21\u306e\u884c\u5217\u5f0f\u3092\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u304b\u3089\u8a08\u7b97\u3059\u308b</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>4\u6b21\u4ee5\u4e0a\u306e\u884c\u5217\u5f0f\u306e\u8a08\u7b97</p> <ul> <li>\u57fa\u672c\u5909\u5f62\u3001\u9084\u5143\u5b9a\u7406\u3001\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u7528\u3044\u305f4\u6b21\u4ee5\u4e0a\u306e\u884c\u5217\u5f0f\u306e\u8a08\u7b97</li> <li>4\u6b21\u4ee5\u4e0a\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u30b3\u30c4</li> <li>google colab \u3092\u7528\u3044\u3066 4\u6b21\u4ee5\u4e0a\u306e\u884c\u5217\u5f0f\u3092\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u304b\u3089\u8a08\u7b97\u3059\u308b</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u7dcf\u5408\u6f14\u7fd2</p> <ul> <li>\u7b2c21\u56de\u301c\u7b2c24\u56de\u3067\u6271\u3063\u305f\u5185\u5bb9\u306e\u7406\u89e3\u5ea6\u3092\u78ba\u304b\u3081\u308b\u30c1\u30a7\u30c3\u30af\u554f\u984c\u3092\u51fa\u984c</li> <li>\u57fa\u790e\u7684\u306a\u5185\u5bb9\u306e\u7406\u89e3\u5ea6\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002</li> <li>\u554f\u984c\u6570\u306f6\u554f\u7a0b\u5ea6</li> <li>\u7b54\u3048\u3082\u63d0\u4f9b\u3057\u3066\u52c9\u5f37\u306b\u5f79\u7acb\u3066\u3066\u3082\u3089\u3046\u3002</li> </ul> </li> </ol>"},{"location":"lectures/LA/#4","title":"\u7b2c4\u90e8: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u57fa\u5e95","text":"<ol> <li> <p>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u790e</p> <ul> <li>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u306f\uff1f</li> <li>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u5b9a\u7fa9\u30fb\u6027\u8cea</li> <li>\u90e8\u5206\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u5b9a\u7fa9\u30fb\u6027\u8cea</li> <li>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3068\u3001\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u5e95</li> <li>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u6b21\u5143</li> <li>2\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u30013\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u5177\u4f53\u4f8b\u3067\u7406\u89e3\u3092\u6df1\u3081\u308b</li> <li>colab \u3067 2\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u30013\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u5e95\u3092\u53ef\u8996\u5316\u3059\u308b</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u5185\u7a4d\u3068\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u30fb\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5</p> <ul> <li>\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u3068\u306f\uff1f</li> <li>\u5185\u7a4d\u7a7a\u9593\u3068\u306f\uff1f</li> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306b\u5bfe\u3059\u308b\u5185\u7a4d\u306e\u5b9a\u7fa9\u3068\u3001\u306a\u3059\u89d2</li> <li>\u5185\u7a4d\u306e\u6027\u8cea</li> <li>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u76f4\u4ea4\u57fa\u5e95</li> <li>\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3068\u30b0\u30e9\u30e0\u30fb\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5</li> <li>\u5185\u7a4d\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u3001Google Colab\u3067\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3</li> <li>\u30b0\u30e9\u30e0\u30fb\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u3092\u3001google colab\u3067\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3068\u5c04\u5f71</p> <ul> <li>\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\uff08\u5fa9\u7fd2\uff09</li> <li>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u76f4\u548c\u5206\u89e3\u3001\u76f4\u4ea4\u76f4\u548c\u5206\u89e3\u3001\u76f4\u4ea4\u88dc\u7a7a\u9593\u306e\u5b9a\u7fa9</li> <li>\u76f4\u4ea4\u5c04\u5f71\u3068\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306e\u95a2\u4fc2</li> <li>\u5c04\u5f71\u884c\u5217\u3068\u306f</li> <li>\u76f4\u4ea4\u5c04\u5f71\u884c\u5217</li> <li>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3068\u76f4\u4ea4\u5c04\u5f71\u884c\u5217</li> <li>\u76f4\u548c\u5206\u89e3\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u3001Google Colab\u3067\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3</li> <li>\u76f4\u4ea4\u5c04\u5f71\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u3001Google Colab\u3067\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u7dcf\u5408\u6f14\u7fd2</p> <ul> <li>\u7b2c26\u56de\u301c\u7b2c28\u56de\u3067\u6271\u3063\u305f\u5185\u5bb9\u306e\u7406\u89e3\u5ea6\u3092\u78ba\u304b\u3081\u308b\u30c1\u30a7\u30c3\u30af\u554f\u984c\u3092\u51fa\u984c</li> <li>\u57fa\u790e\u7684\u306a\u5185\u5bb9\u306e\u7406\u89e3\u5ea6\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002</li> <li>\u554f\u984c\u6570\u306f6\u554f\u7a0b\u5ea6</li> <li>\u7b54\u3048\u3082\u63d0\u4f9b\u3057\u3066\u52c9\u5f37\u306b\u5f79\u7acb\u3066\u3066\u3082\u3089\u3046\u3002</li> </ul> </li> <li> <p>\u30ec\u30dd\u30fc\u30c8\u8ab2\u984c\uff08Report Exam\uff09</p> <ul> <li>\u6388\u696d\u3067\u6271\u3063\u305f\u7dda\u5f62\u4ee3\u6570\u3068\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u95a2\u3059\u308b\u30ec\u30dd\u30fc\u30c8\u8ab2\u984c</li> </ul> </li> <li> <p>\u671f\u672b\u8a66\u9a13</p> <ul> <li>\u7b2c21\u56de\u301c\u7b2c29\u56de\u306e\u5185\u5bb9\u306b\u95a2\u3059\u308b\u8a66\u9a13</li> <li>\u554f\u984c\u6570\u306f4\u554f</li> <li>\u6642\u9593\u306f60\u5206</li> </ul> </li> </ol>"},{"location":"lectures/LA/#5","title":"\u7b2c5\u90e8 : \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb","text":"<ol> <li> <p>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u305f\u3081\u306e\u7dda\u5f62\u5909\u63db\u306e\u57fa\u790e</p> <ul> <li>\u3053\u306e\u56de\u306e\u6388\u696d\u306f\u3001\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u5b66\u3076\u305f\u3081\u306b\u5fc5\u8981\u306a\u7dda\u5f62\u5909\u63db\u3092\u5b66\u3073\u307e\u3059\u3002</li> <li>\u7dda\u5f62\u5909\u63db\u306e\u5b9a\u7fa9</li> <li>\u884c\u5217\u306b\u3088\u308b\u7dda\u5f62\u5909\u63db</li> <li>\u7dda\u5f62\u5909\u63db\u306e\u4f8b</li> <li>Google Colab\u306b\u3088\u308b\u7dda\u5f62\u5909\u63db\u306e\u53ef\u8996\u5316</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb</p> <ul> <li>\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9</li> <li>\u56fa\u6709\u591a\u9805\u5f0f\u3068\u7279\u6027\u65b9\u7a0b\u5f0f\u306e\u5b9a\u7fa9</li> <li>\u7279\u6027\u65b9\u7a0b\u5f0f\u306b\u3088\u308b\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97</li> <li>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5e7e\u4f55\u7684\u89e3\u91c8</li> <li>2\u6b21\u5143\u30fb3\u6b21\u5143\u3067\u306e\u56f3\u793a\u3068\u76f4\u611f\u7684\u7406\u89e3</li> <li>\u56fa\u6709\u5024\u306e\u7dcf\u548c\u3068\u30c8\u30ec\u30fc\u30b9</li> <li>\u56fa\u6709\u5024\u306e\u7a4d\u3068\u884c\u5217\u5f0f</li> <li>Colab\u306b\u3088\u308b\u53ef\u8996\u5316</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u7279\u6027\u65b9\u7a0b\u5f0f\u3068\u5bfe\u89d2\u5316</p> <ul> <li>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u6c42\u3081\u65b9\u306e\u5fa9\u7fd2</li> <li>\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306e\u5b9a\u7fa9</li> <li>\u5bfe\u89d2\u5316\u306f\u53ef\u80fd\u3067\u3042\u308b\u305f\u3081\u306e\u6761\u4ef6</li> <li>\u5bfe\u89d2\u5316\u306e\u624b\u9806\u3068\u8a08\u7b97\u65b9\u6cd5</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306e\u5177\u4f53\u4f8b</li> <li>Colab\u306b\u3088\u308b\u8a08\u7b97</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u5bfe\u79f0\u884c\u5217\u3068\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316</p> <ul> <li>\u5bfe\u79f0\u884c\u5217\u306e\u5b9a\u7fa9</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u304c\u5b9f\u6570\u3067\u3042\u308b\u3053\u3068</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u76f4\u4ea4\u6027</li> <li>\u76f4\u4ea4\u884c\u5217\u306e\u5b9a\u7fa9</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\u3068\u305d\u306e\u624b\u9806</li> <li>\u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u91cd\u8981\u6027</li> <li>Colab\u306b\u3088\u308b\u8a08\u7b97</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u305f\u3081\u306e2\u6b21\u5f62\u5f0f\u3068\u57fa\u790e</p> <ul> <li>2\u6b21\u5f62\u5f0f\u306e\u5b9a\u7fa9</li> <li>\u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024\u306e\u5b9a\u7fa9</li> <li>\u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024\u3068\u56fa\u6709\u5024\u306e\u95a2\u4fc2\u6027</li> <li>\u6b63\u5b9a\u5024\u884c\u5217\u3068\u5206\u6563\u5171\u5206\u6563\u884c\u5217</li> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b2\u6b21\u5f62\u5f0f\u306e\u6d3b\u7528</li> <li>Colab\u306b\u3088\u308b\u56f3\u793a\u3068\u8a08\u7b97</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u7dcf\u5408\u6f14\u7fd2</p> <ul> <li>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u304c\u3067\u304d\u308b\u304b\u306e\u6f14\u7fd2</li> <li>2\u6b21\u5f62\u5f0f\u3068\u6b63\u5b9a\u5024\u884c\u5217</li> <li>\u5bfe\u89d2\u5316\u3092\u5229\u7528\u3057\u305f \\(A^n\\) \u3084\u6570\u5217\u306e\u4e00\u822c\u9805\u306e\u8a08\u7b97</li> </ul> </li> <li> <p>\u4e2d\u9593\u8a66\u9a13\uff08Midterm Exam\uff09</p> <ul> <li>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u3092\u4e2d\u5fc3\u3068\u3057\u305f\u554f\u984c\u30922\u554f\u51fa\u984c</li> <li>\u5bfe\u89d2\u5316\u3092\u5229\u7528\u3057\u305f \\(A^n\\) \u3084\u6570\u5217\u306e\u4e00\u822c\u9805\u306e\u8a08\u7b97\u306e\u554f\u984c\u30922\u554f</li> </ul> </li> </ol>"},{"location":"lectures/LA/#6","title":"\u7b2c6\u90e8 : \u7dda\u5f62\u4ee3\u6570\u5fdc\u7528\u3068\u3057\u3066\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9","text":"<ol> <li> <p>\u7279\u7570\u5024\u5206\u89e3\u306e\u57fa\u790e</p> <ul> <li>\u7279\u7570\u5024\u306e\u6982\u5ff5\u3068\u91cd\u8981\u6027</li> <li>\u7279\u7570\u5024\u5206\u89e3(SVD)\u306e\u5b9a\u7fa9\u3068\u7279\u7570\u5024\u30fb\u7279\u7570\u30d9\u30af\u30c8\u30eb</li> <li>SVD\u306e\u5b58\u5728\u5b9a\u7406</li> <li>\u7279\u7570\u5024\u5206\u89e3\u306e\u8a08\u7b97\u6cd5</li> <li>\u7279\u7570\u5024\u3068\u56fa\u6709\u5024\u306e\u95a2\u4fc2\u3001\u5de6\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97</li> <li>\u7279\u7570\u5024\u5206\u89e3\u306e\u5e7e\u4f55\u7684\u306a\u89e3\u91c8</li> <li>\u7dda\u5f62\u5909\u63db\u306e\u5206\u89e3\u3068\u3057\u3066\u306eSVD, \u5358\u4f4d\u7403\u306e\u5909\u63db\u3001\u4e3b\u8ef8\u3078\u306e\u56de\u8ee2\u3068\u4f38\u7e2e</li> <li>Colab \u306b\u3088\u308b\u7279\u7570\u5024\u5206\u89e3\u3068\u56f3\u793a\u306b\u3088\u308b\u8996\u899a\u7684\u7406\u89e3</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u7279\u7570\u5024\u5206\u89e3\u306e\u5fdc\u7528</p> <ul> <li>SVD\u306e\u5fa9\u7fd2</li> <li>\u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\u3068\u3001SVD\u3092\u7528\u3044\u305f\u8a08\u7b97</li> <li>\u64ec\u4f3c\u9006\u884c\u5217\u306e\u5fdc\u7528\u5148\u306e\u7d39\u4ecb</li> <li>\u884c\u5217\u306e\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c<ul> <li>\u30a8\u30c3\u30ab\u30fc\u30c8\u30fb\u30e4\u30f3\u30b0\u306e\u5b9a\u7406\u306e\u7d39\u4ecb</li> <li>\u30c8\u30e9\u30f3\u30b1\u30fc\u30b7\u30e7\u30f3SVD\u3068\u6700\u9069\u8fd1\u4f3c</li> <li>\u8fd1\u4f3c\u8aa4\u5dee\u8a55\u4fa1</li> </ul> </li> <li>\u30c7\u30fc\u30bf\u5727\u7e2e\u3068\u30ce\u30a4\u30ba\u9664\u53bb<ul> <li>SVD\u306b\u3088\u308b\u753b\u50cf\u5727\u7e2e</li> <li>\u4fe1\u53f7\u51e6\u7406\u306b\u304a\u3051\u308bSVD</li> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u3078\u306eSVD\u306e\u5fdc\u7528</li> </ul> </li> <li>Colab \u306b\u3088\u308bSVD\u306e\u5b9f\u6f14</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u5c0e\u5165</p> <ul> <li>\u4e3b\u6210\u5206\u5206\u6790\u306e\u5c0e\u5165</li> <li>\u4e3b\u6210\u5206\u5206\u6790\u306e\u76ee\u7684</li> <li>\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u3068\u76f8\u95a2\u884c\u5217\u306e\u5b9a\u7fa9\u3068\u6027\u8cea</li> <li>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306b\u57fa\u3065\u304f\u4e3b\u6210\u5206\u5206\u6790<ul> <li>\u5206\u6563\u6700\u5927\u5316\u65b9\u5411\u3092\u6c42\u3081\u308b\u610f\u7fa9</li> <li>\u5206\u6563\u6700\u5927\u5316\u65b9\u5411\uff08\u4e3b\u6210\u5206\uff09\u306e\u6570\u5b66\u7684\u5c0e\u51fa</li> <li>\u4e3b\u6210\u5206\u306e\u3068\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306e\u95a2\u4fc2</li> <li>\u4e3b\u6210\u5206\u306e\u5e7e\u4f55\u7684\u89e3\u91c8</li> </ul> </li> <li>\u76f8\u95a2\u884c\u5217\u306b\u57fa\u3065\u304f\u4e3b\u6210\u5206\u5206\u6790<ul> <li>\u306a\u305c\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3067\u306f\u306a\u304f\u3001\u76f8\u95a2\u884c\u5217\u306b\u57fa\u3065\u304f\u4e3b\u6210\u5206\u5206\u6790\u3092\u884c\u3046\u304b</li> <li>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3092\u7528\u3044\u305f\u5834\u5408\u3068\u3001\u76f8\u95a2\u884c\u5217\u3092\u7528\u3044\u305f\u5834\u5408\u306e\u4e3b\u6210\u5206\u5206\u6790\u306e\u7d50\u679c\u306e\u6bd4\u8f03</li> </ul> </li> <li>Google Colab \u3067\u306e\u5b9f\u6f14\u3068\u8996\u899a\u7684\u7406\u89e3</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u4e3b\u6210\u5206\u5206\u6790\u306e\u7d50\u679c\u306e\u89e3\u91c8\u3068\u6b21\u5143\u524a\u6e1b</p> <ul> <li>\u4e3b\u6210\u5206\u5206\u6790\u306e\u5fa9\u7fd2\uff08\u6982\u5ff5\u30fb\u5c0e\u51fa\u904e\u7a0b\uff09</li> <li>\u4e3b\u6210\u5206\u5206\u6790\u306e\u7d50\u679c\u306e\u89e3\u91c8<ul> <li>\u4e3b\u6210\u5206\u5206\u6790\u306e\u51fa\u529b\u7d50\u679c\u3068\u305d\u306e\u89e3\u91c8\u306b\u3064\u3044\u3066\u3001\u5065\u5eb7\u30fb\u533b\u7642\u30c7\u30fc\u30bf\u306e\u4f8b\u3092\u901a\u3057\u3066\u5b66\u3076</li> <li>\u4e3b\u6210\u5206\u306b\u5bfe\u3059\u308b\u89e3\u91c8\u3092\u3069\u306e\u3088\u3046\u306b\u884c\u3046\u304b\uff1f</li> <li>\u56fa\u6709\u5024\u30fb\u6bd4\u7387\u30fb\u7d2f\u7a4d</li> <li>\u4e3b\u6210\u5206\u3068\u4e3b\u6210\u5206\u30b9\u30b3\u30a2</li> <li>\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3092\u5229\u7528\u3057\u305f\u5916\u308c\u5024\u306e\u7279\u5b9a</li> <li>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8</li> <li>\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8</li> <li>Google Colab\u3067\u5b9f\u884c\u3067\u304d\u308b\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3057\u3001\u56f3\u306a\u3069\u3092\u52b9\u679c\u7684\u306b\u7528\u3044\u306a\u304c\u3089\u7406\u89e3\u3092\u6df1\u3081\u308b\u3002</li> </ul> </li> <li>\u4e3b\u6210\u5206\u56de\u5e30 (Principal component regression) \u3068\u305d\u306e\u5fdc\u7528<ul> <li>\u4e3b\u6210\u5206\u56de\u5e30\u3068\u306f\uff1f</li> <li>\u4e3b\u6210\u5206\u56de\u5e30\u306e\u76ee\u7684\u3068\u610f\u7fa9</li> <li>\u5065\u5eb7\u30fb\u533b\u7642\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e3b\u6210\u5206\u56de\u5e30\u3068\u305d\u306e\u7d50\u679c\u306e\u89e3\u91c8</li> </ul> </li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u7279\u7570\u5024\u5206\u89e3\u30fb\u4e3b\u6210\u5206\u5206\u6790\u3067\u306e\u5fdc\u7528\u4f8b</p> <ul> <li>\u7279\u7570\u5024\u5206\u89e3\u306e\u5fa9\u7fd2</li> <li>\u4e3b\u6210\u5206\u5206\u6790\u306e\u5fa9\u7fd2</li> <li>\u7279\u7570\u5024\u5206\u89e3\u30fb\u4e3b\u6210\u5206\u5206\u6790\u306e\u5fdc\u7528\u4f8b<ul> <li>\u533b\u7642\u753b\u50cf\u51e6\u7406</li> <li>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u89e3\u6790</li> <li>\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u306e\u30c7\u30fc\u30bf\u5206\u6790</li> </ul> </li> <li>Colab \u3067\u306e\u5b9f\u6f14\u3068\u8996\u899a\u7684\u7406\u89e3</li> <li>\u6f14\u7fd2\u554f\u984c</li> </ul> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u306e\u57fa\u790e</p> <ul> <li>\u56e0\u5b50\u5206\u6790\u306e\u6982\u5ff5</li> <li>\u56e0\u5b50\u5206\u6790\u306e\u76ee\u7684</li> <li>\u56e0\u5b50\u5206\u6790\u306b\u304a\u3051\u308b\u6f5c\u5728\u5909\u6570\u30e2\u30c7\u30eb<ul> <li>\u89b3\u6e2c\u5909\u6570\u3068\u6f5c\u5728\u5909\u6570\u306e\u95a2\u4fc2</li> <li>\u56e0\u5b50\u5206\u6790\u306e\u6570\u5b66\u7684\u306a\u30e2\u30c7\u30eb</li> <li>PCA\u3068\u306e\u9055\u3044</li> </ul> </li> <li>\u56e0\u5b50\u8ca0\u8377\u91cf<ul> <li>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u5b9a\u7fa9\u3068\u89e3\u91c8</li> <li>\u56e0\u5b50\u30d1\u30bf\u30fc\u30f3\u884c\u5217</li> </ul> </li> <li>\u5171\u901a\u6027\u3068\u72ec\u81ea\u6027<ul> <li>\u5171\u901a\u56e0\u5b50\u3068\u72ec\u81ea\u56e0\u5b50</li> <li>\u5171\u901a\u6027\u306e\u6982\u5ff5\u3068\u8a08\u7b97</li> <li>\u72ec\u81ea\u6027\u306e\u610f\u5473\u3068\u91cd\u8981\u6027</li> </ul> </li> <li>Colab \u3067\u306e\u5b9f\u6f14\u3068\u8996\u899a\u7684\u306a\u7406\u89e3</li> </ul> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u306e\u63a8\u5b9a\u6cd5</p> <ul> <li>\u56e0\u5b50\u5206\u6790\u306e\u57fa\u672c\u30e2\u30c7\u30eb\u306e\u5fa9\u7fd2</li> <li>\u6700\u5c24\u63a8\u5b9a\u306b\u3088\u308b\u56e0\u5b50\u306e\u63a8\u5b9a</li> <li>\u56e0\u5b50\u56de\u8ee2<ul> <li>\u56de\u8ee2\u306e\u76ee\u7684</li> <li>\u76f4\u4ea4\u56de\u8ee2\uff08varimax\u6cd5\uff09</li> <li>\u659c\u4ea4\u56de\u8ee2\uff08promax\u6cd5\uff09</li> <li>\u5358\u7d14\u69cb\u9020\u306e\u6982\u5ff5</li> </ul> </li> <li>Colab \u3067\u306e\u5b9f\u6f14\u3068\u8996\u899a\u7684\u306a\u7406\u89e3</li> </ul> </li> <li> <p>\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u6cd5\uff1at-SNE\u3068UMAP</p> <ul> <li> <p>\u5c0e\u5165\u3068\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u306e\u5fc5\u8981\u6027\uff0810\u5206\uff09</p> <ul> <li>\u524d\u56de\u307e\u3067\u306e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\uff08PCA\u30fb\u56e0\u5b50\u5206\u6790\uff09\u306e\u9650\u754c</li> <li>\u975e\u7dda\u5f62\u69cb\u9020\u3092\u6301\u3064\u30c7\u30fc\u30bf\u306e\u4f8b\uff08\u533b\u7642\u753b\u50cf\u3001\u30b2\u30ce\u30e0\u30c7\u30fc\u30bf\u3001\u751f\u4f53\u4fe1\u53f7\uff09</li> <li>\u30de\u30cb\u30d5\u30a9\u30fc\u30eb\u30c9\u4eee\u8aac\u306e\u8aac\u660e</li> <li>\u5c40\u6240\u7684\u69cb\u9020\u3068\u5927\u57df\u7684\u69cb\u9020\u306e\u4fdd\u5b58\u306e\u91cd\u8981\u6027</li> </ul> </li> <li> <p>t-SNE\uff08t-distributed Stochastic Neighbor Embedding\uff09\u306e\u7406\u8ad6</p> <ul> <li>\u57fa\u672c\u6982\u5ff5\u3068\u958b\u767a\u80cc\u666f</li> <li>\u6570\u5b66\u7684\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af<ul> <li>\u9ad8\u6b21\u5143\u7a7a\u9593\u3067\u306e\u6761\u4ef6\u4ed8\u304d\u78ba\u7387\u306e\u8a08\u7b97</li> <li>\u4f4e\u6b21\u5143\u7a7a\u9593\u3067\u306e\u78ba\u7387\u5206\u5e03\uff08t\u5206\u5e03\u306e\u63a1\u7528\u7406\u7531\uff09</li> <li>Kullback-Leibler\u767a\u6563\u306e\u6700\u5c0f\u5316</li> </ul> </li> <li>\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5f79\u5272<ul> <li>\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\u306e\u6982\u5ff5\u3068\u8a2d\u5b9a</li> <li>\u5b66\u7fd2\u7387\u3068\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570</li> </ul> </li> <li>\u8a08\u7b97\u91cf\u306e\u554f\u984c\u3068\u9ad8\u901f\u5316\u624b\u6cd5\uff08Barnes-Hut\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff09</li> </ul> </li> <li> <p>UMAP\uff08Uniform Manifold Approximation and Projection\uff09\u306e\u7406\u8ad6\uff0815\u5206\uff09</p> <ul> <li>\u57fa\u672c\u6982\u5ff5\u3068\u958b\u767a\u80cc\u666f</li> </ul> </li> <li>\u6570\u5b66\u7684\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af<ul> <li>\u30ea\u30fc\u30de\u30f3\u5e7e\u4f55\u5b66\u3068\u30c8\u30dd\u30ed\u30b8\u30fc\u306e\u89b3\u70b9</li> <li>\u5c40\u6240\u7684\u8ddd\u96e2\u95a2\u4fc2\u306e\u30e2\u30c7\u30ea\u30f3\u30b0</li> <li>\u30d5\u30a1\u30b8\u30fc\u96c6\u5408\u7406\u8ad6\u306e\u5fdc\u7528</li> <li>\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\u306b\u3088\u308b\u6700\u9069\u5316</li> </ul> </li> <li> <p>\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5f79\u5272</p> <ul> <li>\u8fd1\u508d\u6570</li> <li>\u6700\u5c0f\u8ddd\u96e2</li> <li>\u30cd\u30ac\u30c6\u30a3\u30d6\u30b5\u30f3\u30d7\u30eb\u6bd4\u7387</li> </ul> </li> <li> <p>t-SNE\u3068UMAP\u306e\u7c21\u5358\u306a\u6bd4\u8f03\uff08\u7d50\u679c\u306e\u307f\uff09</p> <ul> <li>\u3069\u306e\u3088\u3046\u306a\u89b3\u70b9\u3067\u4f7f\u3044\u5206\u3051\u304c\u3067\u304d\u308b\u304b</li> </ul> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u79d1\u5b66\u306b\u304a\u3051\u308b\u5fdc\u7528\u4f8b\uff085\u5206\uff09</p> <ul> <li>\u5358\u4e00\u7d30\u80deRNA\u30b7\u30fc\u30b1\u30f3\u30b9\u30c7\u30fc\u30bf\u306e\u8996\u899a\u5316</li> <li>\u533b\u7642\u753b\u50cf\u306e\u7279\u5fb4\u7a7a\u9593\u306b\u304a\u3051\u308b\u60a3\u8005\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0</li> <li>\u751f\u7406\u5b66\u7684\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u30d1\u30bf\u30fc\u30f3\u767a\u898b</li> <li>\u533b\u85ac\u54c1\u958b\u767a\u306b\u304a\u3051\u308b\u5206\u5b50\u69cb\u9020\u306e\u985e\u4f3c\u6027\u30de\u30c3\u30d4\u30f3\u30b0</li> </ul> </li> <li> <p>\u6f14\u7fd2\u554f\u984c</p> </li> </ul> </li> <li> <p>\u7dda\u5f62\u4ee3\u6570\u306e\u5fdc\u7528\u3068\u3057\u3066\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9</p> <ul> <li>\u8b1b\u7fa9\u5168\u4f53\u306e\u7dcf\u62ec<ul> <li>\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u6982\u5ff5</li> <li>SVD</li> <li>\u4e3b\u6210\u5206\u5206\u6790</li> <li>\u56e0\u5b50\u5206\u6790</li> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u91cd\u8981\u6027</li> </ul> </li> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u5fdc\u7528\u4e8b\u4f8b<ul> <li>\u533b\u7642\u753b\u50cf\u89e3\u6790\u306b\u304a\u3051\u308bSVD</li> <li>\u751f\u4f53\u4fe1\u53f7\u306ePCA\u306b\u3088\u308b\u7279\u5fb4\u62bd\u51fa</li> <li>\u5065\u5eb7\u8cea\u554f\u7968\u306e\u56e0\u5b50\u5206\u6790</li> <li>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b</li> </ul> </li> <li>\u6700\u65b0\u7814\u7a76\u52d5\u5411<ul> <li>\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u6cd5\uff08t-SNE, UMAP\uff09</li> <li>\u30b9\u30d1\u30fc\u30b9\u4e3b\u6210\u5206\u5206\u6790</li> <li>\u30c6\u30f3\u30bd\u30eb\u5206\u89e3\u3068\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf</li> <li>\u6df1\u5c64\u5b66\u7fd2\u3068\u306e\u95a2\u9023</li> </ul> </li> </ul> </li> <li> <p>\u671f\u672b\u30ec\u30dd\u30fc\u30c8\uff08\u4e88\u5b9a\uff09</p> <ul> <li>\u4ee5\u4e0b\u306e2\u3064\u306e\u30ec\u30dd\u30fc\u30c8\u3092\u63d0\u51fa\u3059\u308b\u3053\u3068\u3002\u305f\u3060\u3057\u3001\u3053\u308c\u3089\u306e\u5185\u5bb9\u306f\u6388\u696d\u3067\u8aac\u660e\u3057\u305f\u5185\u5bb9\u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u4f5c\u6210\u3057\u306a\u3055\u3044\u3002\u6388\u696d\u3067\u5b66\u3093\u3060\u5185\u5bb9\u306b\u3064\u3044\u3066\u8a00\u53ca\u304c\u4e4f\u3057\u3044\u5834\u5408\u306b\u306f\u6e1b\u70b9\u5bfe\u8c61\u3068\u3059\u308b\u3002<ul> <li>a) \u7dda\u5f62\u4ee3\u6570\u5b66\u304c\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u306a\u305c\u91cd\u8981\u306a\u306e\u304b\u3092\u8ad6\u7406\u7684\u306b\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> <li>b) \u3042\u306a\u305f\u304c\u3053\u306e\u5927\u5b66\u3067\u53d6\u308a\u7d44\u307f\u305f\u3044\u8ab2\u984c\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u3001\u7dda\u5f62\u4ee3\u6570\u5b66\u3067\u5b66\u3093\u3060\u77e5\u8b58\u3001\u307e\u305f\u7d39\u4ecb\u3057\u305f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u624b\u6cd5\u304c\u3069\u306e\u3088\u3046\u306b\u6d3b\u304b\u305b\u308b\u306e\u304b\u3092\u8aac\u660e\u3059\u308b\u3002</li> </ul> </li> </ul> </li> </ol>"},{"location":"lectures/LA/01-introduction/","title":"\u8b1b\u7fa91: \u30b3\u30fc\u30b9\u30a4\u30f3\u30c8\u30ed\u30c0\u30af\u30b7\u30e7\u30f3","text":""},{"location":"lectures/LA/01-introduction/#1_1","title":"1. \u672c\u65e5\u6271\u3046\u5185\u5bb9\u306b\u3064\u3044\u3066\uff08\u6982\u8981\uff09","text":"<ul> <li> <p>\u8b1b\u7fa9\u306e\u76ee\u7684: </p> </li> <li> <p>\u7dda\u5f62\u4ee3\u6570\u5b66\u30b3\u30fc\u30b9\u5168\u4f53\u306e\u6982\u8981\u8aac\u660e  </p> </li> <li>\u8a55\u4fa1\u65b9\u6cd5\u306e\u63d0\u793a  </li> <li>\u4f7f\u7528\u30c4\u30fc\u30eb\uff08ChatGPT, Google Colab\uff09\u306e\u7d39\u4ecb  </li> <li> <p>\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u610f\u7fa9\u3068\u3001\u7d71\u8a08\u5b66\u30fb\u6a5f\u68b0\u5b66\u7fd2\u3001\u533b\u7642\u30c7\u30fc\u30bf\u3001\u753b\u50cf\u51e6\u7406\u3078\u306e\u5fdc\u7528\u4e8b\u4f8b\u306e\u63d0\u793a</p> </li> <li> <p>\u8b1b\u7fa9\u306e\u6d41\u308c: </p> </li> <li> <p>\u8b1b\u7fa9\u5168\u4f53\u306e\u6982\u8981\u304a\u3088\u3073\u8a55\u4fa1\u65b9\u6cd5\u306e\u8aac\u660e  </p> </li> <li>\u4f7f\u7528\u30c4\u30fc\u30eb\uff08ChatGPT\u3068Google Colab\uff09\u306e\u7d39\u4ecb\u3068\u57fa\u672c\u7684\u306a\u4f7f\u3044\u65b9\u306e\u8aac\u660e  </li> <li>\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u610f\u7fa9\u3001\u305d\u3057\u3066\u5404\u5206\u91ce\uff08\u7d71\u8a08\u5b66\u30fb\u6a5f\u68b0\u5b66\u7fd2\u3001\u533b\u7642\u30c7\u30fc\u30bf\u3001\u753b\u50cf\u51e6\u7406\uff09\u3078\u306e\u5fdc\u7528\u4e8b\u4f8b\u306e\u5177\u4f53\u4f8b\u7d39\u4ecb</li> </ul>"},{"location":"lectures/LA/01-introduction/#2","title":"2. \u672c\u65e5\u306e\u5185\u5bb9\u306e\u8aac\u660e\uff08\u6982\u8981\u30fb\u5b9a\u7fa9\u30fb\u5b9a\u7406\u306a\u3069\uff09","text":""},{"location":"lectures/LA/01-introduction/#21","title":"2.1 \u7dda\u5f62\u4ee3\u6570\u5b66\u3068\u306f","text":"<p>\u7dda\u5f62\u4ee3\u6570\u5b66\u306f\u3001\u30d9\u30af\u30c8\u30eb\u3001\u884c\u5217\u3001\u7dda\u5f62\u5199\u50cf\u306a\u3069\u306e\u57fa\u672c\u6982\u5ff5\u3092\u7528\u3044\u3066\u3001\u30c7\u30fc\u30bf\u3084\u30b7\u30b9\u30c6\u30e0\u306e\u69cb\u9020\u3092\u89e3\u6790\u3059\u308b\u6570\u5b66\u306e\u5206\u91ce\u3067\u3059\u3002   - \u57fa\u672c\u6982\u5ff5:     - \u30d9\u30af\u30c8\u30eb: \u5927\u304d\u3055\u3068\u65b9\u5411\u3092\u6301\u3064\u91cf\u3002     - \u884c\u5217: \u6570\u5024\u3084\u95a2\u6570\u3092\u683c\u5b50\u72b6\u306b\u4e26\u3079\u305f\u3082\u306e\u3067\u3001\u8907\u6570\u306e\u30c7\u30fc\u30bf\u3092\u4e00\u62ec\u3057\u3066\u6271\u3046\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3002     - \u7dda\u5f62\u5199\u50cf: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u9593\u306e\u5909\u63db\u3067\u3001\u52a0\u6cd5\u6027\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u6027\u8cea\u3092\u4fdd\u3064\u5199\u50cf\u3002</p> <p>\u3053\u306e\u8b1b\u7fa9\u306f\u3001\u524d\u671f\u306e\u7dda\u5f62\u4ee3\u6570\u5b66I, \u7dda\u5f62\u4ee3\u6570\u5b66\uff08\u57fa\u790e\uff09\u3001\u5f8c\u671f\u306e\u7dda\u5f62\u4ee3\u6570\u5b66II\u306e3\u3064\u306e\u6388\u696d\u3067\u5b8c\u7d50\u3059\u308b\u3088\u3046\u306b\u8a2d\u8a08\u3055\u308c\u3066\u3044\u308b\u3002</p>"},{"location":"lectures/LA/01-introduction/#22","title":"2.2 \u7dda\u5f62\u4ee3\u6570\u3068\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u95a2\u4fc2","text":"<p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u89e3\u304d\u305f\u3044\u8ab2\u984c\u306f\u3001\u5927\u304d\u304f\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u3068\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\u306b\u5927\u5225\u3055\u308c\u308b\u3002</p> <p>\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u3068\u306f\u3001\u4f8b\u3048\u3070\u3001\u533b\u7642\u30c7\u30fc\u30bf\u306b\u304a\u3044\u3066\u3001\u904e\u53bb\u306e\u8a3a\u65ad\u7d50\u679c\u3084\u6cbb\u7642\u8a18\u9332\u3092\u57fa\u306b\u60a3\u8005\u306e\u75c5\u72b6\u3092\u4e88\u6e2c\u3059\u308b\u30b1\u30fc\u30b9\u3084\u3001\u30b9\u30dd\u30fc\u30c4\u30c7\u30fc\u30bf\u306b\u304a\u3044\u3066\u3001\u8a66\u5408\u7d50\u679c\u3084\u9078\u624b\u306e\u6210\u7e3e\u3092\u5143\u306b\u6b21\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u4e88\u6e2c\u3059\u308b\u30b1\u30fc\u30b9\u306e\u3088\u3046\u306b\u3001\u30e9\u30d9\u30eb\u4ed8\u304d\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3057\u3001\u672a\u77e5\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u3092\u884c\u3046\u3053\u3068\u3067\u3042\u308b\u3002</p> <p>\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\u3068\u306f\u3001\u533b\u7642\u30c7\u30fc\u30bf\u3067\u75be\u60a3\u306e\u5171\u901a\u30d1\u30bf\u30fc\u30f3\u3092\u62bd\u51fa\u3057\u305f\u308a\u3001\u30b9\u30dd\u30fc\u30c4\u30c7\u30fc\u30bf\u3067\u9078\u624b\u306e\u30d7\u30ec\u30fc\u30b9\u30bf\u30a4\u30eb\u3084\u30c1\u30fc\u30e0\u306e\u6226\u7565\u7684\u7279\u5fb4\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u305f\u3081\u306b\u3001\u30c7\u30fc\u30bf\u81ea\u4f53\u306e\u69cb\u9020\u3084\u7279\u5fb4\u3092\u89e3\u6790\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002</p> <p>\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u306e\u57fa\u672c\u624b\u6cd5\u3068\u3057\u3066\u6700\u3082\u5e83\u304f\u7528\u3044\u3089\u308c\u3066\u3044\u308b\u306e\u304c\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u3042\u308a\u3001\u305d\u306e\u57fa\u790e\u3068\u306a\u308b\u306e\u306f\u7dda\u5f62\u4ee3\u6570\u5b66\u3067\u5b66\u3076\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u3067\u3042\u308b\u3002\u5bfe\u3057\u3066\u3001\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\u306e\u4ee3\u8868\u7684\u624b\u6cd5\u3067\u3042\u308b\u4e3b\u6210\u5206\u5206\u6790\u306f\u3001\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3\u306b\u57fa\u3065\u3044\u3066\u3044\u308b\u3002</p> <p>\u3053\u308c\u3089\u4e8c\u3064\u306e\u6982\u5ff5\u3092\u78ba\u5b9f\u306b\u7fd2\u5f97\u3059\u308b\u3053\u3068\u304c\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3092\u5b66\u3076\u4e0a\u3067\u306e\u7b2c\u4e00\u6b69\u3067\u3059\u3002\u3053\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u304b\u3089\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3078\u306e\u5fdc\u7528\u3001\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u4e3b\u6210\u5206\u5206\u6790\u3001\u3055\u3089\u306b\u7279\u7570\u5024\u5206\u89e3\u3001\u3055\u3089\u306b\u6642\u9593\u304c\u8a31\u305b\u3070\u56e0\u5b50\u5206\u6790\u306e\u57fa\u790e\u307e\u3067\u3092\u5e45\u5e83\u304f\u30ab\u30d0\u30fc\u3059\u308b\u3002</p>"},{"location":"lectures/LA/01-introduction/#23","title":"2.3 \u6210\u7e3e\u306e\u8a55\u4fa1","text":"<p>\u524d\u671f\u306e\u7dda\u5f62\u4ee3\u6570\u5b66I\u3068\u7dda\u5f62\u4ee3\u6570\u5b66\uff08\u57fa\u790e\uff09\u306f\u30012\u3064\u306e\u6388\u696d\u3092\u5408\u308f\u305b\u3066\u8a55\u4fa1\u3092\u884c\u3044\u307e\u3059\u3002\u8a55\u4fa1\u306f\u3001\u6b21\u306e3\u3064\u3092\u901a\u3057\u3066\u5b9f\u65bd\u3057\u307e\u3059\u3002</p> <ul> <li>\u4e2d\u9593\u8a66\u9a13: \u6301\u3061\u8fbc\u307f\u4e0d\u53ef\u306e60\u5206\u306e\u7b46\u8a18\u8a66\u9a13\u3067\u3059\u3002\u57fa\u672c\u7684\u306a\u884c\u5217\u306e\u8a08\u7b97\u3084\u3001\u6027\u8cea\u304c\u7406\u89e3\u3067\u304d\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\uff0830\uff05\uff09\u3002</li> <li>\u671f\u672b\u8ab2\u984c: \u81ea\u7531\u8a18\u8ff0\u5f62\u5f0f\u306e60\u5206\u306e\u6388\u696d\u5185\u30ec\u30dd\u30fc\u30c8\u8a66\u9a13\u3067\u3059\uff0830\uff05\uff09\u3002</li> <li>\u671f\u672b\u8a66\u9a13: \u6301\u3061\u8fbc\u307f\u4e0d\u53ef\u306e60\u5206\u306e\u7b46\u8a18\u8a66\u9a13\u3067\u3059\u3002\u57fa\u672c\u7684\u306a\u884c\u5217\u306e\u8a08\u7b97\u3084\u3001\u6027\u8cea\u304c\u7406\u89e3\u3067\u304d\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\uff0840\uff05\uff09\u3002</li> </ul> <p>\u4e2d\u9593\u8a66\u9a13\u3068\u671f\u672b\u8a66\u9a13\u306f\u3001\u7dda\u5f62\u4ee3\u6570\u3067\u5b66\u3076\u57fa\u672c\u8a08\u7b97\u306e\u7406\u5c48\u304c\u304d\u3061\u3093\u3068\u7406\u89e3\u3067\u304d\u3066\u3044\u308b\u304b\u3092\u78ba\u304b\u3081\u308b\u305f\u3081\u306e\u3082\u306e\u3067\u3059\u3002\u4e00\u65b9\u3067\u3001\u671f\u672b\u8ab2\u984c\u306f\u7dda\u5f62\u4ee3\u6570\u5b66\u3068\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u7406\u8ad6\u306e\u3064\u306a\u304c\u308a\u3092\u304d\u3061\u3093\u3068\u8aac\u660e\u3067\u304d\u308b\u304b\u3092\u78ba\u304b\u3081\u308b\u305f\u3081\u306e\u7b46\u8a18\u8a66\u9a13\u3067\u3059\u3002\u3053\u308c\u30893\u3064\u306e\u7d50\u679c\u3092\u7dcf\u5408\u3057\u3066\u8a55\u5b9a\u306e\u5224\u65ad\u3092\u884c\u3044\u307e\u3059\u3002\u8a55\u5b9a\u306e\u57fa\u6e96\u306f\u4ee5\u4e0b\u306e\u3068\u304a\u308a\u3067\u3059\u3002</p> <ul> <li>S : 90%\u4ee5\u4e0a</li> <li>A : 80%\u4ee5\u4e0a\u301c89.9%\u4ee5\u4e0b</li> <li>B : 70%\u4ee5\u4e0a\u301c79.9%\u4ee5\u4e0b</li> <li>C : 60%\u4ee5\u4e0a\u301c69.9%\u4ee5\u4e0b</li> <li>D : 59.9%\u4ee5\u4e0b</li> </ul>"},{"location":"lectures/LA/01-introduction/#24-ai","title":"2.4 \u751f\u6210AI\u306e\u6d3b\u7528","text":"<p>\u3053\u306e\u6388\u696d\u3067\u306f\u3001\u751f\u6210AI\u3092\u6d3b\u7528\u3057\u307e\u3059\u3002\u3053\u306e\u6388\u696d\u306b\u306f\u3001\u5fc5\u305aPC\u3092\u6301\u3061\u8fbc\u307f\u307e\u3057\u3087\u3046\u3002\u305d\u306e\u4e0a\u3067\u3001\u308f\u304b\u3089\u306a\u3044\u3053\u3068\u306fChatGPT\uff08\u305d\u306e\u307b\u304b\u306e\u751f\u6210AI\u3067\u3082ok\uff09\u3092\u7528\u3044\u3066\u89e3\u6c7a\u3059\u308b\u3068\u3044\u3046\u306e\u3092\u5fc5\u305a\u5b9f\u8df5\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u307e\u305f\u3001\u8a08\u7b97\u7d50\u679c\u3084\u56f3\u306e\u8868\u793a\u306f\u3001Google Colaboratory\u3092\u5229\u7528\u3057\u307e\u3059\u3002\u3053\u3061\u3089\u306f\u3001ChatGPT\u3092\u7528\u3044\u3066\u751f\u6210\u3057\u305f\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3001\u8a08\u7b97\u7d50\u679c\u3084\u56f3\u3092\u8868\u793a\u3057\u305f\u8996\u899a\u7684\u7406\u89e3\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u304f\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/01-introduction/#3-chatgpt-llm","title":"3. ChatGPT (LLM) \u306e\u6d3b\u7528\u65b9\u6cd5","text":""},{"location":"lectures/LA/01-introduction/#31","title":"3.1 \u6982\u8981","text":"<p>ChatGPT \u306f\u3001OpenAI\u304c\u63d0\u4f9b\u3059\u308b\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb\uff08LLM\uff09\u3067\u3059\u3002\u3053\u306e\u307b\u304bLLM\u306b\u306f\u3001Google\u306eGemini\u3084\u3001Anthropic \u304c\u63d0\u4f9b\u3059\u308b claude \u3082\u3042\u308a\u307e\u3059\u3002\u81ea\u5206\u304c\u597d\u304d\u306a\u3082\u306e\u3092\u5229\u7528\u3059\u308c\u3070\u826f\u3044\u3067\u3059\u3002\u305f\u3060\u3057\u3001\u3053\u306e\u6388\u696d\u3067\u306fChatGPT\u3092\u57fa\u672c\u3068\u3057\u3066\u5229\u7528\u3057\u307e\u3059\u3002\u6388\u696d\u306e\u7406\u89e3\u3092\u6df1\u3081\u308b\u305f\u3081\u306e\u88dc\u52a9\u30c4\u30fc\u30eb\u3068\u3057\u3066\u6d3b\u7528\u3067\u304d\u308b\u306e\u3067\u3001\u7a4d\u6975\u7684\u306b\u5229\u7528\u3057\u307e\u3057\u3087\u3046\u3002</p> <p>\u5f93\u6765\u578b\u306e\u6388\u696d\u3067\u306f\u3001\u6559\u79d1\u66f8\u3068\u6f14\u7fd2\u554f\u984c\uff0b\u53cb\u9054\u3084\u5148\u751f\u306b\u805e\u304f\u304c\u30e1\u30a4\u30f3\u3060\u3063\u305f\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u3053\u308c\u304b\u3089\u306e\u6642\u4ee3\u306f\u3053\u3053\u306bLLM\u3092\u52a0\u3048\u308b\u3053\u3068\u3067\u3001\u3055\u3089\u306b\u5b66\u7fd2\u3059\u308b\u901f\u5ea6\u304c\u52a0\u901f\u3059\u308b\u306f\u305a\u3067\u3059\u3002\u7591\u554f\u70b9\u306e\u89e3\u6d88\u3084\u3001\u88dc\u8db3\u8aac\u660e\u3001\u3055\u3089\u306b\u306f\u7406\u8ad6\u80cc\u666f\u306e\u518d\u78ba\u8a8d\u306a\u3069\u3001\u69d8\u3005\u306a\u7528\u9014\u306b\u5229\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/LA/01-introduction/#step-by-step","title":"Step by Step \u5229\u7528\u65b9\u6cd5","text":"<ol> <li> <p>\u8cea\u554f\u306e\u6e96\u5099: </p> </li> <li> <p>\u8b1b\u7fa9\u4e2d\u306b\u751f\u3058\u305f\u7591\u554f\u70b9\u3084\u3001\u3088\u308a\u6df1\u3044\u7406\u89e3\u304c\u5fc5\u8981\u306a\u7b87\u6240\u3092\u6574\u7406\u3057\u3001\u5177\u4f53\u7684\u306a\u8cea\u554f\u3068\u3057\u3066\u307e\u3068\u3081\u308b\u3002  </p> </li> <li> <p>\u4f8b: \u300c\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u9055\u3044\u306b\u3064\u3044\u3066\u3001\u3082\u3046\u5c11\u3057\u8a73\u3057\u304f\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u300d</p> </li> <li> <p>\u8cea\u554f\u306e\u5165\u529b: </p> </li> <li> <p>ChatGPT\u306e\u30c1\u30e3\u30c3\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\u306b\u6574\u7406\u3057\u305f\u8cea\u554f\u3092\u5165\u529b\u3059\u308b\u3002  </p> </li> <li> <p>\u4f8b: \u300c\u5185\u7a4d\u306e\u8a08\u7b97\u30eb\u30fc\u30eb\u3092\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u300d</p> </li> <li> <p>\u56de\u7b54\u306e\u78ba\u8a8d: </p> </li> <li> <p>ChatGPT\u304b\u3089\u306e\u8fd4\u7b54\u3092\u8aad\u307f\u3001\u8b1b\u7fa9\u8cc7\u6599\u3068\u7167\u3089\u3057\u5408\u308f\u305b\u306a\u304c\u3089\u7406\u89e3\u3092\u6df1\u3081\u308b\u3002  </p> </li> <li> <p>\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u8ffd\u52a0\u8cea\u554f\u3092\u884c\u3044\u3001\u8a73\u7d30\u306a\u8aac\u660e\u3092\u6c42\u3081\u308b\u3002</p> </li> <li> <p>\u30c7\u30a3\u30b9\u30ab\u30c3\u30b7\u30e7\u30f3\u3078\u306e\u5fdc\u7528: </p> </li> <li> <p>\u5f97\u3089\u308c\u305f\u56de\u7b54\u3092\u30b0\u30eb\u30fc\u30d7\u30c7\u30a3\u30b9\u30ab\u30c3\u30b7\u30e7\u30f3\u3067\u5171\u6709\u3057\u3001\u4ed6\u306e\u5b66\u751f\u306e\u610f\u898b\u3082\u53c2\u8003\u306b\u3057\u306a\u304c\u3089\u7406\u89e3\u3092\u5e83\u3052\u308b\u3002</p> </li> <li> <p>\u5fa9\u7fd2\u3068\u4e88\u7fd2: </p> </li> <li> <p>\u8b1b\u7fa9\u5f8c\u306e\u5fa9\u7fd2\u3084\u3001\u6b21\u56de\u306e\u4e88\u7fd2\u306e\u969b\u306b\u3082\u3001ChatGPT\u3092\u5229\u7528\u3057\u3066\u4e0d\u660e\u70b9\u3092\u89e3\u6d88\u3059\u308b\u3002</p> </li> </ol>"},{"location":"lectures/LA/01-introduction/#4-google-colaboratory","title":"4. Google Colaboratory \u306e\u4f7f\u3044\u65b9","text":""},{"location":"lectures/LA/01-introduction/#41","title":"4.1 \u6982\u8981","text":"<p>Google Colaboratory (Colab) \u306f\u3001\u30d6\u30e9\u30a6\u30b6\u4e0a\u3067Python, R\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3067\u304d\u308b\u30aa\u30f3\u30e9\u30a4\u30f3\u74b0\u5883\u3067\u3059\u3002\u8b1b\u7fa9\u3067\u7d39\u4ecb\u3059\u308b\u30b3\u30fc\u30c9\u4f8b\u3084\u6f14\u7fd2\u554f\u984c\u3092\u3001\u5b9f\u969b\u306b\u52d5\u304b\u3057\u3066\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002</p> <p>\u4e00\u822c\u7684\u306b\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u66f8\u304d\u65b9\u3092\u5b66\u3093\u3067\u304b\u3089\u3001Colab\u3067\u5b9f\u969b\u306b\u81ea\u5206\u3067\u66f8\u3044\u3066\u52d5\u304b\u3057\u307e\u3057\u3087\u3046\u3068\u3044\u3046\u3053\u3068\u3092\u3059\u308b\u306e\u3067\u3059\u304c\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u66f8\u304d\u65b9\u3092\u5b66\u3070\u306a\u304f\u3066\u3082\u3001\u5b9f\u969b\u306b\u30b3\u30fc\u30c9\u3092\u52d5\u304b\u3059\u3053\u3068\u306f\u3067\u304d\u307e\u3059\u3002\u3053\u306e\u6388\u696d\u3067\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u66f8\u304d\u65b9\u306f\u6559\u3048\u307e\u305b\u3093\u304c\u3001ChatGPT\u3092\u5229\u7528\u3057\u3066\u30b3\u30fc\u30c9\u3092\u751f\u6210\u3057\u3001\u305d\u308c\u3092Colab\u4e0a\u3067\u52d5\u304b\u3057\u3066\u3001\u52d5\u4f5c\u3092\u78ba\u8a8d\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/01-introduction/#42-step-by-step","title":"4.2 Step by Step \u5229\u7528\u65b9\u6cd5","text":"<ol> <li> <p>Google Colab\u3078\u306e\u30a2\u30af\u30bb\u30b9: </p> </li> <li> <p>\u30a6\u30a7\u30d6\u30d6\u30e9\u30a6\u30b6\u3067 Google Colab \u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3059\u3002  </p> </li> <li> <p>Google\u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u30ed\u30b0\u30a4\u30f3\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u65b0\u3057\u3044\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306e\u4f5c\u6210: </p> </li> <li> <p>\u300c\u65b0\u3057\u3044\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u300d\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u3001\u7a7a\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002  </p> </li> <li> <p>\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306e\u540d\u524d\u3092\u300c\u7dda\u5f62\u4ee3\u6570\u5b66_\u7b2c1\u56de\u300d\u306a\u3069\u306b\u5909\u66f4\u3057\u3066\u3001\u7ba1\u7406\u3057\u3084\u3059\u304f\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u30bb\u30eb\u306e\u4f7f\u3044\u65b9\u306e\u7406\u89e3: </p> </li> <li> <p>Colab\u306b\u306f\u300c\u30b3\u30fc\u30c9\u30bb\u30eb\u300d\u3068\u300c\u30c6\u30ad\u30b9\u30c8\u30bb\u30eb\u300d\u304c\u3042\u308a\u3001\u30b3\u30fc\u30c9\u306e\u5b9f\u884c\u3084\u89e3\u8aac\u306e\u8a18\u5165\u306b\u5229\u7528\u3057\u307e\u3059\u3002  </p> </li> <li> <p>\u30b3\u30fc\u30c9\u30bb\u30eb\u306b\u306f\u8b1b\u7fa9\u306e\u30b3\u30fc\u30c9\u4f8b\u3092\u8cbc\u308a\u4ed8\u3051\u3001\u30c6\u30ad\u30b9\u30c8\u30bb\u30eb\u306b\u306f\u30e1\u30e2\u3084\u89e3\u8aac\u3001\u8cea\u554f\u306a\u3069\u3092\u8a18\u5165\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u30b3\u30fc\u30c9\u306e\u5b9f\u884c: </p> </li> <li> <p>\u30b3\u30fc\u30c9\u30bb\u30eb\u3092\u9078\u629e\u3057\u3001\u5de6\u5074\u306e\u518d\u751f\u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u304b\u3001Shift+Enter\u30ad\u30fc\u3092\u62bc\u3057\u3066\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002  </p> </li> <li> <p>\u5b9f\u884c\u7d50\u679c\u306f\u30bb\u30eb\u306e\u4e0b\u306b\u8868\u793a\u3055\u308c\u3001\u30b0\u30e9\u30d5\u3084\u6570\u5024\u51fa\u529b\u3082\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u6f14\u7fd2\u554f\u984c\u3078\u306e\u5fdc\u7528: </p> </li> <li> <p>\u8b1b\u7fa9\u3067\u51fa\u3055\u308c\u305f\u6f14\u7fd2\u554f\u984c\u306e\u30b3\u30fc\u30c9\u3092Colab\u306b\u5165\u529b\u3057\u3001\u5b9f\u969b\u306b\u5b9f\u884c\u3057\u3066\u52d5\u4f5c\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002  </p> </li> <li> <p>\u5206\u304b\u3089\u306a\u3044\u70b9\u306f\u30bb\u30eb\u5185\u306b\u30b3\u30e1\u30f3\u30c8\u3092\u8ffd\u52a0\u3059\u308b\u306a\u3069\u3057\u3066\u3001\u5f8c\u3067\u898b\u8fd4\u3057\u3084\u3059\u304f\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u30d5\u30a1\u30a4\u30eb\u306e\u4fdd\u5b58\u3068\u5171\u6709: </p> </li> <li> <p>\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u81ea\u52d5\u7684\u306bGoogle\u30c9\u30e9\u30a4\u30d6\u306b\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002  </p> </li> <li>\u300c\u5171\u6709\u300d\u30dc\u30bf\u30f3\u3092\u4f7f\u3063\u3066\u3001\u4ed6\u306e\u5b66\u751f\u3084\u8b1b\u5e2b\u3068\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u5171\u6709\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002</li> </ol>"},{"location":"lectures/LA/01-introduction/#43-ai","title":"4.3 AI\u3067\u30b3\u30fc\u30c9\u3092\u751f\u6210\u3059\u308b","text":"<p>\u3053\u306e\u6388\u696d\u3067\u306f\u3001\u6388\u696d\u4e2d\u306b\u751f\u6210AI\u3067\u30b3\u30fc\u30c9\u3092\u751f\u6210\u3057\u3066\u3001\u305d\u308c\u3092Colab\u306b\u8cbc\u308a\u4ed8\u3051\u308b\u3053\u3068\u3067\u7d50\u679c\u3092\u78ba\u8a8d\u3059\u308b\u3068\u3044\u3046\u4f7f\u3044\u65b9\u304c\u30e1\u30a4\u30f3\u3067\u3059\u304c\u3001\u30b3\u30fc\u30c9\u30bb\u30eb\u306b\u306f\"AI\u3067\u751f\u6210\"\u3068\u3044\u3046\u30dc\u30bf\u30f3\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u30dc\u30bf\u30f3\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u751f\u6210AI\u3092\u7528\u3044\u3066\u81ea\u5206\u304c\u3084\u308a\u305f\u3044\u64cd\u4f5c\u306b\u5bfe\u5fdc\u3059\u308b\u30b3\u30fc\u30c9\u3092\u66f8\u3044\u3066\u304f\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/01-introduction/#5-gpt-colab","title":"5. \u6271\u3046\u5185\u5bb9\u306e\u4f8b\uff08\u5b9f\u4f8b\u3092\u63d0\u793a\u3001GPT, Colab\u306a\u3069\u306f\u5229\u7528\u3057\u306a\u3044\uff09","text":""},{"location":"lectures/LA/01-introduction/#51-pca","title":"5.1 \u7d71\u8a08\u5b66\u30fb\u6a5f\u68b0\u5b66\u7fd2\u3078\u306e\u5fdc\u7528\u4f8b: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306b\u3088\u308b\u6b21\u5143\u524a\u6e1b","text":"<ul> <li>\u6982\u8981:   \u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u57fa\u790e\u6982\u5ff5\uff08\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3084\u884c\u5217\u5206\u89e3\uff09\u3092\u7528\u3044\u3066\u3001\u591a\u6b21\u5143\u30c7\u30fc\u30bf\u304b\u3089\u91cd\u8981\u306a\u60c5\u5831\u3092\u62bd\u51fa\u3057\u3001\u6b21\u5143\u524a\u6e1b\u3092\u884c\u3046\u624b\u6cd5\u3067\u3059\u3002  </li> <li>\u5177\u4f53\u4f8b:   \u5b9f\u969b\u306eIris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3066\u30014\u6b21\u5143\u306e\u30c7\u30fc\u30bf\u30922\u6b21\u5143\u306b\u7e2e\u7d04\u3057\u3001\u30c7\u30fc\u30bf\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u50be\u5411\u3092\u8996\u899a\u5316\u3057\u307e\u3059\u3002  </li> <li>\u30b3\u30fc\u30c9\u4f8b:</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\n\n# Iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# PCA\u30674\u6b21\u5143\u304b\u30892\u6b21\u5143\u306b\u6b21\u5143\u524a\u6e1b\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# \u7d50\u679c\u306e\u6563\u5e03\u56f3\u3092\u4f5c\u6210\nplt.figure(figsize=(8,6))\nfor label in np.unique(y):\n    plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], label=iris.target_names[label])\nplt.xlabel('\u7b2c1\u4e3b\u6210\u5206')\nplt.ylabel('\u7b2c2\u4e3b\u6210\u5206')\nplt.title('Iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306ePCA\u7d50\u679c')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/01-introduction/#52","title":"5.2 \u7d71\u8a08\u5b66\u30fb\u6a5f\u68b0\u5b66\u7fd2\u3078\u306e\u5fdc\u7528\u4f8b: \u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u7dda\u5f62\u56de\u5e30","text":"<ul> <li>\u6982\u8981:   \u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u5fdc\u7528\u3068\u3057\u3066\u3001\u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u7528\u3044\u3066\u4e0e\u3048\u3089\u308c\u305f\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u6700\u9069\u306a\u76f4\u7dda\u3092\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002  </li> <li>\u5177\u4f53\u4f8b:   \u30e9\u30f3\u30c0\u30e0\u306b\u751f\u6210\u3057\u305f\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001\u72ec\u7acb\u5909\u6570\u3068\u5f93\u5c5e\u5909\u6570\u306e\u95a2\u4fc2\u3092\u7dda\u5f62\u30e2\u30c7\u30eb\u3067\u8fd1\u4f3c\u3057\u3001\u6700\u9069\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u5207\u7247\u3068\u50be\u304d\uff09\u3092\u6c42\u3081\u307e\u3059\u3002  </li> <li>\u30b3\u30fc\u30c9\u4f8b:</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u751f\u6210\nnp.random.seed(0)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# \u30d0\u30a4\u30a2\u30b9\u9805\u3092\u542b\u3080\u884c\u5217X_b\u306e\u4f5c\u6210\uff08X\u306b1\u306e\u5217\u3092\u8ffd\u52a0\uff09\nX_b = np.c_[np.ones((100, 1)), X]\n\n# \u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u7528\u3044\u3066\u6700\u5c0f\u4e8c\u4e57\u89e3\u3092\u8a08\u7b97\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\nprint(\"\u6700\u9069\u306a\u30d1\u30e9\u30e1\u30fc\u30bf:\", theta_best)\n\n# \u56de\u5e30\u76f4\u7dda\u3092\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(8,6))\nplt.scatter(X, y, label=\"\u30c7\u30fc\u30bf\")\nX_new = np.array([[0], [2]])\nX_new_b = np.c_[np.ones((2, 1)), X_new]\ny_predict = X_new_b.dot(theta_best)\nplt.plot(X_new, y_predict, color=\"red\", label=\"\u56de\u5e30\u76f4\u7dda\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u6700\u5c0f\u4e8c\u4e57\u89e3\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/01-introduction/#53-pca","title":"5.3 \u533b\u7642\u30c7\u30fc\u30bf\u3078\u306e\u5fdc\u7528\u4f8b: \u4e73\u304c\u3093\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306ePCA","text":"<ul> <li>\u6982\u8981:   \u533b\u7642\u5206\u91ce\u3067\u306f\u3001\u591a\u6570\u306e\u6e2c\u5b9a\u5024\u304b\u3089\u75c5\u6c17\u306e\u72b6\u614b\u3092\u8a55\u4fa1\u3059\u308b\u969b\u306b\u3001\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b\u304c\u6709\u52b9\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u4e73\u304c\u3093\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3066PCA\u306b\u3088\u308a\u30c7\u30fc\u30bf\u306e\u4f4e\u6b21\u5143\u8868\u73fe\u3092\u53d6\u5f97\u3057\u3001\u826f\u6027\u3068\u60aa\u6027\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u8996\u899a\u5316\u3057\u307e\u3059\u3002  </li> <li>\u5177\u4f53\u4f8b:   scikit-learn\u306e\u4e73\u304c\u3093\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5229\u7528\u3057\u3001PCA\u30672\u6b21\u5143\u306b\u7e2e\u7d04\u3057\u3066\u3001\u5404\u30af\u30e9\u30b9\uff08\u826f\u6027\u3001\u60aa\u6027\uff09\u306e\u5206\u5e03\u3092\u6563\u5e03\u56f3\u3067\u793a\u3057\u307e\u3059\u3002  </li> <li>\u30b3\u30fc\u30c9\u4f8b:</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.decomposition import PCA\n\n# \u4e73\u304c\u3093\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target\n\n# PCA\u3067\u7279\u5fb4\u91cf\u30922\u6b21\u5143\u306b\u7e2e\u7d04\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# \u7d50\u679c\u306e\u6563\u5e03\u56f3\u3092\u4f5c\u6210\uff080: \u60aa\u6027, 1: \u826f\u6027\uff09\nplt.figure(figsize=(8,6))\nplt.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], alpha=0.7, label='\u60aa\u6027')\nplt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], alpha=0.7, label='\u826f\u6027')\nplt.xlabel('\u7b2c1\u4e3b\u6210\u5206')\nplt.ylabel('\u7b2c2\u4e3b\u6210\u5206')\nplt.title('\u4e73\u304c\u3093\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306ePCA\u7d50\u679c')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/01-introduction/#54-svd","title":"5.4 \u753b\u50cf\u51e6\u7406\u3078\u306e\u5fdc\u7528\u4f8b: SVD\u306b\u3088\u308b\u753b\u50cf\u306e\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c","text":"<ul> <li>\u6982\u8981:   \u753b\u50cf\u51e6\u7406\u3067\u306f\u3001\u7279\u306b\u753b\u50cf\u5727\u7e2e\u3084\u30ce\u30a4\u30ba\u9664\u53bb\u306e\u5206\u91ce\u3067\u7dda\u5f62\u4ee3\u6570\u5b66\u304c\u6d3b\u7528\u3055\u308c\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u3092\u7528\u3044\u3066\u753b\u50cf\u306e\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u3092\u884c\u3044\u3001\u5143\u306e\u753b\u50cf\u3068\u8fd1\u4f3c\u753b\u50cf\u306e\u6bd4\u8f03\u3092\u884c\u3044\u307e\u3059\u3002  </li> <li>\u5177\u4f53\u4f8b:   skimage\u306e\u30b5\u30f3\u30d7\u30eb\u753b\u50cf\u3092\u7528\u3044\u3066\u3001SVD\u3067\u753b\u50cf\u3092\u5206\u89e3\u3057\u3001\u4e0a\u4f4d\u306e\u7279\u7570\u5024\u306e\u307f\u3092\u7528\u3044\u305f\u518d\u69cb\u6210\u753b\u50cf\u3092\u8868\u793a\u3057\u307e\u3059\u3002  </li> <li>\u30b3\u30fc\u30c9\u4f8b:</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage.color import rgb2gray\n\n# \u30b5\u30f3\u30d7\u30eb\u753b\u50cf\uff08\u30ab\u30e1\u30e9\u753b\u50cf\uff09\u306e\u8aad\u307f\u8fbc\u307f\nimage = data.camera()\n\n# SVD\u5206\u89e3\nU, s, Vt = np.linalg.svd(image, full_matrices=False)\n\n# \u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c: \u7279\u7570\u5024\u306e\u4e0a\u4f4d k \u500b\u3092\u5229\u7528\nk = 50\nS = np.diag(s[:k])\nimage_approx = U[:, :k] @ S @ Vt[:k, :]\n\n# \u753b\u50cf\u306e\u8868\u793a\nplt.figure(figsize=(12, 6))\nplt.subplot(1,2,1)\nplt.imshow(image, cmap='gray')\nplt.title('\u5143\u306e\u753b\u50cf')\nplt.axis('off')\n\nplt.subplot(1,2,2)\nplt.imshow(image_approx, cmap='gray')\nplt.title(f'\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c (k={k})')\nplt.axis('off')\nplt.show()\n</code></pre>"},{"location":"lectures/LA/01-introduction/#6","title":"6. \u6b21\u56de\u306e\u5185\u5bb9","text":"<p>\u6b21\u56de\u304b\u3089\u3001\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u6388\u696d\u3092\u59cb\u3081\u307e\u3059\u3002\u7279\u306b\u3001\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u790e\u304b\u3089\u52c9\u5f37\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u9ad8\u6821\u307e\u3067\u306e\u5185\u5bb9\u3092\u5fd8\u308c\u3066\u3057\u307e\u3063\u305f\u3068\u3044\u3046\u65b9\u3082\u3001\u6700\u521d\u304b\u3089\u4e01\u5be7\u306b\u52c9\u5f37\u3057\u307e\u3059\u306e\u3067\u3001\u3064\u3044\u3066\u304d\u3066\u304f\u3060\u3055\u3044\u306d\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I \u7b2c2\u56de \u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/02-vector-and-matrix/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<ul> <li>\u8b1b\u7fa9\u56de: \u7b2c2\u56de</li> <li>\u30c6\u30fc\u30de: \u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\u3068\u57fa\u672c\u64cd\u4f5c\u2460</li> <li>\u95a2\u9023\u9805\u76ee: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3001\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u3001\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8</li> <li>\u4e88\u7fd2\u5185\u5bb9: \u9ad8\u6821\u6570\u5b66\u306e\u5ea7\u6a19\u5e73\u9762\u3068\u7a7a\u9593\u5ea7\u6a19\u306e\u57fa\u790e\u77e5\u8b58\u3092\u5fa9\u7fd2\u3057\u3066\u304a\u304f\u3053\u3068</li> </ul>"},{"location":"lectures/LA/02-vector-and-matrix/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\u3068\u8868\u8a18\u65b9\u6cd5\u3092\u7406\u89e3\u3059\u308b</li> <li>\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u6f14\u7b97\u898f\u5247\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u7406\u89e3\u3059\u308b</li> <li>Python\u3092\u7528\u3044\u3066\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u3092\u5b9f\u88c5\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> </ol>"},{"location":"lectures/LA/02-vector-and-matrix/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/02-vector-and-matrix/#31","title":"3.1 \u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \\(n\\)\u6b21\u5143\u5b9f\u30d9\u30af\u30c8\u30eb\u3068\u306f\u3001\\(n\\)\u500b\u306e\u5b9f\u6570\u3092\u7e26\u306b\u4e26\u3079\u305f\u3082\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u8a18\u3055\u308c\u308b\uff1a</p> \\[\\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix}\\] <p>\u3053\u3053\u3067 \\(v_1, v_2, \\ldots, v_n\\) \u306f\u5b9f\u6570\u3067\u3042\u308b\u3002</p> <p>\u3053\u306e\u3088\u3046\u306a\u30d9\u30af\u30c8\u30eb\u306e\u96c6\u5408\u3092 \\(\\mathbb{R}^n\\) \u3068\u8868\u3057\u3001\\(n\\)\u6b21\u5143\u5b9f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u547c\u3073\u307e\u3059\u3002\u7279\u306b\u91cd\u8981\u306a\u4f8b\u3068\u3057\u3066\uff1a</p> <ul> <li>\\(\\mathbb{R}^2\\): 2\u6b21\u5143\u5b9f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\uff08\u5e73\u9762\u4e0a\u306e\u30d9\u30af\u30c8\u30eb\uff09</li> <li>\\(\\mathbb{R}^3\\): 3\u6b21\u5143\u5b9f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\uff08\u7a7a\u9593\u4e0a\u306e\u30d9\u30af\u30c8\u30eb\uff09</li> </ul> <p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5404\u30b5\u30f3\u30d7\u30eb\u30921\u3064\u306e\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u6271\u3046\u3053\u3068\u304c\u4e00\u822c\u7684\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u3042\u308b\u4eba\u306e\u300c\u5e74\u9f62\u3001\u8eab\u9577\u3001\u4f53\u91cd\u3001\u8840\u5727\u300d\u3068\u3044\u3063\u305f4\u3064\u306e\u5065\u5eb7\u6307\u6a19\u306f4\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#32","title":"3.2 \u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u8868\u3057\u65b9","text":"<p>\u30d9\u30af\u30c8\u30eb\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u69d8\u3005\u306a\u65b9\u6cd5\u3067\u8868\u8a18\u3055\u308c\u307e\u3059\uff1a</p> <ol> <li> <p>\u6210\u5206\u8868\u793a:    \\(\\(\\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u592a\u5b57\u8a18\u53f7:    \\(\\mathbf{v}\\) \u3084 \\(\\vec{v}\\)</p> </li> <li> <p>\u5217\u30d9\u30af\u30c8\u30eb\uff08\u6a19\u6e96\u7684\u306a\u8868\u8a18\uff09:    \\(\\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^T\\)\\)    \u3053\u3053\u3067 \\(T\\) \u306f\u8ee2\u7f6e\u3092\u8868\u3057\u3001\u5217\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b\u3053\u3068\u3092\u660e\u793a\u3057\u307e\u3059\u3002</p> </li> </ol>"},{"location":"lectures/LA/02-vector-and-matrix/#33","title":"3.3 \u96f6\u30d9\u30af\u30c8\u30eb","text":"<p>\u5b9a\u7fa9: \u96f6\u30d9\u30af\u30c8\u30eb \\(\\mathbf{0}\\) \u3068\u306f\u3001\u3059\u3079\u3066\u306e\u6210\u5206\u304c0\u3067\u3042\u308b\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b\uff1a</p> \\[\\mathbf{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\] <p>\u96f6\u30d9\u30af\u30c8\u30eb\u306f\u52a0\u6cd5\u6f14\u7b97\u306e\u5358\u4f4d\u5143\u3068\u3057\u3066\u306e\u5f79\u5272\u3092\u6301\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/02-vector-and-matrix/#41","title":"4.1 \u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u548c","text":"<p>\u5b9a\u7fa9: 2\u3064\u306e\u540c\u3058\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^T\\) \u3068 \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)^T\\) \u306b\u5bfe\u3057\u3066\u3001\u305d\u306e\u548c \\(\\mathbf{v} + \\mathbf{w}\\) \u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a</p> \\[\\mathbf{v} + \\mathbf{w} = \\begin{pmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\\\ \\vdots \\\\ v_n + w_n \\end{pmatrix}\\] <p>\u3064\u307e\u308a\u3001\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\u306f\u300c\u5bfe\u5fdc\u3059\u308b\u6210\u5206\u540c\u58eb\u3092\u8db3\u3059\u300d\u64cd\u4f5c\u3067\u3059\u3002</p> <p>\u4f8b: \\(\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\end{pmatrix}\\) \u3068 \\(\\mathbf{w} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 4 \\end{pmatrix}\\) \u306e\u3068\u304d</p> \\[\\mathbf{v} + \\mathbf{w} = \\begin{pmatrix} 1 + 2 \\\\ 3 + (-1) \\\\ 5 + 4 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\\\ 9 \\end{pmatrix}\\]"},{"location":"lectures/LA/02-vector-and-matrix/#42","title":"4.2 \u30d9\u30af\u30c8\u30eb\u52a0\u6cd5\u306e\u6027\u8cea","text":"<p>\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\u306b\u306f\u4ee5\u4e0b\u306e\u6027\u8cea\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u7d50\u5408\u6cd5\u5247: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>\u4ea4\u63db\u6cd5\u5247: \\(\\mathbf{v} + \\mathbf{w} = \\mathbf{w} + \\mathbf{v}\\)</li> <li>\u5358\u4f4d\u5143: \u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)</li> <li>\u9006\u5143: \u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\) \u3068\u306a\u308b \\(-\\mathbf{v}\\) \u304c\u5b58\u5728\u3059\u308b</li> </ol> <p>\u3053\u308c\u3089\u306e\u6027\u8cea\u306b\u3088\u308a\u3001\u30d9\u30af\u30c8\u30eb\u306e\u96c6\u5408\u306f\u52a0\u6cd5\u306b\u95a2\u3057\u3066\u300c\u53ef\u63db\u7fa4\u300d\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#43","title":"4.3 \u5b9f\u30d9\u30af\u30c8\u30eb\u306e\u30b9\u30ab\u30e9\u30fc\u500d","text":"<p>\u5b9a\u7fa9: \u30d9\u30af\u30c8\u30eb \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)^T\\) \u3068\u5b9f\u6570 \\(\\alpha\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\mathbf{v}\\) \u306e\u30b9\u30ab\u30e9\u30fc\u500d \\(\\alpha\\mathbf{v}\\) \u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a</p> \\[\\alpha\\mathbf{v} = \\begin{pmatrix} \\alpha v_1 \\\\ \\alpha v_2 \\\\ \\vdots \\\\ \\alpha v_n \\end{pmatrix}\\] <p>\u3064\u307e\u308a\u3001\u30d9\u30af\u30c8\u30eb\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u306f\u300c\u3059\u3079\u3066\u306e\u6210\u5206\u306b\u540c\u3058\u5b9f\u6570\u3092\u304b\u3051\u308b\u300d\u64cd\u4f5c\u3067\u3059\u3002</p> <p>\u4f8b: \\(\\mathbf{v} = \\begin{pmatrix} 2 \\\\ 3 \\\\ -1 \\end{pmatrix}\\) \u306e\u3068\u304d\u3001\\(3\\mathbf{v}\\) \u306f</p> \\[3\\mathbf{v} = \\begin{pmatrix} 3 \\cdot 2 \\\\ 3 \\cdot 3 \\\\ 3 \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 9 \\\\ -3 \\end{pmatrix}\\]"},{"location":"lectures/LA/02-vector-and-matrix/#44","title":"4.4 \u30b9\u30ab\u30e9\u30fc\u500d\u306e\u6027\u8cea","text":"<p>\u30b9\u30ab\u30e9\u30fc\u500d\u306b\u306f\u4ee5\u4e0b\u306e\u6027\u8cea\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\\(1\\mathbf{v} = \\mathbf{v}\\)</li> <li>\\(\\alpha(\\beta\\mathbf{v}) = (\\alpha\\beta)\\mathbf{v}\\)</li> <li>\\((\\alpha + \\beta)\\mathbf{v} = \\alpha\\mathbf{v} + \\beta\\mathbf{v}\\)</li> <li>\\(\\alpha(\\mathbf{v} + \\mathbf{w}) = \\alpha\\mathbf{v} + \\alpha\\mathbf{w}\\)</li> </ol> <p>\u3053\u308c\u3089\u306e\u52a0\u6cd5\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u6027\u8cea\u306b\u3088\u308a\u3001\u30d9\u30af\u30c8\u30eb\u306e\u96c6\u5408\u306f\u300c\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u300d\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#45","title":"4.5 \u30d9\u30af\u30c8\u30eb\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":""},{"location":"lectures/LA/02-vector-and-matrix/#_1","title":"\u4e8c\u6b21\u5143\u5e73\u9762\u4e0a\u306e\u30d9\u30af\u30c8\u30eb","text":"<p>\\(\\mathbb{R}^2\\) \u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v} = (v_1, v_2)^T\\) \u306f\u3001\u539f\u70b9 \\((0,0)\\) \u304b\u3089\u70b9 \\((v_1, v_2)\\) \u3078\u5411\u304b\u3046\u77e2\u5370\u3068\u3057\u3066\u5e7e\u4f55\u5b66\u7684\u306b\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#_2","title":"\u30d9\u30af\u30c8\u30eb\u306e\u548c\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473","text":"<p>2\u3064\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}\\) \u3068 \\(\\mathbf{w}\\) \u306e\u548c \\(\\mathbf{v} + \\mathbf{w}\\) \u306f\u3001\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}\\) \u306e\u7d42\u70b9\u304b\u3089 \\(\\mathbf{w}\\) \u3068\u5e73\u884c\u306a\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u3044\u305f\u3068\u304d\u306e\u7d42\u70b9\u3078\u5411\u304b\u3046\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002\u3053\u308c\u306f\u300c\u5e73\u884c\u56db\u8fba\u5f62\u306e\u6cd5\u5247\u300d\u3068\u3082\u547c\u3070\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#_3","title":"\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473","text":"<p>\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}\\) \u306e\u30b9\u30ab\u30e9\u30fc\u500d \\(\\alpha\\mathbf{v}\\) \u306f\uff1a</p> <ul> <li>\\(\\alpha &gt; 0\\) \u306e\u3068\u304d\uff1a\\(\\mathbf{v}\\) \u3068\u540c\u3058\u65b9\u5411\u3067\u3001\u9577\u3055\u304c \\(|\\alpha|\\) \u500d</li> <li>\\(\\alpha &lt; 0\\) \u306e\u3068\u304d\uff1a\\(\\mathbf{v}\\) \u3068\u53cd\u5bfe\u65b9\u5411\u3067\u3001\u9577\u3055\u304c \\(|\\alpha|\\) \u500d</li> <li>\\(\\alpha = 0\\) \u306e\u3068\u304d\uff1a\u96f6\u30d9\u30af\u30c8\u30eb</li> </ul>"},{"location":"lectures/LA/02-vector-and-matrix/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/02-vector-and-matrix/#51-numpy","title":"5.1 NumPy\u3092\u7528\u3044\u305f\u30d9\u30af\u30c8\u30eb\u6f14\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\nv = np.array([3, 2])\nw = np.array([1, 4])\n\n# \u30d9\u30af\u30c8\u30eb\u306e\u548c\nv_plus_w = v + w\nprint(f\"v + w = {v_plus_w}\")\n\n# \u30b9\u30ab\u30e9\u30fc\u500d\nalpha = 2.5\nalpha_v = alpha * v\nprint(f\"{alpha} * v = {alpha_v}\")\n\n# \u30de\u30a4\u30ca\u30b9\u306e\u30b9\u30ab\u30e9\u30fc\u500d\nbeta = -1.5\nbeta_w = beta * w\nprint(f\"{beta} * w = {beta_w}\")\n</code></pre>"},{"location":"lectures/LA/02-vector-and-matrix/#52","title":"5.2 \u30d9\u30af\u30c8\u30eb\u306e\u53ef\u8996\u5316","text":"<pre><code># \u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\u3059\u308b\u95a2\u6570\ndef plot_vector(vector, origin=[0, 0], color='b', label=None):\n    plt.arrow(origin[0], origin[1], vector[0], vector[1], \n              head_width=0.2, head_length=0.3, fc=color, ec=color, label=label)\n\n# \u30b0\u30e9\u30d5\u306e\u8a2d\u5b9a\nplt.figure(figsize=(10, 8))\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nplt.grid(alpha=0.3)\n\n# \u30d9\u30af\u30c8\u30eb\u306e\u30d7\u30ed\u30c3\u30c8\nplot_vector(v, color='red', label='v')\nplot_vector(w, color='blue', label='w')\nplot_vector(v_plus_w, color='green', label='v + w')\n\n# v + w\u306e\u5225\u89e3\u6cd5: v\u306e\u7d42\u70b9\u304b\u3089w\u3092\u63cf\u304f\nplot_vector(w, origin=v, color='purple', label='w (from v)')\n\n# \u30b9\u30ab\u30e9\u30fc\u500d\u306e\u30d7\u30ed\u30c3\u30c8\nplot_vector(alpha_v, color='orange', label=f'{alpha}v')\nplot_vector(beta_w, color='brown', label=f'{beta}w')\n\n# \u30b0\u30e9\u30d5\u306e\u8a2d\u5b9a\nplt.xlim(-2, 10)\nplt.ylim(-2, 10)\nplt.title('\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.axis('equal')  # \u30a2\u30b9\u30da\u30af\u30c8\u6bd4\u30921:1\u306b\nplt.show()\n</code></pre>"},{"location":"lectures/LA/02-vector-and-matrix/#53","title":"5.3 \u5065\u5eb7\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308b\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u306e\u4f8b","text":"<pre><code># \u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u4f8b\uff08\u5e74\u9f62\u3001\u8eab\u9577\u3001\u4f53\u91cd\u3001\u8840\u5727\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\uff09\npatient1 = np.array([35, 170, 70, 120, 200])\npatient2 = np.array([42, 165, 80, 135, 220])\n\n# \u5e73\u5747\u5024\naverage = (patient1 + patient2) / 2\nprint(\"\u5e73\u5747\u5024:\", average)\n\n# \u5e74\u9f62\u306b\u3088\u308b\u91cd\u307f\u4ed8\u3051\uff08\u5e74\u9f62\u304c\u9ad8\u3044\u307b\u3069\u91cd\u8996\u3059\u308b\u5834\u5408\uff09\nweight1 = patient1[0] / (patient1[0] + patient2[0])  # 35/(35+42) \u2248 0.45\nweight2 = patient2[0] / (patient1[0] + patient2[0])  # 42/(35+42) \u2248 0.55\n\nweighted_avg = weight1 * patient1 + weight2 * patient2\nprint(\"\u5e74\u9f62\u306b\u3088\u308b\u91cd\u307f\u4ed8\u3051\u5e73\u5747:\", weighted_avg)\n</code></pre>"},{"location":"lectures/LA/02-vector-and-matrix/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/02-vector-and-matrix/#_4","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u3092\u884c\u3044\u306a\u3055\u3044\u3002    (a) \\(\\begin{pmatrix} 3 \\\\ -2 \\\\ 5 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 4 \\\\ 2 \\end{pmatrix}\\)    (b) \\(2 \\begin{pmatrix} 4 \\\\ 0 \\\\ -3 \\end{pmatrix} - 3 \\begin{pmatrix} 1 \\\\ -2 \\\\ 2 \\end{pmatrix}\\)</p> </li> <li> <p>\\(\\mathbf{a} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\), \\(\\mathbf{b} = \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}\\) \u3068\u3059\u308b\u3068\u304d\u3001\\(2\\mathbf{a} - \\mathbf{b}\\) \u3092\u6c42\u3081\u306a\u3055\u3044\u3002\u307e\u305f\u3001\u3053\u306e\u8a08\u7b97\u3092\u5e7e\u4f55\u5b66\u7684\u306b\u89e3\u91c8\u3057\u3001\u56f3\u793a\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\) \u3068 \\(\\mathbf{b} = \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\mathbf{x} = 2\\mathbf{a} + \\mathbf{b}\\) \u3068 \\(\\mathbf{y} = \\mathbf{a} - 3\\mathbf{b}\\) \u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/02-vector-and-matrix/#_5","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u3042\u308b\u5730\u57df\u306e5\u65e5\u9593\u306e\u6c17\u6e29\uff08\u00b0C\uff09\u304c \\(\\mathbf{t}_1 = \\begin{pmatrix} 25 \\\\ 27 \\\\ 24 \\\\ 26 \\\\ 28 \\end{pmatrix}\\) \u3067\u3001\u5225\u306e\u5730\u57df\u306e\u540c\u30585\u65e5\u9593\u306e\u6c17\u6e29\u304c \\(\\mathbf{t}_2 = \\begin{pmatrix} 22 \\\\ 23 \\\\ 21 \\\\ 24 \\\\ 25 \\end{pmatrix}\\) \u3060\u3063\u305f\u3068\u3057\u307e\u3059\u3002    (a) \u4e8c\u3064\u306e\u5730\u57df\u306e\u6c17\u6e29\u5dee \\(\\mathbf{t}_1 - \\mathbf{t}_2\\) \u3092\u6c42\u3081\u306a\u3055\u3044\u3002    (b) \u4e21\u5730\u57df\u306e\u5e73\u5747\u6c17\u6e29 \\(\\frac{1}{2}(\\mathbf{t}_1 + \\mathbf{t}_2)\\) \u3092\u6c42\u3081\u306a\u3055\u3044\u3002    (c) \u6c17\u8c61\u4e88\u5831\u306b\u3088\u308b\u3068\u3001\u660e\u65e5\u306e\u6c17\u6e29\u306f\u4eca\u65e5\u3088\u308a2\u00b0C\u4e0a\u6607\u3057\u3001\u660e\u5f8c\u65e5\u306f\u660e\u65e5\u3088\u308a1\u00b0C\u4e0b\u964d\u3059\u308b\u3068\u4e88\u6e2c\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u3092\u7528\u3044\u3066\u3001\u3053\u306e\u4e88\u6e2c\u3092\u8868\u73fe\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>3\u4eba\u306e\u60a3\u8005\u306e\u5065\u5eb7\u30c7\u30fc\u30bf\uff08\u5e74\u9f62\u3001\u53ce\u7e2e\u671f\u8840\u5727\u3001\u62e1\u5f35\u671f\u8840\u5727\u3001\u8840\u7cd6\u5024\uff09\u304c\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u3067\u8868\u3055\u308c\u308b\u3068\u3057\u307e\u3059\uff1a    \\(\\mathbf{p}_1 = \\begin{pmatrix} 45 \\\\ 130 \\\\ 85 \\\\ 95 \\end{pmatrix}\\),     \\(\\mathbf{p}_2 = \\begin{pmatrix} 62 \\\\ 145 \\\\ 90 \\\\ 110 \\end{pmatrix}\\),     \\(\\mathbf{p}_3 = \\begin{pmatrix} 38 \\\\ 120 \\\\ 80 \\\\ 90 \\end{pmatrix}\\)</p> </li> </ol> <p>(a) 3\u4eba\u306e\u5e73\u5747\u5024 \\(\\frac{1}{3}(\\mathbf{p}_1 + \\mathbf{p}_2 + \\mathbf{p}_3)\\) \u3092\u6c42\u3081\u306a\u3055\u3044\u3002    (b) \u6a19\u6e96\u7684\u306a\u5065\u5eb7\u5024\u3092 \\(\\mathbf{s} = \\begin{pmatrix} - \\\\ 120 \\\\ 80 \\\\ 100 \\end{pmatrix}\\) \u3068\u3057\u307e\u3059\uff08\u5e74\u9f62\u306f\u57fa\u6e96\u3068\u3057\u307e\u305b\u3093\uff09\u3002\u5404\u60a3\u8005\u306e\u30c7\u30fc\u30bf\u3068\u6a19\u6e96\u5024\u3068\u306e\u5dee \\(\\mathbf{p}_i - \\mathbf{s}\\) \u3092\u8a08\u7b97\u3057\u3001\u3069\u306e\u60a3\u8005\u304c\u6700\u3082\u6a19\u6e96\u304b\u3089\u96e2\u308c\u3066\u3044\u308b\u304b\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/02-vector-and-matrix/#q1","title":"Q1: \u30d9\u30af\u30c8\u30eb\u306e\u8981\u7d20\uff08\u6210\u5206\uff09\u306f\u5fc5\u305a\u6570\u5024\u3067\u3042\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A1: \u7406\u8ad6\u7684\u306a\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u306f\u3001\u6210\u5206\u306f\u5fc5\u305a\u3057\u3082\u5b9f\u6570\u3067\u3042\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u8907\u7d20\u6570\u3001\u95a2\u6570\u3001\u591a\u9805\u5f0f\u306a\u3069\u3092\u30d9\u30af\u30c8\u30eb\u306e\u8981\u7d20\u3068\u3059\u308b\u5834\u5408\u3082\u3042\u308a\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u672c\u8b1b\u7fa9\u3067\u306f\u4e3b\u306b\u5b9f\u6570\u30d9\u30af\u30c8\u30eb\u3092\u6271\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#q2","title":"Q2: \u7570\u306a\u308b\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u540c\u58eb\u306f\u52a0\u7b97\u3067\u304d\u306a\u3044\u306e\u306f\u306a\u305c\u3067\u3059\u304b\uff1f","text":"<p>A2: \u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\u306f\u300c\u5bfe\u5fdc\u3059\u308b\u6210\u5206\u540c\u58eb\u3092\u8db3\u3059\u300d\u64cd\u4f5c\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u6b21\u5143\uff08\u6210\u5206\u306e\u6570\uff09\u304c\u7570\u306a\u308b\u30d9\u30af\u30c8\u30eb\u540c\u58eb\u3067\u306f\u5bfe\u5fdc\u3059\u308b\u6210\u5206\u304c\u5b9a\u7fa9\u3067\u304d\u305a\u3001\u52a0\u7b97\u306e\u5b9a\u7fa9\u304c\u9069\u7528\u3067\u304d\u306a\u3044\u304b\u3089\u3067\u3059\u3002\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u306f\u3001\u7279\u5fb4\u91cf\u306e\u6570\u304c\u63c3\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u91cd\u8981\u306a\u7406\u7531\u306e\u4e00\u3064\u3067\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#q3","title":"Q3: \u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u3069\u306e\u3088\u3046\u306b\u3057\u3066\u30d9\u30af\u30c8\u30eb\u3092\u6271\u3044\u307e\u3059\u304b\uff1f","text":"<p>A3: \u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\uff08\u4f8b: \u60a3\u8005\u4e00\u4eba\u306e\u30c7\u30fc\u30bf\uff09\u30921\u3064\u306e\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u4e00\u822c\u7684\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u4f53\u91cd\u3001\u8eab\u9577\u3001\u5e74\u9f62\u3001\u8840\u5727\u306a\u3069\u306e\u6e2c\u5b9a\u5024\u306f\u3001\u305d\u306e\u60a3\u8005\u3092\u8868\u3059\u30d9\u30af\u30c8\u30eb\u306e\u6210\u5206\u3068\u306a\u308a\u307e\u3059\u3002\u8907\u6570\u306e\u60a3\u8005\u306e\u30c7\u30fc\u30bf\u306f\u3001\u8907\u6570\u306e\u30d9\u30af\u30c8\u30eb\u306e\u96c6\u307e\u308a\u3001\u3064\u307e\u308a\u884c\u5217\u3068\u3057\u3066\u8868\u73fe\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#q4","title":"Q4: \u30d9\u30af\u30c8\u30eb\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u3068\u5185\u7a4d\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A4: \u30b9\u30ab\u30e9\u30fc\u500d\u306f\u30d9\u30af\u30c8\u30eb\u306b\u5b9f\u6570\u3092\u304b\u3051\u308b\u64cd\u4f5c\u3067\u3001\u7d50\u679c\u306f\u30d9\u30af\u30c8\u30eb\u306b\u306a\u308a\u307e\u3059\u3002\u4e00\u65b9\u3001\u5185\u7a4d\u306f2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u9593\u306e\u6f14\u7b97\u3067\u3001\u7d50\u679c\u306f\u30b9\u30ab\u30e9\u30fc\uff08\u5b9f\u6570\uff09\u306b\u306a\u308a\u307e\u3059\u3002\u5185\u7a4d\u306b\u3064\u3044\u3066\u306f\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u8a73\u3057\u304f\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"lectures/LA/02-vector-and-matrix/#q5","title":"Q5: \u30d9\u30af\u30c8\u30eb\u306f\u5fc5\u305a\u539f\u70b9\u304b\u3089\u59cb\u307e\u308b\u306e\u3067\u3059\u304b\uff1f","text":"<p>A5: \u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u30d9\u30af\u30c8\u30eb\u306f\u300c\u5927\u304d\u3055\u3068\u65b9\u5411\u3092\u6301\u3064\u91cf\u300d\u3067\u3042\u308a\u3001\u5fc5\u305a\u3057\u3082\u539f\u70b9\u304b\u3089\u59cb\u307e\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u305f\u3060\u3057\u3001\u8a08\u7b97\u4e0a\u306f\u539f\u70b9\u304b\u3089\u59cb\u307e\u308b\u30d9\u30af\u30c8\u30eb\uff08\u4f4d\u7f6e\u30d9\u30af\u30c8\u30eb\uff09\u3068\u3057\u3066\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u4fbf\u5229\u3067\u3059\u3002\u4efb\u610f\u306e2\u70b9\u9593\u306e\u30d9\u30af\u30c8\u30eb\u306f\u3001\u305d\u308c\u3089\u306e\u4f4d\u7f6e\u30d9\u30af\u30c8\u30eb\u306e\u5dee\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I \u7b2c3\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8\uff1a\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u64cd\u4f5c\u2461\u3068\u5185\u7a4d\u306e\u5c0e\u5165","text":""},{"location":"lectures/LA/03-vector-and-matrix/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c3\u56de \u95a2\u9023\u9805\u76ee: \u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u3001\u30ce\u30eb\u30e0\u3001\u5c04\u5f71 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u7b2c2\u56de\u8b1b\u7fa9\u3067\u6271\u3063\u305f\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\u3001\u30d9\u30af\u30c8\u30eb\u306e\u548c\u3001\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u6982\u5ff5</p>"},{"location":"lectures/LA/03-vector-and-matrix/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\uff08\u5927\u304d\u3055\uff09\u306e\u5b9a\u7fa9\u3092\u7406\u89e3\u3057\u3001\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u306e\u5b9a\u7fa9\u3068\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u7406\u89e3\u3059\u308b</li> <li>\u5185\u7a4d\u3092\u7528\u3044\u3066\u30d9\u30af\u30c8\u30eb\u9593\u306e\u89d2\u5ea6\u3092\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u30d9\u30af\u30c8\u30eb\u306e\u5c04\u5f71\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3057\u3001\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u3053\u308c\u3089\u306e\u6982\u5ff5\u3092\u30c7\u30fc\u30bf\u5206\u6790\u306e\u6587\u8108\u3067\u5fdc\u7528\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3076</li> </ol>"},{"location":"lectures/LA/03-vector-and-matrix/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/03-vector-and-matrix/#31","title":"3.1 \u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\uff08\u5927\u304d\u3055\uff09","text":"<p>\u5b9a\u7fa9: \\(n\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)^T\\) \u306e\u30ce\u30eb\u30e0\uff08\u9577\u3055\u30fb\u5927\u304d\u3055\uff09\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a</p> \\[\\|\\mathbf{x}\\| = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_n^2}\\] <p>\u30ce\u30eb\u30e0\u306f\u3001\u539f\u70b9\u304b\u3089\u30d9\u30af\u30c8\u30eb\u306e\u7d42\u70b9\u307e\u3067\u306e\u8ddd\u96e2\u3092\u8868\u3057\u307e\u3059\u30022\u6b21\u5143\u30fb3\u6b21\u5143\u306e\u5834\u5408\u306f\u3001\u30d4\u30bf\u30b4\u30e9\u30b9\u306e\u5b9a\u7406\u304b\u3089\u5c0e\u304b\u308c\u308b\u8ddd\u96e2\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u4f8b: \u30d9\u30af\u30c8\u30eb \\(\\mathbf{v} = (3, 4)^T\\) \u306e\u30ce\u30eb\u30e0\u3092\u8a08\u7b97</p> \\[\\|\\mathbf{v}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\\] <p>\u30ce\u30eb\u30e0\u306e\u6027\u8cea:</p> <ol> <li>\u975e\u8ca0\u6027: \\(\\|\\mathbf{x}\\| \\geq 0\\) \uff08\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\u306e\u5834\u5408\u306e\u307f\u7b49\u53f7\u6210\u7acb\uff09</li> <li>\u30b9\u30ab\u30e9\u30fc\u500d: \\(\\|c\\mathbf{x}\\| = |c|\\|\\mathbf{x}\\|\\) \uff08\\(c\\)\u306f\u30b9\u30ab\u30e9\u30fc\uff09</li> <li>\u4e09\u89d2\u4e0d\u7b49\u5f0f: \\(\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|\\)</li> </ol>"},{"location":"lectures/LA/03-vector-and-matrix/#32","title":"3.2 \u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d","text":"<p>\u5b9a\u7fa9: \\(n\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)^T\\) \u3068 \\(\\mathbf{y} = (y_1, y_2, \\ldots, y_n)^T\\) \u306e\u5185\u7a4d\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a</p> \\[\\mathbf{x} \\cdot \\mathbf{y} = x_1y_1 + x_2y_2 + \\ldots + x_ny_n = \\sum_{i=1}^{n} x_iy_i\\] <p>\u5185\u7a4d\u306f\u3001\u4e8c\u3064\u306e\u30d9\u30af\u30c8\u30eb\u306e\u5bfe\u5fdc\u3059\u308b\u6210\u5206\u306e\u7a4d\u306e\u7dcf\u548c\u3068\u3057\u3066\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002</p> <p>\u4f8b: \u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = (2, 3, 1)^T\\) \u3068 \\(\\mathbf{b} = (1, 0, 4)^T\\) \u306e\u5185\u7a4d\u3092\u8a08\u7b97</p> \\[\\mathbf{a} \\cdot \\mathbf{b} = 2 \\times 1 + 3 \\times 0 + 1 \\times 4 = 2 + 0 + 4 = 6\\] <p>\u5185\u7a4d\u306e\u6027\u8cea:</p> <ol> <li>\u5bfe\u79f0\u6027: \\(\\mathbf{x} \\cdot \\mathbf{y} = \\mathbf{y} \\cdot \\mathbf{x}\\)</li> <li>\u7dda\u5f62\u6027: \\((\\alpha\\mathbf{x} + \\beta\\mathbf{y}) \\cdot \\mathbf{z} = \\alpha(\\mathbf{x} \\cdot \\mathbf{z}) + \\beta(\\mathbf{y} \\cdot \\mathbf{z})\\)</li> <li>\u6b63\u5b9a\u5024\u6027: \\(\\mathbf{x} \\cdot \\mathbf{x} \\geq 0\\) \uff08\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\u306e\u5834\u5408\u306e\u307f\u7b49\u53f7\u6210\u7acb\uff09</li> </ol>"},{"location":"lectures/LA/03-vector-and-matrix/#33","title":"3.3 \u5185\u7a4d\u3068\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\u306e\u95a2\u4fc2","text":"<p>\u5185\u7a4d\u3068\u30ce\u30eb\u30e0\u306b\u306f\u4ee5\u4e0b\u306e\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\uff1a</p> \\[\\mathbf{x} \\cdot \\mathbf{x} = \\|\\mathbf{x}\\|^2\\] <p>\u3064\u307e\u308a\u3001\u30d9\u30af\u30c8\u30eb\u81ea\u8eab\u3068\u306e\u5185\u7a4d\u306f\u305d\u306e\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\u306e2\u4e57\u306b\u7b49\u3057\u3044\u3067\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#34","title":"3.4 \u5185\u7a4d\u3068\u89d2\u5ea6\u306e\u95a2\u4fc2","text":"<p>\u5b9a\u7fa9: \u4e8c\u3064\u306e\u975e\u30bc\u30ed\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u3068 \\(\\mathbf{y}\\) \u304c\u306a\u3059\u89d2 \\(\\theta\\) \u306f\u3001\u4ee5\u4e0b\u306e\u5f0f\u3067\u8a08\u7b97\u3067\u304d\u308b\uff1a</p> \\[\\cos \\theta = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}\\] <p>\u3053\u306e\u95a2\u4fc2\u306f\u3001\u5185\u7a4d\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u3068\u3057\u3066\u91cd\u8981\u3067\u3059\u3002</p> <p>\u4f8b: \u30d9\u30af\u30c8\u30eb \\(\\mathbf{u} = (1, 1)^T\\) \u3068 \\(\\mathbf{v} = (0, 1)^T\\) \u306e\u306a\u3059\u89d2\u3092\u8a08\u7b97</p> \\[\\mathbf{u} \\cdot \\mathbf{v} = 1 \\times 0 + 1 \\times 1 = 1$$ $$\\|\\mathbf{u}\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$$ $$\\|\\mathbf{v}\\| = \\sqrt{0^2 + 1^2} = 1\\] \\[\\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|} = \\frac{1}{\\sqrt{2} \\times 1} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2}\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\theta = 45^\\circ\\)\uff08\\(\\pi/4\\) \u30e9\u30b8\u30a2\u30f3\uff09\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#35","title":"3.5 \u76f4\u4ea4\u30d9\u30af\u30c8\u30eb","text":"<p>\u5b9a\u7fa9: \u4e8c\u3064\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u3068 \\(\\mathbf{y}\\) \u304c\u76f4\u4ea4\u3059\u308b\u3068\u306f\u3001\u305d\u306e\u5185\u7a4d\u304c\u30bc\u30ed\u306b\u306a\u308b\u3053\u3068\u3092\u610f\u5473\u3059\u308b\uff1a</p> \\[\\mathbf{x} \\cdot \\mathbf{y} = 0\\] <p>\u3053\u308c\u306f\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u4e8c\u3064\u306e\u30d9\u30af\u30c8\u30eb\u304c90\u5ea6\uff08\\(\\pi/2\\) \u30e9\u30b8\u30a2\u30f3\uff09\u306e\u89d2\u5ea6\u3092\u306a\u3059\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> <p>\u4f8b: \u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = (3, 4)^T\\) \u3068 \\(\\mathbf{b} = (4, -3)^T\\) \u304c\u76f4\u4ea4\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> \\[\\mathbf{a} \\cdot \\mathbf{b} = 3 \\times 4 + 4 \\times (-3) = 12 - 12 = 0\\] <p>\u3088\u3063\u3066\u3001\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a}\\) \u3068 \\(\\mathbf{b}\\) \u306f\u76f4\u4ea4\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/03-vector-and-matrix/#41","title":"4.1 \u30d9\u30af\u30c8\u30eb\u306e\u5c04\u5f71","text":"<p>\u30d9\u30af\u30c8\u30eb\u306e\u5c04\u5f71\u306f\u3001\u3042\u308b\u30d9\u30af\u30c8\u30eb\u3092\u5225\u306e\u30d9\u30af\u30c8\u30eb\u306b\u6295\u5f71\u3057\u305f\u3068\u304d\u306e\u6210\u5206\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9: \u30d9\u30af\u30c8\u30eb \\(\\mathbf{b}\\) \u4e0a\u3078\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a}\\) \u306e\u5c04\u5f71\u30d9\u30af\u30c8\u30eb \\(\\text{proj}_{\\mathbf{b}}\\mathbf{a}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a</p> \\[\\text{proj}_{\\mathbf{b}}\\mathbf{a} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{b}\\|^2} \\mathbf{b}\\] <p>\u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u306e\u5927\u304d\u3055\uff08\u30b9\u30ab\u30e9\u30fc\u5c04\u5f71\uff09\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3055\u308c\u307e\u3059\uff1a</p> \\[\\|\\text{proj}_{\\mathbf{b}}\\mathbf{a}\\| = \\frac{|\\mathbf{a} \\cdot \\mathbf{b}|}{\\|\\mathbf{b}\\|}\\] <p>\u4f8b: \u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = (2, 3)^T\\) \u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{b} = (1, 0)^T\\) \u4e0a\u3078\u306e\u5c04\u5f71\u3092\u8a08\u7b97</p> \\[\\mathbf{a} \\cdot \\mathbf{b} = 2 \\times 1 + 3 \\times 0 = 2$$ $$\\|\\mathbf{b}\\|^2 = 1^2 + 0^2 = 1\\] \\[\\text{proj}_{\\mathbf{b}}\\mathbf{a} = \\frac{2}{1} (1, 0)^T = (2, 0)^T\\] <p>\u3053\u306e\u7d50\u679c\u306f\u3001\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = (2, 3)^T\\) \u306e\\(x\\)\u8ef8\u65b9\u5411\u306e\u6210\u5206\u304c2\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#42","title":"4.2 \u5c04\u5f71\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":"<p>\u5c04\u5f71\u306f\u3001\u3042\u308b\u30d9\u30af\u30c8\u30eb\u3092\u5225\u306e\u30d9\u30af\u30c8\u30eb\u65b9\u5411\u306b\u5206\u89e3\u3059\u308b\u64cd\u4f5c\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4f8b\u3048\u3070\u30013\u6b21\u5143\u7a7a\u9593\u5185\u306e\u30d9\u30af\u30c8\u30eb\u3092\\(x\\)\u8ef8\u3001\\(y\\)\u8ef8\u3001\\(z\\)\u8ef8\u65b9\u5411\u3078\u306e\u5c04\u5f71\u306b\u5206\u89e3\u3059\u308b\u3053\u3068\u3067\u3001\u305d\u306e\u30d9\u30af\u30c8\u30eb\u306e\u5404\u8ef8\u65b9\u5411\u306e\u6210\u5206\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u5c04\u5f71\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u3068\u3057\u3066\u91cd\u8981\u306a\u306e\u306f\uff1a - \u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u306f\u5143\u306e\u30d9\u30af\u30c8\u30eb\u306e\u300c\u5f71\u300d\u306e\u3088\u3046\u306a\u3082\u306e - \u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u306f\u5e38\u306b\u5c04\u5f71\u5148\u306e\u30d9\u30af\u30c8\u30eb\u3068\u540c\u3058\u65b9\u5411\uff08\u307e\u305f\u306f\u305d\u306e\u53cd\u5bfe\u65b9\u5411\uff09 - \u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u3068\u5143\u306e\u30d9\u30af\u30c8\u30eb\u306e\u5dee\u30d9\u30af\u30c8\u30eb\u306f\u3001\u5c04\u5f71\u5148\u306e\u30d9\u30af\u30c8\u30eb\u3068\u76f4\u4ea4\u3059\u308b</p>"},{"location":"lectures/LA/03-vector-and-matrix/#43","title":"4.3 \u76f4\u4ea4\u5206\u89e3","text":"<p>\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a}\\) \u306f\u3001\u3042\u308b\u30d9\u30af\u30c8\u30eb \\(\\mathbf{b}\\) \u306e\u65b9\u5411\u306b\u5e73\u884c\u306a\u6210\u5206\u3068\u5782\u76f4\u306a\u6210\u5206\u306b\u5206\u89e3\u3067\u304d\u307e\u3059\uff1a</p> \\[\\mathbf{a} = \\text{proj}_{\\mathbf{b}}\\mathbf{a} + \\mathbf{a_{\\perp}}\\] <p>\u3053\u3053\u3067\u3001\\(\\mathbf{a_{\\perp}}\\) \u306f \\(\\mathbf{b}\\) \u306b\u5782\u76f4\u306a\u6210\u5206\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a</p> \\[\\mathbf{a_{\\perp}} = \\mathbf{a} - \\text{proj}_{\\mathbf{b}}\\mathbf{a}\\] <p>\u4f8b: \u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = (2, 3)^T\\) \u3092\u30d9\u30af\u30c8\u30eb \\(\\mathbf{b} = (1, 0)^T\\) \u306b\u5e73\u884c\u306a\u6210\u5206\u3068\u5782\u76f4\u306a\u6210\u5206\u306b\u5206\u89e3</p> <p>\u5e73\u884c\u6210\u5206: \\(\\text{proj}_{\\mathbf{b}}\\mathbf{a} = (2, 0)^T\\)\uff08\u4e0a\u8a18\u306e\u8a08\u7b97\u7d50\u679c\uff09 \u5782\u76f4\u6210\u5206: \\(\\mathbf{a_{\\perp}} = \\mathbf{a} - \\text{proj}_{\\mathbf{b}}\\mathbf{a} = (2, 3)^T - (2, 0)^T = (0, 3)^T\\)</p>"},{"location":"lectures/LA/03-vector-and-matrix/#44","title":"4.4 \u30b3\u30fc\u30b7\u30fc\u30fb\u30b7\u30e5\u30ef\u30eb\u30c4\u306e\u4e0d\u7b49\u5f0f","text":"<p>\u5b9a\u7406: \u4efb\u610f\u306e\u4e8c\u3064\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u3068 \\(\\mathbf{y}\\) \u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u4e0d\u7b49\u5f0f\u304c\u6210\u308a\u7acb\u3064\uff1a</p> \\[|\\mathbf{x} \\cdot \\mathbf{y}| \\leq \\|\\mathbf{x}\\| \\|\\mathbf{y}\\|\\] <p>\u7b49\u53f7\u306f\u3001\\(\\mathbf{x}\\) \u3068 \\(\\mathbf{y}\\) \u304c\u5e73\u884c\uff08\u307e\u305f\u306f\u4e00\u65b9\u304c\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\uff09\u306e\u3068\u304d\u306b\u6210\u7acb\u3059\u308b\u3002</p> <p>\u3053\u306e\u4e0d\u7b49\u5f0f\u306f\u3001\u5185\u7a4d\u306e\u7d76\u5bfe\u5024\u304c\u30ce\u30eb\u30e0\u306e\u7a4d\u3092\u8d85\u3048\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u3001\\(|\\cos \\theta| \\leq 1\\) \u3068\u3044\u3046\u4e8b\u5b9f\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/03-vector-and-matrix/#51-numpy","title":"5.1 NumPy\u3092\u4f7f\u3063\u305f\u30d9\u30af\u30c8\u30eb\u6f14\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\na = np.array([2, 3])\nb = np.array([1, 0])\n\n# \u30ce\u30eb\u30e0\u306e\u8a08\u7b97\nnorm_a = np.linalg.norm(a)\nnorm_b = np.linalg.norm(b)\nprint(f\"\u30d9\u30af\u30c8\u30eba\u306e\u30ce\u30eb\u30e0: {norm_a}\")\nprint(f\"\u30d9\u30af\u30c8\u30ebb\u306e\u30ce\u30eb\u30e0: {norm_b}\")\n\n# \u5185\u7a4d\u306e\u8a08\u7b97\ndot_product = np.dot(a, b)\nprint(f\"\u30d9\u30af\u30c8\u30eba\u3068b\u306e\u5185\u7a4d: {dot_product}\")\n\n# \u89d2\u5ea6\u306e\u8a08\u7b97\uff08\u30e9\u30b8\u30a2\u30f3\uff09\nangle_rad = np.arccos(dot_product / (norm_a * norm_b))\n# \u89d2\u5ea6\uff08\u5ea6\uff09\nangle_deg = np.degrees(angle_rad)\nprint(f\"\u30d9\u30af\u30c8\u30eba\u3068b\u306e\u306a\u3059\u89d2: {angle_deg:.2f}\u5ea6\")\n\n# \u5c04\u5f71\u306e\u8a08\u7b97\nproj_a_on_b = (dot_product / (norm_b**2)) * b\nprint(f\"\u30d9\u30af\u30c8\u30eba\u306eb\u3078\u306e\u5c04\u5f71: {proj_a_on_b}\")\n\n# \u76f4\u4ea4\u6210\u5206\u306e\u8a08\u7b97\nperp_component = a - proj_a_on_b\nprint(f\"\u30d9\u30af\u30c8\u30eba\u306eb\u306b\u5782\u76f4\u306a\u6210\u5206: {perp_component}\")\n</code></pre>"},{"location":"lectures/LA/03-vector-and-matrix/#52","title":"5.2 \u30d9\u30af\u30c8\u30eb\u306e\u53ef\u8996\u5316","text":"<pre><code>def plot_vectors_and_projection(a, b, proj_a_on_b, perp_component):\n    plt.figure(figsize=(10, 8))\n\n    # \u30d9\u30af\u30c8\u30eb\u306e\u539f\u70b9\n    origin = np.array([0, 0])\n\n    # \u30d9\u30af\u30c8\u30eba\u3068b\u3092\u63cf\u753b\n    plt.arrow(*origin, *a, head_width=0.2, head_length=0.3, fc='blue', ec='blue', label='\u30d9\u30af\u30c8\u30eba')\n    plt.arrow(*origin, *b, head_width=0.2, head_length=0.3, fc='red', ec='red', label='\u30d9\u30af\u30c8\u30ebb')\n\n    # \u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\n    plt.arrow(*origin, *proj_a_on_b, head_width=0.2, head_length=0.3, fc='green', ec='green', label='\u5c04\u5f71\u30d9\u30af\u30c8\u30eb')\n\n    # \u5782\u76f4\u6210\u5206\u3092\u63cf\u753b\uff08\u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u306e\u7d42\u70b9\u304b\u3089\uff09\n    plt.arrow(*proj_a_on_b, *perp_component, head_width=0.2, head_length=0.3, fc='purple', ec='purple', label='\u5782\u76f4\u6210\u5206')\n\n    # \u5c04\u5f71\u7dda\u3092\u70b9\u7dda\u3067\u63cf\u753b\n    plt.plot([a[0], proj_a_on_b[0]], [a[1], proj_a_on_b[1]], 'k--')\n\n    # \u8ef8\u306e\u8a2d\u5b9a\n    plt.grid(True)\n    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n    # \u30b0\u30e9\u30d5\u306e\u7bc4\u56f2\u8a2d\u5b9a\n    margin = 1\n    max_val = max(np.max(np.abs(a)), np.max(np.abs(b))) + margin\n    plt.xlim(-max_val, max_val)\n    plt.ylim(-max_val, max_val)\n\n    plt.gca().set_aspect('equal')\n    plt.title('\u30d9\u30af\u30c8\u30eb\u306e\u5c04\u5f71\u3068\u76f4\u4ea4\u5206\u89e3')\n    plt.legend()\n    plt.show()\n\n# \u30d9\u30af\u30c8\u30eb\u3092\u53ef\u8996\u5316\nplot_vectors_and_projection(a, b, proj_a_on_b, perp_component)\n</code></pre>"},{"location":"lectures/LA/03-vector-and-matrix/#53","title":"5.3 \u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u5185\u7a4d\u306e\u5fdc\u7528\u4f8b","text":"<pre><code># \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\uff08\u8eab\u9577\u3068\u4f53\u91cd\uff09\nheights = np.array([170, 175, 165, 180, 160])  # cm\nweights = np.array([65, 70, 60, 75, 55])      # kg\n\n# \u5e73\u5747\u3092\u5f15\u3044\u3066\u4e2d\u5fc3\u5316\nheights_centered = heights - np.mean(heights)\nweights_centered = weights - np.mean(weights)\n\n# \u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97\uff08\u5185\u7a4d\u3092\u4f7f\u7528\uff09\ncorrelation = np.dot(heights_centered, weights_centered) / (np.linalg.norm(heights_centered) * np.linalg.norm(weights_centered))\n\nprint(f\"\u8eab\u9577\u3068\u4f53\u91cd\u306e\u76f8\u95a2\u4fc2\u6570: {correlation:.4f}\")\n\n# \u6563\u5e03\u56f3\u306e\u63cf\u753b\nplt.figure(figsize=(8, 6))\nplt.scatter(heights, weights)\nplt.xlabel('\u8eab\u9577 (cm)')\nplt.ylabel('\u4f53\u91cd (kg)')\nplt.title(f'\u8eab\u9577\u3068\u4f53\u91cd\u306e\u95a2\u4fc2 (\u76f8\u95a2\u4fc2\u6570: {correlation:.4f})')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/03-vector-and-matrix/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/03-vector-and-matrix/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li>\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\u305b\u3088\u3002</li> <li>\\(\\mathbf{a} = (1, 2, 3)^T\\)</li> <li>\\(\\mathbf{b} = (5, 0, -5)^T\\)</li> <li> <p>\\(\\mathbf{c} = (2, 2, 2, 2)^T\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u306e\u7d44\u306e\u5185\u7a4d\u3092\u8a08\u7b97\u305b\u3088\u3002</p> </li> <li>\\(\\mathbf{u} = (3, -1, 2)^T\\) \u3068 \\(\\mathbf{v} = (2, 4, 1)^T\\)</li> <li> <p>\\(\\mathbf{p} = (1, 1, 1, 1)^T\\) \u3068 \\(\\mathbf{q} = (2, -2, 3, -3)^T\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u306e\u7d44\u304c\u306a\u3059\u89d2\u5ea6\uff08\u5ea6\uff09\u3092\u8a08\u7b97\u305b\u3088\u3002</p> </li> <li>\\(\\mathbf{a} = (1, 1)^T\\) \u3068 \\(\\mathbf{b} = (1, -1)^T\\)</li> <li> <p>\\(\\mathbf{c} = (3, 0, 4)^T\\) \u3068 \\(\\mathbf{d} = (5, 0, 0)^T\\)</p> </li> <li> <p>\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = (3, 4, 0)^T\\) \u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{b} = (1, 1, 1)^T\\) \u4e0a\u3078\u306e\u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u306e\u7d44\u304c\u76f4\u4ea4\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002</p> </li> <li>\\(\\mathbf{u} = (2, -1, 3)^T\\) \u3068 \\(\\mathbf{v} = (1, 2, 0)^T\\)</li> <li>\\(\\mathbf{p} = (4, 3)^T\\) \u3068 \\(\\mathbf{q} = (3, -4)^T\\)</li> </ol>"},{"location":"lectures/LA/03-vector-and-matrix/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>3\u6b21\u5143\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v} = (2, 3, 4)^T\\) \u3092\u3001\\(\\mathbf{u}_1 = (1, 0, 0)^T\\), \\(\\mathbf{u}_2 = (0, 1, 0)^T\\), \\(\\mathbf{u}_3 = (0, 0, 1)^T\\) \u306e3\u3064\u306e\u65b9\u5411\u3078\u306e\u5c04\u5f71\u306e\u548c\u3068\u3057\u3066\u8868\u305b\u3002\u3053\u308c\u306f\u4f55\u3092\u610f\u5473\u3059\u308b\u304b\u8aac\u660e\u305b\u3088\u3002</p> </li> <li> <p>\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = (2, 1, 3)^T\\) \u3068 \\(\\mathbf{b} = (1, -1, 2)^T\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\mathbf{c} = \\mathbf{a} - \\text{proj}_{\\mathbf{b}}\\mathbf{a}\\) \u3092\u8a08\u7b97\u305b\u3088\u3002\\(\\mathbf{c}\\) \u3068 \\(\\mathbf{b}\\) \u304c\u76f4\u4ea4\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u305b\u3088\u3002</p> </li> <li> <p>\u30b3\u30fc\u30b7\u30fc\u30fb\u30b7\u30e5\u30ef\u30eb\u30c4\u306e\u4e0d\u7b49\u5f0f \\(|\\mathbf{x} \\cdot \\mathbf{y}| \\leq \\|\\mathbf{x}\\| \\|\\mathbf{y}\\|\\) \u3092\u3001\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} = (1, 2)^T\\) \u3068 \\(\\mathbf{y} = (3, 4)^T\\) \u3092\u7528\u3044\u3066\u78ba\u8a8d\u305b\u3088\u3002</p> </li> <li> <p>3\u6b21\u5143\u7a7a\u9593\u5185\u306e\u3042\u308b\u5e73\u9762\u304c\u3001\u6cd5\u7dda\u30d9\u30af\u30c8\u30eb \\(\\mathbf{n} = (1, 2, 3)^T\\) \u3067\u8868\u3055\u308c\u308b\u3068\u3059\u308b\u3002\u70b9 \\(P(2, 3, 4)\\) \u304b\u3089\u3053\u306e\u5e73\u9762\u3078\u306e\u6700\u77ed\u8ddd\u96e2\u3092\u6c42\u3081\u3088\u3002(\u30d2\u30f3\u30c8: \u539f\u70b9\u304b\u3089\u5e73\u9762\u307e\u3067\u306e\u8ddd\u96e2\u3092\u8003\u3048\u3001\u305d\u3053\u304b\u3089\u30d9\u30af\u30c8\u30eb\u306e\u5c04\u5f71\u3092\u5229\u7528\u3059\u308b)</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306e\u5fdc\u7528: 5\u4eba\u306e\u60a3\u8005\u306e\u8840\u5727\uff08\u53ce\u7e2e\u671f/\u62e1\u5f35\u671f\uff09\u30c7\u30fc\u30bf\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u3066\u3044\u308b\u3002</p> </li> <li>\u60a3\u80051: (120, 80)</li> <li>\u60a3\u80052: (130, 85)</li> <li>\u60a3\u80053: (140, 90)</li> <li>\u60a3\u80054: (125, 75)</li> <li>\u60a3\u80055: (135, 88)</li> </ol> <p>\u3053\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u53ce\u7e2e\u671f\u8840\u5727\u3068\u62e1\u5f35\u671f\u8840\u5727\u306e\u76f8\u95a2\u4fc2\u6570\u3092\u5185\u7a4d\u3092\u4f7f\u3063\u3066\u8a08\u7b97\u305b\u3088\u3002\u307e\u305f\u3001\u5404\u60a3\u8005\u306e\u30c7\u30fc\u30bf\u3092\u3001\u5e73\u5747\u5024\u306e\u30d9\u30af\u30c8\u30eb\u3068\u3001\u305d\u308c\u306b\u76f4\u4ea4\u3059\u308b\u65b9\u5411\u306e\u6210\u5206\u306b\u5206\u89e3\u305b\u3088\u3002\u3053\u308c\u306f\u3069\u306e\u3088\u3046\u306a\u610f\u5473\u3092\u6301\u3064\u304b\u8003\u5bdf\u305b\u3088\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/03-vector-and-matrix/#q1","title":"Q1: \u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u3068\u30ce\u30eb\u30e0\u306e\u95a2\u4fc2\u3092\u7c21\u5358\u306b\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A1: \u30d9\u30af\u30c8\u30eb\u81ea\u8eab\u3068\u306e\u5185\u7a4d\u306f\u3001\u305d\u306e\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\u306e2\u4e57\u306b\u306a\u308a\u307e\u3059\u3002\u3064\u307e\u308a\u3001\\(\\mathbf{v} \\cdot \\mathbf{v} = \\|\\mathbf{v}\\|^2\\) \u3067\u3059\u3002\u3053\u308c\u306f\u3001\u5404\u6210\u5206\u306e2\u4e57\u548c\u3092\u8a08\u7b97\u3059\u308b\u70b9\u3067\u5171\u901a\u3057\u3066\u3044\u308b\u305f\u3081\u3067\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#q2-0","title":"Q2: \u5185\u7a4d\u304c0\u306e\u3068\u304d\u3001\u30d9\u30af\u30c8\u30eb\u540c\u58eb\u306f\u3069\u306e\u3088\u3046\u306a\u95a2\u4fc2\u306b\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A2: \u5185\u7a4d\u304c0\u306e\u4e8c\u3064\u306e\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4\uff08\u5782\u76f4\uff09\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u3001\\(\\cos \\theta = 0\\) \u3068\u306a\u308b\u89d2\u5ea6 \\(\\theta = 90\u00b0\\) \uff08\u307e\u305f\u306f \\(\\pi/2\\) \u30e9\u30b8\u30a2\u30f3\uff09\u3067\u4ea4\u308f\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#q3","title":"Q3: \u306a\u305c\u30c7\u30fc\u30bf\u5206\u6790\u3067\u5185\u7a4d\u304c\u91cd\u8981\u306a\u306e\u3067\u3059\u304b\uff1f","text":"<p>A3: \u5185\u7a4d\u306f\u3001\u4e8c\u3064\u306e\u30c7\u30fc\u30bf\u30d9\u30af\u30c8\u30eb\u9593\u306e\u985e\u4f3c\u6027\u3084\u95a2\u9023\u6027\u3092\u6e2c\u308b\u57fa\u672c\u7684\u306a\u9053\u5177\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u6b63\u898f\u5316\u3055\u308c\u305f\u4e8c\u3064\u306e\u30c7\u30fc\u30bf\u30d9\u30af\u30c8\u30eb\u9593\u306e\u5185\u7a4d\u306f\u76f8\u95a2\u4fc2\u6570\u306b\u306a\u308a\u3001\u30c7\u30fc\u30bf\u306e\u7dda\u5f62\u95a2\u4fc2\u3092\u6e2c\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u307e\u305f\u3001\u5185\u7a4d\u306b\u57fa\u3065\u304f\u5c04\u5f71\u306f\u3001\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b\u3084\u7279\u5fb4\u62bd\u51fa\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#q4","title":"Q4: \u30d9\u30af\u30c8\u30eb\u306e\u5c04\u5f71\u3068\u306f\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u304b\uff1f","text":"<p>A4: \u30d9\u30af\u30c8\u30eb \\(\\mathbf{a}\\) \u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{b}\\) \u3078\u306e\u5c04\u5f71\u3068\u306f\u3001\\(\\mathbf{a}\\) \u3092 \\(\\mathbf{b}\\) \u306e\u65b9\u5411\u306b\u843d\u3068\u3057\u305f\u300c\u5f71\u300d\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002\u3053\u308c\u306f \\(\\mathbf{b}\\) \u3068\u540c\u3058\uff08\u307e\u305f\u306f\u53cd\u5bfe\uff09\u65b9\u5411\u3092\u6301\u3064\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308a\u3001\\(\\mathbf{a}\\) \u304c \\(\\mathbf{b}\\) \u306e\u65b9\u5411\u306b\u3069\u308c\u3060\u3051\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/03-vector-and-matrix/#q5","title":"Q5: \u5185\u7a4d\u3068\u30d9\u30af\u30c8\u30eb\u306e\u9577\u3055\u3060\u3051\u304b\u3089\u89d2\u5ea6\u304c\u8a08\u7b97\u3067\u304d\u308b\u306e\u306f\u306a\u305c\u3067\u3059\u304b\uff1f","text":"<p>A5: \u3053\u308c\u306f\u4f59\u5f26\u5b9a\u7406\u306e\u5fdc\u7528\u3067\u3059\u3002\u4e8c\u3064\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a}\\) \u3068 \\(\\mathbf{b}\\) \u306e\u306a\u3059\u89d2\u3092 \\(\\theta\\) \u3068\u3059\u308b\u3068\u3001\u4f59\u5f26\u5b9a\u7406\u304b\u3089 \\(\\|\\mathbf{a} - \\mathbf{b}\\|^2 = \\|\\mathbf{a}\\|^2 + \\|\\mathbf{b}\\|^2 - 2\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\cos\\theta\\) \u304c\u6210\u308a\u7acb\u3061\u307e\u3059\u3002\u307e\u305f\u3001\\(\\|\\mathbf{a} - \\mathbf{b}\\|^2 = (\\mathbf{a} - \\mathbf{b}) \\cdot (\\mathbf{a} - \\mathbf{b}) = \\|\\mathbf{a}\\|^2 + \\|\\mathbf{b}\\|^2 - 2(\\mathbf{a} \\cdot \\mathbf{b})\\) \u3067\u3059\u3002\u3053\u308c\u3089\u4e8c\u3064\u306e\u5f0f\u3092\u6bd4\u8f03\u3059\u308b\u3068\u3001\\(\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\cos\\theta\\) \u304c\u5c0e\u304b\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/04-vector-and-matrix/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I \u7b2c4\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8\uff1a\u884c\u5217\u306e\u5b9a\u7fa9\u30fb\u884c\u5217\u306e\u548c\u30fb\u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d","text":""},{"location":"lectures/LA/04-vector-and-matrix/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<ul> <li>\u8b1b\u7fa9\u56de: \u7b2c4\u56de</li> <li>\u95a2\u9023\u9805\u76ee: \u30d9\u30af\u30c8\u30eb\u6f14\u7b97\uff08\u7b2c2-3\u56de\u306e\u5185\u5bb9\uff09</li> <li>\u4e88\u7fd2\u5185\u5bb9: \u30d9\u30af\u30c8\u30eb\u306e\u548c\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u3001\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u306e\u5fa9\u7fd2</li> </ul>"},{"location":"lectures/LA/04-vector-and-matrix/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u3053\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u80fd\u529b\u3092\u8eab\u306b\u3064\u3051\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3057\u307e\u3059\uff1a</p> <ol> <li>\u884c\u5217\u306e\u5b9a\u7fa9\u3092\u7406\u89e3\u3057\u3001\u9069\u5207\u306b\u8868\u8a18\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u548c\u3092\u6b63\u78ba\u306b\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u3092\u6b63\u78ba\u306b\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u95a2\u4fc2\u6027\u3092\u7406\u89e3\u3067\u304d\u308b</li> <li>Google Colab\u3092\u7528\u3044\u3066\u884c\u5217\u8a08\u7b97\u3092\u5b9f\u884c\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/04-vector-and-matrix/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/04-vector-and-matrix/#31","title":"3.1 \u884c\u5217\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \u884c\u5217\uff08Matrix\uff09\u3068\u306f\u3001\u6570\u3084\u8a18\u53f7\u3092\u7e26\u3068\u6a2a\u306b\u77e9\u5f62\u72b6\u306b\u914d\u7f6e\u3057\u305f\u3082\u306e\u3067\u3059\u3002\\(m\\)\u884c\\(n\\)\u5217\u306e\u884c\u5217\\(A\\)\u306f\u6b21\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}\\] <p>\u3053\u3053\u3067\u3001\\(a_{ij}\\)\u306f\\(i\\)\u884c\\(j\\)\u5217\u76ee\u306e\u8981\u7d20\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u30b5\u30a4\u30ba: \u884c\u5217\u306e\u30b5\u30a4\u30ba\u306f\u884c\u6570\u00d7\u5217\u6570\u3067\u8868\u3057\u3001\\(m \\times n\\)\u884c\u5217\u306a\u3069\u3068\u547c\u3073\u307e\u3059\u3002</p> <p>\u4f8b:</p> \\[A = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{pmatrix}\\] <p>\u3053\u306e\u884c\u5217\\(A\\)\u306f\\(2 \\times 3\\)\u884c\u5217\uff082\u884c3\u5217\u306e\u884c\u5217\uff09\u3067\u3059\u3002</p>"},{"location":"lectures/LA/04-vector-and-matrix/#32","title":"3.2 \u7279\u6b8a\u306a\u5f62\u72b6\u306e\u884c\u5217","text":"<ol> <li>\u6b63\u65b9\u884c\u5217\uff08Square Matrix\uff09: \u884c\u6570\u3068\u5217\u6570\u304c\u7b49\u3057\u3044\u884c\u5217\uff08\\(m = n\\)\uff09</li> </ol> <p>\u4f8b: \\(B = \\begin{pmatrix}    1 &amp; 2 \\\\    3 &amp; 4    \\end{pmatrix}\\) \u306f\\(2 \\times 2\\)\u306e\u6b63\u65b9\u884c\u5217</p> <ol> <li>\u884c\u30d9\u30af\u30c8\u30eb\uff08Row Vector\uff09: 1\u884c\\(n\\)\u5217\u306e\u884c\u5217</li> </ol> <p>\u4f8b: \\(r = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\end{pmatrix}\\) \u306f\\(1 \\times 3\\)\u306e\u884c\u30d9\u30af\u30c8\u30eb</p> <ol> <li>\u5217\u30d9\u30af\u30c8\u30eb\uff08Column Vector\uff09: \\(m\\)\u884c1\u5217\u306e\u884c\u5217</li> </ol> <p>\u4f8b: \\(c = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\) \u306f\\(3 \\times 1\\)\u306e\u5217\u30d9\u30af\u30c8\u30eb</p>"},{"location":"lectures/LA/04-vector-and-matrix/#33","title":"3.3 \u884c\u5217\u306e\u8868\u8a18\u6cd5","text":"<p>\u884c\u5217\u306f\u901a\u5e38\u3001\u5927\u6587\u5b57\u306e\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\uff08\\(A\\), \\(B\\), \\(C\\)\u306a\u3069\uff09\u3067\u8868\u3057\u307e\u3059\u3002\u884c\u5217\u306e\u8981\u7d20\u306f\u5c0f\u6587\u5b57\u306e\u6dfb\u3048\u5b57\u4ed8\u304d\u306e\u6587\u5b57\uff08\\(a_{ij}\\)\u306a\u3069\uff09\u3067\u8868\u3057\u307e\u3059\u3002</p> <ul> <li>\\(A\\): \u884c\u5217\u5168\u4f53</li> <li>\\(a_{ij}\\): \u884c\u5217\\(A\\)\u306e\\(i\\)\u884c\\(j\\)\u5217\u76ee\u306e\u8981\u7d20</li> <li>\\(A_{i,j}\\): \u884c\u5217\\(A\\)\u306e\\(i\\)\u884c\\(j\\)\u5217\u76ee\u306e\u8981\u7d20\uff08\u5225\u8868\u8a18\uff09</li> </ul>"},{"location":"lectures/LA/04-vector-and-matrix/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/04-vector-and-matrix/#41","title":"4.1 \u884c\u5217\u306e\u548c","text":"<p>\u5b9a\u7fa9: \u540c\u3058\u30b5\u30a4\u30ba\u306e\u884c\u5217\\(A\\)\u3068\\(B\\)\u306e\u548c\\(A + B\\)\u306f\u3001\u5bfe\u5fdc\u3059\u308b\u8981\u7d20\u540c\u58eb\u3092\u8db3\u3057\u5408\u308f\u305b\u305f\u884c\u5217\u3067\u3059\uff1a</p> \\[(A + B)_{ij} = a_{ij} + b_{ij}\\] <p>\u6ce8\u610f\u70b9: \u7570\u306a\u308b\u30b5\u30a4\u30ba\u306e\u884c\u5217\u540c\u58eb\u306f\u8db3\u3057\u5408\u308f\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u305b\u3093\u3002</p> <p>\u4f8b:</p> \\[A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix}\\] \\[A + B = \\begin{pmatrix} 1+5 &amp; 2+6 \\\\ 3+7 &amp; 4+8 \\end{pmatrix} = \\begin{pmatrix} 6 &amp; 8 \\\\ 10 &amp; 12 \\end{pmatrix}\\]"},{"location":"lectures/LA/04-vector-and-matrix/#42","title":"4.2 \u884c\u5217\u306e\u548c\u306e\u6027\u8cea","text":"<p>\u884c\u5217\u306e\u548c\u306f\u4ee5\u4e0b\u306e\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\uff1a</p> <ol> <li>\u4ea4\u63db\u6cd5\u5247: \\(A + B = B + A\\)</li> <li>\u7d50\u5408\u6cd5\u5247: \\((A + B) + C = A + (B + C)\\)</li> <li>\u5358\u4f4d\u5143: \u96f6\u884c\u5217 \\(O\\) \u306b\u3064\u3044\u3066 \\(A + O = A\\)</li> <li>\u9006\u5143: \\(-A\\) \u306b\u3064\u3044\u3066 \\(A + (-A) = O\\)</li> </ol>"},{"location":"lectures/LA/04-vector-and-matrix/#43","title":"4.3 \u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d","text":"<p>\u5b9a\u7fa9: \u884c\u5217\\(A\\)\u306e\u30b9\u30ab\u30e9\u30fc\u500d\\(cA\\)\u306f\u3001\\(A\\)\u306e\u5404\u8981\u7d20\u306b\\(c\\)\u3092\u639b\u3051\u305f\u884c\u5217\u3067\u3059\uff1a</p> \\[(cA)_{ij} = c \\cdot a_{ij}\\] <p>\u4f8b:</p> \\[A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}, \\quad c = 3\\] \\[cA = 3 \\cdot \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} = \\begin{pmatrix} 3 \\cdot 1 &amp; 3 \\cdot 2 \\\\ 3 \\cdot 3 &amp; 3 \\cdot 4 \\end{pmatrix} = \\begin{pmatrix} 3 &amp; 6 \\\\ 9 &amp; 12 \\end{pmatrix}\\]"},{"location":"lectures/LA/04-vector-and-matrix/#44","title":"4.4 \u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u6027\u8cea","text":"<p>\u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u306f\u4ee5\u4e0b\u306e\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\uff1a</p> <ol> <li>\\(c(A + B) = cA + cB\\)</li> <li>\\((c + d)A = cA + dA\\)</li> <li>\\(c(dA) = (cd)A\\)</li> <li>\\(1 \\cdot A = A\\)</li> </ol>"},{"location":"lectures/LA/04-vector-and-matrix/#45","title":"4.5 \u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u95a2\u4fc2","text":"<p>\u884c\u5217\u306f\u300c\u30d9\u30af\u30c8\u30eb\u3092\u5217\u306b\u4e26\u3079\u305f\u3082\u306e\u300d\u3068\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\\(n\\)\u6b21\u5143\u306e\u5217\u30d9\u30af\u30c8\u30eb\\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_m\\)\u3092\u8003\u3048\u308b\u3068\u3001\u305d\u308c\u3089\u3092\u6a2a\u306b\u4e26\u3079\u305f\u884c\u5217\\(A\\)\u306f\uff1a</p> \\[A = \\begin{pmatrix} | &amp; | &amp; &amp; | \\\\ \\vec{v}_1 &amp; \\vec{v}_2 &amp; \\cdots &amp; \\vec{v}_m \\\\ | &amp; | &amp; &amp; | \\end{pmatrix}\\] <p>\u540c\u69d8\u306b\u3001\u884c\u5217\u3092\u300c\u884c\u30d9\u30af\u30c8\u30eb\u3092\u7e26\u306b\u7a4d\u307f\u91cd\u306d\u305f\u3082\u306e\u300d\u3068\u898b\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002</p> <p>\u4f8b:</p> <p>\u5217\u30d9\u30af\u30c8\u30eb \\(\\vec{v}_1 = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}\\), \\(\\vec{v}_2 = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix}\\) \u3092\u4e26\u3079\u308b\u3068\u3001</p> \\[A = \\begin{pmatrix} | &amp; | \\\\ \\vec{v}_1 &amp; \\vec{v}_2 \\\\ | &amp; | \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}\\]"},{"location":"lectures/LA/04-vector-and-matrix/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/04-vector-and-matrix/#51-numpy","title":"5.1 NumPy \u3092\u7528\u3044\u305f\u884c\u5217\u306e\u64cd\u4f5c","text":"<p>Python \u306e NumPy \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3057\u3066\u884c\u5217\u306e\u57fa\u672c\u64cd\u4f5c\u3092\u5b9f\u884c\u3059\u308b\u65b9\u6cd5\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nprint(\"\u884c\u5217 A:\")\nprint(A)\nprint(\"\\n\u884c\u5217 B:\")\nprint(B)\n\n# \u884c\u5217\u306e\u548c\nC = A + B\nprint(\"\\nA + B =\")\nprint(C)\n\n# \u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d\nscalar = 3\nD = scalar * A\nprint(f\"\\n{scalar} \u00d7 A =\")\nprint(D)\n\n# \u884c\u5217\u306e\u30b5\u30a4\u30ba\nprint(f\"\\n\u884c\u5217 A \u306e\u30b5\u30a4\u30ba: {A.shape}\")\n</code></pre>"},{"location":"lectures/LA/04-vector-and-matrix/#52","title":"5.2 \u884c\u5217\u306e\u53ef\u8996\u5316","text":"<pre><code>def plot_matrix(matrix, title):\n    plt.figure(figsize=(6, 6))\n    plt.imshow(matrix, cmap='viridis')\n    plt.colorbar(label='\u5024')\n    plt.title(title)\n\n    # \u5024\u3092\u8868\u793a\n    rows, cols = matrix.shape\n    for i in range(rows):\n        for j in range(cols):\n            plt.text(j, i, f'{matrix[i, j]}', \n                     ha='center', va='center', color='white')\n\n    plt.tight_layout()\n    plt.show()\n\n# \u884c\u5217\u306e\u53ef\u8996\u5316\nplot_matrix(A, '\u884c\u5217 A')\nplot_matrix(B, '\u884c\u5217 B')\nplot_matrix(C, '\u884c\u5217 A + B')\nplot_matrix(D, f'\u884c\u5217 {scalar} \u00d7 A')\n</code></pre>"},{"location":"lectures/LA/04-vector-and-matrix/#53","title":"5.3 \u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u95a2\u4fc2\u306e\u53ef\u8996\u5316","text":"<pre><code># \u30d9\u30af\u30c8\u30eb\u304b\u3089\u884c\u5217\u3092\u69cb\u6210\nv1 = np.array([1, 3])\nv2 = np.array([2, 4])\n\n# \u5217\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u7d50\u5408\nA_from_columns = np.column_stack((v1, v2))\nprint(\"\u5217\u30d9\u30af\u30c8\u30eb\u304b\u3089\u69cb\u6210\u3057\u305f\u884c\u5217:\")\nprint(A_from_columns)\n\n# \u884c\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u7d50\u5408\nrow1 = np.array([1, 2])\nrow2 = np.array([3, 4])\nA_from_rows = np.vstack((row1, row2))\nprint(\"\\n\u884c\u30d9\u30af\u30c8\u30eb\u304b\u3089\u69cb\u6210\u3057\u305f\u884c\u5217:\")\nprint(A_from_rows)\n\n# \u53ef\u8996\u5316\nplt.figure(figsize=(10, 5))\n\n# v1, v2 \u3092\u5225\u3005\u306b\u63cf\u753b\nplt.subplot(1, 2, 1)\nplt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='r', label='v1')\nplt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='b', label='v2')\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.grid()\nplt.title('\u30d9\u30af\u30c8\u30eb v1, v2')\nplt.legend()\n\n# \u884c\u5217 A \u306e\u5217\u30d9\u30af\u30c8\u30eb\u8868\u73fe\nplt.subplot(1, 2, 2)\nplt.quiver(0, 0, A[0, 0], A[1, 0], angles='xy', scale_units='xy', scale=1, color='r', label='A[:,0]')\nplt.quiver(0, 0, A[0, 1], A[1, 1], angles='xy', scale_units='xy', scale=1, color='b', label='A[:,1]')\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.grid()\nplt.title('\u884c\u5217 A \u306e\u5217\u30d9\u30af\u30c8\u30eb')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/04-vector-and-matrix/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/04-vector-and-matrix/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li>\u6b21\u306e\u884c\u5217\u306e\u30b5\u30a4\u30ba\u3092\u7b54\u3048\u306a\u3055\u3044\u3002</li> </ol> <p>(a) \\(A = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{pmatrix}\\)</p> <p>(b) \\(B = \\begin{pmatrix} 7 &amp; 8 \\\\ 9 &amp; 10 \\\\ 11 &amp; 12 \\end{pmatrix}\\)</p> <p>(c) \\(C = \\begin{pmatrix} 13 &amp; 14 &amp; 15 &amp; 16 \\end{pmatrix}\\)</p> <ol> <li>\u6b21\u306e\u884c\u5217\u306e\u548c\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</li> </ol> <p>\\(A = \\begin{pmatrix} 2 &amp; 0 \\\\ -1 &amp; 3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 4 &amp; -2 \\\\ 1 &amp; 5 \\end{pmatrix}\\)</p> <ol> <li>\u6b21\u306e\u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</li> </ol> <p>\\(A = \\begin{pmatrix} 1 &amp; -2 &amp; 3 \\\\ 0 &amp; 4 &amp; -5 \\end{pmatrix}, \\quad c = -2\\)</p> <ol> <li>\u6b21\u306e\u8a08\u7b97\u3092\u305b\u3088\u3002</li> </ol> <p>\\(2A - 3B\\), \u305f\u3060\u3057 \\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}, B = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix}\\)</p>"},{"location":"lectures/LA/04-vector-and-matrix/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u5217\u30d9\u30af\u30c8\u30eb \\(\\vec{v}_1 = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}\\), \\(\\vec{v}_2 = \\begin{pmatrix} 0 \\\\ -1 \\\\ 4 \\end{pmatrix}\\), \\(\\vec{v}_3 = \\begin{pmatrix} -2 \\\\ 5 \\\\ 1 \\end{pmatrix}\\) \u304b\u3089\u69cb\u6210\u3055\u308c\u308b\u884c\u5217 \\(A\\) \u3092\u66f8\u304d\u306a\u3055\u3044\u3002\u307e\u305f\u3001\\(2\\vec{v}_1 - \\vec{v}_2 + 3\\vec{v}_3\\) \u3092\u8a08\u7b97\u3057\u3001\u3053\u308c\u3092\u884c\u5217 \\(A\\) \u3068\u9069\u5207\u306a\u30d9\u30af\u30c8\u30eb \\(\\vec{x}\\) \u3092\u7528\u3044\u3066 \\(A\\vec{x}\\) \u3068\u8868\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u60a3\u8005\u30c7\u30fc\u30bf\u884c\u5217 \\(P\\) \u304c\u3042\u308a\u307e\u3059\uff1a</p> </li> </ol> <p>\\(\\(P = \\begin{pmatrix}     120 &amp; 80 &amp; 90 \\\\    130 &amp; 85 &amp; 110 \\\\    125 &amp; 75 &amp; 95 \\\\    140 &amp; 90 &amp; 120    \\end{pmatrix}\\)\\)</p> <p>\u5404\u884c\u306f\u60a3\u8005\u3001\u5404\u5217\u306f\u7570\u306a\u308b\u5065\u5eb7\u6307\u6a19\uff08\u4f8b\uff1a\u8840\u5727\u3001\u4f53\u91cd\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\uff09\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u5168\u3066\u306e\u60a3\u8005\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001\u6a19\u6e96\u5316\u306e\u305f\u3081\u306b\u4ee5\u4e0b\u306e\u64cd\u4f5c\u3092\u884c\u3044\u307e\u3059\uff1a</p> <ul> <li>\u8840\u5727\uff081\u5217\u76ee\uff09\u304b\u3089100\u3092\u5f15\u304f</li> <li>\u4f53\u91cd\uff082\u5217\u76ee\uff09\u304b\u308970\u3092\u5f15\u304f</li> <li>\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\uff083\u5217\u76ee\uff09\u304b\u308980\u3092\u5f15\u304f</li> </ul> <p>\u3053\u306e\u64cd\u4f5c\u3092\u884c\u5217\u306e\u8a08\u7b97\u3068\u3057\u3066\u8868\u73fe\u3057\u3001\u7d50\u679c\u306e\u884c\u5217\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> <ol> <li>Google Colab\u3092\u4f7f\u3063\u3066\u3001\u4ee5\u4e0b\u306e\u30d8\u30eb\u30b9\u30c7\u30fc\u30bf\u306b\u95a2\u3059\u308b\u884c\u5217\u6f14\u7b97\u3092\u5b9f\u88c5\u3057\u306a\u3055\u3044\uff1a</li> <li>\u60a3\u800510\u4eba\u00d7\u5065\u5eb7\u6307\u6a195\u3064\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u884c\u5217\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3059\u308b</li> <li>\u5404\u5065\u5eb7\u6307\u6a19\u306e\u5e73\u5747\u5024\u3092\u6c42\u3081\u308b</li> <li>\u3059\u3079\u3066\u306e\u5024\u3092\u6b63\u898f\u5316\u3059\u308b\uff08\u5404\u5217\u306e\u5e73\u5747\u304c0\u3001\u6a19\u6e96\u504f\u5dee\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\uff09</li> <li>\u7d50\u679c\u3092\u53ef\u8996\u5316\u3059\u308b\uff08\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\uff09</li> </ol>"},{"location":"lectures/LA/04-vector-and-matrix/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":"<p>Q1: \u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A1: \u30d9\u30af\u30c8\u30eb\u306f\u884c\u5217\u306e\u7279\u6b8a\u306a\u5834\u5408\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u5217\u30d9\u30af\u30c8\u30eb\u306f\\(n \\times 1\\)\u884c\u5217\u3001\u884c\u30d9\u30af\u30c8\u30eb\u306f\\(1 \\times m\\)\u884c\u5217\u3067\u3059\u3002\u884c\u5217\u306f\u30d9\u30af\u30c8\u30eb\u3092\u8907\u6570\u4e26\u3079\u305f\u3082\u306e\u3068\u3082\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>Q2: \u7570\u306a\u308b\u30b5\u30a4\u30ba\u306e\u884c\u5217\u540c\u58eb\u3092\u8db3\u3057\u5408\u308f\u305b\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u3059\u304b\uff1f</p> <p>A2: \u3067\u304d\u307e\u305b\u3093\u3002\u884c\u5217\u306e\u52a0\u7b97\u306f\u3001\u5bfe\u5fdc\u3059\u308b\u8981\u7d20\u540c\u58eb\u3092\u8db3\u3057\u5408\u308f\u305b\u308b\u64cd\u4f5c\u3067\u3042\u308b\u305f\u3081\u3001\u884c\u6570\u3068\u5217\u6570\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>Q3: \u884c\u5217\u306e\u548c\u3084\u30b9\u30ab\u30e9\u30fc\u500d\u304c\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u3069\u306e\u3088\u3046\u306b\u4f7f\u308f\u308c\u307e\u3059\u304b\uff1f</p> <p>A3: \u884c\u5217\u306e\u548c\u3084\u30b9\u30ab\u30e9\u30fc\u500d\u306f\u3001\u30c7\u30fc\u30bf\u306e\u6b63\u898f\u5316\u3001\u7279\u5fb4\u91cf\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3001\u8907\u6570\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u7d50\u5408\u3001\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u79fb\u52d5\u5e73\u5747\u306e\u8a08\u7b97\u306a\u3069\u3001\u69d8\u3005\u306a\u30c7\u30fc\u30bf\u524d\u51e6\u7406\u3084\u5206\u6790\u624b\u6cd5\u3067\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\u307e\u305f\u3001\u6a5f\u68b0\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u5185\u90e8\u8a08\u7b97\uff08\u52fe\u914d\u964d\u4e0b\u6cd5\u306a\u3069\uff09\u3067\u3082\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002</p> <p>Q4: \u884c\u5217\u306e\u8981\u7d20\u3092\u4e26\u3079\u308b\u9806\u5e8f\u306f\u91cd\u8981\u3067\u3059\u304b\uff1f</p> <p>A4: \u975e\u5e38\u306b\u91cd\u8981\u3067\u3059\u3002\u884c\u5217\u3067\u306f\u8981\u7d20\u306e\u4f4d\u7f6e\uff08\u884c\u756a\u53f7\u3068\u5217\u756a\u53f7\uff09\u304c\u60c5\u5831\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\u884c\u3068\u5217\u3092\u5165\u308c\u66ff\u3048\u308b\u3068\u3001\u5168\u304f\u7570\u306a\u308b\u884c\u5217\u306b\u306a\u308a\u307e\u3059\u3002\u7279\u306b\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u884c\u306f\u901a\u5e38\u30b5\u30f3\u30d7\u30eb\uff08\u89b3\u6e2c\u5024\uff09\u3001\u5217\u306f\u7279\u5fb4\u91cf\uff08\u5909\u6570\uff09\u3092\u8868\u3059\u3053\u3068\u304c\u591a\u3044\u305f\u3081\u3001\u305d\u306e\u69cb\u9020\u3092\u4fdd\u3064\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p> <p>Q5: \u306a\u305cPython\u306eNumPy\u3092\u4f7f\u3063\u3066\u884c\u5217\u8a08\u7b97\u3092\u3059\u308b\u306e\u3067\u3059\u304b\uff1f</p> <p>A5: NumPy\u306f\u884c\u5217\u8a08\u7b97\u306b\u6700\u9069\u5316\u3055\u308c\u305f\u52b9\u7387\u7684\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u5927\u898f\u6a21\u306a\u884c\u5217\u3067\u3082\u9ad8\u901f\u306b\u8a08\u7b97\u3067\u304d\u3001\u307e\u305f\u8c4a\u5bcc\u306a\u95a2\u6570\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u4f5c\u696d\u3092\u52b9\u7387\u5316\u3067\u304d\u307e\u3059\u3002\u307e\u305f\u3001\u6b63\u78ba\u306a\u6570\u5024\u8a08\u7b97\u304c\u4fdd\u8a3c\u3055\u308c\u3066\u304a\u308a\u3001\u884c\u5217\u306e\u69d8\u3005\u306a\u64cd\u4f5c\u3084\u5206\u89e3\u3092\u7c21\u5358\u306b\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/04-vector-and-matrix/#8","title":"8. \u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u884c\u5217\u306e\u57fa\u672c\u6982\u5ff5\u3001\u8868\u8a18\u6cd5\u3001\u884c\u5217\u306e\u548c\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u8a08\u7b97\u65b9\u6cd5\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\u3002\u307e\u305f\u3001\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u95a2\u4fc2\u6027\u306b\u3064\u3044\u3066\u3082\u8003\u5bdf\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u6982\u5ff5\u306f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u790e\u3068\u306a\u308b\u91cd\u8981\u306a\u9053\u5177\u3067\u3059\u3002\u7279\u306b\u3001\u30c7\u30fc\u30bf\u306e\u8868\u73fe\u3084\u5909\u63db\u3001\u30e2\u30c7\u30ea\u30f3\u30b0\u306b\u304a\u3044\u3066\u3001\u884c\u5217\u6f14\u7b97\u306f\u4e2d\u5fc3\u7684\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u884c\u5217\u306e\u7a4d\u306b\u3064\u3044\u3066\u5b66\u7fd2\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u7dda\u5f62\u5909\u63db\u3084\u30c7\u30fc\u30bf\u306e\u5909\u63db\u306b\u304a\u3044\u3066\u975e\u5e38\u306b\u91cd\u8981\u306a\u64cd\u4f5c\u3067\u3059\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I - \u8b1b\u7fa9\u30ce\u30fc\u30c8 \u7b2c5\u56de","text":""},{"location":"lectures/LA/05-vector-and-matrix/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c5\u56de \u30c6\u30fc\u30de: \u884c\u5217\u306e\u7a4d \u95a2\u9023\u9805\u76ee: \u884c\u5217\u7a4d\u306e\u5b9a\u7fa9\u3001\u8a08\u7b97\u65b9\u6cd5\u3001\u6ce8\u610f\u70b9\u3001\u9006\u884c\u5217\u306e\u5c0e\u5165 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u7b2c4\u56de\u306e\u5185\u5bb9\uff08\u884c\u5217\u306e\u5b9a\u7fa9\u3001\u884c\u5217\u306e\u548c\u3001\u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d\uff09</p>"},{"location":"lectures/LA/05-vector-and-matrix/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\uff1a</p> <ol> <li>\u884c\u5217\u306e\u7a4d\u306e\u5b9a\u7fa9\u3092\u7406\u89e3\u3057\u3001\u6b63\u78ba\u306b\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u7a4d\u306e\u6027\u8cea\uff08\u7d50\u5408\u6cd5\u5247\u3001\u5206\u914d\u6cd5\u5247\u306a\u3069\uff09\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u7a4d\u306e\u975e\u53ef\u63db\u6027\u3092\u7406\u89e3\u3057\u3001\u305d\u306e\u610f\u5473\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u9006\u884c\u5217\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3057\u30012\u6b21\u306e\u6b63\u5247\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u884c\u5217\u7a4d\u306e\u610f\u5473\u3068\u5fdc\u7528\u4f8b\u3092\u8aac\u660e\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/05-vector-and-matrix/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/05-vector-and-matrix/#31","title":"3.1 \u884c\u5217\u7a4d\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9 3.1.1\uff08\u884c\u5217\u7a4d\uff09 \\(A\\) \u3092 \\(m \\times n\\) \u884c\u5217\u3001\\(B\\) \u3092 \\(n \\times p\\) \u884c\u5217\u3068\u3059\u308b\u3002\u3053\u306e\u3068\u304d\u3001\\(A\\) \u3068 \\(B\\) \u306e\u7a4d \\(AB\\) \u306f \\(m \\times p\\) \u306e\u884c\u5217\u3067\u3042\u308a\u3001\u305d\u306e \\((i,j)\\) \u6210\u5206\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a</p> <p>\\((AB)_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj} = a_{i1}b_{1j} + a_{i2}b_{2j} + \\cdots + a_{in}b_{nj}\\)</p> <p>\u3053\u3053\u3067\u91cd\u8981\u306a\u306e\u306f\u3001\u884c\u5217\u306e\u7a4d \\(AB\\) \u304c\u5b9a\u7fa9\u3055\u308c\u308b\u305f\u3081\u306b\u306f\u3001\u5de6\u5074\u306e\u884c\u5217 \\(A\\) \u306e\u5217\u6570\u3068\u53f3\u5074\u306e\u884c\u5217 \\(B\\) \u306e\u884c\u6570\u304c\u4e00\u81f4\u3057\u3066\u3044\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p> <p>\u4f8b 3.1.1\uff1a \\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}\\)\uff08\\(2 \\times 2\\) \u884c\u5217\uff09\u3068 \\(B = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix}\\)\uff08\\(2 \\times 2\\) \u884c\u5217\uff09\u306e\u7a4d\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\\((AB)_{11} = a_{11}b_{11} + a_{12}b_{21} = 1 \\times 5 + 2 \\times 7 = 5 + 14 = 19\\)</p> <p>\\((AB)_{12} = a_{11}b_{12} + a_{12}b_{22} = 1 \\times 6 + 2 \\times 8 = 6 + 16 = 22\\)</p> <p>\\((AB)_{21} = a_{21}b_{11} + a_{22}b_{21} = 3 \\times 5 + 4 \\times 7 = 15 + 28 = 43\\)</p> <p>\\((AB)_{22} = a_{21}b_{12} + a_{22}b_{22} = 3 \\times 6 + 4 \\times 8 = 18 + 32 = 50\\)</p> <p>\u3088\u3063\u3066\u3001\\(AB = \\begin{pmatrix} 19 &amp; 22 \\\\ 43 &amp; 50 \\end{pmatrix}\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#32","title":"3.2 \u884c\u5217\u7a4d\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":"<p>\u884c\u5217\u7a4d\u306f\u7dda\u5f62\u5909\u63db\u306e\u5408\u6210\u3068\u3057\u3066\u5e7e\u4f55\u5b66\u7684\u306b\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002\u884c\u5217 \\(A\\) \u3068\u884c\u5217 \\(B\\) \u304c\u305d\u308c\u305e\u308c\u7dda\u5f62\u5909\u63db\u3092\u8868\u3059\u3068\u304d\u3001\\(AB\\) \u306f\u307e\u305a \\(B\\) \u306b\u3088\u308b\u5909\u63db\u3092\u884c\u3044\u3001\u6b21\u306b \\(A\\) \u306b\u3088\u308b\u5909\u63db\u3092\u884c\u3046\u3068\u3044\u3046\u5408\u6210\u5909\u63db\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u7279\u306b\u3001\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u306b\u5bfe\u3057\u3066\u884c\u5217 \\(A\\) \u3092\u4f5c\u7528\u3055\u305b\u308b\u3068\u3001\\(A\\mathbf{x}\\) \u306f \\(\\mathbf{x}\\) \u3092\u7dda\u5f62\u5909\u63db\u3057\u305f\u7d50\u679c\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/05-vector-and-matrix/#41","title":"4.1 \u884c\u5217\u7a4d\u306e\u57fa\u672c\u7684\u306a\u6027\u8cea","text":"<p>\u884c\u5217\u7a4d\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u91cd\u8981\u306a\u6027\u8cea\u304c\u3042\u308a\u307e\u3059\uff1a</p> <p>\u6027\u8cea 4.1.1\uff08\u7d50\u5408\u6cd5\u5247\uff09 \u884c\u5217 \\(A\\), \\(B\\), \\(C\\) \u306b\u5bfe\u3057\u3066\u3001\\((AB)C = A(BC)\\) \u304c\u6210\u308a\u7acb\u3064\uff08\u305f\u3060\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u7a4d\u304c\u5b9a\u7fa9\u3055\u308c\u308b\u3068\u3059\u308b\uff09\u3002</p> <p>\u6027\u8cea 4.1.2\uff08\u5206\u914d\u6cd5\u5247\uff09 \u884c\u5217 \\(A\\), \\(B\\), \\(C\\) \u306b\u5bfe\u3057\u3066\u3001\\(A(B+C) = AB + AC\\) \u304a\u3088\u3073 \\((A+B)C = AC + BC\\) \u304c\u6210\u308a\u7acb\u3064\uff08\u305f\u3060\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u548c\u3068\u7a4d\u304c\u5b9a\u7fa9\u3055\u308c\u308b\u3068\u3059\u308b\uff09\u3002</p> <p>\u6027\u8cea 4.1.3\uff08\u30b9\u30ab\u30e9\u30fc\u500d\u3068\u306e\u95a2\u4fc2\uff09 \u30b9\u30ab\u30e9\u30fc \\(c\\) \u3068\u884c\u5217 \\(A\\), \\(B\\) \u306b\u5bfe\u3057\u3066\u3001\\(c(AB) = (cA)B = A(cB)\\) \u304c\u6210\u308a\u7acb\u3064\u3002</p> <p>\u4f8b 4.1.1\uff1a \\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}\\), \\(B = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix}\\), \\(C = \\begin{pmatrix} 9 &amp; 10 \\\\ 11 &amp; 12 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\\((A+B)C\\) \u3068 \\(AC + BC\\) \u3092\u8a08\u7b97\u3057\u3001\u5206\u914d\u6cd5\u5247\u3092\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046\u3002</p> <p>\\(A + B = \\begin{pmatrix} 1+5 &amp; 2+6 \\\\ 3+7 &amp; 4+8 \\end{pmatrix} = \\begin{pmatrix} 6 &amp; 8 \\\\ 10 &amp; 12 \\end{pmatrix}\\)</p> <p>\\((A+B)C = \\begin{pmatrix} 6 &amp; 8 \\\\ 10 &amp; 12 \\end{pmatrix} \\begin{pmatrix} 9 &amp; 10 \\\\ 11 &amp; 12 \\end{pmatrix} = \\begin{pmatrix} 6 \\times 9 + 8 \\times 11 &amp; 6 \\times 10 + 8 \\times 12 \\\\ 10 \\times 9 + 12 \\times 11 &amp; 10 \\times 10 + 12 \\times 12 \\end{pmatrix} = \\begin{pmatrix} 54 + 88 &amp; 60 + 96 \\\\ 90 + 132 &amp; 100 + 144 \\end{pmatrix} = \\begin{pmatrix} 142 &amp; 156 \\\\ 222 &amp; 244 \\end{pmatrix}\\)</p> <p>\u4e00\u65b9\u3001 \\(AC = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} \\begin{pmatrix} 9 &amp; 10 \\\\ 11 &amp; 12 \\end{pmatrix} = \\begin{pmatrix} 31 &amp; 34 \\\\ 71 &amp; 78 \\end{pmatrix}\\)</p> <p>\\(BC = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix} \\begin{pmatrix} 9 &amp; 10 \\\\ 11 &amp; 12 \\end{pmatrix} = \\begin{pmatrix} 111 &amp; 122 \\\\ 151 &amp; 166 \\end{pmatrix}\\)</p> <p>\\(AC + BC = \\begin{pmatrix} 31 &amp; 34 \\\\ 71 &amp; 78 \\end{pmatrix} + \\begin{pmatrix} 111 &amp; 122 \\\\ 151 &amp; 166 \\end{pmatrix} = \\begin{pmatrix} 142 &amp; 156 \\\\ 222 &amp; 244 \\end{pmatrix}\\)</p> <p>\u3088\u3063\u3066\u3001\\((A+B)C = AC + BC\\) \u3067\u3042\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#42","title":"4.2 \u884c\u5217\u7a4d\u306e\u975e\u53ef\u63db\u6027","text":"<p>\u884c\u5217\u306e\u7a4d\u306b\u306f\u3001\u4e00\u822c\u306b\u4ea4\u63db\u6cd5\u5247\u304c\u6210\u308a\u7acb\u3061\u307e\u305b\u3093\u3002\u3059\u306a\u308f\u3061\u3001\\(AB \\neq BA\\) \u3068\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u4f8b 4.2.1\uff1a \\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}\\) \u3068 \\(B = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\\(AB\\) \u3068 \\(BA\\) \u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\\(AB = \\begin{pmatrix} 19 &amp; 22 \\\\ 43 &amp; 50 \\end{pmatrix}\\)\uff08\u5148\u307b\u3069\u8a08\u7b97\u3057\u305f\u901a\u308a\uff09</p> <p>\\(BA = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} = \\begin{pmatrix} 5 \\times 1 + 6 \\times 3 &amp; 5 \\times 2 + 6 \\times 4 \\\\ 7 \\times 1 + 8 \\times 3 &amp; 7 \\times 2 + 8 \\times 4 \\end{pmatrix} = \\begin{pmatrix} 5 + 18 &amp; 10 + 24 \\\\ 7 + 24 &amp; 14 + 32 \\end{pmatrix} = \\begin{pmatrix} 23 &amp; 34 \\\\ 31 &amp; 46 \\end{pmatrix}\\)</p> <p>\\(AB \\neq BA\\) \u3067\u3042\u308b\u305f\u3081\u3001\u884c\u5217\u306e\u7a4d\u306f\u4e00\u822c\u306b\u53ef\u63db\u3067\u306f\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u3053\u306e\u975e\u53ef\u63db\u6027\u306f\u3001\u884c\u5217\u304c\u8868\u3059\u7dda\u5f62\u5909\u63db\u306e\u9806\u5e8f\u304c\u91cd\u8981\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#43","title":"4.3 \u7279\u6b8a\u306a\u884c\u5217\u3068\u884c\u5217\u7a4d","text":""},{"location":"lectures/LA/05-vector-and-matrix/#431","title":"4.3.1 \u5358\u4f4d\u884c\u5217","text":"<p>\u5b9a\u7fa9 4.3.1\uff08\u5358\u4f4d\u884c\u5217\uff09 \\(n\\) \u6b21\u306e\u5358\u4f4d\u884c\u5217 \\(I_n\\) \u306f\u3001\u4e3b\u5bfe\u89d2\u7dda\u4e0a\u306e\u6210\u5206\u304c\u3059\u3079\u3066 \\(1\\) \u3067\u3001\u305d\u308c\u4ee5\u5916\u306e\u6210\u5206\u304c\u3059\u3079\u3066 \\(0\\) \u3067\u3042\u308b \\(n \\times n\\) \u306e\u6b63\u65b9\u884c\u5217\u3067\u3059\uff1a</p> <p>\\(I_n = \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{pmatrix}\\)</p> <p>\u5358\u4f4d\u884c\u5217\u306f\u3001\u4efb\u610f\u306e \\(n \\times m\\) \u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(I_n A = A\\) \u304b\u3064 \\(A I_m = A\\) \u3092\u6e80\u305f\u3057\u307e\u3059\u3002\u3053\u306e\u6027\u8cea\u304b\u3089\u3001\u5358\u4f4d\u884c\u5217\u306f\u884c\u5217\u306e\u7a4d\u306b\u95a2\u3059\u308b\u300c\u5358\u4f4d\u5143\u300d\u3068\u547c\u3070\u308c\u307e\u3059\u3002</p> <p>\u4f8b 4.3.1\uff1a \\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}\\) \u3068 \\(2\\) \u6b21\u306e\u5358\u4f4d\u884c\u5217 \\(I_2 = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\\(I_2 A\\) \u3068 \\(A I_2\\) \u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\\(I_2 A = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\times 1 + 0 \\times 3 &amp; 1 \\times 2 + 0 \\times 4 \\\\ 0 \\times 1 + 1 \\times 3 &amp; 0 \\times 2 + 1 \\times 4 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} = A\\)</p> <p>\\(A I_2 = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\times 1 + 2 \\times 0 &amp; 1 \\times 0 + 2 \\times 1 \\\\ 3 \\times 1 + 4 \\times 0 &amp; 3 \\times 0 + 4 \\times 1 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} = A\\)</p> <p>\u3088\u3063\u3066\u3001\\(I_2 A = A I_2 = A\\) \u304c\u78ba\u8a8d\u3067\u304d\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#432","title":"4.3.2 \u96f6\u884c\u5217","text":"<p>\u3059\u3079\u3066\u306e\u6210\u5206\u304c \\(0\\) \u3067\u3042\u308b\u884c\u5217\u3092\u96f6\u884c\u5217\u3068\u547c\u3073\u3001\\(O\\) \u3067\u8868\u3057\u307e\u3059\u3002\u4efb\u610f\u306e\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(A + O = A\\) \u304a\u3088\u3073 \\(A \\times O = O \\times A = O\\) \u304c\u6210\u308a\u7acb\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#44","title":"4.4 \u9006\u884c\u5217","text":"<p>\u5b9a\u7fa9 4.4.1\uff08\u9006\u884c\u5217\uff09 \\(n\\) \u6b21\u6b63\u65b9\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(AB = BA = I_n\\) \u3092\u6e80\u305f\u3059 \\(n\\) \u6b21\u6b63\u65b9\u884c\u5217 \\(B\\) \u304c\u5b58\u5728\u3059\u308b\u3068\u304d\u3001\\(B\\) \u3092 \\(A\\) \u306e\u9006\u884c\u5217\u3068\u3044\u3044\u3001\\(A^{-1}\\) \u3068\u8868\u3057\u307e\u3059\u3002</p> <p>\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u884c\u5217\u3092\u6b63\u5247\u884c\u5217\uff08\u307e\u305f\u306f\u53ef\u9006\u884c\u5217\uff09\u3068\u547c\u3073\u307e\u3059\u3002\u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u306a\u3044\u884c\u5217\u306f\u7279\u7570\u884c\u5217\uff08\u307e\u305f\u306f\u975e\u53ef\u9006\u884c\u5217\uff09\u3068\u547c\u3070\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#441-2","title":"4.4.1 2\u6b21\u306e\u884c\u5217\u306e\u9006\u884c\u5217\u306e\u8a08\u7b97","text":"<p>2\u6b21\u306e\u884c\u5217 \\(A = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\det(A) = ad - bc \\neq 0\\) \u3067\u3042\u308c\u3070\u3001\\(A\\) \u306f\u6b63\u5247\u3067\u3042\u308a\u3001\u305d\u306e\u9006\u884c\u5217\u306f\u6b21\u306e\u5f0f\u3067\u4e0e\u3048\u3089\u308c\u307e\u3059\uff1a</p> <p>\\(A^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} d &amp; -b \\\\ -c &amp; a \\end{pmatrix} = \\frac{1}{ad-bc} \\begin{pmatrix} d &amp; -b \\\\ -c &amp; a \\end{pmatrix}\\)</p> <p>\u4f8b 4.4.1\uff1a \\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 2 \\end{pmatrix}\\) \u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u307e\u3057\u3087\u3046\u3002</p> <p>\u307e\u305a\u3001\\(\\det(A) = 3 \\times 2 - 1 \\times 2 = 6 - 2 = 4 \\neq 0\\) \u306a\u306e\u3067\u3001\\(A\\) \u306f\u6b63\u5247\u3067\u3059\u3002</p> <p>\\(A^{-1} = \\frac{1}{4} \\begin{pmatrix} 2 &amp; -1 \\\\ -2 &amp; 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} &amp; -\\frac{1}{4} \\\\ -\\frac{1}{2} &amp; \\frac{3}{4} \\end{pmatrix}\\)</p> <p>\u691c\u7b97\u3068\u3057\u3066\u3001\\(A A^{-1}\\) \u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> <p>\\(A A^{-1} = \\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} &amp; -\\frac{1}{4} \\\\ -\\frac{1}{2} &amp; \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} 3 \\times \\frac{1}{2} + 1 \\times (-\\frac{1}{2}) &amp; 3 \\times (-\\frac{1}{4}) + 1 \\times \\frac{3}{4} \\\\ 2 \\times \\frac{1}{2} + 2 \\times (-\\frac{1}{2}) &amp; 2 \\times (-\\frac{1}{4}) + 2 \\times \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} - \\frac{1}{2} &amp; -\\frac{3}{4} + \\frac{3}{4} \\\\ 1 - 1 &amp; -\\frac{1}{2} + \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} = I_2\\)</p> <p>\u3088\u3063\u3066\u3001\u6c42\u3081\u305f\u9006\u884c\u5217\u304c\u6b63\u3057\u3044\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>NumPy\u3092\u4f7f\u7528\u3057\u3066\u884c\u5217\u306e\u7a4d\u3068\u9006\u884c\u5217\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\nprint(\"\u884c\u5217A:\\n\", A)\nprint(\"\u884c\u5217B:\\n\", B)\n\n# \u884c\u5217\u306e\u7a4d\nC = np.dot(A, B)  # \u307e\u305f\u306f C = A @ B \uff08Python 3.5\u4ee5\u964d\uff09\nprint(\"A\u00d7B:\\n\", C)\n\n# \u884c\u5217\u7a4d\u306e\u975e\u53ef\u63db\u6027\u3092\u78ba\u8a8d\nD = np.dot(B, A)\nprint(\"B\u00d7A:\\n\", D)\nprint(\"A\u00d7B = B\u00d7A\u306f\", np.array_equal(C, D))\n\n# \u5358\u4f4d\u884c\u5217\nI = np.eye(2)  # 2\u6b21\u306e\u5358\u4f4d\u884c\u5217\nprint(\"\u5358\u4f4d\u884c\u5217I:\\n\", I)\nprint(\"I\u00d7A:\\n\", np.dot(I, A))\n\n# \u9006\u884c\u5217\u306e\u8a08\u7b97\nA_inv = np.linalg.inv(A)\nprint(\"A\u306e\u9006\u884c\u5217:\\n\", A_inv)\n\n# \u9006\u884c\u5217\u306e\u691c\u8a3c\nprint(\"A\u00d7A^(-1):\\n\", np.dot(A, A_inv))\nprint(\"A^(-1)\u00d7A:\\n\", np.dot(A_inv, A))\n</code></pre>"},{"location":"lectures/LA/05-vector-and-matrix/#51","title":"5.1 \u7dda\u5f62\u5909\u63db\u3068\u3057\u3066\u306e\u884c\u5217\u7a4d\u306e\u53ef\u8996\u5316","text":"<p>\u884c\u5217\u304c\u7dda\u5f62\u5909\u63db\u3092\u8868\u3059\u3053\u3068\u3092\u53ef\u8996\u5316\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# 2x2\u306e\u884c\u5217\uff08\u7dda\u5f62\u5909\u63db\uff09\u3092\u5b9a\u7fa9\nA = np.array([[1, 0.5], [0.5, 1]])\nB = np.array([[0, -1], [1, 0]])  # 90\u5ea6\u56de\u8ee2\nC = np.dot(A, B)  # \u5408\u6210\u5909\u63db\n\n# \u53ef\u8996\u5316\u306e\u305f\u3081\u306e\u683c\u5b50\u70b9\u3092\u751f\u6210\nx = np.linspace(-3, 3, 7)\ny = np.linspace(-3, 3, 7)\nX, Y = np.meshgrid(x, y)\npoints = np.vstack([X.flatten(), Y.flatten()])\n\n# \u5404\u5909\u63db\u3092\u9069\u7528\npoints_A = np.dot(A, points)\npoints_B = np.dot(B, points)\npoints_C = np.dot(C, points)\n\n# \u53ef\u8996\u5316\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.scatter(points_A[0], points_A[1], c='red', s=50)\nplt.scatter(points[0], points[1], c='blue', s=20)\nplt.grid(True)\nplt.title('\u5909\u63dbA')\nplt.axis('equal')\n\nplt.subplot(1, 3, 2)\nplt.scatter(points_B[0], points_B[1], c='red', s=50)\nplt.scatter(points[0], points[1], c='blue', s=20)\nplt.grid(True)\nplt.title('\u5909\u63dbB\uff0890\u5ea6\u56de\u8ee2\uff09')\nplt.axis('equal')\n\nplt.subplot(1, 3, 3)\nplt.scatter(points_C[0], points_C[1], c='red', s=50)\nplt.scatter(points[0], points[1], c='blue', s=20)\nplt.grid(True)\nplt.title('\u5408\u6210\u5909\u63dbC = A\u00d7B')\nplt.axis('equal')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/05-vector-and-matrix/#6","title":"6. \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u5fdc\u7528\u4f8b","text":""},{"location":"lectures/LA/05-vector-and-matrix/#61","title":"6.1 \u7dda\u5f62\u56de\u5e30\u306b\u304a\u3051\u308b\u884c\u5217\u6f14\u7b97","text":"<p>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u3001\u8aac\u660e\u5909\u6570\u3068\u76ee\u7684\u5909\u6570\u306e\u95a2\u4fc2\u3092\u884c\u5217\u3067\u8868\u73fe\u3057\u307e\u3059\u3002\\(n\\)\u500b\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u3068\\(p\\)\u500b\u306e\u7279\u5fb4\u91cf\u304c\u3042\u308b\u3068\u304d\u3001\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u306f \\(n \\times p\\) \u306e\u884c\u5217\u3001\u76ee\u7684\u5909\u6570\u30d9\u30af\u30c8\u30eb \\(\\mathbf{y}\\) \u306f \\(n\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002\u7dda\u5f62\u56de\u5e30\u306e\u4fc2\u6570\u30d9\u30af\u30c8\u30eb \\(\\mathbf{\\beta}\\) \u306f\u3001\u6b63\u898f\u65b9\u7a0b\u5f0f \\(X^T X \\mathbf{\\beta} = X^T \\mathbf{y}\\) \u306e\u89e3\u3068\u3057\u3066\u4e0e\u3048\u3089\u308c\u3001\u3053\u308c\u306f \\((X^T X)^{-1} X^T \\mathbf{y}\\) \u3067\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u751f\u6210\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\nX_matrix = np.column_stack((np.ones(X.shape[0]), X))  # \u5207\u7247\u9805\u3092\u8ffd\u52a0\n\n# \u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u4f7f\u3063\u305f\u7dda\u5f62\u56de\u5e30\nbeta = np.linalg.inv(X_matrix.T @ X_matrix) @ X_matrix.T @ y\nprint(\"\u56de\u5e30\u4fc2\u6570:\", beta)\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, alpha=0.7)\nplt.plot(X, beta[0] + beta[1] * X, color='red', linewidth=2)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/05-vector-and-matrix/#62","title":"6.2 \u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u884c\u5217\u6f14\u7b97\u306e\u5fdc\u7528","text":"<p>\u533b\u7642\u753b\u50cf\u51e6\u7406\u3084\u751f\u4f53\u4fe1\u53f7\u51e6\u7406\u306a\u3069\u306e\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u884c\u5217\u6f14\u7b97\u304c\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u533b\u7642\u753b\u50cf\u306e\u5909\u63db\u3001\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3001\u7279\u5fb4\u62bd\u51fa\u306a\u3069\u306b\u306f\u884c\u5217\u306e\u7a4d\u304c\u983b\u7e41\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\n\n# \u624b\u66f8\u304d\u6570\u5b57\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\uff08\u533b\u7642\u753b\u50cf\u306e\u4ee3\u308f\u308a\u3068\u3057\u3066\uff09\ndigits = load_digits()\nimage = digits.images[0]\n\n# \u884c\u5217\u3092\u4f7f\u3063\u305f\u753b\u50cf\u306e\u64cd\u4f5c\uff08\u4f8b\uff1a\u30a8\u30c3\u30b8\u691c\u51fa\u30d5\u30a3\u30eb\u30bf\uff09\nedge_filter = np.array([[-1, -1, -1],\n                         [-1,  8, -1],\n                         [-1, -1, -1]])\n\n# \u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u51e6\u7406\uff08\u7573\u307f\u8fbc\u307f\uff09\ndef convolve2d(image, kernel):\n    output = np.zeros_like(image)\n    padding = kernel.shape[0] // 2\n    padded_image = np.pad(image, padding, mode='constant')\n\n    for i in range(image.shape[0]):\n        for j in range(image.shape[1]):\n            output[i, j] = np.sum(\n                padded_image[i:i+kernel.shape[0], j:j+kernel.shape[1]] * kernel\n            )\n    return output\n\nfiltered_image = convolve2d(image, edge_filter)\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(image, cmap='gray')\nplt.title('\u539f\u753b\u50cf')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(filtered_image, cmap='gray')\nplt.title('\u30a8\u30c3\u30b8\u691c\u51fa\u5f8c')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/05-vector-and-matrix/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/05-vector-and-matrix/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u7a4d\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    (a) \\(\\begin{pmatrix} 2 &amp; 3 \\\\ 4 &amp; 5 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 2 &amp; 3 \\end{pmatrix}\\)    (b) \\(\\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{pmatrix} \\begin{pmatrix} 7 &amp; 8 \\\\ 9 &amp; 10 \\\\ 11 &amp; 12 \\end{pmatrix}\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217 \\(A\\) \u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u3001\\(A A^{-1} = I\\) \u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002    \\(A = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix}\\)</p> </li> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}\\) \u3068 \\(B = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\\(AB\\) \u3068 \\(BA\\) \u3092\u8a08\u7b97\u3057\u3001\\(AB \\neq BA\\) \u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\\(A = \\begin{pmatrix} 3 &amp; 0 \\\\ 0 &amp; 3 \\end{pmatrix}\\) \u3068 \\(B = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\\(AB = BA\\) \u3068\u306a\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u884c\u5217\u306e\u7a4d\u306f\u4ea4\u63db\u53ef\u80fd\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u304b\uff1f</p> </li> </ol>"},{"location":"lectures/LA/05-vector-and-matrix/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\\(A^2\\), \\(A^3\\), \\(A^4\\) \u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002\u898f\u5247\u6027\u3092\u898b\u3064\u3051\u3001\\(A^n\\) \u306e\u4e00\u822c\u5f62\u3092\u4e88\u60f3\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\) \u304c\u6b63\u5247\u3067\u3042\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f \\(\\det(A) = ad - bc \\neq 0\\) \u3067\u3059\u3002\u3053\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3055\u306a\u3044\u4f8b\u3092\u6319\u3052\u3001\u305d\u306e\u884c\u5217\u304c\u9006\u884c\u5217\u3092\u6301\u305f\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u3042\u308b\u4f1a\u793e\u306e\u88fd\u54c1 A, B, C \u306e3\u3064\u306e\u6210\u5206 X, Y, Z \u306e\u542b\u6709\u91cf\uff08\u5358\u4f4d: g\uff09\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> </li> <li>\u88fd\u54c1 A: X = 2, Y = 1, Z = 3</li> <li>\u88fd\u54c1 B: X = 1, Y = 2, Z = 2</li> <li>\u88fd\u54c1 C: X = 3, Y = 1, Z = 1</li> </ol> <p>\u5404\u6210\u5206\u306e\u5358\u4fa1\uff08\u5186/g\uff09\u306f X = 100, Y = 200, Z = 150 \u3067\u3059\u3002\u884c\u5217\u306e\u7a4d\u3092\u4f7f\u3063\u3066\u3001\u5404\u88fd\u54c1\u306e\u539f\u4fa1\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002</p> <ol> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5fdc\u7528\u554f\u984c\uff1a3\u4eba\u306e\u60a3\u8005\uff08\u60a3\u80051, 2, 3\uff09\u306e\u8840\u5727\u3001\u8840\u7cd6\u5024\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u306e\u6a19\u6e96\u5316\u30b9\u30b3\u30a2\uff08\u5e73\u57470\u3001\u6a19\u6e96\u504f\u5dee1\u306b\u5909\u63db\u3057\u305f\u3082\u306e\uff09\u304c\u4ee5\u4e0b\u306e\u884c\u5217 \\(X\\) \u3067\u8868\u3055\u308c\u308b\u3068\u3057\u307e\u3059\uff1a</li> </ol> <p>\\(X = \\begin{pmatrix}     0.5 &amp; 1.2 &amp; -0.3 \\\\    -0.8 &amp; 0.4 &amp; 0.2 \\\\    1.3 &amp; -0.1 &amp; 0.7    \\end{pmatrix}\\)</p> <p>\u3053\u3053\u3067\u3001\u884c\u306f\u60a3\u8005\u3001\u5217\u306f\u9806\u306b\u8840\u5727\u3001\u8840\u7cd6\u5024\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u307e\u305f\u3001\u3053\u308c\u3089\u306e\u5024\u3068\u5fc3\u75be\u60a3\u30ea\u30b9\u30af\u306e\u95a2\u9023\u3092\u8868\u3059\u91cd\u307f\u4fc2\u6570\u304c \\(w = \\begin{pmatrix} 0.4 \\\\ 0.3 \\\\ 0.5 \\end{pmatrix}\\) \u3067\u4e0e\u3048\u3089\u308c\u3066\u3044\u308b\u3068\u3057\u307e\u3059\u3002</p> <p>(a) \u884c\u5217\u306e\u7a4d \\(Xw\\) \u3092\u8a08\u7b97\u3057\u3001\u5404\u60a3\u8005\u306e\u5fc3\u75be\u60a3\u30ea\u30b9\u30af\u30b9\u30b3\u30a2\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    (b) \u3069\u306e\u60a3\u8005\u304c\u6700\u3082\u30ea\u30b9\u30af\u304c\u9ad8\u3044\u3067\u3057\u3087\u3046\u304b\uff1f    (c) \u5404\u6e2c\u5b9a\u5024\uff08\u8840\u5727\u3001\u8840\u7cd6\u5024\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\uff09\u304c\u30ea\u30b9\u30af\u30b9\u30b3\u30a2\u306b\u3069\u306e\u7a0b\u5ea6\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u5206\u6790\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/05-vector-and-matrix/#q1","title":"Q1: \u884c\u5217\u306e\u7a4d\u304c\u5b9a\u7fa9\u3055\u308c\u308b\u305f\u3081\u306e\u6761\u4ef6\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A1: \u884c\u5217 \\(A\\) \u3068 \\(B\\) \u306e\u7a4d \\(AB\\) \u304c\u5b9a\u7fa9\u3055\u308c\u308b\u305f\u3081\u306b\u306f\u3001\\(A\\) \u306e\u5217\u6570\u3068 \\(B\\) \u306e\u884c\u6570\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3059\u306a\u308f\u3061\u3001\\(A\\) \u304c \\(m \\times n\\) \u884c\u5217\u3067 \\(B\\) \u304c \\(p \\times q\\) \u884c\u5217\u306e\u3068\u304d\u3001\\(n = p\\) \u3067\u3042\u308c\u3070\u7a4d \\(AB\\) \u304c\u5b9a\u7fa9\u3067\u304d\u3001\u7d50\u679c\u306f \\(m \\times q\\) \u884c\u5217\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#q2-ab-neq-ba","title":"Q2: \u884c\u5217\u306e\u7a4d\u304c\u53ef\u63db\u3067\u306a\u3044\uff08\\(AB \\neq BA\\)\uff09\u306e\u306f\u306a\u305c\u3067\u3059\u304b\uff1f","text":"<p>A2: \u884c\u5217\u306e\u7a4d\u306f\u3001\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u7dda\u5f62\u5909\u63db\u306e\u5408\u6210\u3092\u8868\u3057\u307e\u3059\u3002\u4e00\u822c\u306b\u3001\u5909\u63db\u306e\u9069\u7528\u9806\u5e8f\u3092\u5909\u3048\u308b\u3068\u7d50\u679c\u3082\u5909\u308f\u308b\u305f\u3081\u3001\u884c\u5217\u306e\u7a4d\u306f\u53ef\u63db\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u305f\u3060\u3057\u3001\u7279\u5225\u306a\u5834\u5408\uff08\u4f8b\u3048\u3070\u3001\u5bfe\u89d2\u884c\u5217\u540c\u58eb\u306e\u7a4d\u306a\u3069\uff09\u306b\u306f\u53ef\u63db\u306b\u306a\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#q3","title":"Q3: \u884c\u5217\u304c\u6b63\u5247\u3067\u3042\u308b\u3053\u3068\u3068\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u306f\u540c\u3058\u610f\u5473\u3067\u3059\u304b\uff1f","text":"<p>A3: \u306f\u3044\u3001\u540c\u3058\u610f\u5473\u3067\u3059\u3002\u6b63\u65b9\u884c\u5217 \\(A\\) \u304c\u6b63\u5247\uff08\u53ef\u9006\uff09\u3067\u3042\u308b\u3068\u306f\u3001\u305d\u306e\u9006\u884c\u5217 \\(A^{-1}\\) \u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u30022\u6b21\u306e\u884c\u5217\u306e\u5834\u5408\u3001\\(\\det(A) \\neq 0\\) \u304c\u6b63\u5247\u3067\u3042\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u3067\u3059\u3002</p>"},{"location":"lectures/LA/05-vector-and-matrix/#q4","title":"Q4: \u884c\u5217\u306e\u7a4d\u306e\u8a08\u7b97\u3067\u6700\u3082\u3088\u304f\u3042\u308b\u9593\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A4: \u884c\u5217\u306e\u7a4d\u306e\u8a08\u7b97\u3067\u3088\u304f\u3042\u308b\u9593\u9055\u3044\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u884c\u5217\u306e\u6b21\u5143\u3092\u78ba\u8a8d\u305b\u305a\u306b\u8a08\u7b97\u3057\u3088\u3046\u3068\u3059\u308b\u3053\u3068\uff08\u5de6\u5074\u306e\u884c\u5217\u306e\u5217\u6570\u3068\u53f3\u5074\u306e\u884c\u5217\u306e\u884c\u6570\u304c\u4e00\u81f4\u3057\u306a\u3044\u5834\u5408\u3001\u7a4d\u306f\u5b9a\u7fa9\u3055\u308c\u307e\u305b\u3093\uff09 2. \u6210\u5206\u3054\u3068\u306e\u7a4d\uff08\u30a2\u30c0\u30de\u30fc\u30eb\u7a4d\uff09\u3068\u884c\u5217\u7a4d\u3092\u6df7\u540c\u3059\u308b\u3053\u3068\uff08\u884c\u5217\u7a4d\u306f\u5185\u7a4d\u306e\u96c6\u307e\u308a\u3067\u3042\u308a\u3001\u5bfe\u5fdc\u3059\u308b\u6210\u5206\u540c\u58eb\u306e\u7a4d\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff09 3. \u884c\u5217\u306e\u7a4d\u306e\u975e\u53ef\u63db\u6027\u3092\u5fd8\u308c\u3001\\(AB = BA\\) \u3068\u8aa4\u3063\u3066\u4eee\u5b9a\u3059\u308b\u3053\u3068 4. \u8a08\u7b97\u904e\u7a0b\u3067\u306e\u6dfb\u5b57\u306e\u7ba1\u7406\u30df\u30b9\uff08\u7279\u306b\u5927\u304d\u306a\u884c\u5217\u3067\u306f\u6dfb\u5b57\u306e\u6271\u3044\u306b\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\uff09</p>"},{"location":"lectures/LA/05-vector-and-matrix/#q5","title":"Q5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u306a\u305c\u884c\u5217\u306e\u7a4d\u304c\u91cd\u8981\u306a\u306e\u3067\u3059\u304b\uff1f","text":"<p>A5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u884c\u5217\u306e\u7a4d\u306f\u4ee5\u4e0b\u306e\u7406\u7531\u3067\u91cd\u8981\u3067\u3059\uff1a - \u591a\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u52b9\u7387\u7684\u306b\u51e6\u7406\u3067\u304d\u308b\uff08\u591a\u6570\u306e\u89b3\u6e2c\u5024\u3068\u7279\u5fb4\u91cf\u3092\u4e00\u5ea6\u306b\u6271\u3048\u308b\uff09 - \u7dda\u5f62\u5909\u63db\u3084\u5ea7\u6a19\u5909\u63db\u3092\u8868\u73fe\u3067\u304d\u308b\uff08\u6b21\u5143\u524a\u6e1b\u3084\u7279\u5fb4\u62bd\u51fa\u306a\u3069\uff09 - \u7dda\u5f62\u56de\u5e30\u3084\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u306e\u7d71\u8a08\u7684\u624b\u6cd5\u306e\u57fa\u790e\u3068\u306a\u308b - \u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306e\u95a2\u4fc2\u6027\u3092\u7c21\u6f54\u306b\u8a18\u8ff0\u3067\u304d\u308b - \u6a5f\u68b0\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u591a\u304f\u306f\u884c\u5217\u8a08\u7b97\u306b\u57fa\u3065\u3044\u3066\u3044\u308b\uff08\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5c64\u9593\u306e\u8a08\u7b97\u306a\u3069\uff09</p>"},{"location":"lectures/LA/05-vector-and-matrix/#q6","title":"Q6: \u9006\u884c\u5217\u306f\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u5b58\u5728\u3057\u307e\u305b\u3093\u304b\uff1f","text":"<p>A6: \u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u306a\u3044\uff08\u7279\u7570\u884c\u5217\u3067\u3042\u308b\uff09\u306e\u306f\u3001\u4ee5\u4e0b\u306e\u5834\u5408\u3067\u3059\uff1a - \u884c\u5217\u5f0f\u304c0\u3067\u3042\u308b\u5834\u5408\uff08\\(\\det(A) = 0\\)\uff09 - \u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u884c\u6570\uff08\u307e\u305f\u306f\u5217\u6570\uff09\u3088\u308a\u3082\u5c0f\u3055\u3044\u5834\u5408 - \u884c\u5217\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u304c\u7dda\u5f62\u5f93\u5c5e\u3067\u3042\u308b\u5834\u5408\uff08\u4e00\u3064\u306e\u884c\u304c\u4ed6\u306e\u884c\u306e\u7dda\u5f62\u7d50\u5408\u3067\u8868\u305b\u308b\uff09 - \u6b63\u65b9\u884c\u5217\u3067\u306a\u3044\u5834\u5408\uff08\u884c\u6570\u3068\u5217\u6570\u304c\u7570\u306a\u308b\u5834\u5408\uff09</p>"},{"location":"lectures/LA/06-vector-and-matrix/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II","text":""},{"location":"lectures/LA/06-vector-and-matrix/#6","title":"\u7b2c6\u56de \u8b1b\u7fa9\u30ce\u30fc\u30c8\uff1a\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u5fc5\u8981\u306a\u884c\u5217","text":""},{"location":"lectures/LA/06-vector-and-matrix/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<ul> <li>\u8b1b\u7fa9\u56de: \u7b2c6\u56de</li> <li>\u95a2\u9023\u9805\u76ee: \u884c\u5217\u306e\u7279\u6b8a\u5f62\u3001\u884c\u5217\u306e\u6027\u8cea</li> <li>\u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: </li> <li>\u884c\u5217\u306e\u5b9a\u7fa9\uff08\u7b2c4\u56de\uff09</li> <li>\u884c\u5217\u306e\u548c\u3068\u30b9\u30ab\u30e9\u30fc\u500d\uff08\u7b2c4\u56de\uff09</li> <li>\u884c\u5217\u306e\u7a4d\uff08\u7b2c5\u56de\uff09</li> </ul>"},{"location":"lectures/LA/06-vector-and-matrix/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u5358\u4f4d\u884c\u5217\u306e\u5b9a\u7fa9\u3068\u6027\u8cea\u3092\u7406\u89e3\u3057\u3001\u5fdc\u7528\u3067\u304d\u308b</li> <li>\u8ee2\u7f6e\u884c\u5217\u306e\u5b9a\u7fa9\u3068\u6027\u8cea\u3092\u7406\u89e3\u3057\u3001\u5fdc\u7528\u3067\u304d\u308b</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u5b9a\u7fa9\u3068\u6027\u8cea\u3092\u7406\u89e3\u3057\u3001\u5fdc\u7528\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u7279\u6b8a\u5f62\u304c\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u3069\u306e\u3088\u3046\u306b\u6d3b\u7528\u3055\u308c\u308b\u304b\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/06-vector-and-matrix/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/06-vector-and-matrix/#31-identity-matrix","title":"3.1 \u5358\u4f4d\u884c\u5217\uff08Identity Matrix\uff09","text":"<p>\u5b9a\u7fa9: n\u6b21\u306e\u5358\u4f4d\u884c\u5217 \\(I_n\\) \u306f\u3001\u4e3b\u5bfe\u89d2\u7dda\u4e0a\u306e\u8981\u7d20\u304c\u3059\u3079\u30661\u3067\u3001\u305d\u308c\u4ee5\u5916\u306e\u8981\u7d20\u304c\u3059\u3079\u30660\u3067\u3042\u308b\u6b63\u65b9\u884c\u5217\u3067\u3042\u308b\u3002</p> \\[I_n = \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{pmatrix}\\] <p>\u4f8b: 2\u6b21\u5358\u4f4d\u884c\u5217\u30683\u6b21\u5358\u4f4d\u884c\u5217</p> \\[I_2 = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}, \\quad I_3 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] <p>\u5358\u4f4d\u884c\u5217\u306e\u6027\u8cea:</p> <ol> <li>\u4efb\u610f\u306e\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066: \\(AI = IA = A\\) \uff08\u305f\u3060\u3057 \\(I\\) \u306f\u9069\u5207\u306a\u30b5\u30a4\u30ba\u306e\u5358\u4f4d\u884c\u5217\uff09</li> <li>\u5358\u4f4d\u884c\u5217 \\(I\\) \u306f\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b</li> <li>\u5358\u4f4d\u884c\u5217 \\(I\\) \u306e\u9006\u884c\u5217\u306f \\(I\\) \u81ea\u8eab\u3067\u3042\u308b: \\(I^{-1} = I\\)</li> <li>\u5358\u4f4d\u884c\u5217 \\(I\\) \u306e\u30e9\u30f3\u30af\u306f \\(n\\) \u3067\u3042\u308b\uff08\\(I\\) \u304c \\(n \\times n\\) \u884c\u5217\u306e\u5834\u5408\uff09</li> </ol> <p>\u6570\u5024\u4f8b:</p> <p>\u3042\u308b \\(2 \\times 2\\) \u884c\u5217 \\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 4 \\end{pmatrix}\\) \u3068\u5358\u4f4d\u884c\u5217 \\(I_2\\) \u3068\u306e\u7a4d\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[A \\cdot I_2 = \\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 4 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\cdot 1 + 1 \\cdot 0 &amp; 3 \\cdot 0 + 1 \\cdot 1 \\\\ 2 \\cdot 1 + 4 \\cdot 0 &amp; 2 \\cdot 0 + 4 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 4 \\end{pmatrix} = A\\] <p>\u540c\u69d8\u306b \\(I_2 \\cdot A\\) \u3082\u8a08\u7b97\u3059\u308b\u3068\u7d50\u679c\u306f \\(A\\) \u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/06-vector-and-matrix/#32-transpose-matrix","title":"3.2 \u8ee2\u7f6e\u884c\u5217\uff08Transpose Matrix\uff09","text":"<p>\u5b9a\u7fa9: \u884c\u5217 \\(A\\) \u306e\u8ee2\u7f6e\u884c\u5217 \\(A^T\\) \u306f\u3001\\(A\\) \u306e\u884c\u3068\u5217\u3092\u5165\u308c\u66ff\u3048\u305f\u884c\u5217\u3067\u3042\u308b\u3002</p> <p>\\(A\\) \u304c \\(m \\times n\\) \u884c\u5217\u306e\u5834\u5408\u3001\\(A^T\\) \u306f \\(n \\times m\\) \u884c\u5217\u3068\u306a\u308b\u3002</p> <p>\u5177\u4f53\u7684\u306b\u306f\u3001\\(A = (a_{ij})\\) \u306b\u5bfe\u3057\u3066\u3001\\(A^T = (a_{ji})\\) \u3067\u3042\u308b\u3002</p> <p>\u4f8b: \u884c\u5217 \\(A = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{pmatrix}\\) \u306e\u8ee2\u7f6e\u884c\u5217\u306f:</p> \\[A^T = \\begin{pmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{pmatrix}\\] <p>\u8ee2\u7f6e\u884c\u5217\u306e\u6027\u8cea:</p> <ol> <li>\\((A^T)^T = A\\)</li> <li>\\((A + B)^T = A^T + B^T\\)</li> <li>\\((cA)^T = cA^T\\) \uff08\\(c\\) \u306f\u30b9\u30ab\u30e9\u30fc\uff09</li> <li>\\((AB)^T = B^T A^T\\) \uff08\u884c\u5217\u306e\u7a4d\u306e\u8ee2\u7f6e\u306f\u3001\u8ee2\u7f6e\u306e\u7a4d\u306e\u9806\u5e8f\u3092\u9006\u306b\u3057\u305f\u3082\u306e\u306b\u7b49\u3057\u3044\uff09</li> <li>\\(\\text{rank}(A) = \\text{rank}(A^T)\\) \uff08\u884c\u5217\u3068\u305d\u306e\u8ee2\u7f6e\u306e\u30e9\u30f3\u30af\u306f\u7b49\u3057\u3044\uff09</li> </ol> <p>\u6570\u5024\u4f8b:</p> <p>\u884c\u5217 \\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}\\) \u3068 \\(B = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\\((A + B)^T\\) \u3068 \\(A^T + B^T\\) \u304c\u7b49\u3057\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> \\[A + B = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} + \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix} = \\begin{pmatrix} 6 &amp; 8 \\\\ 10 &amp; 12 \\end{pmatrix}\\] \\[(A + B)^T = \\begin{pmatrix} 6 &amp; 10 \\\\ 8 &amp; 12 \\end{pmatrix}\\] \\[A^T + B^T = \\begin{pmatrix} 1 &amp; 3 \\\\ 2 &amp; 4 \\end{pmatrix} + \\begin{pmatrix} 5 &amp; 7 \\\\ 6 &amp; 8 \\end{pmatrix} = \\begin{pmatrix} 6 &amp; 10 \\\\ 8 &amp; 12 \\end{pmatrix}\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\\((A + B)^T = A^T + B^T\\) \u304c\u6210\u308a\u7acb\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/LA/06-vector-and-matrix/#33-symmetric-matrix","title":"3.3 \u5bfe\u79f0\u884c\u5217\uff08Symmetric Matrix\uff09","text":"<p>\u5b9a\u7fa9: \u6b63\u65b9\u884c\u5217 \\(A\\) \u304c\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u3068\u306f\u3001\\(A = A^T\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3067\u3042\u308b\u3002\u3064\u307e\u308a\u3001\\(a_{ij} = a_{ji}\\) \u304c\u3059\u3079\u3066\u306e \\(i, j\\) \u306b\u3064\u3044\u3066\u6210\u308a\u7acb\u3064\u3002</p> <p>\u4f8b: </p> \\[A = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 6 \\end{pmatrix}\\] <p>\u3053\u306e\u884c\u5217\u3067\u306f\u3001\\(a_{12} = a_{21} = 2\\), \\(a_{13} = a_{31} = 3\\), \\(a_{23} = a_{32} = 5\\) \u3068\u306a\u3063\u3066\u304a\u308a\u3001\u4e3b\u5bfe\u89d2\u7dda\u306b\u95a2\u3057\u3066\u5bfe\u79f0\u306a\u4f4d\u7f6e\u306b\u3042\u308b\u8981\u7d20\u304c\u7b49\u3057\u3044\u305f\u3081\u3001\u5bfe\u79f0\u884c\u5217\u3067\u3059\u3002</p> <p>\u5bfe\u79f0\u884c\u5217\u306e\u6027\u8cea:</p> <ol> <li>\u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u8981\u7d20 \\(a_{ii}\\) \u306f\u5b9f\u6570\u3067\u3042\u308b</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u306f\u3059\u3079\u3066\u5b9f\u6570\u3067\u3042\u308b\uff08\u5f8c\u306e\u8b1b\u7fa9\u3067\u8a73\u7d30\u306b\u8aac\u660e\uff09</li> <li>\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b\uff08\u5f8c\u306e\u8b1b\u7fa9\u3067\u8a73\u7d30\u306b\u8aac\u660e\uff09</li> <li>\u4efb\u610f\u306e\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(A^T A\\) \u3068 \\(A A^T\\) \u306f\u5e38\u306b\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b</li> <li>\u5bfe\u79f0\u884c\u5217\u540c\u58eb\u306e\u548c\u3082\u5bfe\u79f0\u884c\u5217\u306b\u306a\u308b</li> </ol> <p>\u6570\u5024\u4f8b:</p> <p>\u4efb\u610f\u306e\u884c\u5217\u304b\u3089\u5bfe\u79f0\u884c\u5217\u3092\u4f5c\u308b\u65b9\u6cd5\u3068\u3057\u3066\u3001\\(A^T A\\) \u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}\\] \\[A^T = \\begin{pmatrix} 1 &amp; 3 \\\\ 2 &amp; 4 \\end{pmatrix}\\] \\[A^T A = \\begin{pmatrix} 1 &amp; 3 \\\\ 2 &amp; 4 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 3 \\cdot 3 &amp; 1 \\cdot 2 + 3 \\cdot 4 \\\\ 2 \\cdot 1 + 4 \\cdot 3 &amp; 2 \\cdot 2 + 4 \\cdot 4 \\end{pmatrix} = \\begin{pmatrix} 10 &amp; 14 \\\\ 14 &amp; 20 \\end{pmatrix}\\] <p>\u7d50\u679c\u306e\u884c\u5217\u304c\u5bfe\u79f0\u884c\u5217\u306b\u306a\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/06-vector-and-matrix/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/06-vector-and-matrix/#41","title":"4.1 \u5358\u4f4d\u884c\u5217\u306e\u5fdc\u7528","text":"<ol> <li> <p>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5:    \u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f \\(Ax = b\\) \u306b\u304a\u3044\u3066\u3001\u4e21\u8fba\u306b \\(A\\) \u306e\u9006\u884c\u5217 \\(A^{-1}\\) \u3092\u304b\u3051\u308b\u3068:    \\(A^{-1}Ax = A^{-1}b\\) \\(Ix = A^{-1}b\\) \\(x = A^{-1}b\\)    \u3068\u306a\u308a\u3001\u89e3\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u884c\u5217\u306e\u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6:    \u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(AB = BA = I\\) \u3068\u306a\u308b\u884c\u5217 \\(B\\) \u304c\u5b58\u5728\u3059\u308b\u3068\u304d\u3001\\(B\\) \u3092 \\(A\\) \u306e\u9006\u884c\u5217\u3068\u3044\u3044\u3001\\(A^{-1}\\) \u3068\u8868\u3057\u307e\u3059\u3002\u3059\u3079\u3066\u306e\u884c\u5217\u306b\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u308f\u3051\u3067\u306f\u306a\u304f\u3001\\(A\\) \u304c\u6b63\u5247\uff08\u884c\u5217\u5f0f\u304c0\u3067\u306a\u3044\uff09\u306e\u3068\u304d\u306e\u307f\u5b58\u5728\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u7dda\u5f62\u5909\u63db:    \u5358\u4f4d\u884c\u5217 \\(I\\) \u306f\u3001\u7a7a\u9593\u3092\u5909\u5316\u3055\u305b\u306a\u3044\u7dda\u5f62\u5909\u63db\uff08\u6052\u7b49\u5909\u63db\uff09\u3092\u8868\u3057\u307e\u3059\u3002</p> </li> </ol>"},{"location":"lectures/LA/06-vector-and-matrix/#42","title":"4.2 \u8ee2\u7f6e\u884c\u5217\u306e\u5fdc\u7528","text":"<ol> <li> <p>\u5185\u7a4d\u306e\u8a08\u7b97:    \u30d9\u30af\u30c8\u30eb \\(x, y\\) \u306e\u5185\u7a4d\u306f \\(x^T y\\) \u3067\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u4e8c\u6b21\u5f62\u5f0f:    \\(x^T A x\\) \u306e\u5f62\u306e\u5f0f\u306f\u4e8c\u6b21\u5f62\u5f0f\u3068\u547c\u3070\u308c\u3001\u591a\u5909\u91cf\u7d71\u8a08\u89e3\u6790\u3067\u91cd\u8981\u3067\u3059\u3002</p> </li> <li> <p>\u6b63\u898f\u65b9\u7a0b\u5f0f:    \u7dda\u5f62\u56de\u5e30\u306b\u304a\u3051\u308b\u6b63\u898f\u65b9\u7a0b\u5f0f \\(X^T X \\beta = X^T y\\) \u306e\u5c0e\u51fa\u306b\u8ee2\u7f6e\u884c\u5217\u304c\u4f7f\u308f\u308c\u307e\u3059\u3002</p> </li> </ol>"},{"location":"lectures/LA/06-vector-and-matrix/#43","title":"4.3 \u5bfe\u79f0\u884c\u5217\u306e\u5fdc\u7528","text":"<ol> <li> <p>\u5171\u5206\u6563\u884c\u5217:    \u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u306f\u5e38\u306b\u5bfe\u79f0\u884c\u5217\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u4e8c\u6b21\u5f62\u5f0f\u3068\u534a\u6b63\u5b9a\u5024\u6027:    \u5bfe\u79f0\u884c\u5217 \\(A\\) \u304c\u534a\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3068\u306f\u3001\u4efb\u610f\u306e\u975e\u30bc\u30ed\u30d9\u30af\u30c8\u30eb \\(x\\) \u306b\u5bfe\u3057\u3066 \\(x^T A x \\geq 0\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3067\u3059\u3002\u5171\u5206\u6563\u884c\u5217\u306f\u534a\u6b63\u5b9a\u5024\u3067\u3059\u3002</p> </li> <li> <p>\u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3:    \u5bfe\u79f0\u884c\u5217 \\(A\\) \u306f\u56fa\u6709\u5024 \\(\\lambda_i\\) \u3068\u5bfe\u5fdc\u3059\u308b\u6b63\u898f\u76f4\u4ea4\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(u_i\\) \u3092\u7528\u3044\u3066\u3001\\(A = \\sum_{i=1}^{n} \\lambda_i u_i u_i^T\\) \u3068\u5206\u89e3\u3067\u304d\u307e\u3059\uff08\u5f8c\u306e\u8b1b\u7fa9\u3067\u8a73\u7d30\u306b\u8aac\u660e\uff09\u3002</p> </li> </ol>"},{"location":"lectures/LA/06-vector-and-matrix/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>NumPy\u3092\u4f7f\u3063\u3066\u3001\u5358\u4f4d\u884c\u5217\u3001\u8ee2\u7f6e\u884c\u5217\u3001\u5bfe\u79f0\u884c\u5217\u306e\u57fa\u672c\u7684\u306a\u64cd\u4f5c\u3092\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyArrowPatch\nfrom mpl_toolkits.mplot3d import proj3d\n\n# \u5358\u4f4d\u884c\u5217\u306e\u751f\u6210\nI2 = np.eye(2)  # 2\u6b21\u306e\u5358\u4f4d\u884c\u5217\nI3 = np.eye(3)  # 3\u6b21\u306e\u5358\u4f4d\u884c\u5217\nprint(\"2\u6b21\u306e\u5358\u4f4d\u884c\u5217:\\n\", I2)\nprint(\"\\n3\u6b21\u306e\u5358\u4f4d\u884c\u5217:\\n\", I3)\n\n# \u884c\u5217\u306e\u8ee2\u7f6e\nA = np.array([[1, 2, 3], [4, 5, 6]])\nAT = A.T\nprint(\"\\n\u884c\u5217A:\\n\", A)\nprint(\"\\n\u884c\u5217A\u306e\u8ee2\u7f6e:\\n\", AT)\n\n# \u5bfe\u79f0\u884c\u5217\u306e\u751f\u6210\u3068\u691c\u8a3c\nB = np.array([[1, 2, 3], [2, 4, 5], [3, 5, 6]])\nBT = B.T\nis_symmetric = np.array_equal(B, BT)\nprint(\"\\n\u884c\u5217B:\\n\", B)\nprint(\"\\n\u884c\u5217B\u306f\u5bfe\u79f0\u884c\u5217\u304b?:\", is_symmetric)\n\n# \u4efb\u610f\u306e\u884c\u5217\u304b\u3089\u5bfe\u79f0\u884c\u5217\u3092\u4f5c\u308b\nC = np.array([[1, 2], [3, 4]])\nC_symmetric = C.T @ C  # C^T * C\nprint(\"\\n\u884c\u5217C:\\n\", C)\nprint(\"\\n\u884c\u5217C^T * C (\u5bfe\u79f0\u884c\u5217):\\n\", C_symmetric)\n\n# \u5358\u4f4d\u884c\u5217\u306e\u6027\u8cea\u3092\u8996\u899a\u5316\uff08\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u7a4d\uff09\n# 2\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3092\u5358\u4f4d\u884c\u5217\u3067\u5909\u63db\uff08\u5909\u5316\u306a\u3057\uff09\u3092\u793a\u3059\nv = np.array([2, 1])  # \u5143\u306e\u30d9\u30af\u30c8\u30eb\nv_transformed = I2 @ v  # \u5358\u4f4d\u884c\u5217\u3068\u306e\u7a4d\n\nplt.figure(figsize=(8, 6))\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nplt.grid(alpha=0.3)\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='b', label='\u5143\u306e\u30d9\u30af\u30c8\u30eb v')\nplt.quiver(0, 0, v_transformed[0], v_transformed[1], angles='xy', scale_units='xy', scale=1, color='r', label='I*v')\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nplt.title(\"\u5358\u4f4d\u884c\u5217\u306b\u3088\u308b\u5909\u63db\uff08\u6052\u7b49\u5909\u63db\uff09\")\nplt.legend()\nplt.axis('equal')\nplt.show()\n\n# \u5bfe\u79f0\u884c\u5217\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u8868\u793a\nplt.figure(figsize=(10, 8))\nplt.subplot(1, 2, 1)\nplt.imshow(B, cmap='viridis')\nplt.colorbar()\nplt.title(\"\u5bfe\u79f0\u884c\u5217B\")\nfor i in range(B.shape[0]):\n    for j in range(B.shape[1]):\n        plt.text(j, i, str(B[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n\n# \u975e\u5bfe\u79f0\u884c\u5217\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\nD = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nplt.subplot(1, 2, 2)\nplt.imshow(D, cmap='viridis')\nplt.colorbar()\nplt.title(\"\u975e\u5bfe\u79f0\u884c\u5217D\")\nfor i in range(D.shape[0]):\n    for j in range(D.shape[1]):\n        plt.text(j, i, str(D[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/06-vector-and-matrix/#6_1","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/06-vector-and-matrix/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>3\u6b21\u306e\u5358\u4f4d\u884c\u5217\u3092\u624b\u3067\u66f8\u304d\u4e0b\u3052\u3001\u6b21\u306e\u884c\u5217\u3068\u306e\u7a4d\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(A = \\begin{pmatrix} 2 &amp; 0 &amp; 1 \\\\ 3 &amp; 1 &amp; 4 \\\\ 0 &amp; 2 &amp; 5 \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u306b\u5bfe\u3057\u3066\u3001\u8ee2\u7f6e\u884c\u5217\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    a) \\(\\(A = \\begin{pmatrix} 3 &amp; -1 \\\\ 2 &amp; 4 \\\\ 0 &amp; 5 \\end{pmatrix}\\)\\)    b) \\(\\(B = \\begin{pmatrix} 1 &amp; 0 &amp; -2 \\\\ 3 &amp; 1 &amp; 4 \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u304c\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002    a) \\(\\(A = \\begin{pmatrix} 3 &amp; 1 &amp; 2 \\\\ 1 &amp; 5 &amp; 0 \\\\ 2 &amp; 0 &amp; 4 \\end{pmatrix}\\)\\)    b) \\(\\(B = \\begin{pmatrix} 2 &amp; 1 &amp; 3 \\\\ 1 &amp; 4 &amp; 2 \\\\ 2 &amp; 2 &amp; 5 \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(A^T A\\) \u3068 \\(A A^T\\) \u3092\u8a08\u7b97\u3057\u3001\u3069\u3061\u3089\u3082\u5bfe\u79f0\u884c\u5217\u306b\u306a\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002    \\(\\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{pmatrix}\\)\\)</p> </li> <li> <p>\\(3 \\times 3\\) \u306e\u5bfe\u79f0\u884c\u5217 \\(S\\) \u30921\u3064\u4f5c\u6210\u3057\u3001\\(S = S^T\\) \u3092\u6e80\u305f\u3059\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/06-vector-and-matrix/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u884c\u5217 \\(A\\) \u3068 \\(B\\) \u304c\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u3068\u304d\u3001\\(A + B\\) \u3082\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u306a\u3055\u3044\u3002\u307e\u305f\u3001\\(AB\\) \u304c\u4e00\u822c\u306b\u5bfe\u79f0\u884c\u5217\u306b\u306a\u308b\u3068\u306f\u9650\u3089\u306a\u3044\u3053\u3068\u3092\u4f8b\u3092\u6319\u3052\u3066\u793a\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u4efb\u610f\u306e\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(A + A^T\\) \u304c\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u306a\u3055\u3044\u3002\u540c\u69d8\u306b\u3001\\(A - A^T\\) \u304c\u53cd\u5bfe\u79f0\u884c\u5217\uff08\\(B^T = -B\\) \u3092\u6e80\u305f\u3059\u884c\u5217\uff09\u3067\u3042\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u5bfe\u79f0\u884c\u5217 \\(A\\) \u3068\u975e\u5bfe\u79f0\u884c\u5217 \\(B\\) \u306e\u7a4d \\(AB\\) \u3092\u4f7f\u3063\u3066\u5bfe\u79f0\u884c\u5217\u3092\u4f5c\u308b\u65b9\u6cd5\u3092\u8003\u3048\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5fdc\u7528\u554f\u984c\uff1a    \u60a3\u8005\u306e\u8840\u5727\uff08\u53ce\u7e2e\u671f\u3001\u62e1\u5f35\u671f\uff09\u3001\u5fc3\u62cd\u6570\u3001\u4f53\u6e29\u3001\u304a\u3088\u3073\u9178\u7d20\u98fd\u548c\u5ea6\u306e5\u3064\u306e\u751f\u4f53\u30c7\u30fc\u30bf\u304c\u3042\u308b\u3068\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u9593\u306e\u76f8\u95a2\u3092\u8868\u3059\u76f8\u95a2\u884c\u5217\u306f\u5bfe\u79f0\u884c\u5217\u306b\u306a\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306e10\u4eba\u5206\u306e\u30c7\u30fc\u30bf\u304c\u3042\u308b\u3068\u304d\u3001\u76f8\u95a2\u884c\u5217\u3092\u8a08\u7b97\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u307e\u305f\u3001\u3053\u306e\u76f8\u95a2\u884c\u5217\u304b\u3089\u5206\u304b\u308b\u5065\u5eb7\u6307\u6a19\u9593\u306e\u95a2\u4fc2\u3092\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <pre><code># \u5065\u5eb7\u30c7\u30fc\u30bf\uff0810\u4eba\u5206\uff09\n# \u53ce\u7e2e\u671f\u8840\u5727, \u62e1\u5f35\u671f\u8840\u5727, \u5fc3\u62cd\u6570, \u4f53\u6e29, \u9178\u7d20\u98fd\u548c\u5ea6\nhealth_data = np.array([\n    [120, 80, 72, 36.5, 98],\n    [130, 85, 78, 36.8, 97],\n    [125, 82, 70, 36.6, 99],\n    [140, 90, 85, 37.0, 96],\n    [115, 75, 65, 36.4, 98],\n    [135, 88, 80, 36.9, 97],\n    [122, 78, 68, 36.5, 99],\n    [128, 84, 75, 36.7, 98],\n    [132, 86, 82, 36.8, 96],\n    [118, 76, 67, 36.4, 99]\n])\n\n# \u30d2\u30f3\u30c8\uff1a\u76f8\u95a2\u884c\u5217\u306e\u8a08\u7b97\u306b\u306f numpy.corrcoef() \u95a2\u6570\u304c\u4f7f\u3048\u307e\u3059\n</code></pre> <ol> <li>\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(A^T A\\) \u304c\u3069\u306e\u3088\u3046\u306a\u6761\u4ef6\u4e0b\u3067\u5358\u4f4d\u884c\u5217\u306b\u306a\u308b\u304b\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002\u307e\u305f\u3001\u305d\u306e\u3088\u3046\u306a\u884c\u5217 \\(A\\) \u306e\u4f8b\u3092\u6319\u3052\u306a\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/LA/06-vector-and-matrix/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":"<p>Q1: \u5bfe\u79f0\u884c\u5217\u3068\u5358\u4f4d\u884c\u5217\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A1: \u5358\u4f4d\u884c\u5217\u306f\u3001\u4e3b\u5bfe\u89d2\u7dda\u4e0a\u306e\u8981\u7d20\u304c\u3059\u3079\u30661\u3067\u3001\u305d\u308c\u4ee5\u5916\u306e\u8981\u7d20\u304c\u3059\u3079\u30660\u3067\u3042\u308b\u7279\u6b8a\u306a\u5bfe\u79f0\u884c\u5217\u3067\u3059\u3002\u5bfe\u79f0\u884c\u5217\u306f \\(A = A^T\\) \u3092\u6e80\u305f\u3059\u884c\u5217\u3067\u3001\u4e3b\u5bfe\u89d2\u7dda\u306b\u95a2\u3057\u3066\u5bfe\u79f0\u306a\u4f4d\u7f6e\u306b\u3042\u308b\u8981\u7d20\u304c\u7b49\u3057\u3044\u3082\u306e\u3067\u3059\u304c\u3001\u5bfe\u89d2\u8981\u7d20\u3084\u975e\u5bfe\u89d2\u8981\u7d20\u306e\u5024\u306b\u5236\u7d04\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> <p>Q2: \u8ee2\u7f6e\u884c\u5217\u3092\u6c42\u3081\u308b\u969b\u306b\u3088\u304f\u3042\u308b\u9593\u9055\u3044\u306f\uff1f</p> <p>A2: \u8ee2\u7f6e\u884c\u5217\u3092\u6c42\u3081\u308b\u969b\u306b\u3088\u304f\u3042\u308b\u9593\u9055\u3044\u306f\u3001\u884c\u3068\u5217\u3092\u5165\u308c\u66ff\u3048\u308b\u64cd\u4f5c\u3092\u6b63\u78ba\u306b\u884c\u308f\u306a\u3044\u3053\u3068\u3067\u3059\u3002\u7279\u306b\u5927\u304d\u306a\u884c\u5217\u306e\u5834\u5408\u3001\u8981\u7d20\u306e\u4f4d\u7f6e\u3092\u6df7\u540c\u3057\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002\u8ee2\u7f6e\u64cd\u4f5c\u3067\u306f\u3001\\(a_{ij}\\) \u304c \\(a_{ji}\\) \u306b\u306a\u308b\u3053\u3068\u3092\u5e38\u306b\u5ff5\u982d\u306b\u7f6e\u304f\u3053\u3068\u304c\u5927\u5207\u3067\u3059\u3002</p> <p>Q3: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306a\u305c\u5bfe\u79f0\u884c\u5217\u304c\u91cd\u8981\u306a\u306e\u3067\u3059\u304b\uff1f</p> <p>A3: \u5bfe\u79f0\u884c\u5217\u306f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u975e\u5e38\u306b\u91cd\u8981\u3067\u3059\u3002\u7279\u306b\u5171\u5206\u6563\u884c\u5217\u3084\u76f8\u95a2\u884c\u5217\u306f\u5e38\u306b\u5bfe\u79f0\u884c\u5217\u306b\u306a\u308a\u3001\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3084\u56e0\u5b50\u5206\u6790\u306a\u3069\u306e\u591a\u5909\u91cf\u89e3\u6790\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u5bfe\u79f0\u884c\u5217\u306f\u5b9f\u6570\u306e\u56fa\u6709\u5024\u3092\u6301\u3061\u3001\u76f4\u4ea4\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6301\u3064\u3068\u3044\u3046\u7279\u6b8a\u306a\u6027\u8cea\u304c\u3042\u308b\u305f\u3081\u3001\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u62bd\u51fa\u3084\u6b21\u5143\u524a\u6e1b\u306b\u5229\u7528\u3055\u308c\u307e\u3059\u3002</p> <p>Q4: \u884c\u5217 \\(A^T A\\) \u306f\u306a\u305c\u3044\u3064\u3082\u5bfe\u79f0\u884c\u5217\u306b\u306a\u308b\u306e\u3067\u3059\u304b\uff1f</p> <p>A4: \u884c\u5217 \\(A^T A\\) \u306e\u8ee2\u7f6e\u3092\u8003\u3048\u308b\u3068\uff1a \\((A^T A)^T = A^T (A^T)^T = A^T A\\) \u3068\u306a\u308a\u3001\u5b9a\u7fa9\u304b\u3089\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u4efb\u610f\u306e\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u6210\u308a\u7acb\u3061\u307e\u3059\u3002</p> <p>Q5: \u5358\u4f4d\u884c\u5217\u306f\u3069\u306e\u3088\u3046\u306b\u3057\u3066Python\u3067\u751f\u6210\u3057\u307e\u3059\u304b\uff1f</p> <p>A5: NumPy\u3092\u4f7f\u3063\u3066\u5358\u4f4d\u884c\u5217\u3092\u751f\u6210\u3059\u308b\u306b\u306f\u3001<code>numpy.eye(n)</code> \u95a2\u6570\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u3053\u3053\u3067 <code>n</code> \u306f\u884c\u5217\u306e\u30b5\u30a4\u30ba\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001<code>numpy.eye(3)</code> \u306f3\u6b21\u306e\u5358\u4f4d\u884c\u5217\u3092\u751f\u6210\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nI3 = np.eye(3)  # 3\u6b21\u306e\u5358\u4f4d\u884c\u5217\u3092\u751f\u6210\nprint(I3)\n</code></pre>"},{"location":"lectures/LA/06-vector-and-matrix/#8","title":"8. \u307e\u3068\u3081","text":"<p>\u3053\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u7279\u306b\u91cd\u8981\u306a\u7279\u6b8a\u306a\u884c\u5217\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\uff1a</p> <ol> <li>\u5358\u4f4d\u884c\u5217\uff1a\u4e3b\u5bfe\u89d2\u7dda\u4e0a\u306e\u8981\u7d20\u304c\u3059\u3079\u30661\u3067\u3001\u305d\u308c\u4ee5\u5916\u306f0\u306e\u884c\u5217\u3002\u4efb\u610f\u306e\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066 \\(AI = IA = A\\) \u304c\u6210\u308a\u7acb\u3064\u3002</li> <li>\u8ee2\u7f6e\u884c\u5217\uff1a\u884c\u3068\u5217\u3092\u5165\u308c\u66ff\u3048\u305f\u884c\u5217\u3002\\((AB)^T = B^T A^T\\) \u306a\u3069\u91cd\u8981\u306a\u6027\u8cea\u3092\u6301\u3064\u3002</li> <li>\u5bfe\u79f0\u884c\u5217\uff1a\u8ee2\u7f6e\u884c\u5217\u3068\u7b49\u3057\u3044\u884c\u5217 (\\(A = A^T\\))\u3002\u5171\u5206\u6563\u884c\u5217\u3084\u76f8\u95a2\u884c\u5217\u306a\u3069\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u983b\u7e41\u306b\u73fe\u308c\u308b\u3002</li> </ol> <p>\u3053\u308c\u3089\u306e\u7279\u6b8a\u306a\u884c\u5217\u306e\u6027\u8cea\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u306f\u3001\u5f8c\u306e\u8b1b\u7fa9\u3067\u6271\u3046\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3084\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u306e\u9ad8\u5ea6\u306a\u30c8\u30d4\u30c3\u30af\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002\u7279\u306b\u3001\u5bfe\u79f0\u884c\u5217\u306e\u6027\u8cea\u306f\u4e3b\u6210\u5206\u5206\u6790\u306e\u7406\u8ad6\u7684\u57fa\u76e4\u3068\u306a\u308b\u91cd\u8981\u306a\u6982\u5ff5\u3067\u3059\u3002</p> <p>\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u30011\u6b21\u5143\u30c7\u30fc\u30bf\u3068\u30d9\u30af\u30c8\u30eb\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u5b66\u3073\u3001\u7d71\u8a08\u91cf\u306e\u8a08\u7b97\u306b\u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u304c\u3069\u306e\u3088\u3046\u306b\u6d3b\u7528\u3055\u308c\u308b\u304b\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I - \u7b2c7\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/07-vector-and-matrix/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c7\u56de \u30c6\u30fc\u30de: 1\u6b21\u5143\u30c7\u30fc\u30bf\u3068\u30d9\u30af\u30c8\u30eb\u306e\u548c\u3068\u7a4d \u95a2\u9023\u9805\u76ee: \u30c7\u30fc\u30bf\u306e\u5e73\u5747\u3001\u504f\u5dee\u3001\u5206\u6563\u3001\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u8868\u73fe \u4e88\u7fd2\u5185\u5bb9:  - \u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u6f14\u7b97\uff08\u5185\u7a4d\u3001\u30ce\u30eb\u30e0\uff09 - \u7d71\u8a08\u5b66\u306b\u304a\u3051\u308b\u57fa\u672c\u7684\u306a\u6982\u5ff5\uff08\u5e73\u5747\u3001\u5206\u6563\uff09</p>"},{"location":"lectures/LA/07-vector-and-matrix/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> <ol> <li>1\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u308b</li> <li>\u30d9\u30af\u30c8\u30eb\u306e\u6f14\u7b97\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u5024\u3092\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u30d9\u30af\u30c8\u30eb\u306e\u6f14\u7b97\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u306e\u504f\u5dee\u3092\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u30d9\u30af\u30c8\u30eb\u306e\u6f14\u7b97\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u8a08\u7b97\u3067\u304d\u308b</li> <li>Google Colab\u3092\u7528\u3044\u3066\u3053\u308c\u3089\u306e\u8a08\u7b97\u3092\u5b9f\u88c5\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/07-vector-and-matrix/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/07-vector-and-matrix/#31","title":"3.1 \u30c7\u30fc\u30bf\u3068\u30d9\u30af\u30c8\u30eb\u8868\u73fe","text":"<p>\u73fe\u5b9f\u4e16\u754c\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u591a\u6570\u306e\u89b3\u6e2c\u5024\u3092\u51e6\u7406\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u5b66\u751f\u306e\u8a66\u9a13\u306e\u70b9\u6570\u3084\u3001\u60a3\u8005\u306e\u8840\u5727\u6e2c\u5b9a\u5024\u306a\u3069\u3067\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u3092\u52b9\u7387\u7684\u306b\u6271\u3046\u305f\u3081\u306b\u3001\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u73fe\u3057\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u30c7\u30fc\u30bf\u30d9\u30af\u30c8\u30eb \\(n\\)\u500b\u306e\u89b3\u6e2c\u5024 \\(x_1, x_2, \\ldots, x_n\\) \u3092\u6301\u30641\u6b21\u5143\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u3001\\(n\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\\) \u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u30015\u4eba\u306e\u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf[36.5, 36.8, 37.2, 36.4, 36.9]\u2103\u306f\u3001\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\uff1a</p> \\[\\mathbf{x} = \\begin{pmatrix} 36.5 \\\\ 36.8 \\\\ 37.2 \\\\ 36.4 \\\\ 36.9 \\end{pmatrix}\\]"},{"location":"lectures/LA/07-vector-and-matrix/#32-mathbf1","title":"3.2 \u30d9\u30af\u30c8\u30eb \\(\\mathbf{1}\\) \u306e\u5b9a\u7fa9","text":"<p>\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u3084\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u969b\u306b\u91cd\u8981\u3068\u306a\u308b\u7279\u5225\u306a\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u3001\u3059\u3079\u3066\u306e\u8981\u7d20\u304c1\u3067\u3042\u308b\u30d9\u30af\u30c8\u30eb \\(\\mathbf{1}\\) \u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u5168\u3066\u306e\u8981\u7d20\u304c1\u306e\u30d9\u30af\u30c8\u30eb \\(n\\)\u6b21\u5143\u306e\u300c1\u30d9\u30af\u30c8\u30eb\u300d\\(\\mathbf{1}_n\\)\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[\\mathbf{1}_n = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\\] <p>\u4f8b\u3048\u3070\u30013\u6b21\u5143\u306e\u5834\u5408\u306f \\(\\mathbf{1}_3 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u3053\u306e\u30d9\u30af\u30c8\u30eb\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u3084\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u969b\u306b\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/07-vector-and-matrix/#41","title":"4.1 \u30c7\u30fc\u30bf\u306e\u5e73\u5747","text":"<p>\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u5024\u306f\u3001\u3059\u3079\u3066\u306e\u89b3\u6e2c\u5024\u306e\u548c\u3092\u30c7\u30fc\u30bf\u6570\u3067\u5272\u3063\u305f\u5024\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u5024 \u30c7\u30fc\u30bf\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\\) \u306e\u5e73\u5747\u5024 \\(\\bar{x}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\] <p>\u3053\u308c\u3092\u30d9\u30af\u30c8\u30eb\u306e\u8868\u8a18\u3067\u8868\u73fe\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[\\bar{x} = \\frac{1}{n} \\mathbf{1}_n^T \\mathbf{x}\\] <p>\u3053\u3053\u3067\u3001\\(\\mathbf{1}_n^T\\) \u306f\u5168\u3066\u306e\u8981\u7d20\u304c1\u306e\u30d9\u30af\u30c8\u30eb\u306e\u8ee2\u7f6e\u3067\u3042\u308a\u3001\\(\\mathbf{1}_n^T \\mathbf{x}\\) \u306f\u30d9\u30af\u30c8\u30eb \\(\\mathbf{1}_n\\) \u3068 \\(\\mathbf{x}\\) \u306e\u5185\u7a4d\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u4f8b\u984c4.1.1\uff1a 5\u4eba\u306e\u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf \\(\\mathbf{x} = \\begin{pmatrix} 36.5 \\\\ 36.8 \\\\ 37.2 \\\\ 36.4 \\\\ 36.9 \\end{pmatrix}\\) \u306e\u5e73\u5747\u5024\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\u89e3\u7b54\uff1a \\(\\(\\bar{x} = \\frac{1}{5} \\mathbf{1}_5^T \\mathbf{x} = \\frac{1}{5} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 36.5 \\\\ 36.8 \\\\ 37.2 \\\\ 36.4 \\\\ 36.9 \\end{pmatrix} = \\frac{1}{5}(36.5 + 36.8 + 37.2 + 36.4 + 36.9) = \\frac{183.8}{5} = 36.76\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u3053\u306e5\u4eba\u306e\u5b66\u751f\u306e\u5e73\u5747\u4f53\u6e29\u306f36.76\u2103\u3067\u3059\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/#42","title":"4.2 \u30c7\u30fc\u30bf\u306e\u504f\u5dee","text":"<p>\u30c7\u30fc\u30bf\u306e\u504f\u5dee\u306f\u3001\u5404\u30c7\u30fc\u30bf\u70b9\u3068\u5e73\u5747\u5024\u3068\u306e\u5dee\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u30c7\u30fc\u30bf\u306e\u504f\u5dee \u30c7\u30fc\u30bf \\(x_i\\) \u306e\u504f\u5dee \\(d_i\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[d_i = x_i - \\bar{x}\\] <p>\u30d9\u30af\u30c8\u30eb\u8868\u8a18\u3067\u306f\u3001\u30c7\u30fc\u30bf\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u306e\u504f\u5dee\u30d9\u30af\u30c8\u30eb \\(\\mathbf{d}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[\\mathbf{d} = \\mathbf{x} - \\bar{x}\\mathbf{1}_n\\] <p>\u3053\u3053\u3067\u6ce8\u610f\u3059\u3079\u304d\u70b9\u306f\u3001\u504f\u5dee\u306e\u548c\u306f\u5e38\u306b0\u306b\u306a\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\uff1a</p> \\[\\sum_{i=1}^{n} d_i = 0\\] <p>\u30d9\u30af\u30c8\u30eb\u8868\u8a18\u3067\u306f\uff1a</p> \\[\\mathbf{1}_n^T \\mathbf{d} = 0\\] <p>\u4f8b\u984c4.2.1\uff1a 5\u4eba\u306e\u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf \\(\\mathbf{x} = \\begin{pmatrix} 36.5 \\\\ 36.8 \\\\ 37.2 \\\\ 36.4 \\\\ 36.9 \\end{pmatrix}\\) \u306e\u504f\u5dee\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\u89e3\u7b54\uff1a \u5148\u307b\u3069\u8a08\u7b97\u3057\u305f\u5e73\u5747\u5024 \\(\\bar{x} = 36.76\\) \u3092\u7528\u3044\u3066\uff1a</p> \\[\\mathbf{d} = \\mathbf{x} - \\bar{x}\\mathbf{1}_5 = \\begin{pmatrix} 36.5 \\\\ 36.8 \\\\ 37.2 \\\\ 36.4 \\\\ 36.9 \\end{pmatrix} - 36.76 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 36.5 - 36.76 \\\\ 36.8 - 36.76 \\\\ 37.2 - 36.76 \\\\ 36.4 - 36.76 \\\\ 36.9 - 36.76 \\end{pmatrix} = \\begin{pmatrix} -0.26 \\\\ 0.04 \\\\ 0.44 \\\\ -0.36 \\\\ 0.14 \\end{pmatrix}\\] <p>\u504f\u5dee\u306e\u548c\u3092\u78ba\u8a8d\u3059\u308b\u3068\uff1a</p> \\[\\mathbf{1}_5^T \\mathbf{d} = -0.26 + 0.04 + 0.44 + (-0.36) + 0.14 = 0\\] <p>\uff08\u5c11\u6570\u8a08\u7b97\u306e\u8aa4\u5dee\u3092\u7121\u8996\u3059\u308b\u3068\uff09\u504f\u5dee\u306e\u548c\u306f0\u306b\u306a\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/#43","title":"4.3 \u30c7\u30fc\u30bf\u306e\u5206\u6563","text":"<p>\u5206\u6563\u306f\u3001\u30c7\u30fc\u30bf\u306e\u6563\u3089\u3070\u308a\u5177\u5408\u3092\u8868\u3059\u6307\u6a19\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u30c7\u30fc\u30bf\u306e\u5206\u6563 \u30c7\u30fc\u30bf\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u306e\u5206\u6563 \\(\\sigma^2\\) \u306f\u3001\u504f\u5dee\u306e\u4e8c\u4e57\u548c\u3092\u30c7\u30fc\u30bf\u6570\u3067\u5272\u3063\u305f\u5024\u3067\u3059\uff1a</p> \\[\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} d_i^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\] <p>\u30d9\u30af\u30c8\u30eb\u8868\u8a18\u3067\u306f\uff1a</p> \\[\\sigma^2 = \\frac{1}{n} \\mathbf{d}^T \\mathbf{d} = \\frac{1}{n} (\\mathbf{x} - \\bar{x}\\mathbf{1}_n)^T (\\mathbf{x} - \\bar{x}\\mathbf{1}_n)\\] <p>\u4f8b\u984c4.3.1\uff1a 5\u4eba\u306e\u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\u89e3\u7b54\uff1a \u5148\u307b\u3069\u8a08\u7b97\u3057\u305f\u504f\u5dee\u30d9\u30af\u30c8\u30eb \\(\\mathbf{d} = \\begin{pmatrix} -0.26 \\\\ 0.04 \\\\ 0.44 \\\\ -0.36 \\\\ 0.14 \\end{pmatrix}\\) \u3092\u7528\u3044\u3066\uff1a</p> \\[\\sigma^2 = \\frac{1}{5} \\mathbf{d}^T \\mathbf{d} = \\frac{1}{5} \\begin{pmatrix} -0.26 &amp; 0.04 &amp; 0.44 &amp; -0.36 &amp; 0.14 \\end{pmatrix} \\begin{pmatrix} -0.26 \\\\ 0.04 \\\\ 0.44 \\\\ -0.36 \\\\ 0.14 \\end{pmatrix}\\] \\[= \\frac{1}{5} ((-0.26)^2 + 0.04^2 + 0.44^2 + (-0.36)^2 + 0.14^2)\\] \\[= \\frac{1}{5} (0.0676 + 0.0016 + 0.1936 + 0.1296 + 0.0196)\\] \\[= \\frac{1}{5} \\cdot 0.412 = 0.0824\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\u3053\u306e5\u4eba\u306e\u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf\u306e\u5206\u6563\u306f0.0824(\u2103\u00b2)\u3067\u3059\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/#44","title":"4.4 \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u504f\u5dee","text":"<p>\u6a19\u6e96\u504f\u5dee\u306f\u3001\u5206\u6563\u306e\u5e73\u65b9\u6839\u3067\u3042\u308a\u3001\u30c7\u30fc\u30bf\u306e\u3070\u3089\u3064\u304d\u3092\u5143\u306e\u30c7\u30fc\u30bf\u3068\u540c\u3058\u5358\u4f4d\u3067\u8868\u3057\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u504f\u5dee \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u504f\u5dee \\(\\sigma\\) \u306f\u3001\u5206\u6563\u306e\u5e73\u65b9\u6839\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\] <p>\u30d9\u30af\u30c8\u30eb\u8868\u8a18\u3067\u306f\uff1a</p> \\[\\sigma = \\sqrt{\\frac{1}{n} \\mathbf{d}^T \\mathbf{d}}\\] <p>\u4f8b\u984c4.4.1\uff1a 5\u4eba\u306e\u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\u89e3\u7b54\uff1a \u5148\u307b\u3069\u8a08\u7b97\u3057\u305f\u5206\u6563 \\(\\sigma^2 = 0.0824\\) \u3092\u7528\u3044\u3066\uff1a</p> \\[\\sigma = \\sqrt{\\sigma^2} = \\sqrt{0.0824} \\approx 0.287\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\u3053\u306e5\u4eba\u306e\u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u504f\u5dee\u306f\u7d040.287\u2103\u3067\u3059\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/#45","title":"4.5 \u5206\u6563\u306e\u5225\u516c\u5f0f","text":"<p>\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u5225\u306e\u516c\u5f0f\u3068\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u3082\u306e\u304c\u3042\u308a\u307e\u3059\uff1a</p> <p>\u5b9a\u7406\uff1a\u5206\u6563\u306e\u8a08\u7b97\u516c\u5f0f \u30c7\u30fc\u30bf\u306e\u5206\u6563\u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u3082\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a</p> \\[\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\bar{x}^2 = \\frac{1}{n} \\mathbf{x}^T \\mathbf{x} - \\bar{x}^2\\] <p>\u8a3c\u660e\uff1a </p> \\[ \\begin{align} \\sigma^2 &amp;= \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} (x_i^2 - 2x_i\\bar{x} + \\bar{x}^2)\\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\frac{2\\bar{x}}{n} \\sum_{i=1}^{n} x_i + \\frac{1}{n} \\sum_{i=1}^{n} \\bar{x}^2\\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\frac{2\\bar{x}}{n} \\cdot n\\bar{x} + \\frac{1}{n} \\cdot n\\bar{x}^2\\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - 2\\bar{x}^2 + \\bar{x}^2\\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\bar{x}^2\\\\ &amp;= \\frac{1}{n} \\mathbf{x}^T \\mathbf{x} - \\bar{x}^2 \\end{align} \\] <p>\u4f8b\u984c4.5.1\uff1a 5\u4eba\u306e\u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u5225\u516c\u5f0f\u3092\u7528\u3044\u3066\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\u89e3\u7b54\uff1a \\(\\mathbf{x} = \\begin{pmatrix} 36.5 \\\\ 36.8 \\\\ 37.2 \\\\ 36.4 \\\\ 36.9 \\end{pmatrix}\\)\u3001\\(\\bar{x} = 36.76\\) \u3092\u7528\u3044\u3066\uff1a</p> \\[\\sigma^2 = \\frac{1}{5} \\mathbf{x}^T \\mathbf{x} - \\bar{x}^2\\] \\[= \\frac{1}{5} \\begin{pmatrix} 36.5 &amp; 36.8 &amp; 37.2 &amp; 36.4 &amp; 36.9 \\end{pmatrix} \\begin{pmatrix} 36.5 \\\\ 36.8 \\\\ 37.2 \\\\ 36.4 \\\\ 36.9 \\end{pmatrix} - 36.76^2\\] \\[= \\frac{1}{5} (36.5^2 + 36.8^2 + 37.2^2 + 36.4^2 + 36.9^2) - 36.76^2\\] \\[= \\frac{1}{5} (1332.25 + 1354.24 + 1383.84 + 1324.96 + 1361.61) - 36.76^2\\] \\[= \\frac{1}{5} \\cdot 6756.9 - 1351.3\\] \\[= 1351.38 - 1351.3 = 0.08\\] <p>\uff08\u5c11\u6570\u8a08\u7b97\u306e\u8aa4\u5dee\u3092\u8003\u616e\u3059\u308b\u3068\uff09\u5148\u307b\u3069\u306e\u8a08\u7b97\u7d50\u679c\u3068\u307b\u307c\u4e00\u81f4\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u3053\u3053\u3067\u306f\u3001Google Colaboratory\u74b0\u5883\u3067\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u5206\u6790\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf\ntemperature_data = np.array([36.5, 36.8, 37.2, 36.4, 36.9])\n\n# \u30c7\u30fc\u30bf\u3092\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u793a\nprint(\"\u30c7\u30fc\u30bf\u30d9\u30af\u30c8\u30eb:\")\nprint(temperature_data.reshape(-1, 1))  # column vector\u3068\u3057\u3066\u8868\u793a\n\n# \u5e73\u5747\u306e\u8a08\u7b97\nn = len(temperature_data)\none_vector = np.ones(n)\nmean = (1/n) * np.dot(one_vector, temperature_data)\nprint(f\"\\n\u5e73\u5747\u5024\uff08\u30d9\u30af\u30c8\u30eb\u8a08\u7b97\uff09: {mean:.4f}\u2103\")\nprint(f\"\u5e73\u5747\u5024\uff08numpy\u95a2\u6570\uff09: {np.mean(temperature_data):.4f}\u2103\")\n\n# \u504f\u5dee\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\ndeviation = temperature_data - mean\nprint(\"\\n\u504f\u5dee\u30d9\u30af\u30c8\u30eb:\")\nprint(deviation.reshape(-1, 1))\n\n# \u504f\u5dee\u306e\u548c\u3092\u78ba\u8a8d\nprint(f\"\\n\u504f\u5dee\u306e\u548c: {np.sum(deviation):.10f}\")  # 0\u306b\u975e\u5e38\u306b\u8fd1\u3044\u5024\uff08\u6d6e\u52d5\u5c0f\u6570\u70b9\u8aa4\u5dee\uff09\n\n# \u5206\u6563\u306e\u8a08\u7b97\uff08\u504f\u5dee\u3092\u4f7f\u7528\uff09\nvariance1 = (1/n) * np.dot(deviation, deviation)\nprint(f\"\\n\u5206\u6563\uff08\u504f\u5dee\u304b\u3089\u8a08\u7b97\uff09: {variance1:.4f}(\u2103\u00b2)\")\n\n# \u5206\u6563\u306e\u8a08\u7b97\uff08\u5225\u516c\u5f0f\uff09\nvariance2 = (1/n) * np.dot(temperature_data, temperature_data) - mean**2\nprint(f\"\u5206\u6563\uff08\u5225\u516c\u5f0f\uff09: {variance2:.4f}(\u2103\u00b2)\")\nprint(f\"\u5206\u6563\uff08numpy\u95a2\u6570\uff09: {np.var(temperature_data, ddof=0):.4f}(\u2103\u00b2)\")\n\n# \u6a19\u6e96\u504f\u5dee\u306e\u8a08\u7b97\nstd_dev = np.sqrt(variance1)\nprint(f\"\\n\u6a19\u6e96\u504f\u5dee: {std_dev:.4f}\u2103\")\nprint(f\"\u6a19\u6e96\u504f\u5dee\uff08numpy\u95a2\u6570\uff09: {np.std(temperature_data, ddof=0):.4f}\u2103\")\n\n# \u4f53\u6e29\u30c7\u30fc\u30bf\u3068\u5e73\u5747\u5024\u3001\u6a19\u6e96\u504f\u5dee\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, n+1), temperature_data, color='skyblue', alpha=0.7)\nplt.axhline(y=mean, color='red', linestyle='-', label=f'\u5e73\u5747: {mean:.2f}\u2103')\nplt.axhline(y=mean+std_dev, color='green', linestyle='--', label=f'\u5e73\u5747+\u6a19\u6e96\u504f\u5dee: {mean+std_dev:.2f}\u2103')\nplt.axhline(y=mean-std_dev, color='green', linestyle='--', label=f'\u5e73\u5747-\u6a19\u6e96\u504f\u5dee: {mean-std_dev:.2f}\u2103')\nplt.xlabel('\u5b66\u751f')\nplt.ylabel('\u4f53\u6e29 (\u2103)')\nplt.title('5\u4eba\u306e\u5b66\u751f\u306e\u4f53\u6e29\u30c7\u30fc\u30bf')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# \u504f\u5dee\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, n+1), deviation, color='lightgreen', alpha=0.7)\nplt.axhline(y=0, color='black', linestyle='-')\nplt.xlabel('\u5b66\u751f')\nplt.ylabel('\u504f\u5dee (\u2103)')\nplt.title('\u4f53\u6e29\u30c7\u30fc\u30bf\u306e\u504f\u5dee')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/07-vector-and-matrix/#51","title":"5.1 \u5b9f\u884c\u7d50\u679c\u306e\u4f8b","text":"<p>\u4e0a\u8a18\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u5f97\u3089\u308c\u307e\u3059\uff1a</p> <pre><code>\u30c7\u30fc\u30bf\u30d9\u30af\u30c8\u30eb:\n[[36.5]\n [36.8]\n [37.2]\n [36.4]\n [36.9]]\n\n\u5e73\u5747\u5024\uff08\u30d9\u30af\u30c8\u30eb\u8a08\u7b97\uff09: 36.7600\u2103\n\u5e73\u5747\u5024\uff08numpy\u95a2\u6570\uff09: 36.7600\u2103\n\n\u504f\u5dee\u30d9\u30af\u30c8\u30eb:\n[[-0.26]\n [ 0.04]\n [ 0.44]\n [-0.36]\n [ 0.14]]\n\n\u504f\u5dee\u306e\u548c: 0.0000000000\n\n\u5206\u6563\uff08\u504f\u5dee\u304b\u3089\u8a08\u7b97\uff09: 0.0824(\u2103\u00b2)\n\u5206\u6563\uff08\u5225\u516c\u5f0f\uff09: 0.0824(\u2103\u00b2)\n\u5206\u6563\uff08numpy\u95a2\u6570\uff09: 0.0824(\u2103\u00b2)\n\n\u6a19\u6e96\u504f\u5dee: 0.2870\u2103\n\u6a19\u6e96\u504f\u5dee\uff08numpy\u95a2\u6570\uff09: 0.2870\u2103\n</code></pre> <p>\u307e\u305f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u53ef\u8996\u5316\u30b0\u30e9\u30d5\u304c\u751f\u6210\u3055\u308c\u307e\u3059\uff1a 1. \u4f53\u6e29\u30c7\u30fc\u30bf\u306e\u30b0\u30e9\u30d5\uff08\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u306e\u7bc4\u56f2\u3092\u793a\u3059\uff09 2. \u504f\u5dee\u306e\u30b0\u30e9\u30d5\uff080\u3092\u4e2d\u5fc3\u3068\u3057\u305f\u3070\u3089\u3064\u304d\u3092\u793a\u3059\uff09</p>"},{"location":"lectures/LA/07-vector-and-matrix/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/07-vector-and-matrix/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c1 \u4ee5\u4e0b\u306e7\u4eba\u306e\u60a3\u8005\u306e\u53ce\u7e2e\u671f\u8840\u5727\u30c7\u30fc\u30bf\uff08\u5358\u4f4d\uff1ammHg\uff09\u3092\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8003\u3048\u3001\u5e73\u5747\u3001\u504f\u5dee\u3001\u5206\u6563\u3001\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u305b\u3088\u3002 \\(\\mathbf{x} = \\begin{pmatrix} 120 \\\\ 135 \\\\ 115 \\\\ 122 \\\\ 128 \\\\ 130 \\\\ 118 \\end{pmatrix}\\)</p> <p>\u554f\u984c2 \u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix}\\) \u3068 \\(\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\u4ee5\u4e0b\u3092\u8a08\u7b97\u305b\u3088\u3002 a) \\(\\mathbf{a}\\) \u306e\u5e73\u5747 b) \\(\\mathbf{a}\\) \u306e\u504f\u5dee\u30d9\u30af\u30c8\u30eb c) \\(\\mathbf{a}\\) \u306e\u5206\u6563</p> <p>\u554f\u984c3 \u5206\u6563\u306e\u516c\u5f0f \\(\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\bar{x}^2\\) \u3092\u7528\u3044\u3066\u3001\u30c7\u30fc\u30bf \\(\\{3, 5, 7, 9, 11\\}\\) \u306e\u5206\u6563\u3092\u8a08\u7b97\u305b\u3088\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c4 \u3042\u308b\u30af\u30e9\u30b9\u306e10\u4eba\u306e\u5b66\u751f\u306e\u8eab\u9577\u30c7\u30fc\u30bf\uff08\u5358\u4f4d\uff1acm\uff09\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u3066\u3044\u308b\u3002 \\(\\mathbf{h} = \\begin{pmatrix} 168 \\\\ 172 \\\\ 165 \\\\ 170 \\\\ 175 \\\\ 167 \\\\ 169 \\\\ 173 \\\\ 168 \\\\ 171 \\end{pmatrix}\\)</p> <p>a) \u5e73\u5747\u8eab\u9577\u3092\u8a08\u7b97\u305b\u3088\u3002 b) \u504f\u5dee\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3088\u3002 c) \u5206\u6563\u3068\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u305b\u3088\u3002 d) \u5404\u5b66\u751f\u306e\u8eab\u9577\u3092\u6a19\u6e96\u5316\uff08\\(z\\)\u30b9\u30b3\u30a2\u5316\uff09\u305b\u3088\u3002\u6a19\u6e96\u5316\u3055\u308c\u305f\u5024 \\(z_i\\) \u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u5b9a\u7fa9\u3055\u308c\u308b\uff1a    \\(z_i = \\frac{x_i - \\bar{x}}{\\sigma}\\) e) \u6a19\u6e96\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u3068\u5206\u6563\u3092\u8a08\u7b97\u3057\u3001\u305d\u308c\u305e\u308c0\u30681\u306b\u306a\u308b\u3053\u3068\u3092\u78ba\u8a8d\u305b\u3088\u3002</p> <p>\u554f\u984c5\uff08Health Data Science\u95a2\u9023\uff09 \u3042\u308b\u5065\u5eb7\u8abf\u67fb\u3067\u300110\u4eba\u306e\u53c2\u52a0\u8005\u306e\u5b89\u9759\u6642\u5fc3\u62cd\u6570\uff08bpm\uff09\u30681\u65e5\u306e\u6b69\u6570\uff08\u5343\u6b69\u5358\u4f4d\uff09\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6e2c\u5b9a\u3055\u308c\u305f\uff1a</p> <p>\u5fc3\u62cd\u6570: \\(\\mathbf{hr} = \\begin{pmatrix} 68 \\\\ 72 \\\\ 65 \\\\ 70 \\\\ 75 \\\\ 62 \\\\ 69 \\\\ 73 \\\\ 67 \\\\ 71 \\end{pmatrix}\\)</p> <p>\u6b69\u6570: \\(\\mathbf{steps} = \\begin{pmatrix} 8.2 \\\\ 6.5 \\\\ 7.8 \\\\ 9.3 \\\\ 5.9 \\\\ 10.2 \\\\ 7.5 \\\\ 6.8 \\\\ 8.9 \\\\ 7.2 \\end{pmatrix}\\)</p> <p>a) \u4e21\u65b9\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u3064\u3044\u3066\u3001\u5e73\u5747\u3001\u5206\u6563\u3001\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u305b\u3088\u3002 b) \u6b69\u6570\u30c7\u30fc\u30bf\u3092\u5e73\u57470\u3001\u5206\u65631\u306b\u306a\u308b\u3088\u3046\u306b\u6a19\u6e96\u5316\u305b\u3088\u3002 c) Python\uff08NumPy\uff09\u3092\u7528\u3044\u3066\u3001\u5fc3\u62cd\u6570\u3068\u6b69\u6570\u306e\u9593\u306e\u5171\u5206\u6563\u3092\u8a08\u7b97\u305b\u3088\u3002\u5171\u5206\u6563\u306e\u516c\u5f0f\u306f\u4ee5\u4e0b\u306e\u3068\u304a\u308a\uff1a    \\(\\text{cov}(\\mathbf{hr}, \\mathbf{steps}) = \\frac{1}{n} \\sum_{i=1}^{n} (hr_i - \\overline{hr})(steps_i - \\overline{steps})\\) d) \u5fc3\u62cd\u6570\u3068\u6b69\u6570\u306e\u9593\u306b\u4f55\u304b\u95a2\u4fc2\u6027\u304c\u898b\u3089\u308c\u308b\u304b\u8003\u5bdf\u305b\u3088\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":"<p>Q1: \u30c7\u30fc\u30bf\u306e\u5e73\u5747\u3068\u5206\u6563\u306f\u4f55\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u304b\uff1f</p> <p>A1: \u30c7\u30fc\u30bf\u306e\u5e73\u5747\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u300c\u4e2d\u5fc3\u7684\u306a\u5024\u300d\u3084\u300c\u4ee3\u8868\u5024\u300d\u3092\u8868\u3057\u307e\u3059\u3002\u4e00\u65b9\u3001\u5206\u6563\u306f\u30c7\u30fc\u30bf\u306e\u6563\u3089\u3070\u308a\u5177\u5408\u3092\u8868\u3059\u6307\u6a19\u3067\u3001\u5024\u304c\u5927\u304d\u3044\u307b\u3069\u30c7\u30fc\u30bf\u304c\u5e73\u5747\u304b\u3089\u96e2\u308c\u3066\u5e83\u304c\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u6a19\u6e96\u504f\u5dee\u306f\u5206\u6563\u306e\u5e73\u65b9\u6839\u3067\u3001\u5143\u306e\u30c7\u30fc\u30bf\u3068\u540c\u3058\u5358\u4f4d\u3067\u6563\u3089\u3070\u308a\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>Q2: \u306a\u305c\u30c7\u30fc\u30bf\u306e\u504f\u5dee\u306e\u548c\u306f\u5e38\u306b0\u306b\u306a\u308b\u306e\u3067\u3059\u304b\uff1f</p> <p>A2: \u3053\u308c\u306f\u5e73\u5747\u306e\u5b9a\u7fa9\u304b\u3089\u5c0e\u304b\u308c\u308b\u6027\u8cea\u3067\u3059\u3002\u5e73\u5747\u306f \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\) \u3068\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001\\(n\\bar{x} = \\sum_{i=1}^{n} x_i\\) \u3067\u3059\u3002\u504f\u5dee\u306e\u548c\u3092\u8a08\u7b97\u3059\u308b\u3068\uff1a</p> <p>\\(\\sum_{i=1}^{n} (x_i - \\bar{x}) = \\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} \\bar{x} = \\sum_{i=1}^{n} x_i - n\\bar{x} = \\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} x_i = 0\\)</p> <p>Q3: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u3001\u306a\u305c\u30d9\u30af\u30c8\u30eb\u3068\u3057\u30661\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u6271\u3046\u3053\u3068\u304c\u91cd\u8981\u306a\u306e\u3067\u3059\u304b\uff1f</p> <p>A3: \u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u30c7\u30fc\u30bf\u3092\u6271\u3046\u3053\u3068\u3067\u3001\u7dda\u5f62\u4ee3\u6570\u306e\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u3092\u6d3b\u7528\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u5206\u6790\u624b\u6cd5\uff08\u4e3b\u6210\u5206\u5206\u6790\u3001\u7dda\u5f62\u56de\u5e30\u306a\u3069\uff09\u3092\u7d71\u4e00\u7684\u306a\u67a0\u7d44\u307f\u3067\u7406\u89e3\u30fb\u5b9f\u88c5\u3067\u304d\u307e\u3059\u3002\u307e\u305f\u3001\u5927\u91cf\u306e\u30c7\u30fc\u30bf\u3092\u52b9\u7387\u7684\u306b\u51e6\u7406\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>Q4: \u504f\u5dee\u3068\u6a19\u6e96\u504f\u5dee\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A4: \u504f\u5dee\u306f\u5404\u30c7\u30fc\u30bf\u70b9\u3068\u5e73\u5747\u5024\u306e\u5dee\u3092\u8868\u3057\u3001\u6b63\u8ca0\u306e\u5024\u3092\u53d6\u308a\u307e\u3059\u3002\u6a19\u6e96\u504f\u5dee\u306f\u504f\u5dee\u306e\u4e8c\u4e57\u306e\u5e73\u5747\u306e\u5e73\u65b9\u6839\u3067\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5168\u4f53\u306e\u3070\u3089\u3064\u304d\u3092\u8868\u3059\u6b63\u306e\u5024\u3067\u3059\u3002\u504f\u5dee\u306f\u5404\u30c7\u30fc\u30bf\u70b9\u306b\u5bfe\u3059\u308b\u5024\u3067\u3059\u304c\u3001\u6a19\u6e96\u504f\u5dee\u306f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5168\u4f53\u306b\u5bfe\u3059\u308b\u5358\u4e00\u306e\u5024\u3067\u3059\u3002</p> <p>Q5: \u5206\u6563\u306e\u8a08\u7b97\u306b\u5225\u516c\u5f0f \\(\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\bar{x}^2\\) \u3092\u4f7f\u3046\u30e1\u30ea\u30c3\u30c8\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A5: \u3053\u306e\u516c\u5f0f\u306f\u3001\u7279\u306b\u5927\u898f\u6a21\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5bfe\u3057\u3066\u8a08\u7b97\u52b9\u7387\u304c\u826f\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u30c7\u30fc\u30bf\u306e\u5408\u8a08\u3068\u4e8c\u4e57\u306e\u5408\u8a08\u3060\u3051\u3092\u4fdd\u6301\u3059\u308c\u3070\u5206\u6563\u3092\u8a08\u7b97\u3067\u304d\u308b\u305f\u3081\u3001\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u304c\u5c11\u306a\u304f\u3066\u6e08\u307f\u307e\u3059\u3002\u307e\u305f\u3001\u30aa\u30f3\u30e9\u30a4\u30f3\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff08\u30c7\u30fc\u30bf\u3092\u4e00\u5ea6\u306b\u5168\u90e8\u8aad\u307f\u8fbc\u307e\u305a\u306b\u9010\u6b21\u51e6\u7406\u3059\u308b\u65b9\u6cd5\uff09\u306b\u3082\u9069\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/07-vector-and-matrix/#8","title":"8. \u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u30011\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u73fe\u3057\u3001\u30d9\u30af\u30c8\u30eb\u306e\u6f14\u7b97\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u3001\u504f\u5dee\u3001\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3057\u305f\u3002\u4e3b\u306a\u30dd\u30a4\u30f3\u30c8\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>1\u6b21\u5143\u30c7\u30fc\u30bf\u306f \\(n\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u308b</li> <li>\u5e73\u5747\u5024\u306f \\(\\bar{x} = \\frac{1}{n} \\mathbf{1}_n^T \\mathbf{x}\\) \u3067\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u504f\u5dee\u30d9\u30af\u30c8\u30eb\u306f \\(\\mathbf{d} = \\mathbf{x} - \\bar{x}\\mathbf{1}_n\\) \u3067\u8a08\u7b97\u3067\u304d\u3001\u305d\u306e\u548c\u306f\u5e38\u306b0\u306b\u306a\u308b</li> <li>\u5206\u6563\u306f \\(\\sigma^2 = \\frac{1}{n} \\mathbf{d}^T \\mathbf{d}\\) \u307e\u305f\u306f \\(\\sigma^2 = \\frac{1}{n} \\mathbf{x}^T \\mathbf{x} - \\bar{x}^2\\) \u3067\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u6a19\u6e96\u504f\u5dee\u306f\u5206\u6563\u306e\u5e73\u65b9\u6839 \\(\\sigma = \\sqrt{\\sigma^2}\\) \u3067\u3042\u308b</li> </ol> <p>\u3053\u308c\u3089\u306e\u6982\u5ff5\u306f\u3001\u3088\u308a\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u5206\u6790\u624b\u6cd5\uff08\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u5206\u6790\u3001\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3001\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\uff09\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u6982\u5ff5\u30922\u6b21\u5143\u30c7\u30fc\u30bf\u306b\u62e1\u5f35\u3057\u3001\u884c\u5217\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u8868\u73fe\u3092\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u57fa\u790e \u7b2c8\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/08-vector-and-matrix/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c8\u56de \u30c6\u30fc\u30de: 2\u6b21\u5143\u30c7\u30fc\u30bf\u3068\u884c\u5217\u306e\u7a4d \u95a2\u9023\u9805\u76ee: \u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u3001\u76f8\u95a2\u4fc2\u6570\u3001\u884c\u5217\u8868\u73fe \u4e88\u7fd2\u5185\u5bb9: \u7b2c7\u56de\u300c1\u6b21\u5143\u30c7\u30fc\u30bf\u3068\u30d9\u30af\u30c8\u30eb\u306e\u548c\u3068\u7a4d\u300d\u306e\u5fa9\u7fd2\u3001\u7279\u306b\u5e73\u5747\u30fb\u5206\u6563\u306e\u8a08\u7b97\u65b9\u6cd5</p>"},{"location":"lectures/LA/08-vector-and-matrix/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>2\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3059\u308b</li> <li>\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u5171\u5206\u6563\u306e\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u5b9f\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u5171\u5206\u6563\u30fb\u76f8\u95a2\u4fc2\u6570\u306e\u89e3\u91c8\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>Google Colab\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u5206\u6790\u306e\u57fa\u672c\u7684\u306a\u624b\u6cd5\u3092\u8eab\u306b\u3064\u3051\u308b</li> </ol>"},{"location":"lectures/LA/08-vector-and-matrix/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/08-vector-and-matrix/#31-2","title":"3.1 2\u6b21\u5143\u30c7\u30fc\u30bf\u3068\u306f","text":"<p>2\u6b21\u5143\u30c7\u30fc\u30bf\u3068\u306f\u3001\u5404\u89b3\u6e2c\u5bfe\u8c61\u306b\u5bfe\u3057\u30662\u3064\u306e\u5909\u6570\u306e\u5024\u304c\u8a18\u9332\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u306e\u3053\u3068\u3067\u3059\u3002\u4f8b\u3048\u3070\uff1a</p> <ul> <li>\u5b66\u751f\u306e\u8eab\u9577\u3068\u4f53\u91cd</li> <li>\u90fd\u5e02\u306e\u6c17\u6e29\u3068\u964d\u6c34\u91cf</li> <li>\u5546\u54c1\u306e\u4fa1\u683c\u3068\u8ca9\u58f2\u6570</li> </ul> <p>2\u6b21\u5143\u30c7\u30fc\u30bf\u306f\u3001\\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\) \u306e\u3088\u3046\u306a\u5f62\u5f0f\u3067\u8868\u3055\u308c\u307e\u3059\u3002\u3053\u3053\u3067: - \\(x_i\\): \\(i\\)\u756a\u76ee\u306e\u30c7\u30fc\u30bf\u306e\u7b2c1\u5909\u6570\u306e\u5024 - \\(y_i\\): \\(i\\)\u756a\u76ee\u306e\u30c7\u30fc\u30bf\u306e\u7b2c2\u5909\u6570\u306e\u5024 - \\(n\\): \u30c7\u30fc\u30bf\u6570</p>"},{"location":"lectures/LA/08-vector-and-matrix/#32","title":"3.2 \u5171\u5206\u6563\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: 2\u3064\u306e\u5909\u6570 \\(X\\) \u3068 \\(Y\\) \u306e\u5171\u5206\u6563 \\(\\mathrm{Cov}(X,Y)\\) \u306f\u3001\u5404\u5909\u6570\u306e\u5e73\u5747\u304b\u3089\u306e\u504f\u5dee\u306e\u7a4d\u306e\u5e73\u5747\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002</p> \\[\\mathrm{Cov}(X,Y) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\\] <p>\u3053\u3053\u3067\u3001\\(\\bar{x}\\) \u3068 \\(\\bar{y}\\) \u306f\u305d\u308c\u305e\u308c\u5909\u6570 \\(X\\) \u3068 \\(Y\\) \u306e\u5e73\u5747\u5024\u3067\u3059\u3002</p> <p>\u5171\u5206\u6563\u306f2\u3064\u306e\u5909\u6570\u306e\u95a2\u9023\u6027\u3092\u6e2c\u308b\u6307\u6a19\u3067\u3042\u308a\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u6027\u8cea\u3092\u6301\u3061\u307e\u3059:</p> <ul> <li>\\(\\mathrm{Cov}(X,Y) &gt; 0\\) : \\(X\\) \u304c\u5927\u304d\u304f\u306a\u308b\u3068 \\(Y\\) \u3082\u5927\u304d\u304f\u306a\u308b\u50be\u5411\uff08\u6b63\u306e\u76f8\u95a2\uff09</li> <li>\\(\\mathrm{Cov}(X,Y) &lt; 0\\) : \\(X\\) \u304c\u5927\u304d\u304f\u306a\u308b\u3068 \\(Y\\) \u306f\u5c0f\u3055\u304f\u306a\u308b\u50be\u5411\uff08\u8ca0\u306e\u76f8\u95a2\uff09</li> <li>\\(\\mathrm{Cov}(X,Y) \\approx 0\\) : \\(X\\) \u3068 \\(Y\\) \u306b\u660e\u78ba\u306a\u95a2\u9023\u304c\u306a\u3044\uff08\u7121\u76f8\u95a2\uff09</li> <li>\\(\\mathrm{Cov}(X,X) = \\mathrm{Var}(X)\\) : \u5909\u6570\u81ea\u8eab\u3068\u306e\u5171\u5206\u6563\u306f\u5206\u6563\u306b\u7b49\u3057\u3044</li> </ul>"},{"location":"lectures/LA/08-vector-and-matrix/#33","title":"3.3 \u76f8\u95a2\u4fc2\u6570\u306e\u5b9a\u7fa9","text":"<p>\u5171\u5206\u6563\u306f\u5909\u6570\u306e\u30b9\u30b1\u30fc\u30eb\u306b\u4f9d\u5b58\u3059\u308b\u305f\u3081\u3001\u5909\u6570\u9593\u306e\u95a2\u9023\u6027\u3092\u6a19\u6e96\u5316\u3057\u305f\u6307\u6a19\u3068\u3057\u3066\u76f8\u95a2\u4fc2\u6570\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9: 2\u3064\u306e\u5909\u6570 \\(X\\) \u3068 \\(Y\\) \u306e\u76f8\u95a2\u4fc2\u6570 \\(\\rho_{XY}\\) \u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002</p> \\[\\rho_{XY} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\] <p>\u3053\u3053\u3067\u3001\\(\\sigma_X\\) \u3068 \\(\\sigma_Y\\) \u306f\u305d\u308c\u305e\u308c\u5909\u6570 \\(X\\) \u3068 \\(Y\\) \u306e\u6a19\u6e96\u504f\u5dee\u3067\u3059\u3002</p> <p>\u76f8\u95a2\u4fc2\u6570\u306e\u6027\u8cea:</p> <ul> <li>\\(-1 \\leq \\rho_{XY} \\leq 1\\)</li> <li>\\(\\rho_{XY} = 1\\) : \u5b8c\u5168\u306a\u6b63\u306e\u76f8\u95a2\uff08\u76f4\u7dda\u7684\u306a\u6bd4\u4f8b\u95a2\u4fc2\uff09</li> <li>\\(\\rho_{XY} = -1\\) : \u5b8c\u5168\u306a\u8ca0\u306e\u76f8\u95a2\uff08\u76f4\u7dda\u7684\u306a\u53cd\u6bd4\u4f8b\u95a2\u4fc2\uff09</li> <li>\\(\\rho_{XY} = 0\\) : \u7121\u76f8\u95a2\uff08\u7dda\u5f62\u306e\u95a2\u9023\u6027\u304c\u306a\u3044\uff09</li> </ul>"},{"location":"lectures/LA/08-vector-and-matrix/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/08-vector-and-matrix/#41","title":"4.1 \u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306b\u3088\u308b\u30c7\u30fc\u30bf\u8868\u73fe","text":"<p>2\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3067\u8868\u73fe\u3057\u307e\u3057\u3087\u3046\u3002</p> <p>\u307e\u305a\u3001\u30c7\u30fc\u30bf\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u30d9\u30af\u30c8\u30eb\u3067\u8868\u3057\u307e\u3059: - \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T\\) : \u7b2c1\u5909\u6570\u306e\u5024\u3092\u96c6\u3081\u305f\u30d9\u30af\u30c8\u30eb - \\(\\mathbf{y} = [y_1, y_2, \\ldots, y_n]^T\\) : \u7b2c2\u5909\u6570\u306e\u5024\u3092\u96c6\u3081\u305f\u30d9\u30af\u30c8\u30eb</p> <p>\u307e\u305f\u306f\u3001\u30c7\u30fc\u30bf\u884c\u5217 \\(\\mathbf{X}\\) \u3068\u3057\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3059\u3053\u3068\u3082\u3067\u304d\u307e\u3059:</p> \\[\\mathbf{X} =  \\begin{bmatrix}  x_1 &amp; y_1 \\\\ x_2 &amp; y_2 \\\\ \\vdots &amp; \\vdots \\\\ x_n &amp; y_n \\end{bmatrix}\\]"},{"location":"lectures/LA/08-vector-and-matrix/#42","title":"4.2 \u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u5171\u5206\u6563\u306e\u8a08\u7b97","text":"<p>\u30d9\u30af\u30c8\u30eb\u8868\u8a18\u3092\u7528\u3044\u305f\u5171\u5206\u6563\u306e\u8a08\u7b97\u65b9\u6cd5\u3092\u793a\u3057\u307e\u3059\u3002</p> <p>\u307e\u305a\u3001\u5404\u5909\u6570\u306e\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u307e\u3059: - \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\) : \u7b2c1\u5909\u6570\u306e\u5e73\u5747 - \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i\\) : \u7b2c2\u5909\u6570\u306e\u5e73\u5747</p> <p>\u504f\u5dee\u30d9\u30af\u30c8\u30eb\u3092\u5b9a\u7fa9\u3057\u307e\u3059: - \\(\\mathbf{x}_{dev} = \\mathbf{x} - \\bar{x}\\mathbf{1}\\) : \u7b2c1\u5909\u6570\u306e\u504f\u5dee\u30d9\u30af\u30c8\u30eb - \\(\\mathbf{y}_{dev} = \\mathbf{y} - \\bar{y}\\mathbf{1}\\) : \u7b2c2\u5909\u6570\u306e\u504f\u5dee\u30d9\u30af\u30c8\u30eb</p> <p>\u3053\u3053\u3067\u3001\\(\\mathbf{1}\\) \u306f\u5168\u3066\u306e\u8981\u7d20\u304c1\u3067\u3042\u308b\u9577\u3055 \\(n\\) \u306e\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p> <p>\u3053\u306e\u3068\u304d\u3001\u5171\u5206\u6563\u306f\u6b21\u306e\u3088\u3046\u306b\u8a08\u7b97\u3055\u308c\u307e\u3059:</p> \\[\\mathrm{Cov}(X,Y) = \\frac{1}{n}\\mathbf{x}_{dev}^T\\mathbf{y}_{dev}\\]"},{"location":"lectures/LA/08-vector-and-matrix/#43","title":"4.3 \u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97","text":"<p>\u76f8\u95a2\u4fc2\u6570\u306f\u3001\u5171\u5206\u6563\u3092\u5404\u5909\u6570\u306e\u6a19\u6e96\u504f\u5dee\u3067\u9664\u3059\u308b\u3053\u3068\u3067\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002</p> <p>\u5404\u5909\u6570\u306e\u5206\u6563\u3092\u8a08\u7b97\u3057\u307e\u3059: - \\(\\mathrm{Var}(X) = \\frac{1}{n}\\mathbf{x}_{dev}^T\\mathbf{x}_{dev}\\) : \u7b2c1\u5909\u6570\u306e\u5206\u6563 - \\(\\mathrm{Var}(Y) = \\frac{1}{n}\\mathbf{y}_{dev}^T\\mathbf{y}_{dev}\\) : \u7b2c2\u5909\u6570\u306e\u5206\u6563</p> <p>\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3057\u307e\u3059: - \\(\\sigma_X = \\sqrt{\\mathrm{Var}(X)}\\) : \u7b2c1\u5909\u6570\u306e\u6a19\u6e96\u504f\u5dee - \\(\\sigma_Y = \\sqrt{\\mathrm{Var}(Y)}\\) : \u7b2c2\u5909\u6570\u306e\u6a19\u6e96\u504f\u5dee</p> <p>\u76f8\u95a2\u4fc2\u6570\u306f\u6b21\u306e\u5f0f\u3067\u8a08\u7b97\u3055\u308c\u307e\u3059:</p> \\[\\rho_{XY} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{\\mathbf{x}_{dev}^T\\mathbf{y}_{dev}}{\\sqrt{(\\mathbf{x}_{dev}^T\\mathbf{x}_{dev})(\\mathbf{y}_{dev}^T\\mathbf{y}_{dev})}}\\]"},{"location":"lectures/LA/08-vector-and-matrix/#44","title":"4.4 \u5171\u5206\u6563\u884c\u5217","text":"<p>\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u3092\u6271\u3046\u969b\u306b\u306f\u3001\u5168\u3066\u306e\u5909\u6570\u306e\u30da\u30a2\u306b\u5bfe\u3059\u308b\u5171\u5206\u6563\u3092\u542b\u3080\u5171\u5206\u6563\u884c\u5217\u304c\u91cd\u8981\u306b\u306a\u308a\u307e\u3059\u30022\u5909\u6570 \\(X\\) \u3068 \\(Y\\) \u306e\u5834\u5408\u3001\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{\\Sigma}\\) \u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059:</p> \\[\\mathbf{\\Sigma} =  \\begin{bmatrix}  \\mathrm{Var}(X) &amp; \\mathrm{Cov}(X,Y) \\\\ \\mathrm{Cov}(X,Y) &amp; \\mathrm{Var}(Y) \\end{bmatrix}\\] <p>\u4e00\u822c\u306b\u3001\u30c7\u30fc\u30bf\u884c\u5217 \\(\\mathbf{X}\\) \u306e\u5404\u5217\u304c\u5909\u6570\u3092\u8868\u3059\u5834\u5408\u3001\u5171\u5206\u6563\u884c\u5217\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059:</p> \\[\\mathbf{\\Sigma} = \\frac{1}{n}(\\mathbf{X} - \\mathbf{1}\\boldsymbol{\\mu}^T)^T(\\mathbf{X} - \\mathbf{1}\\boldsymbol{\\mu}^T)\\] <p>\u3053\u3053\u3067\u3001\\(\\boldsymbol{\\mu}\\) \u306f\u5404\u5909\u6570\u306e\u5e73\u5747\u5024\u3092\u542b\u3080\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#5","title":"5. \u8a08\u7b97\u4f8b\u3068\u5b9f\u88c5","text":""},{"location":"lectures/LA/08-vector-and-matrix/#51","title":"5.1 \u6570\u5024\u4f8b: \u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97","text":"<p>\u4ee5\u4e0b\u306e5\u3064\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306b\u5bfe\u3057\u3066\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046:</p> \u30c7\u30fc\u30bf\u756a\u53f7 \\(X\\) (\u8eab\u9577 cm) \\(Y\\) (\u4f53\u91cd kg) 1 160 55 2 170 65 3 180 75 4 165 60 5 175 70 <p>\u30b9\u30c6\u30c3\u30d71: \u5404\u5909\u6570\u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 - \\(\\bar{x} = \\frac{160 + 170 + 180 + 165 + 175}{5} = 170\\) - \\(\\bar{y} = \\frac{55 + 65 + 75 + 60 + 70}{5} = 65\\)</p> <p>\u30b9\u30c6\u30c3\u30d72: \u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u504f\u5dee\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \u30c7\u30fc\u30bf\u756a\u53f7 \\(x_i - \\bar{x}\\) \\(y_i - \\bar{y}\\) \\((x_i - \\bar{x})(y_i - \\bar{y})\\) 1 -10 -10 100 2 0 0 0 3 10 10 100 4 -5 -5 25 5 5 5 25 <p>\u30b9\u30c6\u30c3\u30d73: \u5171\u5206\u6563\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 - \\(\\mathrm{Cov}(X,Y) = \\frac{100 + 0 + 100 + 25 + 25}{5} = \\frac{250}{5} = 50\\)</p> <p>\u30b9\u30c6\u30c3\u30d74: \u5404\u5909\u6570\u306e\u5206\u6563\u3068\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 - \\(\\mathrm{Var}(X) = \\frac{(-10)^2 + 0^2 + 10^2 + (-5)^2 + 5^2}{5} = \\frac{250}{5} = 50\\) - \\(\\mathrm{Var}(Y) = \\frac{(-10)^2 + 0^2 + 10^2 + (-5)^2 + 5^2}{5} = \\frac{250}{5} = 50\\) - \\(\\sigma_X = \\sqrt{50} = 7.07\\) - \\(\\sigma_Y = \\sqrt{50} = 7.07\\)</p> <p>\u30b9\u30c6\u30c3\u30d75: \u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 - \\(\\rho_{XY} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{50}{7.07 \\times 7.07} = \\frac{50}{50} = 1\\)</p> <p>\u3053\u306e\u4f8b\u3067\u306f\u3001\u76f8\u95a2\u4fc2\u6570\u304c1\u3067\u3042\u308a\u3001\u8eab\u9577\u3068\u4f53\u91cd\u306e\u9593\u306b\u5b8c\u5168\u306a\u6b63\u306e\u7dda\u5f62\u76f8\u95a2\u304c\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#52","title":"5.2 \u884c\u5217\u3092\u7528\u3044\u305f\u8a08\u7b97\u4f8b","text":"<p>\u540c\u3058\u30c7\u30fc\u30bf\u3092\u884c\u5217\u5f62\u5f0f\u3067\u8868\u73fe\u3057\u3001\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code>x = [160, 170, 180, 165, 175]^T\ny = [55, 65, 75, 60, 70]^T\n</code></pre> <p>\u30b9\u30c6\u30c3\u30d71: \u5404\u5909\u6570\u306e\u5e73\u5747\u5024\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 - \\(\\bar{x} = 170\\) - \\(\\bar{y} = 65\\)</p> <p>\u30b9\u30c6\u30c3\u30d72: \u504f\u5dee\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 - \\(\\mathbf{x}_{dev} = [160-170, 170-170, 180-170, 165-170, 175-170]^T = [-10, 0, 10, -5, 5]^T\\) - \\(\\mathbf{y}_{dev} = [55-65, 65-65, 75-65, 60-65, 70-65]^T = [-10, 0, 10, -5, 5]^T\\)</p> <p>\u30b9\u30c6\u30c3\u30d73: \u5171\u5206\u6563\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 - \\(\\mathrm{Cov}(X,Y) = \\frac{1}{5}\\mathbf{x}_{dev}^T\\mathbf{y}_{dev} = \\frac{1}{5}((-10) \\times (-10) + 0 \\times 0 + 10 \\times 10 + (-5) \\times (-5) + 5 \\times 5) = \\frac{250}{5} = 50\\)</p> <p>\u30b9\u30c6\u30c3\u30d74: \u5206\u6563\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 - \\(\\mathrm{Var}(X) = \\frac{1}{5}\\mathbf{x}_{dev}^T\\mathbf{x}_{dev} = \\frac{1}{5}((-10)^2 + 0^2 + 10^2 + (-5)^2 + 5^2) = \\frac{250}{5} = 50\\) - \\(\\mathrm{Var}(Y) = \\frac{1}{5}\\mathbf{y}_{dev}^T\\mathbf{y}_{dev} = \\frac{1}{5}((-10)^2 + 0^2 + 10^2 + (-5)^2 + 5^2) = \\frac{250}{5} = 50\\)</p> <p>\u30b9\u30c6\u30c3\u30d75: \u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 - \\(\\rho_{XY} = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X) \\times \\mathrm{Var}(Y)}} = \\frac{50}{\\sqrt{50 \\times 50}} = \\frac{50}{50} = 1\\)</p>"},{"location":"lectures/LA/08-vector-and-matrix/#53-python","title":"5.3 Python\u306b\u3088\u308b\u5b9f\u88c5\u4f8b","text":"<p>Google Colaboratory\u3092\u7528\u3044\u3066\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3059\u308bPython\u5b9f\u88c5\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# \u30c7\u30fc\u30bf\u306e\u4f5c\u6210\nheight = np.array([160, 170, 180, 165, 175])\nweight = np.array([55, 65, 75, 60, 70])\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210\ndata = pd.DataFrame({\n    '\u8eab\u9577': height,\n    '\u4f53\u91cd': weight\n})\nprint(\"\u30c7\u30fc\u30bf:\")\nprint(data)\n\n# \u5404\u5909\u6570\u306e\u5e73\u5747\u3092\u8a08\u7b97\nmean_height = np.mean(height)\nmean_weight = np.mean(weight)\nprint(\"\\n\u5e73\u5747\u5024:\")\nprint(f\"\u5e73\u5747\u8eab\u9577: {mean_height} cm\")\nprint(f\"\u5e73\u5747\u4f53\u91cd: {mean_weight} kg\")\n\n# \u5404\u5909\u6570\u306e\u504f\u5dee\u3092\u8a08\u7b97\ndev_height = height - mean_height\ndev_weight = weight - mean_weight\nprint(\"\\n\u504f\u5dee:\")\nfor i in range(len(height)):\n    print(f\"\u30c7\u30fc\u30bf{i+1}: \u8eab\u9577\u504f\u5dee = {dev_height[i]}, \u4f53\u91cd\u504f\u5dee = {dev_weight[i]}\")\n\n# \u5171\u5206\u6563\u306e\u8a08\u7b97\uff08\u624b\u52d5\uff09\ncov_manual = np.sum(dev_height * dev_weight) / len(height)\nprint(\"\\n\u624b\u52d5\u3067\u8a08\u7b97\u3057\u305f\u5171\u5206\u6563:\")\nprint(f\"Cov(\u8eab\u9577, \u4f53\u91cd) = {cov_manual}\")\n\n# \u5171\u5206\u6563\u306e\u8a08\u7b97\uff08NumPy\u95a2\u6570\u4f7f\u7528\uff09\ncov_numpy = np.cov(height, weight, bias=True)[0, 1]\nprint(\"\\nNumPy\u3067\u8a08\u7b97\u3057\u305f\u5171\u5206\u6563:\")\nprint(f\"Cov(\u8eab\u9577, \u4f53\u91cd) = {cov_numpy}\")\n\n# \u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97\uff08\u624b\u52d5\uff09\nvar_height = np.sum(dev_height**2) / len(height)\nvar_weight = np.sum(dev_weight**2) / len(weight)\ncorr_manual = cov_manual / (np.sqrt(var_height) * np.sqrt(var_weight))\nprint(\"\\n\u624b\u52d5\u3067\u8a08\u7b97\u3057\u305f\u76f8\u95a2\u4fc2\u6570:\")\nprint(f\"\u76f8\u95a2\u4fc2\u6570 = {corr_manual}\")\n\n# \u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97\uff08NumPy\u95a2\u6570\u4f7f\u7528\uff09\ncorr_numpy = np.corrcoef(height, weight)[0, 1]\nprint(\"\\nNumPy\u3067\u8a08\u7b97\u3057\u305f\u76f8\u95a2\u4fc2\u6570:\")\nprint(f\"\u76f8\u95a2\u4fc2\u6570 = {corr_numpy}\")\n\n# \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.scatter(height, weight, color='blue', s=100)\nplt.title('\u8eab\u9577\u3068\u4f53\u91cd\u306e\u6563\u5e03\u56f3')\nplt.xlabel('\u8eab\u9577 (cm)')\nplt.ylabel('\u4f53\u91cd (kg)')\nplt.grid(True)\n\n# \u56de\u5e30\u76f4\u7dda\u306e\u8ffd\u52a0\nslope, intercept, r_value, p_value, std_err = stats.linregress(height, weight)\nx = np.linspace(min(height)-5, max(height)+5, 100)\ny = slope * x + intercept\nplt.plot(x, y, 'r--', label=f'\u56de\u5e30\u76f4\u7dda: y = {slope:.2f}x + {intercept:.2f}, r = {r_value:.2f}')\nplt.legend()\nplt.show()\n\n# \u5171\u5206\u6563\u884c\u5217\u306e\u8a08\u7b97\ncov_matrix = np.cov(np.vstack([height, weight]), bias=True)\nprint(\"\\n\u5171\u5206\u6563\u884c\u5217:\")\nprint(cov_matrix)\n\n# \u76f8\u95a2\u884c\u5217\u306e\u8a08\u7b97\ncorr_matrix = np.corrcoef(np.vstack([height, weight]))\nprint(\"\\n\u76f8\u95a2\u884c\u5217:\")\nprint(corr_matrix)\n</code></pre> <p>\u5b9f\u884c\u7d50\u679c\uff08\u53c2\u8003\uff09: <pre><code>\u30c7\u30fc\u30bf:\n    \u8eab\u9577  \u4f53\u91cd\n0  160  55\n1  170  65\n2  180  75\n3  165  60\n4  175  70\n\n\u5e73\u5747\u5024:\n\u5e73\u5747\u8eab\u9577: 170.0 cm\n\u5e73\u5747\u4f53\u91cd: 65.0 kg\n\n\u504f\u5dee:\n\u30c7\u30fc\u30bf1: \u8eab\u9577\u504f\u5dee = -10.0, \u4f53\u91cd\u504f\u5dee = -10.0\n\u30c7\u30fc\u30bf2: \u8eab\u9577\u504f\u5dee = 0.0, \u4f53\u91cd\u504f\u5dee = 0.0\n\u30c7\u30fc\u30bf3: \u8eab\u9577\u504f\u5dee = 10.0, \u4f53\u91cd\u504f\u5dee = 10.0\n\u30c7\u30fc\u30bf4: \u8eab\u9577\u504f\u5dee = -5.0, \u4f53\u91cd\u504f\u5dee = -5.0\n\u30c7\u30fc\u30bf5: \u8eab\u9577\u504f\u5dee = 5.0, \u4f53\u91cd\u504f\u5dee = 5.0\n\n\u624b\u52d5\u3067\u8a08\u7b97\u3057\u305f\u5171\u5206\u6563:\nCov(\u8eab\u9577, \u4f53\u91cd) = 50.0\n\nNumPy\u3067\u8a08\u7b97\u3057\u305f\u5171\u5206\u6563:\nCov(\u8eab\u9577, \u4f53\u91cd) = 50.0\n\n\u624b\u52d5\u3067\u8a08\u7b97\u3057\u305f\u76f8\u95a2\u4fc2\u6570:\n\u76f8\u95a2\u4fc2\u6570 = 1.0\n\nNumPy\u3067\u8a08\u7b97\u3057\u305f\u76f8\u95a2\u4fc2\u6570:\n\u76f8\u95a2\u4fc2\u6570 = 1.0\n\n\u5171\u5206\u6563\u884c\u5217:\n[[50. 50.]\n [50. 50.]]\n\n\u76f8\u95a2\u884c\u5217:\n[[1. 1.]\n [1. 1.]]\n</code></pre></p>"},{"location":"lectures/LA/08-vector-and-matrix/#54","title":"5.4 \u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u4f8b","text":"<p>\u3088\u308a\u73fe\u5b9f\u7684\u306a\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97\u3068\u89e3\u91c8\u306e\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u8a2d\u5b9a\uff08\u518d\u73fe\u6027\u306e\u305f\u3081\uff09\nnp.random.seed(42)\n\n# \u73fe\u5b9f\u7684\u306a\u30c7\u30fc\u30bf\u306e\u751f\u6210\uff08\u8eab\u9577\u3068\u4f53\u91cd\uff09\nn = 50  # \u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\n\n# \u7537\u6027\u306e\u30c7\u30fc\u30bf\u751f\u6210\uff08\u5e73\u5747\u8eab\u9577175cm\u3001\u6a19\u6e96\u504f\u5dee6cm\u3001\u5e73\u5747\u4f53\u91cd70kg\u3001\u6a19\u6e96\u504f\u5dee8kg\uff09\nmale_height = np.random.normal(175, 6, n)\nmale_weight = 0.9 * (male_height - 175) + 70 + np.random.normal(0, 4, n)\n\n# \u5973\u6027\u306e\u30c7\u30fc\u30bf\u751f\u6210\uff08\u5e73\u5747\u8eab\u9577162cm\u3001\u6a19\u6e96\u504f\u5dee5cm\u3001\u5e73\u5747\u4f53\u91cd53kg\u3001\u6a19\u6e96\u504f\u5dee6kg\uff09\nfemale_height = np.random.normal(162, 5, n)\nfemale_weight = 0.8 * (female_height - 162) + 53 + np.random.normal(0, 3, n)\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210\nmale_df = pd.DataFrame({\n    '\u8eab\u9577': male_height,\n    '\u4f53\u91cd': male_weight,\n    '\u6027\u5225': '\u7537\u6027'\n})\n\nfemale_df = pd.DataFrame({\n    '\u8eab\u9577': female_height,\n    '\u4f53\u91cd': female_weight,\n    '\u6027\u5225': '\u5973\u6027'\n})\n\n# \u30c7\u30fc\u30bf\u306e\u7d50\u5408\ndf = pd.concat([male_df, female_df], ignore_index=True)\n\n# \u30c7\u30fc\u30bf\u306e\u57fa\u672c\u7d71\u8a08\u91cf\nprint(\"\u30c7\u30fc\u30bf\u306e\u57fa\u672c\u7d71\u8a08\u91cf:\")\nprint(df.groupby('\u6027\u5225').describe())\n\n# \u5171\u5206\u6563\u884c\u5217\u306e\u8a08\u7b97\nprint(\"\\n\u7537\u6027\u306e\u5171\u5206\u6563\u884c\u5217:\")\nmale_cov = male_df[['\u8eab\u9577', '\u4f53\u91cd']].cov()\nprint(male_cov)\n\nprint(\"\\n\u5973\u6027\u306e\u5171\u5206\u6563\u884c\u5217:\")\nfemale_cov = female_df[['\u8eab\u9577', '\u4f53\u91cd']].cov()\nprint(female_cov)\n\nprint(\"\\n\u5168\u4f53\u306e\u5171\u5206\u6563\u884c\u5217:\")\ntotal_cov = df[['\u8eab\u9577', '\u4f53\u91cd']].cov()\nprint(total_cov)\n\n# \u76f8\u95a2\u884c\u5217\u306e\u8a08\u7b97\nprint(\"\\n\u7537\u6027\u306e\u76f8\u95a2\u884c\u5217:\")\nmale_corr = male_df[['\u8eab\u9577', '\u4f53\u91cd']].corr()\nprint(male_corr)\n\nprint(\"\\n\u5973\u6027\u306e\u76f8\u95a2\u884c\u5217:\")\nfemale_corr = female_df[['\u8eab\u9577', '\u4f53\u91cd']].corr()\nprint(female_corr)\n\nprint(\"\\n\u5168\u4f53\u306e\u76f8\u95a2\u884c\u5217:\")\ntotal_corr = df[['\u8eab\u9577', '\u4f53\u91cd']].corr()\nprint(total_corr)\n\n# \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(12, 8))\n\n# \u6563\u5e03\u56f3\nplt.subplot(2, 2, 1)\nsns.scatterplot(data=df, x='\u8eab\u9577', y='\u4f53\u91cd', hue='\u6027\u5225', s=70)\nplt.title('\u8eab\u9577\u3068\u4f53\u91cd\u306e\u6563\u5e03\u56f3')\nplt.grid(True)\n\n# \u7537\u6027\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u56de\u5e30\u76f4\u7dda\nplt.subplot(2, 2, 2)\nsns.regplot(data=male_df, x='\u8eab\u9577', y='\u4f53\u91cd', scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\nplt.title(f'\u7537\u6027\u306e\u8eab\u9577\u3068\u4f53\u91cd\u306e\u95a2\u4fc2 (\u76f8\u95a2\u4fc2\u6570: {male_corr.iloc[0, 1]:.2f})')\nplt.grid(True)\n\n# \u5973\u6027\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u56de\u5e30\u76f4\u7dda\nplt.subplot(2, 2, 3)\nsns.regplot(data=female_df, x='\u8eab\u9577', y='\u4f53\u91cd', scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\nplt.title(f'\u5973\u6027\u306e\u8eab\u9577\u3068\u4f53\u91cd\u306e\u95a2\u4fc2 (\u76f8\u95a2\u4fc2\u6570: {female_corr.iloc[0, 1]:.2f})')\nplt.grid(True)\n\n# \u5168\u4f53\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\nplt.subplot(2, 2, 4)\nsns.heatmap(total_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('\u76f8\u95a2\u884c\u5217\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7')\n\nplt.tight_layout()\nplt.show()\n\n# \u30b0\u30eb\u30fc\u30d7\u5225\u306e\u5171\u5206\u6563\u30fb\u76f8\u95a2\u4fc2\u6570\u6bd4\u8f03\ncomparison = pd.DataFrame({\n    '\u30b0\u30eb\u30fc\u30d7': ['\u7537\u6027', '\u5973\u6027', '\u5168\u4f53'],\n    '\u5171\u5206\u6563': [male_cov.iloc[0, 1], female_cov.iloc[0, 1], total_cov.iloc[0, 1]],\n    '\u76f8\u95a2\u4fc2\u6570': [male_corr.iloc[0, 1], female_corr.iloc[0, 1], total_corr.iloc[0, 1]]\n})\n\nprint(\"\\n\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u6bd4\u8f03:\")\nprint(comparison)\n</code></pre>"},{"location":"lectures/LA/08-vector-and-matrix/#6","title":"6. \u8aa4\u89e3\u3057\u3084\u3059\u3044\u30dd\u30a4\u30f3\u30c8","text":""},{"location":"lectures/LA/08-vector-and-matrix/#61","title":"6.1 \u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u89e3\u91c8","text":"<p>\u274c \u3088\u304f\u3042\u308b\u8aa4\u308a: \u5171\u5206\u6563\u3084\u76f8\u95a2\u4fc2\u6570\u304c0\u306b\u8fd1\u3044\u30682\u3064\u306e\u5909\u6570\u306b\u306f\u95a2\u4fc2\u304c\u306a\u3044\u3002</p> <p>\u2705 \u6b63\u3057\u3044\u7406\u89e3: \u5171\u5206\u6563\u3084\u76f8\u95a2\u4fc2\u6570\u304c0\u306b\u8fd1\u3044\u5834\u5408\u3001\u7dda\u5f62\u7684\u306a\u95a2\u4fc2\u304c\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u304c\u3001\u975e\u7dda\u5f62\u306a\u95a2\u4fc2\u304c\u3042\u308b\u53ef\u80fd\u6027\u306f\u3042\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\\(y = x^2\\) \u306e\u3088\u3046\u306a\u4e8c\u6b21\u95a2\u6570\u306e\u95a2\u4fc2\u3067\u306f\u3001\u76f8\u95a2\u4fc2\u6570\u304c0\u306b\u8fd1\u304f\u306a\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#62","title":"6.2 \u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u9055\u3044","text":"<p>\u274c \u3088\u304f\u3042\u308b\u8aa4\u308a: \u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306f\u540c\u3058\u60c5\u5831\u3092\u63d0\u4f9b\u3057\u3066\u3044\u308b\u3002</p> <p>\u2705 \u6b63\u3057\u3044\u7406\u89e3: \u5171\u5206\u6563\u306f\u30c7\u30fc\u30bf\u306e\u5358\u4f4d\u306b\u4f9d\u5b58\u3057\u307e\u3059\u304c\u3001\u76f8\u95a2\u4fc2\u6570\u306f\u5358\u4f4d\u306b\u4f9d\u5b58\u3057\u306a\u3044\u6a19\u6e96\u5316\u3055\u308c\u305f\u6307\u6a19\u3067\u3059\u3002\u76f8\u95a2\u4fc2\u6570\u306f\u5e38\u306b-1\u304b\u30891\u306e\u7bc4\u56f2\u5185\u306b\u306a\u308b\u305f\u3081\u3001\u7570\u306a\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u9593\u3067\u306e\u6bd4\u8f03\u304c\u5bb9\u6613\u3067\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#63","title":"6.3 \u56e0\u679c\u95a2\u4fc2\u3068\u306e\u6df7\u540c","text":"<p>\u274c \u3088\u304f\u3042\u308b\u8aa4\u308a: \u76f8\u95a2\u4fc2\u6570\u304c\u9ad8\u3044\u3053\u3068\u306f\u56e0\u679c\u95a2\u4fc2\u3092\u793a\u3057\u3066\u3044\u308b\u3002</p> <p>\u2705 \u6b63\u3057\u3044\u7406\u89e3: \u76f8\u95a2\u306f\u95a2\u9023\u6027\u306e\u5f37\u3055\u3092\u793a\u3059\u3082\u306e\u3067\u3042\u308a\u3001\u56e0\u679c\u95a2\u4fc2\u3092\u793a\u3059\u3082\u306e\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u300c\u76f8\u95a2\u306f\u56e0\u679c\u3092\u610f\u5473\u3057\u306a\u3044\u300d\u3068\u3044\u3046\u683c\u8a00\u304c\u3042\u308a\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u30a2\u30a4\u30b9\u30af\u30ea\u30fc\u30e0\u306e\u58f2\u4e0a\u3068\u6c34\u96e3\u4e8b\u6545\u306e\u767a\u751f\u6570\u306b\u306f\u6b63\u306e\u76f8\u95a2\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u4e21\u65b9\u304c\u6c17\u6e29\u3068\u3044\u3046\u7b2c\u4e09\u306e\u8981\u56e0\u306b\u5f71\u97ff\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3067\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#64","title":"6.4 \u5916\u308c\u5024\u306e\u5f71\u97ff","text":"<p>\u274c \u3088\u304f\u3042\u308b\u8aa4\u308a: \u76f8\u95a2\u4fc2\u6570\u306f\u5e38\u306b\u5b89\u5b9a\u3057\u305f\u6307\u6a19\u3067\u3042\u308b\u3002</p> <p>\u2705 \u6b63\u3057\u3044\u7406\u89e3: \u76f8\u95a2\u4fc2\u6570\u306f\u5916\u308c\u5024\u306b\u654f\u611f\u3067\u3059\u3002\u5c11\u6570\u306e\u6975\u7aef\u306a\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u304c\u76f8\u95a2\u4fc2\u6570\u306e\u5024\u3092\u5927\u304d\u304f\u5909\u3048\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u30c7\u30fc\u30bf\u5206\u6790\u3092\u884c\u3046\u969b\u306b\u306f\u3001\u5916\u308c\u5024\u306e\u691c\u51fa\u3068\u9069\u5207\u306a\u51e6\u7406\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/08-vector-and-matrix/#71","title":"7.1 \u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c1: \u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u3001\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u3092\u624b\u8a08\u7b97\u3067\u6c42\u3081\u306a\u3055\u3044\u3002</p> \u5b66\u751f \u30c6\u30b9\u30c8\u70b9\u6570 \\((X)\\) \u52c9\u5f37\u6642\u9593 \\((Y)\\) (\u6642\u9593) A 85 10 B 70 7 C 90 12 D 65 6 E 80 9 <p>\u554f\u984c2: \u4ee5\u4e0b\u306e2\u30bb\u30c3\u30c8\u306e\u30c7\u30fc\u30bf\u306e\u76f8\u95a2\u4fc2\u6570\u3092\u6bd4\u8f03\u3057\u306a\u3055\u3044\u3002\u307e\u305f\u3001\u305d\u306e\u7d50\u679c\u304b\u3089\u3069\u306e\u3088\u3046\u306a\u89e3\u91c8\u304c\u3067\u304d\u308b\u304b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> <p>\u30c7\u30fc\u30bf\u30bb\u30c3\u30c81: <pre><code>X: [1, 2, 3, 4, 5]\nY: [2, 4, 6, 8, 10]\n</code></pre></p> <p>\u30c7\u30fc\u30bf\u30bb\u30c3\u30c82: <pre><code>X: [1, 2, 3, 4, 5]\nY: [5, 4, 3, 2, 1]\n</code></pre></p> <p>\u554f\u984c3: \u5171\u5206\u6563\u884c\u5217\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u3066\u3044\u308b\u5834\u5408\u3001\u76f8\u95a2\u884c\u5217\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> \\[\\mathbf{\\Sigma} =  \\begin{bmatrix}  16 &amp; 12 \\\\ 12 &amp; 25 \\end{bmatrix}\\] <p>\u554f\u984c4: Python \u3092\u7528\u3044\u3066\u3001\u6b21\u306e\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3057\u3001\u6563\u5e03\u56f3\u3068\u3068\u3082\u306b\u793a\u3057\u306a\u3055\u3044\u3002</p> <pre><code>X = [68, 71, 65, 73, 70, 69, 75, 72, 67, 74]  # \u8eab\u9577\uff08\u30a4\u30f3\u30c1\uff09\nY = [165, 180, 155, 190, 175, 170, 200, 185, 160, 195]  # \u4f53\u91cd\uff08\u30dd\u30f3\u30c9\uff09\n</code></pre>"},{"location":"lectures/LA/08-vector-and-matrix/#72","title":"7.2 \u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c5: \u3042\u308b\u30af\u30e9\u30b9\u306e10\u4eba\u306e\u5b66\u751f\u306b\u3064\u3044\u3066\u3001\u6570\u5b66\u306e\u30c6\u30b9\u30c8\u70b9\u6570 \\((X)\\)\u3001\u82f1\u8a9e\u306e\u30c6\u30b9\u30c8\u70b9\u6570 \\((Y)\\)\u3001\u52c9\u5f37\u6642\u9593 \\((Z)\\) \uff08\u6642\u9593/\u9031\uff09\u306e\u30c7\u30fc\u30bf\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5f97\u3089\u308c\u305f\u3002</p> <pre><code>\u6570\u5b66: [85, 70, 90, 65, 80, 75, 95, 60, 85, 75]\n\u82f1\u8a9e: [80, 75, 85, 70, 75, 80, 90, 65, 80, 70]\n\u52c9\u5f37\u6642\u9593: [10, 7, 12, 6, 9, 8, 14, 5, 11, 8]\n</code></pre> <p>(1) 3\u3064\u306e\u5909\u6570\u306e\u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002 (2) 3\u3064\u306e\u5909\u6570\u306e\u76f8\u95a2\u884c\u5217\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002 (3) \u6570\u5b66\u306e\u70b9\u6570\u3068\u82f1\u8a9e\u306e\u70b9\u6570\u306e\u95a2\u4fc2\u3001\u6570\u5b66\u306e\u70b9\u6570\u3068\u52c9\u5f37\u6642\u9593\u3068\u52c9\u5f37\u91cf\u306e\u95a2\u4fc2\u3001\u82f1\u8a9e\u306e\u70b9\u6570\u3068\u52c9\u5f37\u6642\u9593\u306e\u95a2\u4fc2\u3092\u6bd4\u8f03\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u76f8\u95a2\u306e\u5f37\u3055\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</p> <p>\u554f\u984c6: \u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u95a2\u9023\u3059\u308b\u554f\u984c\u3068\u3057\u3066\u3001\u3042\u308b\u7814\u7a76\u306750\u4eba\u306e\u60a3\u8005\u304b\u3089\u53ce\u96c6\u3057\u305f\u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3057\u306a\u3055\u3044\u3002</p> <pre><code># \u60a3\u8005\u30c7\u30fc\u30bf\nage = [45, 50, 35, 65, 55, 40, 60, 42, 58, 70, 48, 53, 62, 39, 67, \n       52, 47, 59, 43, 64, 38, 57, 72, 46, 61, 49, 56, 41, 68, 54,\n       63, 37, 66, 51, 44, 69, 36, 60, 52, 48, 71, 43, 58, 50, 64, \n       39, 55, 47, 62, 53]  # \u5e74\u9f62\n\nsystolic_bp = [120, 135, 115, 150, 140, 125, 145, 130, 138, 155, 128, 142, 148, 122, 152,\n              136, 126, 144, 124, 149, 118, 143, 160, 127, 146, 132, 141, 123, 153, 139,\n              147, 116, 151, 134, 125, 154, 117, 145, 136, 129, 158, 122, 143, 133, 150,\n              119, 140, 128, 146, 137]  # \u53ce\u7e2e\u671f\u8840\u5727\n\nbmi = [22.5, 28.3, 21.0, 31.5, 27.8, 24.1, 29.4, 25.6, 28.9, 33.2, 26.3, 29.7, 30.8, 23.5, 32.1,\n      28.4, 25.9, 30.2, 23.8, 31.0, 22.3, 29.8, 34.5, 26.7, 30.5, 27.2, 29.3, 24.4, 32.7, 28.6,\n      30.9, 21.8, 32.3, 27.9, 25.2, 33.8, 22.1, 30.6, 28.8, 26.5, 34.0, 23.7, 29.9, 27.5, 31.2,\n      22.7, 29.1, 26.8, 30.7, 28.2]  # BMI\uff08\u4f53\u683c\u6307\u6570\uff09\n</code></pre> <p>(1) \u5e74\u9f62\u3068\u53ce\u7e2e\u671f\u8840\u5727\u306e\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002 (2) \u5e74\u9f62\u3068BMI\u306e\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002 (3) \u53ce\u7e2e\u671f\u8840\u5727\u3068BMI\u306e\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002 (4) 3\u3064\u306e\u5909\u6570\u306e\u6563\u5e03\u56f3\u884c\u5217\uff08scatter matrix\uff09\u3092\u4f5c\u6210\u3057\u3001\u8996\u899a\u7684\u306b\u95a2\u4fc2\u6027\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002 (5) \u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u5206\u6790\u3068\u3057\u3066\u3001\u3053\u308c\u3089\u306e\u7d50\u679c\u304b\u3089\u3069\u306e\u3088\u3046\u306a\u533b\u5b66\u7684\u793a\u5506\u304c\u5f97\u3089\u308c\u308b\u304b\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/08-vector-and-matrix/#q1","title":"Q1: \u5171\u5206\u6563\u304c\u6b63\u306e\u5024\u304b\u8ca0\u306e\u5024\u304b\u306f\u4f55\u3092\u610f\u5473\u3057\u307e\u3059\u304b\uff1f","text":"<p>A1: \u5171\u5206\u6563\u304c\u6b63\u306e\u5024\u3067\u3042\u308b\u5834\u5408\u30012\u3064\u306e\u5909\u6570\u306f\u540c\u3058\u65b9\u5411\u306b\u5909\u5316\u3059\u308b\u50be\u5411\u304c\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u4e00\u65b9\u306e\u5909\u6570\u304c\u5897\u52a0\u3059\u308b\u3068\u4ed6\u65b9\u3082\u5897\u52a0\u3059\u308b\u50be\u5411\u304c\u3042\u308a\u307e\u3059\u3002\u5171\u5206\u6563\u304c\u8ca0\u306e\u5024\u3067\u3042\u308b\u5834\u5408\u306f\u30012\u3064\u306e\u5909\u6570\u304c\u9006\u65b9\u5411\u306b\u5909\u5316\u3059\u308b\u50be\u5411\u304c\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002\u4e00\u65b9\u304c\u5897\u52a0\u3059\u308b\u3068\u4ed6\u65b9\u306f\u6e1b\u5c11\u3057\u307e\u3059\u3002\u5171\u5206\u6563\u304c0\u306b\u8fd1\u3044\u5834\u5408\u306f\u30012\u3064\u306e\u5909\u6570\u306e\u9593\u306b\u660e\u78ba\u306a\u7dda\u5f62\u306e\u95a2\u4fc2\u304c\u306a\u3044\u3053\u3068\u3092\u793a\u5506\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#q2","title":"Q2: \u76f8\u95a2\u4fc2\u6570\u306e\u5024\u306f\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3059\u3079\u304d\u3067\u3059\u304b\uff1f","text":"<p>A2: \u76f8\u95a2\u4fc2\u6570\u306f-1\u304b\u30891\u306e\u9593\u306e\u5024\u3092\u3068\u308a\u307e\u3059\u3002\u5024\u304c1\u306b\u8fd1\u3044\u307b\u3069\u5f37\u3044\u6b63\u306e\u76f8\u95a2\u3001-1\u306b\u8fd1\u3044\u307b\u3069\u5f37\u3044\u8ca0\u306e\u76f8\u95a2\u30010\u306b\u8fd1\u3044\u307b\u3069\u76f8\u95a2\u304c\u5f31\u3044\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002\u4e00\u822c\u7684\u306a\u76ee\u5b89\u3068\u3057\u3066\uff1a - |r| &gt; 0.7\uff1a\u5f37\u3044\u76f8\u95a2 - 0.4 &lt; |r| &lt; 0.7\uff1a\u4e2d\u7a0b\u5ea6\u306e\u76f8\u95a2 - 0.2 &lt; |r| &lt; 0.4\uff1a\u5f31\u3044\u76f8\u95a2 - |r| &lt; 0.2\uff1a\u307b\u3068\u3093\u3069\u76f8\u95a2\u306a\u3057 \u305f\u3060\u3057\u3001\u3053\u306e\u57fa\u6e96\u306f\u5206\u91ce\u306b\u3088\u3063\u3066\u7570\u306a\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#q3","title":"Q3: \u76f8\u95a2\u4fc2\u6570\u304c\u9ad8\u3044\u3053\u3068\u306f\u56e0\u679c\u95a2\u4fc2\u3092\u610f\u5473\u3057\u307e\u3059\u304b\uff1f","text":"<p>A3: \u3044\u3044\u3048\u3001\u76f8\u95a2\u306f\u5fc5\u305a\u3057\u3082\u56e0\u679c\u95a2\u4fc2\u3092\u610f\u5473\u3057\u307e\u305b\u3093\u3002\u76f8\u95a2\u4fc2\u6570\u306f2\u3064\u306e\u5909\u6570\u9593\u306e\u7dda\u5f62\u95a2\u4fc2\u306e\u5f37\u3055\u3092\u6e2c\u308b\u3082\u306e\u3067\u3042\u308a\u3001\u4e00\u65b9\u304c\u4ed6\u65b9\u306e\u539f\u56e0\u3067\u3042\u308b\u304b\u3069\u3046\u304b\u3092\u793a\u3059\u3082\u306e\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u4f8b\u3048\u3070\u3001\u30a2\u30a4\u30b9\u30af\u30ea\u30fc\u30e0\u306e\u58f2\u4e0a\u3068\u6c34\u96e3\u4e8b\u6545\u306e\u6570\u306b\u306f\u6b63\u306e\u76f8\u95a2\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u4e21\u65b9\u304c\u6c17\u6e29\u3068\u3044\u3046\u7b2c\u4e09\u306e\u8981\u56e0\u306b\u5f71\u97ff\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3067\u3059\u3002\u56e0\u679c\u95a2\u4fc2\u3092\u7279\u5b9a\u3059\u308b\u306b\u306f\u3001\u9069\u5207\u306a\u5b9f\u9a13\u8a08\u753b\u3084\u7d71\u8a08\u7684\u624b\u6cd5\u304c\u5fc5\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#q4","title":"Q4: \u5171\u5206\u6563\u884c\u5217\u306e\u5bfe\u89d2\u6210\u5206\u306f\u4f55\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u304b\uff1f","text":"<p>A4: \u5171\u5206\u6563\u884c\u5217\u306e\u5bfe\u89d2\u6210\u5206\u306f\u305d\u308c\u305e\u308c\u306e\u5909\u6570\u306e\u5206\u6563\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u4f8b\u3048\u3070\u30012\u5909\u6570X\u3001Y\u306e\u5171\u5206\u6563\u884c\u5217\u3067\u306f\u3001\u5de6\u4e0a\u306e\u8981\u7d20\u306fX\u306e\u5206\u6563\u3001\u53f3\u4e0b\u306e\u8981\u7d20\u306fY\u306e\u5206\u6563\u3092\u8868\u3057\u307e\u3059\u3002\u975e\u5bfe\u89d2\u6210\u5206\u306fX\u3068Y\u306e\u5171\u5206\u6563\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#q5-02","title":"Q5: \u76f8\u95a2\u4fc2\u6570\u304c0\u306e\u5834\u5408\u30012\u3064\u306e\u5909\u6570\u306b\u306f\u95a2\u4fc2\u304c\u306a\u3044\u3068\u8a00\u3048\u307e\u3059\u304b\uff1f","text":"<p>A5: \u76f8\u95a2\u4fc2\u6570\u304c0\u306e\u5834\u5408\u30012\u3064\u306e\u5909\u6570\u306e\u9593\u306b\u7dda\u5f62\u95a2\u4fc2\u304c\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u975e\u7dda\u5f62\u7684\u306a\u95a2\u4fc2\u304c\u3042\u308b\u53ef\u80fd\u6027\u306f\u3042\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001Y = X^2\u306e\u3088\u3046\u306a\u653e\u7269\u7dda\u306e\u95a2\u4fc2\u3067\u306f\u3001\u76f8\u95a2\u4fc2\u6570\u304c0\u306b\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u660e\u3089\u304b\u306b\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\u3002\u30c7\u30fc\u30bf\u306e\u6563\u5e03\u56f3\u3092\u8996\u899a\u7684\u306b\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#q6","title":"Q6: \u5916\u308c\u5024\u306f\u76f8\u95a2\u4fc2\u6570\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u307e\u3059\u304b\uff1f","text":"<p>A6: \u5916\u308c\u5024\u306f\u76f8\u95a2\u4fc2\u6570\u306b\u5927\u304d\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u30021\u3064\u306e\u6975\u7aef\u306a\u5916\u308c\u5024\u3060\u3051\u3067\u3001\u76f8\u95a2\u4fc2\u6570\u306e\u5024\u304c\u5927\u304d\u304f\u5909\u308f\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u5916\u308c\u5024\u306e\u5b58\u5728\u3092\u78ba\u8a8d\u3057\u3001\u305d\u306e\u5f71\u97ff\u3092\u8a55\u4fa1\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u5916\u308c\u5024\u3092\u542b\u3080\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3068\u3001\u305d\u308c\u3092\u9664\u5916\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e21\u65b9\u3067\u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u3001\u5916\u308c\u5024\u306e\u5f71\u97ff\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#q7","title":"Q7: \u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97\u306b\u304a\u3044\u3066\u3001\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u306f\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3057\u307e\u3059\u304b\uff1f","text":"<p>A7: \u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u304c\u5c0f\u3055\u3044\u5834\u5408\u3001\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u63a8\u5b9a\u5024\u306f\u4e0d\u5b89\u5b9a\u306b\u306a\u308a\u3084\u3059\u304f\u3001\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u8aa4\u5dee\u304c\u5927\u304d\u304f\u306a\u308a\u307e\u3059\u3002\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u304c\u5927\u304d\u304f\u306a\u308b\u306b\u3064\u308c\u3066\u3001\u63a8\u5b9a\u5024\u306f\u3088\u308a\u5b89\u5b9a\u3057\u3001\u6bcd\u96c6\u56e3\u306e\u771f\u306e\u5024\u306b\u8fd1\u3065\u304f\u50be\u5411\u304c\u3042\u308a\u307e\u3059\u3002\u4e00\u822c\u7684\u306b\u3001\u4fe1\u983c\u3067\u304d\u308b\u76f8\u95a2\u4fc2\u6570\u306e\u63a8\u5b9a\u306b\u306f\u3001\u5c11\u306a\u304f\u3068\u308230\u4ee5\u4e0a\u306e\u30b5\u30f3\u30d7\u30eb\u304c\u63a8\u5968\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#q8","title":"Q8: \u76f8\u95a2\u4fc2\u6570\u3092\u7528\u3044\u3066\u56de\u5e30\u76f4\u7dda\u306e\u50be\u304d\u3092\u6c42\u3081\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u3059\u304b\uff1f","text":"<p>A8: \u306f\u3044\u3001\u6a19\u6e96\u5316\u3055\u308c\u305f\u56de\u5e30\u4fc2\u6570\uff08\u30d9\u30fc\u30bf\u4fc2\u6570\uff09\u306f\u76f8\u95a2\u4fc2\u6570\u3092\u7528\u3044\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a \u03b2 = r \u00d7 (\u03c3Y/\u03c3X) \u3053\u3053\u3067\u3001r\u306f\u76f8\u95a2\u4fc2\u6570\u3001\u03c3Y\u306fY\u306e\u6a19\u6e96\u504f\u5dee\u3001\u03c3X\u306fX\u306e\u6a19\u6e96\u504f\u5dee\u3067\u3059\u3002\u3082\u3057\u5909\u6570\u304c\u6a19\u6e96\u5316\u3055\u308c\u3066\u3044\u308b\u5834\u5408\uff08\u5e73\u57470\u3001\u6a19\u6e96\u504f\u5dee1\uff09\u3001\u56de\u5e30\u76f4\u7dda\u306e\u50be\u304d\u306f\u76f8\u95a2\u4fc2\u6570\u3068\u7b49\u3057\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/08-vector-and-matrix/#9","title":"9. \u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u30012\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u5206\u6790\u306b\u5fc5\u8981\u306a\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u6982\u5ff5\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u672c\u7684\u306a\u30c4\u30fc\u30eb\u3067\u3042\u308a\u3001\u5909\u6570\u9593\u306e\u95a2\u9023\u6027\u3092\u5b9a\u91cf\u7684\u306b\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002</p> <p>\u4e3b\u8981\u306a\u30dd\u30a4\u30f3\u30c8\uff1a</p> <ol> <li>\u5171\u5206\u6563\u306f2\u3064\u306e\u5909\u6570\u306e\u95a2\u9023\u6027\u306e\u6307\u6a19\u3067\u3042\u308a\u3001\u4e21\u65b9\u306e\u5909\u6570\u304c\u5e73\u5747\u304b\u3089\u3069\u306e\u65b9\u5411\u306b\u3069\u308c\u3060\u3051\u504f\u5dee\u304c\u3042\u308b\u304b\u3092\u793a\u3057\u307e\u3059\u3002</li> <li>\u76f8\u95a2\u4fc2\u6570\u306f\u5171\u5206\u6563\u3092\u6a19\u6e96\u5316\u3057\u305f\u3082\u306e\u3067\u3001-1\u304b\u30891\u306e\u7bc4\u56f2\u3067\u5909\u6570\u9593\u306e\u7dda\u5f62\u95a2\u4fc2\u306e\u5f37\u3055\u3068\u65b9\u5411\u3092\u793a\u3057\u307e\u3059\u3002</li> <li>\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u5171\u5206\u6563\u3068\u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97\u3092\u52b9\u7387\u7684\u306b\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> <li>\u5171\u5206\u6563\u884c\u5217\u306f\u8907\u6570\u306e\u5909\u6570\u9593\u306e\u95a2\u4fc2\u3092\u4e00\u5ea6\u306b\u8868\u73fe\u3059\u308b\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u3067\u3059\u3002</li> <li>\u76f8\u95a2\u306f\u56e0\u679c\u95a2\u4fc2\u3092\u793a\u3059\u3082\u306e\u3067\u306f\u306a\u304f\u3001\u30c7\u30fc\u30bf\u5206\u6790\u306e\u969b\u306b\u306f\u6ce8\u610f\u6df1\u3044\u89e3\u91c8\u304c\u5fc5\u8981\u3067\u3059\u3002</li> </ol> <p>\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u6982\u5ff5\u3092\u5fdc\u7528\u3057\u3066\u3001\u3088\u308a\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u5206\u6790\u624b\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002\u7279\u306b\u3001\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306e\u5206\u6790\u3068\u53ef\u8996\u5316\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/09-exercise/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I \u8b1b\u7fa9\u30ce\u30fc\u30c8 - \u7b2c9\u56de\uff1a\u7dcf\u5408\u6f14\u7fd2","text":""},{"location":"lectures/LA/09-exercise/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<ul> <li>\u8b1b\u7fa9\u56de: \u7b2c9\u56de</li> <li>\u8b1b\u7fa9\u5185\u5bb9: \u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u306e\u57fa\u672c\u6982\u5ff5\u306e\u7dcf\u5408\u6f14\u7fd2</li> <li>\u95a2\u9023\u9805\u76ee: \u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u3001\u884c\u5217\u6f14\u7b97\u3001\u5185\u7a4d\u3001\u30c7\u30fc\u30bf\u306e\u7d71\u8a08\u91cf</li> <li>\u4e88\u7fd2\u5185\u5bb9: \u7b2c1\u56de\uff5e\u7b2c8\u56de\u306e\u8b1b\u7fa9\u5185\u5bb9\u3092\u5fa9\u7fd2\u3057\u3066\u304a\u304f\u3053\u3068</li> </ul>"},{"location":"lectures/LA/09-exercise/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\uff1a</p> <ol> <li>\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u64cd\u4f5c\uff08\u548c\u3001\u30b9\u30ab\u30e9\u30fc\u500d\u3001\u5185\u7a4d\u3001\u30ce\u30eb\u30e0\uff09\u3092\u6b63\u78ba\u306b\u7406\u89e3\u3057\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u57fa\u672c\u64cd\u4f5c\uff08\u548c\u3001\u30b9\u30ab\u30e9\u30fc\u500d\u3001\u7a4d\u3001\u8ee2\u7f6e\uff09\u3092\u6b63\u78ba\u306b\u7406\u89e3\u3057\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u306e\u7d71\u8a08\u91cf\uff08\u5e73\u5747\u3001\u5206\u6563\u3001\u5171\u5206\u6563\u3001\u76f8\u95a2\u4fc2\u6570\uff09\u3092\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u7dda\u5f62\u4ee3\u6570\u306e\u57fa\u790e\u6982\u5ff5\u3092\u5b9f\u30c7\u30fc\u30bf\u306b\u9069\u7528\u3067\u304d\u308b</li> <li>\u7dda\u5f62\u4ee3\u6570\u306e\u8996\u70b9\u304b\u3089\u30c7\u30fc\u30bf\u306e\u69cb\u9020\u3092\u7406\u89e3\u3057\u89e3\u91c8\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/09-exercise/#3","title":"3. \u57fa\u672c\u6982\u5ff5\u306e\u5fa9\u7fd2","text":""},{"location":"lectures/LA/09-exercise/#31","title":"3.1 \u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u6982\u5ff5","text":"<p>\u5b9a\u7fa9: \u30d9\u30af\u30c8\u30eb</p> <p>n\u6b21\u5143\u306e\u5b9f\u30d9\u30af\u30c8\u30eb\u306f\u3001n\u500b\u306e\u5b9f\u6570\u3092\u7e26\u306b\u4e26\u3079\u305f\u3082\u306e\u3067\u3042\u308a\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u8a18\u3055\u308c\u308b\uff1a</p> \\[\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\\] <p>\u3053\u3053\u3067\u3001\\(x_i\\)\u306f\u5b9f\u6570\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/09-exercise/#_1","title":"\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u6f14\u7b97","text":"<ol> <li> <p>\u548c: \\(\\mathbf{x} + \\mathbf{y} = \\begin{pmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\\)</p> </li> <li> <p>\u30b9\u30ab\u30e9\u30fc\u500d: \\(\\alpha \\mathbf{x} = \\begin{pmatrix} \\alpha x_1 \\\\ \\alpha x_2 \\\\ \\vdots \\\\ \\alpha x_n \\end{pmatrix}\\)</p> </li> <li> <p>\u5185\u7a4d: \\(\\mathbf{x} \\cdot \\mathbf{y} = x_1y_1 + x_2y_2 + \\cdots + x_ny_n = \\sum_{i=1}^n x_i y_i\\)</p> </li> <li> <p>\u30ce\u30eb\u30e0: \\(\\|\\mathbf{x}\\| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} = \\sqrt{\\mathbf{x} \\cdot \\mathbf{x}}\\)</p> </li> </ol>"},{"location":"lectures/LA/09-exercise/#32","title":"3.2 \u884c\u5217\u306e\u57fa\u672c\u6982\u5ff5","text":"<p>\u5b9a\u7fa9: \u884c\u5217</p> <p>m\u884cn\u5217\u306e\u5b9f\u884c\u5217\u306f\u3001mn\u500b\u306e\u5b9f\u6570\u3092\u9577\u65b9\u5f62\u72b6\u306b\u914d\u7f6e\u3057\u305f\u3082\u306e\u3067\u3042\u308a\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u8a18\u3055\u308c\u308b\uff1a</p> \\[A = \\begin{pmatrix}  a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}\\] <p>\u3053\u3053\u3067\u3001\\(a_{ij}\\)\u306f\u5b9f\u6570\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/09-exercise/#_2","title":"\u884c\u5217\u306e\u57fa\u672c\u6f14\u7b97","text":"<ol> <li> <p>\u548c: \\((A + B)_{ij} = a_{ij} + b_{ij}\\)</p> </li> <li> <p>\u30b9\u30ab\u30e9\u30fc\u500d: \\((\\alpha A)_{ij} = \\alpha a_{ij}\\)</p> </li> <li> <p>\u7a4d: \\((AB)_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}\\)</p> </li> <li> <p>\u8ee2\u7f6e: \\((A^T)_{ij} = a_{ji}\\)</p> </li> </ol>"},{"location":"lectures/LA/09-exercise/#33","title":"3.3 \u7279\u6b8a\u306a\u884c\u5217\u3068\u305d\u306e\u6027\u8cea","text":"<ol> <li>\u5358\u4f4d\u884c\u5217:    \\(\\(I_n = \\begin{pmatrix}    1 &amp; 0 &amp; \\cdots &amp; 0 \\\\    0 &amp; 1 &amp; \\cdots &amp; 0 \\\\    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\    0 &amp; 0 &amp; \\cdots &amp; 1    \\end{pmatrix}\\)\\)</li> </ol> <p>\u6027\u8cea: \u4efb\u610f\u306e\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066 \\(AI = IA = A\\)</p> <ol> <li>\u8ee2\u7f6e\u884c\u5217:    \\(A^T\\) \u306f \\(A\\) \u306e\u8ee2\u7f6e</li> </ol> <p>\u6027\u8cea:     - \\((A^T)^T = A\\)    - \\((A + B)^T = A^T + B^T\\)    - \\((AB)^T = B^T A^T\\)</p> <ol> <li>\u5bfe\u79f0\u884c\u5217:    \\(A = A^T\\) \u3092\u6e80\u305f\u3059\u884c\u5217</li> </ol> <p>\u6027\u8cea:    - \u5bfe\u89d2\u8981\u7d20\u4ee5\u5916\u306e\u8981\u7d20\u304c\u5bfe\u79f0\u7684\u306b\u914d\u7f6e\u3055\u308c\u308b    - \u56fa\u6709\u5024\u304c\u5b9f\u6570\u306b\u306a\u308b</p>"},{"location":"lectures/LA/09-exercise/#34","title":"3.4 \u30c7\u30fc\u30bf\u3068\u884c\u5217\u30fb\u30d9\u30af\u30c8\u30eb\u306e\u95a2\u4fc2","text":"<p>\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u304c \\(n\\) \u500b\u306e\u30b5\u30f3\u30d7\u30eb\u3068 \\(p\\) \u500b\u306e\u5909\u6570\u3092\u6301\u3064\u5834\u5408\uff1a</p> \\[X = \\begin{pmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{pmatrix}\\] <p>\u3053\u3053\u3067\u3001\\(x_{ij}\\) \u306f\u30b5\u30f3\u30d7\u30eb \\(i\\) \u306e\u5909\u6570 \\(j\\) \u306e\u5024\u3092\u8868\u3059\u3002</p>"},{"location":"lectures/LA/09-exercise/#4","title":"4. \u30c7\u30fc\u30bf\u306e\u7d71\u8a08\u8a08\u7b97\u3068\u30d9\u30af\u30c8\u30eb\u30fb\u884c\u5217\u6f14\u7b97","text":""},{"location":"lectures/LA/09-exercise/#41","title":"4.1 \u5e73\u5747\u30d9\u30af\u30c8\u30eb","text":"<p>n\u500b\u306e\u30b5\u30f3\u30d7\u30eb\u306ep\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3055\u308c\u308b\uff1a</p> \\[\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i = \\frac{1}{n} \\begin{pmatrix} \\sum_{i=1}^n x_{i1} \\\\ \\sum_{i=1}^n x_{i2} \\\\ \\vdots \\\\ \\sum_{i=1}^n x_{ip} \\end{pmatrix}\\] <p>\u884c\u5217\u8868\u8a18\u3067\u306f\uff1a</p> \\[\\bar{\\mathbf{x}} = \\frac{1}{n}X^T\\mathbf{1}_n\\] <p>\u3053\u3053\u3067\u3001\\(\\mathbf{1}_n\\) \u306f\u5168\u3066\u306e\u8981\u7d20\u304c1\u306e\\(n\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/09-exercise/#42","title":"4.2 \u5206\u6563\u30fb\u5171\u5206\u6563\u884c\u5217","text":"<p>\u30c7\u30fc\u30bf\u306e\u5206\u6563\u30fb\u5171\u5206\u6563\u884c\u5217\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3055\u308c\u308b\uff1a</p> \\[S = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^T\\] <p>\u884c\u5217\u8868\u8a18\u3067\u306f\uff1a</p> \\[S = \\frac{1}{n}(X - \\mathbf{1}_n\\bar{\\mathbf{x}}^T)^T(X - \\mathbf{1}_n\\bar{\\mathbf{x}}^T)\\] <p>\u3053\u3053\u3067\u3001\\(S\\) \u306e\u5bfe\u89d2\u8981\u7d20 \\(s_{jj}\\) \u306f\u5909\u6570 \\(j\\) \u306e\u5206\u6563\u3001\u975e\u5bfe\u89d2\u8981\u7d20 \\(s_{jk}\\) \u306f\u5909\u6570 \\(j\\) \u3068\u5909\u6570 \\(k\\) \u306e\u5171\u5206\u6563\u3092\u8868\u3059\u3002</p>"},{"location":"lectures/LA/09-exercise/#43","title":"4.3 \u76f8\u95a2\u4fc2\u6570\u884c\u5217","text":"<p>\u76f8\u95a2\u4fc2\u6570\u884c\u5217 \\(R\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3055\u308c\u308b\uff1a</p> \\[r_{jk} = \\frac{s_{jk}}{\\sqrt{s_{jj}s_{kk}}}\\] <p>\u3053\u3053\u3067\u3001\\(r_{jk}\\) \u306f\u5909\u6570 \\(j\\) \u3068\u5909\u6570 \\(k\\) \u306e\u76f8\u95a2\u4fc2\u6570\u3092\u8868\u3059\u3002</p> <p>\u884c\u5217\u8868\u8a18\u3067\u306f\uff1a</p> \\[R = D^{-1/2}SD^{-1/2}\\] <p>\u3053\u3053\u3067\u3001\\(D\\) \u306f \\(S\\) \u306e\u5bfe\u89d2\u8981\u7d20\u3092\u5bfe\u89d2\u306b\u6301\u3064\u5bfe\u89d2\u884c\u5217\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/09-exercise/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/09-exercise/#51","title":"5.1 \u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u306e\u57fa\u672c\u64cd\u4f5c","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\nx = np.array([1, 2, 3])\ny = np.array([4, 5, 6])\n\n# \u30d9\u30af\u30c8\u30eb\u306e\u548c\nz = x + y\nprint(\"\u30d9\u30af\u30c8\u30eb\u306e\u548c:\", z)  # [5 7 9]\n\n# \u30b9\u30ab\u30e9\u30fc\u500d\nalpha = 2\nscaled_x = alpha * x\nprint(\"\u30b9\u30ab\u30e9\u30fc\u500d:\", scaled_x)  # [2 4 6]\n\n# \u5185\u7a4d\ndot_product = np.dot(x, y)\nprint(\"\u5185\u7a4d:\", dot_product)  # 32\n\n# \u30ce\u30eb\u30e0\nnorm_x = np.linalg.norm(x)\nprint(\"\u30ce\u30eb\u30e0:\", norm_x)  # 3.7416573867739413\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\n# \u884c\u5217\u306e\u548c\nC = A + B\nprint(\"\u884c\u5217\u306e\u548c:\\n\", C)  # [[6 8], [10 12]]\n\n# \u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d\nscaled_A = 2 * A\nprint(\"\u884c\u5217\u306e\u30b9\u30ab\u30e9\u30fc\u500d:\\n\", scaled_A)  # [[2 4], [6 8]]\n\n# \u884c\u5217\u306e\u7a4d\nD = np.matmul(A, B)\nprint(\"\u884c\u5217\u306e\u7a4d:\\n\", D)  # [[19 22], [43 50]]\n\n# \u884c\u5217\u306e\u8ee2\u7f6e\nA_transpose = A.T\nprint(\"\u884c\u5217\u306e\u8ee2\u7f6e:\\n\", A_transpose)  # [[1 3], [2 4]]\n</code></pre>"},{"location":"lectures/LA/09-exercise/#52","title":"5.2 \u30c7\u30fc\u30bf\u306e\u7d71\u8a08\u8a08\u7b97","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u751f\u6210\uff08\u8eab\u9577\u3068\u4f53\u91cd\u306e\u30c7\u30fc\u30bf\u3092\u60f3\u5b9a\uff09\nnp.random.seed(42)\nheight = 170 + 10 * np.random.randn(100)  # \u5e73\u5747170cm\u3001\u6a19\u6e96\u504f\u5dee10cm\u306e\u8eab\u9577\nweight = 60 + 0.6 * (height - 170) + 5 * np.random.randn(100)  # \u8eab\u9577\u3068\u76f8\u95a2\u306e\u3042\u308b\u4f53\u91cd\n\n# \u30c7\u30fc\u30bf\u884c\u5217\u306e\u4f5c\u6210\nX = np.column_stack((height, weight))\nprint(\"\u30c7\u30fc\u30bf\u884c\u5217\u306e\u5f62\u72b6:\", X.shape)  # (100, 2)\n\n# \u5e73\u5747\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\nmean_vector = np.mean(X, axis=0)\nprint(\"\u5e73\u5747\u30d9\u30af\u30c8\u30eb:\", mean_vector)\n\n# \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u8a08\u7b97\ncov_matrix = np.cov(X, rowvar=False)\nprint(\"\u5206\u6563\u5171\u5206\u6563\u884c\u5217:\\n\", cov_matrix)\n\n# \u76f8\u95a2\u4fc2\u6570\u884c\u5217\u306e\u8a08\u7b97\ncorr_matrix = np.corrcoef(X, rowvar=False)\nprint(\"\u76f8\u95a2\u4fc2\u6570\u884c\u5217:\\n\", corr_matrix)\n\n# \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\n\n# \u6563\u5e03\u56f3\nplt.scatter(X[:, 0], X[:, 1])\nplt.xlabel('\u8eab\u9577 (cm)')\nplt.ylabel('\u4f53\u91cd (kg)')\nplt.title('\u8eab\u9577\u3068\u4f53\u91cd\u306e\u6563\u5e03\u56f3')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/09-exercise/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/09-exercise/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c1: \u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u306b\u3064\u3044\u3066\u3001\u30d9\u30af\u30c8\u30eb\u306e\u548c\u3001\u5dee\u3001\u5185\u7a4d\u3001\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\u305b\u3088\u3002 \\(\\(\\mathbf{a} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 4 \\end{pmatrix}, \\mathbf{b} = \\begin{pmatrix} 2 \\\\ 5 \\\\ 0 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c2: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u7a4d\u3092\u8a08\u7b97\u305b\u3088\u3002 \\(\\(A = \\begin{pmatrix} 2 &amp; 1 \\\\ 3 &amp; 4 \\end{pmatrix}, B = \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c3: \u4ee5\u4e0b\u306e\u884c\u5217 \\(A\\) \u306b\u3064\u3044\u3066\u3001\\(A^TA\\) \u3068 \\(AA^T\\) \u3092\u8a08\u7b97\u305b\u3088\u3002 \\(\\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c4: \u6b21\u306e\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u305b\u3088\u3002 \\(\\(X = \\begin{pmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c5: \u4ee5\u4e0b\u306e\u884c\u5217\u304c\u5bfe\u79f0\u884c\u5217\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002 \\(\\(C = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 6 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c6: 2\u3064\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = (1, 2, 3)^T\\) \u3068 \\(\\mathbf{b} = (4, 5, 6)^T\\) \u306e\u306a\u3059\u89d2\u5ea6\uff08\u30e9\u30b8\u30a2\u30f3\uff09\u3092\u8a08\u7b97\u305b\u3088\u3002\uff08\u30d2\u30f3\u30c8\uff1a\\(\\cos \\theta = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}||\\mathbf{b}|}\\)\uff09</p>"},{"location":"lectures/LA/09-exercise/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c7: \u3042\u308b\u5065\u5eb7\u8abf\u67fb\u3067\u306f\u30015\u4eba\u306e\u60a3\u8005\u304b\u3089\u4f53\u6e29(\u2103)\u3068\u8108\u62cd(bpm)\u306e\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3057\u307e\u3057\u305f\u3002</p> <pre><code>\u60a3\u80051: \u4f53\u6e29=36.5, \u8108\u62cd=72\n\u60a3\u80052: \u4f53\u6e29=37.2, \u8108\u62cd=85\n\u60a3\u80053: \u4f53\u6e29=36.8, \u8108\u62cd=78\n\u60a3\u80054: \u4f53\u6e29=37.0, \u8108\u62cd=80\n\u60a3\u80055: \u4f53\u6e29=36.7, \u8108\u62cd=75\n</code></pre> <p>\u3053\u306e\u30c7\u30fc\u30bf\u3092\u884c\u5217\u3068\u3057\u3066\u8868\u73fe\u3057\u3001\u4f53\u6e29\u3068\u8108\u62cd\u306e\u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u305b\u3088\u3002</p> <p>\u554f\u984c8: 3\u4eba\u306e\u5b66\u751f\u306e\u30c6\u30b9\u30c8\u30b9\u30b3\u30a2\uff08\u6570\u5b66\u3001\u7269\u7406\u3001\u82f1\u8a9e\uff09\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u3066\u3044\u307e\u3059\u3002</p> <pre><code>\u5b66\u751fA: \u6570\u5b66=85, \u7269\u7406=78, \u82f1\u8a9e=92\n\u5b66\u751fB: \u6570\u5b66=90, \u7269\u7406=85, \u82f1\u8a9e=88\n\u5b66\u751fC: \u6570\u5b66=75, \u7269\u7406=80, \u82f1\u8a9e=85\n</code></pre> <p>\u3053\u306e\u30c7\u30fc\u30bf\u3092\u884c\u5217 \\(X\\) \u3068\u3057\u3066\u8868\u3057\u3001\u4ee5\u4e0b\u306e\u554f\u3044\u306b\u7b54\u3048\u3088\u3002 (a) \u5404\u79d1\u76ee\u306e\u5e73\u5747\u70b9\u3092\u6c42\u3081\u3088\u3002 (b) \u6570\u5b66\u3068\u7269\u7406\u306e\u76f8\u95a2\u4fc2\u6570\u3092\u6c42\u3081\u3088\u3002</p> <p>\u554f\u984c9: 2\u6b21\u5143\u30c7\u30fc\u30bf\u70b9 \\((1, 2)\\), \\((3, 4)\\), \\((5, 6)\\) \u306b\u3064\u3044\u3066\u3001\u3053\u308c\u3089\u306e\u70b9\u306e\u91cd\u5fc3\uff08\u5e73\u5747\uff09\u304b\u3089\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u305b\u3088\u3002</p> <p>\u554f\u984c10: \u884c\u5217 \\(A = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{pmatrix}\\) \u3092\u7528\u3044\u3066\u3001\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\) \u3092\u5909\u63db\u3057\u305f\u7d50\u679c\u3092\u6c42\u3081\u3088\u3002\u307e\u305f\u3001\u5909\u63db\u524d\u5f8c\u306e\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\u3092\u6bd4\u8f03\u305b\u3088\u3002</p>"},{"location":"lectures/LA/09-exercise/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/09-exercise/#71","title":"7.1 \u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u306e\u57fa\u672c","text":"<p>Q1: \u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A1: \u30d9\u30af\u30c8\u30eb\u306f\u4e00\u6b21\u5143\u306e\u914d\u5217\u3067\u3042\u308a\u3001\u5927\u304d\u3055\u3068\u65b9\u5411\u3092\u6301\u3064\u91cf\u3092\u8868\u3057\u307e\u3059\u3002\u4e00\u65b9\u3001\u884c\u5217\u306f\u4e8c\u6b21\u5143\u306e\u914d\u5217\u3067\u3042\u308a\u3001\u8907\u6570\u306e\u30c7\u30fc\u30bf\u3084\u7dda\u5f62\u5909\u63db\u3092\u8868\u73fe\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\u6570\u5b66\u7684\u306b\u306f\u3001\u30d9\u30af\u30c8\u30eb\u306f\u7279\u6b8a\u306a\u884c\u5217\uff081\u5217\u884c\u5217\u307e\u305f\u306f1\u884c\u884c\u5217\uff09\u3068\u8003\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002</p> <p>Q2: \u884c\u5217\u306e\u7a4d\u304c\u53ef\u63db\u3067\u306a\u3044\u7406\u7531\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A2: \u884c\u5217\u306e\u7a4d \\(AB\\) \u306f\u3001\u884c\u5217 \\(A\\) \u306e\u5217\u6570\u3068\u884c\u5217 \\(B\\) \u306e\u884c\u6570\u304c\u4e00\u81f4\u3059\u308b\u5834\u5408\u306b\u306e\u307f\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u3068\u304d\u3001\u7d50\u679c\u306e\u884c\u5217\u306f \\(A\\) \u306e\u884c\u6570\u3068 \\(B\\) \u306e\u5217\u6570\u3092\u6301\u3061\u307e\u3059\u3002\u4e00\u822c\u306b\u3001\\(A\\) \u3068 \\(B\\) \u306e\u6b21\u5143\u304c\u7570\u306a\u308b\u5834\u5408\u3001\\(AB\\) \u3068 \\(BA\\) \u306f\u6b21\u5143\u304c\u7570\u306a\u308b\u304b\u3001\u4e00\u65b9\u304c\u5b9a\u7fa9\u3055\u308c\u306a\u3044\u305f\u3081\u3001\u7b49\u3057\u304f\u306a\u308a\u307e\u305b\u3093\u3002\u307e\u305f\u3001\u540c\u3058\u6b21\u5143\u306e\u6b63\u65b9\u884c\u5217\u3067\u3082\u3001\u884c\u5217\u306e\u7a4d\u306e\u5b9a\u7fa9\u304b\u3089\u3001\u4e00\u822c\u306b\u306f \\(AB \\neq BA\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/09-exercise/#72","title":"7.2 \u30c7\u30fc\u30bf\u89e3\u6790\u3068\u7dda\u5f62\u4ee3\u6570","text":"<p>Q3: \u306a\u305c\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u884c\u5217\u304c\u91cd\u8981\u306a\u306e\u3067\u3059\u304b\uff1f</p> <p>A3: \u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306f\u81ea\u7136\u306b\u884c\u5217\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\u3002\u5404\u884c\u304c\u30b5\u30f3\u30d7\u30eb\u3001\u5404\u5217\u304c\u5909\u6570\u3092\u8868\u3059\u30c7\u30fc\u30bf\u884c\u5217\u306f\u3001\u591a\u304f\u306e\u30c7\u30fc\u30bf\u5206\u6790\u624b\u6cd5\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u7dda\u5f62\u4ee3\u6570\u306e\u6f14\u7b97\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u5909\u63db\u3001\u6b21\u5143\u524a\u6e1b\u3001\u30d1\u30bf\u30fc\u30f3\u767a\u898b\u306a\u3069\u306e\u8907\u96d1\u306a\u5206\u6790\u3092\u52b9\u7387\u7684\u306b\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u4e3b\u6210\u5206\u5206\u6790\u3084\u56e0\u5b50\u5206\u6790\u306a\u3069\u306e\u591a\u5909\u91cf\u89e3\u6790\u624b\u6cd5\u306f\u3001\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3\u3084\u7279\u7570\u5024\u5206\u89e3\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002</p> <p>Q4: \u76f8\u95a2\u4fc2\u6570\u3068\u5171\u5206\u6563\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A4: \u5171\u5206\u6563\u306f2\u3064\u306e\u5909\u6570\u306e\u7dda\u5f62\u95a2\u4fc2\u306e\u5f37\u3055\u3092\u793a\u3059\u6307\u6a19\u3067\u3059\u304c\u3001\u5909\u6570\u306e\u30b9\u30b1\u30fc\u30eb\u306b\u4f9d\u5b58\u3057\u307e\u3059\u3002\u4e00\u65b9\u3001\u76f8\u95a2\u4fc2\u6570\u306f\u5171\u5206\u6563\u3092\u5404\u5909\u6570\u306e\u6a19\u6e96\u504f\u5dee\u3067\u5272\u3063\u3066\u6a19\u6e96\u5316\u3057\u305f\u3082\u306e\u3067\u3001-1\u304b\u30891\u306e\u7bc4\u56f2\u306e\u5024\u3092\u3068\u308a\u307e\u3059\u3002\u76f8\u95a2\u4fc2\u6570\u306f\u5909\u6570\u306e\u30b9\u30b1\u30fc\u30eb\u306b\u4f9d\u5b58\u305b\u305a\u30012\u3064\u306e\u5909\u6570\u306e\u7dda\u5f62\u95a2\u4fc2\u306e\u5f37\u3055\u3068\u65b9\u5411\u3092\u8868\u3059\u6307\u6a19\u3067\u3059\u3002</p>"},{"location":"lectures/LA/09-exercise/#73","title":"7.3 \u8a08\u7b97\u3068\u5b9f\u88c5","text":"<p>Q5: \u5927\u304d\u306a\u884c\u5217\u306e\u8a08\u7b97\u3092\u52b9\u7387\u3088\u304f\u884c\u3046\u306b\u306f\u3069\u3046\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f</p> <p>A5: \u5927\u304d\u306a\u884c\u5217\u306e\u8a08\u7b97\u306b\u306f\u3001NumPy\u3084SciPy\u306a\u3069\u306e\u6700\u9069\u5316\u3055\u308c\u305f\u6570\u5024\u8a08\u7b97\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u3001\u6700\u9069\u5316\u3055\u308c\u305f\u4f4e\u30ec\u30d9\u30eb\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5229\u7528\u3057\u3066\u304a\u308a\u3001\u5927\u898f\u6a21\u306a\u884c\u5217\u8a08\u7b97\u3092\u975e\u5e38\u306b\u52b9\u7387\u7684\u306b\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>Q6: \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\u3068\u6b63\u898f\u5316\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A6: \u6a19\u6e96\u5316\u306f\u30c7\u30fc\u30bf\u3092\u5e73\u57470\u3001\u5206\u65631\u306b\u5909\u63db\u3059\u308b\u51e6\u7406\u3067\u3001\\((x - \\mu) / \\sigma\\) \u3067\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002\u3053\u308c\u306f\u30c7\u30fc\u30bf\u306e\u76f8\u5bfe\u7684\u306a\u4f4d\u7f6e\u3092\u4fdd\u3061\u306a\u304c\u3089\u3001\u7570\u306a\u308b\u30b9\u30b1\u30fc\u30eb\u306e\u5909\u6570\u3092\u6bd4\u8f03\u53ef\u80fd\u306b\u3057\u307e\u3059\u3002\u4e00\u65b9\u3001\u6b63\u898f\u5316\u306f\u30c7\u30fc\u30bf\u3092\u7279\u5b9a\u306e\u7bc4\u56f2\uff08\u901a\u5e38\u306f[0,1]\uff09\u306b\u53ce\u3081\u308b\u51e6\u7406\u3067\u3001\\((x - \\min) / (\\max - \\min)\\) \u3067\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/09-exercise/#8","title":"8. \u6f14\u7fd2\u554f\u984c\u89e3\u7b54\u4f8b","text":""},{"location":"lectures/LA/09-exercise/#_3","title":"\u57fa\u672c\u554f\u984c\u306e\u89e3\u7b54","text":"<p>\u554f\u984c1: \u30d9\u30af\u30c8\u30eb\u306e\u548c: \\(\\mathbf{a} + \\mathbf{b} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 5 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 6 \\\\ 4 \\end{pmatrix}\\)</p> <p>\u30d9\u30af\u30c8\u30eb\u306e\u5dee: \\(\\mathbf{a} - \\mathbf{b} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 5 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -4 \\\\ 4 \\end{pmatrix}\\)</p> <p>\u5185\u7a4d: \\(\\mathbf{a} \\cdot \\mathbf{b} = 3 \\times 2 + 1 \\times 5 + 4 \\times 0 = 6 + 5 + 0 = 11\\)</p> <p>\u30ce\u30eb\u30e0: \\(\\|\\mathbf{a}\\| = \\sqrt{3^2 + 1^2 + 4^2} = \\sqrt{9 + 1 + 16} = \\sqrt{26} \\approx 5.10\\) \\(\\|\\mathbf{b}\\| = \\sqrt{2^2 + 5^2 + 0^2} = \\sqrt{4 + 25 + 0} = \\sqrt{29} \\approx 5.39\\)</p> <p>\u554f\u984c2: \\(AB = \\begin{pmatrix} 2 &amp; 1 \\\\ 3 &amp; 4 \\end{pmatrix} \\begin{pmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{pmatrix} = \\begin{pmatrix} 2 \\times 5 + 1 \\times 7 &amp; 2 \\times 6 + 1 \\times 8 \\\\ 3 \\times 5 + 4 \\times 7 &amp; 3 \\times 6 + 4 \\times 8 \\end{pmatrix} = \\begin{pmatrix} 17 &amp; 20 \\\\ 43 &amp; 50 \\end{pmatrix}\\)</p> <p>\u554f\u984c3: \\(A^TA = \\begin{pmatrix} 1 &amp; 3 \\\\ 2 &amp; 4 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\times 1 + 3 \\times 3 &amp; 1 \\times 2 + 3 \\times 4 \\\\ 2 \\times 1 + 4 \\times 3 &amp; 2 \\times 2 + 4 \\times 4 \\end{pmatrix} = \\begin{pmatrix} 10 &amp; 14 \\\\ 14 &amp; 20 \\end{pmatrix}\\)</p> <p>\\(AA^T = \\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 3 \\\\ 2 &amp; 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\times 1 + 2 \\times 2 &amp; 1 \\times 3 + 2 \\times 4 \\\\ 3 \\times 1 + 4 \\times 2 &amp; 3 \\times 3 + 4 \\times 4 \\end{pmatrix} = \\begin{pmatrix} 5 &amp; 11 \\\\ 11 &amp; 25 \\end{pmatrix}\\)</p> <p>\u554f\u984c4: \u5e73\u5747\u30d9\u30af\u30c8\u30eb: \\(\\bar{\\mathbf{x}} = \\frac{1}{3} \\begin{pmatrix} 1 + 2 + 3 \\\\ 4 + 5 + 6 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}\\)</p> <p>\u554f\u984c5: \u884c\u5217 \\(C\\) \u304c\u5bfe\u79f0\u884c\u5217\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u3059\u308b\u306b\u306f\u3001\\(C = C^T\\) \u304c\u6210\u308a\u7acb\u3064\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> <p>\\(C^T = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 6 \\end{pmatrix}^T = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 6 \\end{pmatrix}\\)</p> <p>\\(C = C^T\\) \u304c\u6210\u308a\u7acb\u3064\u306e\u3067\u3001\u884c\u5217 \\(C\\) \u306f\u5bfe\u79f0\u884c\u5217\u3067\u3059\u3002</p> <p>\u554f\u984c6: 2\u3064\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a} = (1, 2, 3)^T\\) \u3068 \\(\\mathbf{b} = (4, 5, 6)^T\\) \u306e\u306a\u3059\u89d2\u5ea6\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> <p>\u5185\u7a4d: \\(\\mathbf{a} \\cdot \\mathbf{b} = 1 \\times 4 + 2 \\times 5 + 3 \\times 6 = 4 + 10 + 18 = 32\\)</p> <p>\u30ce\u30eb\u30e0: \\(\\|\\mathbf{a}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14} \\approx 3.74\\) \\(\\|\\mathbf{b}\\| = \\sqrt{4^2 + 5^2 + 6^2} = \\sqrt{16 + 25 + 36} = \\sqrt{77} \\approx 8.78\\)</p> <p>\u306a\u3059\u89d2\u306e\u30b3\u30b5\u30a4\u30f3: \\(\\cos \\theta = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}||\\mathbf{b}|} = \\frac{32}{3.74 \\times 8.78} \\approx \\frac{32}{32.84} \\approx 0.974\\)</p> <p>\u306a\u3059\u89d2: \\(\\theta = \\arccos(0.974) \\approx 0.23\\) \u30e9\u30b8\u30a2\u30f3\uff08\u7d0413\u5ea6\uff09</p>"},{"location":"lectures/LA/09-exercise/#_4","title":"\u5fdc\u7528\u554f\u984c\u306e\u89e3\u7b54","text":"<p>\u554f\u984c7: \u307e\u305a\u3001\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> \\[X = \\begin{pmatrix}  36.5 &amp; 72 \\\\ 37.2 &amp; 85 \\\\ 36.8 &amp; 78 \\\\ 37.0 &amp; 80 \\\\ 36.7 &amp; 75 \\end{pmatrix}\\] <p>\u6b21\u306b\u3001\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[\\bar{\\mathbf{x}} = \\begin{pmatrix}  \\frac{36.5 + 37.2 + 36.8 + 37.0 + 36.7}{5} \\\\ \\frac{72 + 85 + 78 + 80 + 75}{5} \\end{pmatrix} = \\begin{pmatrix}  36.84 \\\\ 78 \\end{pmatrix}\\] <p>\u6b21\u306b\u3001\u4e2d\u5fc3\u5316\u30c7\u30fc\u30bf\u884c\u5217\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[X - \\mathbf{1}\\bar{\\mathbf{x}}^T = \\begin{pmatrix}  36.5 - 36.84 &amp; 72 - 78 \\\\ 37.2 - 36.84 &amp; 85 - 78 \\\\ 36.8 - 36.84 &amp; 78 - 78 \\\\ 37.0 - 36.84 &amp; 80 - 78 \\\\ 36.7 - 36.84 &amp; 75 - 78 \\end{pmatrix} = \\begin{pmatrix}  -0.34 &amp; -6 \\\\ 0.36 &amp; 7 \\\\ -0.04 &amp; 0 \\\\ 0.16 &amp; 2 \\\\ -0.14 &amp; -3 \\end{pmatrix}\\] <p>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[S = \\frac{1}{5}(X - \\mathbf{1}\\bar{\\mathbf{x}}^T)^T(X - \\mathbf{1}\\bar{\\mathbf{x}}^T) = \\frac{1}{5}\\begin{pmatrix}  (-0.34)^2 + 0.36^2 + (-0.04)^2 + 0.16^2 + (-0.14)^2 &amp; (-0.34)(-6) + 0.36 \\times 7 + (-0.04) \\times 0 + 0.16 \\times 2 + (-0.14)(-3) \\\\ (-0.34)(-6) + 0.36 \\times 7 + (-0.04) \\times 0 + 0.16 \\times 2 + (-0.14)(-3) &amp; (-6)^2 + 7^2 + 0^2 + 2^2 + (-3)^2 \\end{pmatrix}\\] \\[S = \\frac{1}{5}\\begin{pmatrix}  0.3112 &amp; 5.66 \\\\ 5.66 &amp; 118 \\end{pmatrix} = \\begin{pmatrix}  0.06224 &amp; 1.132 \\\\ 1.132 &amp; 23.6 \\end{pmatrix}\\] <p>\u5404\u5909\u6570\u306e\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[\\sigma_1 = \\sqrt{0.06224} \\approx 0.2495$$ $$\\sigma_2 = \\sqrt{23.6} \\approx 4.8580\\] <p>\u76f8\u95a2\u4fc2\u6570\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[r = \\frac{s_{12}}{\\sigma_1 \\sigma_2} = \\frac{1.132}{0.2495 \\times 4.8580} \\approx \\frac{1.132}{1.212} \\approx 0.934\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\u4f53\u6e29\u3068\u8108\u62cd\u306e\u9593\u306b\u306f\u5f37\u3044\u6b63\u306e\u76f8\u95a2\u95a2\u4fc2\uff08\u76f8\u95a2\u4fc2\u6570 \u2248 0.934\uff09\u304c\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p> <p>\u554f\u984c8: \u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> \\[X = \\begin{pmatrix}  85 &amp; 78 &amp; 92 \\\\ 90 &amp; 85 &amp; 88 \\\\ 75 &amp; 80 &amp; 85 \\end{pmatrix}\\] <p>(a) \u5404\u79d1\u76ee\u306e\u5e73\u5747\u70b9\u3092\u6c42\u3081\u307e\u3059\u3002</p> \\[\\bar{\\mathbf{x}} = \\begin{pmatrix}  \\frac{85 + 90 + 75}{3} \\\\ \\frac{78 + 85 + 80}{3} \\\\ \\frac{92 + 88 + 85}{3} \\end{pmatrix} = \\begin{pmatrix}  83.33 \\\\ 81 \\\\ 88.33 \\end{pmatrix}\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\u6570\u5b66\u306e\u5e73\u5747\u70b9\u306f83.33\u70b9\u3001\u7269\u7406\u306e\u5e73\u5747\u70b9\u306f81\u70b9\u3001\u82f1\u8a9e\u306e\u5e73\u5747\u70b9\u306f88.33\u70b9\u3067\u3059\u3002</p> <p>(b) \u6570\u5b66\u3068\u7269\u7406\u306e\u76f8\u95a2\u4fc2\u6570\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\u307e\u305a\u3001\u4e2d\u5fc3\u5316\u30c7\u30fc\u30bf\u884c\u5217\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[X - \\mathbf{1}\\bar{\\mathbf{x}}^T = \\begin{pmatrix}  85 - 83.33 &amp; 78 - 81 &amp; 92 - 88.33 \\\\ 90 - 83.33 &amp; 85 - 81 &amp; 88 - 88.33 \\\\ 75 - 83.33 &amp; 80 - 81 &amp; 85 - 88.33 \\end{pmatrix} = \\begin{pmatrix}  1.67 &amp; -3 &amp; 3.67 \\\\ 6.67 &amp; 4 &amp; -0.33 \\\\ -8.33 &amp; -1 &amp; -3.33 \\end{pmatrix}\\] <p>\u6b21\u306b\u3001\u6570\u5b66\u3068\u7269\u7406\u306e\u5217\u306b\u95a2\u3059\u308b\u5171\u5206\u6563\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 \\(\\(s_{12} = \\frac{1}{3}[1.67 \\times (-3) + 6.67 \\times 4 + (-8.33) \\times (-1)] = \\frac{1}{3}[-5.01 + 26.68 + 8.33] = \\frac{30}{3} = 10\\)\\)</p> <p>\u6570\u5b66\u306e\u5206\u6563: \\(\\(s_{11} = \\frac{1}{3}[1.67^2 + 6.67^2 + (-8.33)^2] = \\frac{1}{3}[2.79 + 44.49 + 69.39] = \\frac{116.67}{3} \\approx 38.89\\)\\)</p> <p>\u7269\u7406\u306e\u5206\u6563: \\(\\(s_{22} = \\frac{1}{3}[(-3)^2 + 4^2 + (-1)^2] = \\frac{1}{3}[9 + 16 + 1] = \\frac{26}{3} \\approx 8.67\\)\\)</p> <p>\u76f8\u95a2\u4fc2\u6570: \\(\\(r_{12} = \\frac{s_{12}}{\\sqrt{s_{11}s_{22}}} = \\frac{10}{\\sqrt{38.89 \\times 8.67}} = \\frac{10}{\\sqrt{337.18}} = \\frac{10}{18.36} \\approx 0.54\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u6570\u5b66\u3068\u7269\u7406\u306e\u9593\u306b\u306f\u4e2d\u7a0b\u5ea6\u306e\u6b63\u306e\u76f8\u95a2\u95a2\u4fc2\uff08\u76f8\u95a2\u4fc2\u6570 \u2248 0.54\uff09\u304c\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p> <p>\u554f\u984c9: 2\u6b21\u5143\u30c7\u30fc\u30bf\u70b9 \\((1, 2)\\), \\((3, 4)\\), \\((5, 6)\\) \u306e\u91cd\u5fc3\uff08\u5e73\u5747\uff09\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[\\bar{\\mathbf{x}} = \\frac{1}{3} \\begin{pmatrix} 1 + 3 + 5 \\\\ 2 + 4 + 6 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}\\] <p>\u6b21\u306b\u3001\u5404\u70b9\u304b\u3089\u91cd\u5fc3\u307e\u3067\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> <p>\u70b9 \\((1, 2)\\) \u304b\u3089\u91cd\u5fc3\u307e\u3067\u306e\u8ddd\u96e2: \\(\\(d_1 = \\sqrt{(1 - 3)^2 + (2 - 4)^2} = \\sqrt{4 + 4} = \\sqrt{8} \\approx 2.83\\)\\)</p> <p>\u70b9 \\((3, 4)\\) \u304b\u3089\u91cd\u5fc3\u307e\u3067\u306e\u8ddd\u96e2: \\(\\(d_2 = \\sqrt{(3 - 3)^2 + (4 - 4)^2} = \\sqrt{0 + 0} = 0\\)\\)</p> <p>\u70b9 \\((5, 6)\\) \u304b\u3089\u91cd\u5fc3\u307e\u3067\u306e\u8ddd\u96e2: \\(\\(d_3 = \\sqrt{(5 - 3)^2 + (6 - 4)^2} = \\sqrt{4 + 4} = \\sqrt{8} \\approx 2.83\\)\\)</p> <p>\u554f\u984c10: \u884c\u5217 \\(A = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{pmatrix}\\) \u306b\u3088\u308b\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\) \u306e\u5909\u63db\u7d50\u679c\u3092\u6c42\u3081\u307e\u3059\u3002</p> \\[A\\mathbf{v} = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\times 2 + 1 \\times 1 \\\\ 1 \\times 2 + 3 \\times 1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\end{pmatrix}\\] <p>\u5909\u63db\u524d\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}\\) \u306e\u30ce\u30eb\u30e0: \\(\\(\\|\\mathbf{v}\\| = \\sqrt{2^2 + 1^2} = \\sqrt{4 + 1} = \\sqrt{5} \\approx 2.24\\)\\)</p> <p>\u5909\u63db\u5f8c\u306e\u30d9\u30af\u30c8\u30eb \\(A\\mathbf{v}\\) \u306e\u30ce\u30eb\u30e0: \\(\\(\\|A\\mathbf{v}\\| = \\sqrt{5^2 + 5^2} = \\sqrt{25 + 25} = \\sqrt{50} \\approx 7.07\\)\\)</p> <p>\u5909\u63db\u306b\u3088\u3063\u3066\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\u306f\u7d043.16\u500d\u306b\u306a\u308a\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/09-exercise/#9","title":"9. \u307e\u3068\u3081","text":"<p>\u3053\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u57fa\u672c\u6982\u5ff5\u3067\u3042\u308b\u300c\u30d9\u30af\u30c8\u30eb\u300d\u3068\u300c\u884c\u5217\u300d\u306e\u57fa\u672c\u64cd\u4f5c\u304b\u3089\u59cb\u3081\u3066\u3001\u305d\u308c\u3089\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u89e3\u6790\u306e\u57fa\u790e\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\u3002\u30d9\u30af\u30c8\u30eb\u306e\u548c\u30fb\u5dee\u30fb\u5185\u7a4d\u30fb\u30ce\u30eb\u30e0\u3001\u884c\u5217\u306e\u548c\u30fb\u5dee\u30fb\u7a4d\u30fb\u8ee2\u7f6e\u306a\u3069\u306e\u57fa\u672c\u64cd\u4f5c\u3092\u7406\u89e3\u3057\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u306b\u9069\u7528\u3067\u304d\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p> <p>\u7279\u306b\u3001\u591a\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u7d71\u8a08\u91cf\uff08\u5e73\u5747\u3001\u5206\u6563\u3001\u5171\u5206\u6563\u3001\u76f8\u95a2\u4fc2\u6570\uff09\u3092\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u8868\u73fe\u30fb\u8a08\u7b97\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u6982\u5ff5\u306f\u3001\u6b21\u56de\u4ee5\u964d\u306e\u8b1b\u7fa9\u3067\u5b66\u3076\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3084\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u624b\u6cd5\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u6f14\u7fd2\u554f\u984c\u3092\u901a\u3058\u3066\u3001\u57fa\u672c\u7684\u306a\u8a08\u7b97\u30b9\u30ad\u30eb\u3092\u78e8\u304f\u3068\u3068\u3082\u306b\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u306e\u6587\u8108\u3067\u306e\u5fdc\u7528\u65b9\u6cd5\u306b\u3064\u3044\u3066\u3082\u7406\u89e3\u3092\u6df1\u3081\u307e\u3057\u305f\u3002\u7dda\u5f62\u4ee3\u6570\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u76e4\u3068\u306a\u308b\u91cd\u8981\u306a\u6570\u5b66\u7684\u30c4\u30fc\u30eb\u3067\u3059\u3002</p> <p>\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3068\u305d\u306e\u89e3\u6cd5\u306b\u7126\u70b9\u3092\u5f53\u3066\u3001\u7dda\u5f62\u4ee3\u6570\u3068\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u95a2\u9023\u3092\u3055\u3089\u306b\u6df1\u304f\u63a2\u6c42\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u3053\u306e\u7dcf\u5408\u6f14\u7fd2\u3092\u901a\u3058\u3066\u3001\u3053\u308c\u307e\u3067\u306b\u5b66\u3093\u3060\u6982\u5ff5\u3092\u6574\u7406\u30fb\u5f37\u5316\u3057\u3001\u4eca\u5f8c\u306e\u5fdc\u7528\u306b\u5099\u3048\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/","title":"\u7b2c10\u56de \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u8868\u73fe\u3068\u89e3\u6cd5\u306e\u57fa\u790e","text":""},{"location":"lectures/LA/10-system-of-linear-equation/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<ul> <li>\u8b1b\u7fa9\u56de: \u7b2c10\u56de</li> <li>\u95a2\u9023\u9805\u76ee: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3001\u884c\u5217\u8868\u73fe\u3001\u89e3\u6cd5\u306e\u57fa\u790e</li> <li>\u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u884c\u5217\u306e\u57fa\u672c\u6f14\u7b97\uff08\u7b2c4-6\u56de\uff09\u3001\u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u306e\u95a2\u4fc2\uff08\u7b2c7-8\u56de\uff09</li> </ul>"},{"location":"lectures/LA/10-system-of-linear-equation/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\uff1a</p> <ol> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u7406\u89e3\u3059\u308b</li> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u57fa\u672c\u7684\u306a\u89e3\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e\u3068\u305d\u306e\u7279\u5fb4\u3092\u7406\u89e3\u3059\u308b</li> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092Python\u3067\u89e3\u304f\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> </ol>"},{"location":"lectures/LA/10-system-of-linear-equation/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/10-system-of-linear-equation/#31","title":"3.1 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3068\u306f","text":"<p>\u5b9a\u7fa9: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3068\u306f\u3001\u8907\u6570\u306e\u4e00\u6b21\u65b9\u7a0b\u5f0f\uff08\u5909\u6570\u306e\u6b21\u6570\u304c\u5168\u30661\u6b21\u3067\u3042\u308b\u65b9\u7a0b\u5f0f\uff09\u3092\u540c\u6642\u306b\u6e80\u305f\u3059\u89e3\u3092\u6c42\u3081\u308b\u554f\u984c\u3067\u3059\u3002</p> <p>\u4e00\u822c\u7684\u306a\u5f62\u5f0f\uff1a</p> \\[ \\begin{cases} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1\\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2\\\\ \\vdots\\\\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n = b_m \\end{cases} \\] <p>\u3053\u3053\u3067\u3001\\(a_{ij}\\)\u306f\u4fc2\u6570\u3001\\(x_j\\)\u306f\u672a\u77e5\u5909\u6570\u3001\\(b_i\\)\u306f\u5b9a\u6570\u9805\u3067\u3059\u3002\\(m\\)\u306f\u65b9\u7a0b\u5f0f\u306e\u6570\u3001\\(n\\)\u306f\u672a\u77e5\u5909\u6570\u306e\u6570\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u4f8b: \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002 $$ \\begin{cases} 2x + 3y = 8\\ 4x - y = 2 \\end{cases} $$</p> <p>\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306b\u306f2\u3064\u306e\u672a\u77e5\u5909\u6570\uff08\\(x\\)\u3068\\(y\\)\uff09\u30682\u3064\u306e\u65b9\u7a0b\u5f0f\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#32","title":"3.2 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306f\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u3001\u3088\u308a\u7c21\u6f54\u306b\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9: \\(m\\)\u500b\u306e\u65b9\u7a0b\u5f0f\u3068\\(n\\)\u500b\u306e\u672a\u77e5\u6570\u304b\u3089\u306a\u308b\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306f\u3001\u6b21\u306e\u884c\u5217\u65b9\u7a0b\u5f0f\u3067\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> \\[A\\mathbf{x} = \\mathbf{b}\\] <p>\u3053\u3053\u3067\u3001\\(A\\)\u306f\\(m \\times n\\)\u306e\u4fc2\u6570\u884c\u5217\u3001\\(\\mathbf{x}\\)\u306f\\(n\\)\u6b21\u5143\u306e\u672a\u77e5\u6570\u30d9\u30af\u30c8\u30eb\u3001\\(\\mathbf{b}\\)\u306f\\(m\\)\u6b21\u5143\u306e\u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p> <p>\u5177\u4f53\u7684\u306b\u306f\uff1a</p> \\[ \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n}\\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\begin{pmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{pmatrix} = \\begin{pmatrix} b_1\\\\ b_2\\\\ \\vdots\\\\ b_m \\end{pmatrix} \\] <p>\u4f8b: \u5148\u307b\u3069\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u884c\u5217\u8868\u73fe\u3059\u308b\u3068\uff1a</p> \\[ \\begin{pmatrix} 2 &amp; 3\\\\ 4 &amp; -1 \\end{pmatrix} \\begin{pmatrix} x\\\\ y \\end{pmatrix} = \\begin{pmatrix} 8\\\\ 2 \\end{pmatrix} \\] <p>\u3053\u308c\u306b\u3088\u308a\u3001\u8907\u96d1\u306a\u9023\u7acb\u65b9\u7a0b\u5f0f\u3082\u7c21\u6f54\u306b\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#33","title":"3.3 \u62e1\u5927\u4fc2\u6570\u884c\u5217","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u969b\u306b\u306f\u3001\u300c\u62e1\u5927\u4fc2\u6570\u884c\u5217\u300d\u3068\u3044\u3046\u6982\u5ff5\u304c\u5f79\u7acb\u3061\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9: \u62e1\u5927\u4fc2\u6570\u884c\u5217\u3068\u306f\u3001\u4fc2\u6570\u884c\u5217\\(A\\)\u306b\u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30eb\\(\\mathbf{b}\\)\u3092\u53f3\u5074\u306b\u8ffd\u52a0\u3057\u305f\u884c\u5217\u3067\u3059\u3002</p> \\[[A|\\mathbf{b}] =  \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1\\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; | &amp; \\vdots\\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} &amp; | &amp; b_m \\end{pmatrix}\\] <p>\u4f8b: \u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u4f8b</p> \\[[A|\\mathbf{b}] =  \\begin{pmatrix} 2 &amp; 3 &amp; | &amp; 8\\\\ 4 &amp; -1 &amp; | &amp; 2 \\end{pmatrix}\\] <p>\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306f\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u969b\u306e\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306a\u3069\u3067\u7279\u306b\u91cd\u8981\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/10-system-of-linear-equation/#41","title":"4.1 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u305f\u3081\u306e\u57fa\u672c\u7684\u306a\u65b9\u6cd5\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3082\u306e\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u4ee3\u5165\u6cd5: \u4e00\u3064\u306e\u65b9\u7a0b\u5f0f\u304b\u3089\u5909\u6570\u3092\u4ed6\u306e\u5909\u6570\u3067\u8868\u3057\u3001\u305d\u308c\u3092\u4ed6\u306e\u65b9\u7a0b\u5f0f\u306b\u4ee3\u5165\u3059\u308b\u65b9\u6cd5</li> <li>\u52a0\u6e1b\u6cd5: \u8907\u6570\u306e\u65b9\u7a0b\u5f0f\u3092\u52a0\u3048\u305f\u308a\u5f15\u3044\u305f\u308a\u3057\u3066\u5909\u6570\u3092\u6d88\u53bb\u3059\u308b\u65b9\u6cd5</li> <li>\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u7528\u3044\u308b\u65b9\u6cd5: \\(A\\mathbf{x} = \\mathbf{b}\\) \u304b\u3089 \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\) \u3092\u6c42\u3081\u308b\u65b9\u6cd5</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5: \u884c\u306e\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u4e0a\u4e09\u89d2\u884c\u5217\u307e\u305f\u306f\u884c\u7c21\u7d04\u5f62\u306b\u5909\u5f62\u3059\u308b\u65b9\u6cd5</li> </ol> <p>\u3053\u3053\u3067\u306f\u3001\u305d\u308c\u305e\u308c\u306e\u65b9\u6cd5\u306b\u3064\u3044\u3066\u5177\u4f53\u4f8b\u3092\u7528\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#411","title":"4.1.1 \u4ee3\u5165\u6cd5","text":"<p>\u4f8b: \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u4ee3\u5165\u6cd5\u3067\u89e3\u304d\u307e\u3059\u3002 $$ \\begin{cases} 2x + 3y = 8\\ 4x - y = 2 \\end{cases} $$</p> <p>\u89e3\u6cd5: 1. 2\u756a\u76ee\u306e\u65b9\u7a0b\u5f0f\u304b\u3089\\(y\\)\u306b\u3064\u3044\u3066\u89e3\u304d\u307e\u3059\uff1a\\(y = 4x - 2\\) 2. \u3053\u306e\\(y\\)\u306e\u5f0f\u30921\u756a\u76ee\u306e\u65b9\u7a0b\u5f0f\u306b\u4ee3\u5165\u3057\u307e\u3059\uff1a\\(2x + 3(4x - 2) = 8\\) 3. \u6574\u7406\u3057\u307e\u3059\uff1a\\(2x + 12x - 6 = 8\\) 4. \u3055\u3089\u306b\u6574\u7406\uff1a\\(14x = 14\\) 5. \u3088\u3063\u3066\\(x = 1\\) 6. \\(x = 1\\)\u3092\\(y = 4x - 2\\)\u306b\u4ee3\u5165\uff1a\\(y = 4 \\cdot 1 - 2 = 2\\) 7. \u89e3\u306f\\((x, y) = (1, 2)\\)\u3067\u3059\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#412","title":"4.1.2 \u52a0\u6e1b\u6cd5","text":"<p>\u4f8b: \u540c\u3058\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u52a0\u6e1b\u6cd5\u3067\u89e3\u304d\u307e\u3059\u3002 $$ \\begin{cases} 2x + 3y = 8 \\quad \\ldots (1)\\ 4x - y = 2 \\quad \\ldots (2) \\end{cases} $$</p> <p>\u89e3\u6cd5: 1. \u65b9\u7a0b\u5f0f(1)\u306e\u4e21\u8fba\u30922\u500d\u3057\u307e\u3059\uff1a\\(4x + 6y = 16 \\quad \\ldots (1')\\) 2. \u65b9\u7a0b\u5f0f(1')\u304b\u3089\u65b9\u7a0b\u5f0f(2)\u3092\u5f15\u304d\u307e\u3059\uff1a\\(6y + y = 16 - 2\\) 3. \u6574\u7406\u3059\u308b\u3068\uff1a\\(7y = 14\\) 4. \u3088\u3063\u3066\\(y = 2\\) 5. \\(y = 2\\)\u3092\u65b9\u7a0b\u5f0f(2)\u306b\u4ee3\u5165\uff1a\\(4x - 2 = 2\\) 6. \u6574\u7406\u3059\u308b\u3068\uff1a\\(4x = 4\\) 7. \u3088\u3063\u3066\\(x = 1\\) 8. \u89e3\u306f\\((x, y) = (1, 2)\\)\u3067\u3059\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#413","title":"4.1.3 \u884c\u5217\u306e\u9006\u884c\u5217\u3092\u7528\u3044\u308b\u65b9\u6cd5","text":"<p>\u4f8b: \u540c\u3058\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u7528\u3044\u3066\u89e3\u304d\u307e\u3059\u3002 \\(\\(A\\mathbf{x} = \\mathbf{b}\\)\\) \u3053\u3053\u3067\u3001 \\(\\(A = \\begin{pmatrix} 2 &amp; 3\\\\ 4 &amp; -1 \\end{pmatrix}, \\quad  \\mathbf{x} = \\begin{pmatrix} x\\\\ y \\end{pmatrix}, \\quad  \\mathbf{b} = \\begin{pmatrix} 8\\\\ 2 \\end{pmatrix}\\)\\)</p> <p>\u89e3\u6cd5: 1. \\(A\\)\u306e\u9006\u884c\u5217\\(A^{-1}\\)\u3092\u6c42\u3081\u307e\u3059\u3002    \\(\\det(A) = 2 \\cdot (-1) - 3 \\cdot 4 = -2 - 12 = -14\\)</p> <p>\\(A^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix}    -1 &amp; -3\\\\    -4 &amp; 2    \\end{pmatrix} = \\frac{1}{-14} \\begin{pmatrix}    -1 &amp; -3\\\\    -4 &amp; 2    \\end{pmatrix} = \\begin{pmatrix}    \\frac{1}{14} &amp; \\frac{3}{14}\\\\    \\frac{4}{14} &amp; -\\frac{2}{14}    \\end{pmatrix}\\)</p> <ol> <li> <p>\\(\\mathbf{x} = A^{-1}\\mathbf{b}\\)\u3088\u308a\uff1a    \\(\\(\\mathbf{x} = \\begin{pmatrix}    \\frac{1}{14} &amp; \\frac{3}{14}\\\\    \\frac{4}{14} &amp; -\\frac{2}{14}    \\end{pmatrix} \\begin{pmatrix}    8\\\\    2    \\end{pmatrix} = \\begin{pmatrix}    \\frac{1}{14} \\cdot 8 + \\frac{3}{14} \\cdot 2\\\\    \\frac{4}{14} \\cdot 8 + (-\\frac{2}{14}) \\cdot 2    \\end{pmatrix} = \\begin{pmatrix}    \\frac{8 + 6}{14}\\\\    \\frac{32 - 4}{14}    \\end{pmatrix} = \\begin{pmatrix}    \\frac{14}{14}\\\\    \\frac{28}{14}    \\end{pmatrix} = \\begin{pmatrix}    1\\\\    2    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u89e3\u306f\\((x, y) = (1, 2)\\)\u3067\u3059\u3002</p> </li> </ol>"},{"location":"lectures/LA/10-system-of-linear-equation/#42","title":"4.2 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":""},{"location":"lectures/LA/10-system-of-linear-equation/#421-21","title":"4.2.1 2\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":"<p>2\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306f\u5e73\u9762\u4e0a\u306e\u76f4\u7dda\u3068\u3057\u3066\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> <ul> <li>\u5404\u65b9\u7a0b\u5f0f\u306f\u5e73\u9762\u4e0a\u306e1\u672c\u306e\u76f4\u7dda\u3092\u8868\u3057\u307e\u3059</li> <li>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306f\u3001\u3053\u308c\u3089\u306e\u76f4\u7dda\u306e\u4ea4\u70b9\u306b\u5bfe\u5fdc\u3057\u307e\u3059</li> </ul> <p>\u4f8b: \u5148\u307b\u3069\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8</p> \\[ \\begin{cases} 2x + 3y = 8 \\quad \\ldots \\text{\u76f4\u7dda1}\\\\ 4x - y = 2 \\quad \\ldots \\text{\u76f4\u7dda2} \\end{cases} \\] <p>\u3053\u306e2\u3064\u306e\u76f4\u7dda\u306f\u70b9\\((1, 2)\\)\u3067\u4ea4\u308f\u308a\u307e\u3059\u3002</p> <p>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u306b\u3064\u3044\u3066\u306f\u3001\u6b21\u306e3\u3064\u306e\u30b1\u30fc\u30b9\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u552f\u4e00\u89e3: 2\u3064\u306e\u76f4\u7dda\u304c1\u70b9\u3067\u4ea4\u308f\u308b\uff08\u4e00\u822c\u7684\u306a\u30b1\u30fc\u30b9\uff09</li> <li>\u7121\u6570\u306e\u89e3: 2\u3064\u306e\u76f4\u7dda\u304c\u4e00\u81f4\u3059\u308b\uff08\u7121\u9650\u500b\u306e\u89e3\uff09</li> <li>\u89e3\u306a\u3057: 2\u3064\u306e\u76f4\u7dda\u304c\u5e73\u884c\u3067\u4ea4\u308f\u3089\u306a\u3044</li> </ol>"},{"location":"lectures/LA/10-system-of-linear-equation/#422-31","title":"4.2.2 3\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":"<p>3\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306f3\u6b21\u5143\u7a7a\u9593\u5185\u306e\u5e73\u9762\u3068\u3057\u3066\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> <ul> <li>\u5404\u65b9\u7a0b\u5f0f\u306f3\u6b21\u5143\u7a7a\u9593\u5185\u306e1\u3064\u306e\u5e73\u9762\u3092\u8868\u3057\u307e\u3059</li> <li>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306f\u3001\u3053\u308c\u3089\u306e\u5e73\u9762\u306e\u5171\u901a\u90e8\u5206\uff08\u4ea4\u70b9\u3084\u4ea4\u7dda\uff09\u306b\u5bfe\u5fdc\u3057\u307e\u3059</li> </ul> <p>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u306b\u3064\u3044\u3066\u306f\u3001\u6b21\u306e\u30b1\u30fc\u30b9\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u552f\u4e00\u89e3: 3\u3064\u306e\u5e73\u9762\u304c1\u70b9\u3067\u4ea4\u308f\u308b</li> <li>\u7121\u6570\u306e\u89e3: 3\u3064\u306e\u5e73\u9762\u304c\u4e00\u76f4\u7dda\u307e\u305f\u306f\u5e73\u9762\u3067\u4ea4\u308f\u308b</li> <li>\u89e3\u306a\u3057: 3\u3064\u306e\u5e73\u9762\u306b\u5171\u901a\u90e8\u5206\u304c\u306a\u3044</li> </ol>"},{"location":"lectures/LA/10-system-of-linear-equation/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/10-system-of-linear-equation/#51-numpy","title":"5.1 NumPy\u3092\u7528\u3044\u305f\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5","text":"<p>NumPy\u30e9\u30a4\u30d6\u30e9\u30ea\u306e<code>numpy.linalg.solve</code>\u95a2\u6570\u3092\u4f7f\u3046\u3068\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u7c21\u5358\u306b\u89e3\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u4fc2\u6570\u884c\u5217\u3068\u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30eb\nA = np.array([[2, 3], [4, -1]])\nb = np.array([8, 2])\n\n# \u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\nx = np.linalg.solve(A, b)\nprint(f\"\u89e3\u306f x = {x[0]}, y = {x[1]}\")\n</code></pre> <p>\u51fa\u529b: <pre><code>\u89e3\u306f x = 1.0, y = 2.0\n</code></pre></p>"},{"location":"lectures/LA/10-system-of-linear-equation/#52-21","title":"5.2 2\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u53ef\u8996\u5316","text":"<p>2\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u3092\u8996\u899a\u5316\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u76f4\u7dda\u3092\u63cf\u753b\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\ndef plot_line(a, b, c, label):\n    \"\"\"ax + by = c \u306e\u76f4\u7dda\u3092\u63cf\u753b\"\"\"\n    if b != 0:\n        x = np.linspace(-2, 4, 100)\n        y = (c - a * x) / b\n    else:\n        y = np.linspace(-2, 4, 100)\n        x = c / a * np.ones_like(y)\n    plt.plot(x, y, label=label)\n\n# \u30b0\u30e9\u30d5\u306e\u8a2d\u5b9a\nplt.figure(figsize=(8, 6))\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nplt.grid(True, alpha=0.3)\n\n# 2\u3064\u306e\u76f4\u7dda\u3092\u63cf\u753b\nplot_line(2, 3, 8, '2x + 3y = 8')\nplot_line(4, -1, 2, '4x - y = 2')\n\n# \u89e3\u3092\u70b9\u3067\u30d7\u30ed\u30c3\u30c8\nx_sol = 1\ny_sol = 2\nplt.scatter(x_sol, y_sol, color='red', s=100, label='\u89e3 (1, 2)')\n\nplt.xlim(-2, 4)\nplt.ylim(-2, 4)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('\u9023\u7acb\u65b9\u7a0b\u5f0f 2x + 3y = 8, 4x - y = 2 \u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/10-system-of-linear-equation/#53-31","title":"5.3 3\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u4f8b\u3068\u89e3\u6cd5","text":"<pre><code>import numpy as np\n\n# 3\u5143\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u4f8b\n# 2x + y - z = 8\n# -3x + 4y + 2z = -2\n# x + 2y + 3z = 11\n\n# \u4fc2\u6570\u884c\u5217\u3068\u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30eb\nA = np.array([[2, 1, -1], [-3, 4, 2], [1, 2, 3]])\nb = np.array([8, -2, 11])\n\n# \u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\nx = np.linalg.solve(A, b)\nprint(f\"\u89e3\u306f x = {x[0]}, y = {x[1]}, z = {x[2]}\")\n</code></pre> <p>\u51fa\u529b: <pre><code>\u89e3\u306f x = 2.0, y = 1.0, z = 3.0\n</code></pre></p>"},{"location":"lectures/LA/10-system-of-linear-equation/#54","title":"5.4 \u69d8\u3005\u306a\u30b1\u30fc\u30b9\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f","text":""},{"location":"lectures/LA/10-system-of-linear-equation/#541","title":"5.4.1 \u552f\u4e00\u89e3\u3092\u6301\u3064\u5834\u5408","text":"<pre><code>import numpy as np\n\n# \u552f\u4e00\u89e3\u3092\u6301\u3064\u9023\u7acb\u65b9\u7a0b\u5f0f\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 11])\n\ntry:\n    x = np.linalg.solve(A, b)\n    print(f\"\u552f\u4e00\u89e3: x = {x[0]}, y = {x[1]}\")\nexcept np.linalg.LinAlgError as e:\n    print(\"\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u304b\u3001\u4e00\u610f\u306b\u5b9a\u307e\u308a\u307e\u305b\u3093:\", e)\n</code></pre> <p>\u51fa\u529b: <pre><code>\u552f\u4e00\u89e3: x = 1.0, y = 2.0\n</code></pre></p>"},{"location":"lectures/LA/10-system-of-linear-equation/#542","title":"5.4.2 \u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408","text":"<pre><code>import numpy as np\n\n# \u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u9023\u7acb\u65b9\u7a0b\u5f0f\uff08\u5e73\u884c\u306a\u76f4\u7dda\uff09\nA = np.array([[1, 2], [1, 2]])\nb = np.array([5, 6])  # \u7570\u306a\u308b\u5b9a\u6570\u9805\n\ntry:\n    x = np.linalg.solve(A, b)\n    print(f\"\u89e3: x = {x[0]}, y = {x[1]}\")\nexcept np.linalg.LinAlgError as e:\n    print(\"\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u304b\u3001\u4e00\u610f\u306b\u5b9a\u307e\u308a\u307e\u305b\u3093:\", e)\n</code></pre> <p>\u51fa\u529b: <pre><code>\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u304b\u3001\u4e00\u610f\u306b\u5b9a\u307e\u308a\u307e\u305b\u3093: Singular matrix\n</code></pre></p>"},{"location":"lectures/LA/10-system-of-linear-equation/#543","title":"5.4.3 \u7121\u6570\u306e\u89e3\u3092\u6301\u3064\u5834\u5408","text":"<p>\u7121\u6570\u306e\u89e3\u3092\u6301\u3064\u5834\u5408\u306f\u3001<code>numpy.linalg.solve</code>\u3067\u306f\u76f4\u63a5\u6271\u3048\u307e\u305b\u3093\u3002\u4ee3\u308f\u308a\u306b\u3001\u4e00\u822c\u89e3\u3092\u6c42\u3081\u308b\u65b9\u6cd5\u3068\u3057\u3066<code>numpy.linalg.lstsq</code>\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002</p> <pre><code>import numpy as np\n\n# \u7121\u6570\u306e\u89e3\u3092\u6301\u3064\u9023\u7acb\u65b9\u7a0b\u5f0f\uff08\u540c\u3058\u76f4\u7dda\u3092\u8868\u3059\u65b9\u7a0b\u5f0f\uff09\nA = np.array([[1, 2], [2, 4]])\nb = np.array([5, 10])\n\n# \u30e9\u30f3\u30af\u3092\u78ba\u8a8d\nrank_A = np.linalg.matrix_rank(A)\nrank_Ab = np.linalg.matrix_rank(np.column_stack([A, b]))\n\nif rank_A == rank_Ab and rank_A &lt; A.shape[1]:\n    print(\"\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\")\n    # \u6700\u5c0f\u4e8c\u4e57\u89e3\u3092\u6c42\u3081\u308b\uff08\u4e00\u822c\u89e3\u306e\u4e00\u4f8b\uff09\n    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n    print(f\"\u4e00\u822c\u89e3\u306e\u4e00\u4f8b: x = {x[0]}, y = {x[1]}\")\nelse:\n    try:\n        x = np.linalg.solve(A, b)\n        print(f\"\u552f\u4e00\u89e3: x = {x[0]}, y = {x[1]}\")\n    except np.linalg.LinAlgError:\n        print(\"\u89e3\u304c\u5b58\u5728\u3057\u307e\u305b\u3093\")\n</code></pre> <p>\u51fa\u529b: <pre><code>\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\n\u4e00\u822c\u89e3\u306e\u4e00\u4f8b: x = 1.0, y = 2.0\n</code></pre></p>"},{"location":"lectures/LA/10-system-of-linear-equation/#6","title":"6. \u5065\u5eb7\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308b\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u5fdc\u7528","text":"<p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306f\u3055\u307e\u3056\u307e\u306a\u5834\u9762\u3067\u6d3b\u7528\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#61","title":"6.1 \u85ac\u7269\u52d5\u614b\u30e2\u30c7\u30eb","text":"<p>\u85ac\u7269\u52d5\u614b\u5b66\u3067\u306f\u3001\u85ac\u7269\u304c\u4f53\u5185\u3067\u3069\u306e\u3088\u3046\u306b\u5438\u53ce\u3001\u5206\u5e03\u3001\u4ee3\u8b1d\u3001\u6392\u6cc4\u3055\u308c\u308b\u304b\u3092\u6570\u5b66\u7684\u306b\u30e2\u30c7\u30eb\u5316\u3057\u307e\u3059\u3002\u6700\u3082\u57fa\u672c\u7684\u306a\u30b3\u30f3\u30d1\u30fc\u30c8\u30e1\u30f3\u30c8\u30e2\u30c7\u30eb\u3067\u306f\u3001\u9023\u7acb\u5fae\u5206\u65b9\u7a0b\u5f0f\u304c\u7528\u3044\u3089\u308c\u3001\u305d\u306e\u5b9a\u5e38\u72b6\u614b\u89e3\u6790\u306b\u306f\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u4f8b: 2\u30b3\u30f3\u30d1\u30fc\u30c8\u30e1\u30f3\u30c8\u30e2\u30c7\u30eb</p> <p>\u3042\u308b\u85ac\u7269\u304c\u8840\u6db2\uff08\u30b3\u30f3\u30d1\u30fc\u30c8\u30e1\u30f3\u30c81\uff09\u3068\u7d44\u7e54\uff08\u30b3\u30f3\u30d1\u30fc\u30c8\u30e1\u30f3\u30c82\uff09\u306e\u9593\u3067\u79fb\u52d5\u3059\u308b\u30e2\u30c7\u30eb\u3092\u8003\u3048\u307e\u3059\uff1a</p> \\[ \\begin{cases} k_{12}x_1 - k_{21}x_2 = 0\\\\ x_1 + x_2 = D \\end{cases} \\] <p>\u3053\u3053\u3067\u3001\\(k_{12}\\)\u306f\u8840\u6db2\u304b\u3089\u7d44\u7e54\u3078\u306e\u79fb\u884c\u901f\u5ea6\u5b9a\u6570\u3001\\(k_{21}\\)\u306f\u7d44\u7e54\u304b\u3089\u8840\u6db2\u3078\u306e\u79fb\u884c\u901f\u5ea6\u5b9a\u6570\u3001\\(D\\)\u306f\u6295\u4e0e\u3055\u308c\u305f\u85ac\u7269\u306e\u7dcf\u91cf\u3067\u3059\u3002</p> <p>\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3053\u3068\u3067\u3001\u5b9a\u5e38\u72b6\u614b\u306b\u304a\u3051\u308b\u5404\u30b3\u30f3\u30d1\u30fc\u30c8\u30e1\u30f3\u30c8\u306e\u85ac\u7269\u91cf\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <pre><code>import numpy as np\n\n# \u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\nk_12 = 0.3  # \u8840\u6db2\u304b\u3089\u7d44\u7e54\u3078\u306e\u79fb\u884c\u901f\u5ea6\u5b9a\u6570\nk_21 = 0.2  # \u7d44\u7e54\u304b\u3089\u8840\u6db2\u3078\u306e\u79fb\u884c\u901f\u5ea6\u5b9a\u6570\nD = 100     # \u85ac\u7269\u306e\u7dcf\u91cf(mg)\n\n# \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u4fc2\u6570\u884c\u5217\u3068\u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30eb\nA = np.array([[k_12, -k_21], [1, 1]])\nb = np.array([0, D])\n\n# \u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\nx = np.linalg.solve(A, b)\nprint(f\"\u8840\u6db2\u4e2d\u306e\u85ac\u7269\u91cf: {x[0]:.1f} mg\")\nprint(f\"\u7d44\u7e54\u4e2d\u306e\u85ac\u7269\u91cf: {x[1]:.1f} mg\")\n</code></pre> <p>\u51fa\u529b: <pre><code>\u8840\u6db2\u4e2d\u306e\u85ac\u7269\u91cf: 40.0 mg\n\u7d44\u7e54\u4e2d\u306e\u85ac\u7269\u91cf: 60.0 mg\n</code></pre></p>"},{"location":"lectures/LA/10-system-of-linear-equation/#62","title":"6.2 \u6804\u990a\u7d20\u30d0\u30e9\u30f3\u30b9\u306e\u6700\u9069\u5316","text":"<p>\u6804\u990a\u8a08\u753b\u3067\u306f\u3001\u8907\u6570\u306e\u6804\u990a\u7d20\u8981\u4ef6\u3092\u6e80\u305f\u3059\u98df\u4e8b\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u6c42\u3081\u308b\u554f\u984c\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p> <p>\u4f8b: 3\u7a2e\u985e\u306e\u98df\u54c1\u304b\u30892\u7a2e\u985e\u306e\u6804\u990a\u7d20\u8981\u4ef6\u3092\u6e80\u305f\u3059\u554f\u984c</p> \\[ \\begin{cases} a_{11}x_1 + a_{12}x_2 + a_{13}x_3 = b_1\\\\ a_{21}x_1 + a_{22}x_2 + a_{23}x_3 = b_2 \\end{cases} \\] <p>\u3053\u3053\u3067\u3001\\(a_{ij}\\)\u306f\u98df\u54c1\\(j\\)\u306b\u542b\u307e\u308c\u308b\u6804\u990a\u7d20\\(i\\)\u306e\u91cf\u3001\\(x_j\\)\u306f\u98df\u54c1\\(j\\)\u306e\u6442\u53d6\u91cf\u3001\\(b_i\\)\u306f\u6804\u990a\u7d20\\(i\\)\u306e\u5fc5\u8981\u91cf\u3067\u3059\u3002</p> <pre><code>import numpy as np\nfrom scipy.optimize import nnls\n\n# \u98df\u54c1\u3054\u3068\u306e\u6804\u990a\u7d20\u542b\u6709\u91cf\uff08100g\u3042\u305f\u308a\uff09\n# \u98df\u54c1: [\u30bf\u30f3\u30d1\u30af\u8cea(g), \u30ab\u30eb\u30b7\u30a6\u30e0(mg)]\nfood_nutrition = np.array([\n    [20, 10],    # \u98df\u54c11: \u9d8f\u8089\n    [5, 120],    # \u98df\u54c12: \u725b\u4e73\n    [2, 30]      # \u98df\u54c13: \u91ce\u83dc\n])\n\n# 1\u65e5\u306e\u5fc5\u8981\u6804\u990a\u7d20\u91cf\nrequired_nutrition = np.array([60, 800])  # [\u30bf\u30f3\u30d1\u30af\u8cea(g), \u30ab\u30eb\u30b7\u30a6\u30e0(mg)]\n\n# \u6700\u5c0f\u4e8c\u4e57\u6cd5\u3067\u975e\u8ca0\u306e\u89e3\u3092\u6c42\u3081\u308b\uff08\u73fe\u5b9f\u7684\u306b\u306f\u98df\u54c1\u306e\u6442\u53d6\u91cf\u306f\u975e\u8ca0\uff09\namounts, residual = nnls(food_nutrition.T, required_nutrition)\n\nprint(\"\u5404\u98df\u54c1\u306e\u6700\u9069\u6442\u53d6\u91cf\uff08g\uff09:\")\nfood_names = [\"\u9d8f\u8089\", \"\u725b\u4e73\", \"\u91ce\u83dc\"]\nfor i, name in enumerate(food_names):\n    print(f\"{name}: {amounts[i]:.1f}g\")\n\n# \u5b9f\u969b\u306b\u6442\u53d6\u3055\u308c\u308b\u6804\u990a\u7d20\u91cf\u3092\u8a08\u7b97\nactual_nutrition = food_nutrition.T @ amounts\nprint(\"\\n\u5b9f\u969b\u306e\u6804\u990a\u7d20\u6442\u53d6\u91cf:\")\nprint(f\"\u30bf\u30f3\u30d1\u30af\u8cea: {actual_nutrition[0]:.1f}g\")\nprint(f\"\u30ab\u30eb\u30b7\u30a6\u30e0: {actual_nutrition[1]:.1f}mg\")\n</code></pre> <p>\u51fa\u529b: <pre><code>\u5404\u98df\u54c1\u306e\u6700\u9069\u6442\u53d6\u91cf\uff08g\uff09:\n\u9d8f\u8089: 275.0g\n\u725b\u4e73: 633.3g\n\u91ce\u83dc: 0.0g\n\n\u5b9f\u969b\u306e\u6804\u990a\u7d20\u6442\u53d6\u91cf:\n\u30bf\u30f3\u30d1\u30af\u8cea: 60.0g\n\u30ab\u30eb\u30b7\u30a6\u30e0: 800.0mg\n</code></pre></p>"},{"location":"lectures/LA/10-system-of-linear-equation/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/10-system-of-linear-equation/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304d\u306a\u3055\u3044\u3002    $$    \\begin{cases}    3x + 2y = 7\\    x - 4y = 9    \\end{cases}    $$</p> </li> <li> <p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u884c\u5217\u8868\u73fe\u3057\u306a\u3055\u3044\u3002    $$    \\begin{cases}    2x - 3y + z = 7\\    5x + y - 2z = 4\\    -x + 4y + 3z = 10    \\end{cases}    $$</p> </li> <li> <p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304d\u306a\u3055\u3044\u3002    $$    \\begin{cases}    x + 2y + 3z = 14\\    2x - y + z = 4\\    3x + y - z = 2    \\end{cases}    $$</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u65b9\u7a0b\u5f0f\u3092\u89e3\u304d\u306a\u3055\u3044\u3002    $$    \\begin{pmatrix}    2 &amp; 5\\    1 &amp; 3    \\end{pmatrix}    \\begin{pmatrix}    x\\    y    \\end{pmatrix}    =    \\begin{pmatrix}    16\\    10    \\end{pmatrix}    $$</p> </li> </ol>"},{"location":"lectures/LA/10-system-of-linear-equation/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u3068\u4e00\u610f\u6027\u3092\u5224\u5b9a\u3057\u3001\u89e3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u6c42\u3081\u306a\u3055\u3044\u3002    $$    \\begin{cases}    2x + 4y = 6\\    x + 2y = 3    \\end{cases}    $$</p> </li> <li> <p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u3068\u4e00\u610f\u6027\u3092\u5224\u5b9a\u3057\u3001\u89e3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u6c42\u3081\u306a\u3055\u3044\u3002    $$    \\begin{cases}    2x + 4y = 6\\    x + 2y = 4    \\end{cases}    $$</p> </li> <li> <p>\u5065\u5eb7\u98df\u54c1\u306e\u6804\u990a\u7d20\u8a08\u7b97   \u3042\u308b\u30c0\u30a4\u30a8\u30c3\u30c8\u98df\u54c1\u4f1a\u793e\u304c\u30012\u7a2e\u985e\u306e\u5065\u5eb7\u98df\u54c1\uff08\u88fd\u54c1A\u3068B\uff09\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u6804\u990a\u30d0\u30e9\u30f3\u30b9\u98df\u3092\u958b\u767a\u3057\u3066\u3044\u307e\u3059\u3002\u88fd\u54c1A\u306f1\u888b\u3042\u305f\u308a\u30bf\u30f3\u30d1\u30af\u8cea15g\u3001\u98df\u7269\u7e4a\u7dad5g\u3092\u542b\u307f\u3001\u4fa1\u683c\u306f300\u5186\u3067\u3059\u3002\u88fd\u54c1B\u306f1\u888b\u3042\u305f\u308a\u30bf\u30f3\u30d1\u30af\u8cea10g\u3001\u98df\u7269\u7e4a\u7dad8g\u3092\u542b\u307f\u3001\u4fa1\u683c\u306f250\u5186\u3067\u3059\u3002\u6804\u990a\u58eb\u306e\u6307\u5c0e\u306b\u3088\u308b\u3068\u30011\u65e5\u3042\u305f\u308a\u30bf\u30f3\u30d1\u30af\u8cea50g\u3068\u98df\u7269\u7e4a\u7dad30g\u3092\u6442\u53d6\u3059\u308b\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u3066\u3044\u307e\u3059\u30021\u65e5\u306e\u98df\u4e8b\u3067\u63a8\u5968\u6804\u990a\u7d20\u91cf\u3092\u3061\u3087\u3046\u3069\u6442\u53d6\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u88fd\u54c1A\u3068B\u3092\u305d\u308c\u305e\u308c\u4f55\u888b\u305a\u3064\u6442\u53d6\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304b\uff1f\u307e\u305f\u3001\u305d\u306e\u5834\u5408\u306e1\u65e5\u3042\u305f\u308a\u306e\u7dcf\u8cbb\u7528\u306f\u3044\u304f\u3089\u306b\u306a\u308a\u307e\u3059\u304b\uff1f</p> </li> <li> <p>\u85ac\u7269\u6295\u4e0e\u306e\u6700\u9069\u5316:    \u3042\u308b\u75c5\u9662\u306e\u85ac\u5264\u5e2b\u304c\u3001\u75bc\u75db\u7de9\u548c\u306e\u305f\u3081\u306b2\u7a2e\u985e\u306e\u93ae\u75db\u85ac\uff08\u85ac\u5264X\u3068Y\uff09\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u691c\u8a0e\u3057\u3066\u3044\u307e\u3059\u3002\u81e8\u5e8a\u7814\u7a76\u304b\u3089\u3001\u52b9\u679c\u7684\u306a\u75bc\u75db\u7de9\u548c\u3068\u526f\u4f5c\u7528\u306e\u767a\u751f\u7387\u3092\u6700\u5c0f\u5316\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u5fc5\u8981\u304c\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u3066\u3044\u307e\u3059\uff1a</p> <ul> <li>\u85ac\u5264X\u306e\u6d3b\u6027\u6210\u5206\u3068\u85ac\u5264Y\u306e\u6d3b\u6027\u6210\u5206\u306e\u5408\u8a08\u304c40mg\u306b\u306a\u308b\u3088\u3046\u306b\u3059\u308b</li> <li>\u85ac\u5264X\u306e\u6d3b\u6027\u6210\u5206\u304c\u85ac\u5264Y\u306e\u6d3b\u6027\u6210\u5206\u306e60%\u306b\u306a\u308b\u3088\u3046\u306b\u3059\u308b  </li> </ul> </li> </ol> <p>\u85ac\u5264X\u306f1\u9320\u3042\u305f\u308a5mg\u306e\u6d3b\u6027\u6210\u5206\u3092\u542b\u307f\u3001\u85ac\u5264Y\u306f1\u9320\u3042\u305f\u308a10mg\u306e\u6d3b\u6027\u6210\u5206\u3092\u542b\u3093\u3067\u3044\u307e\u3059\u3002\u60a3\u8005\u306b\u51e6\u65b9\u3059\u3079\u304d\u85ac\u5264X\u3068Y\u306e\u9320\u6570\u3092\u305d\u308c\u305e\u308c\u6c42\u3081\u306a\u3055\u3044\u3002</p> <ol> <li>\u5065\u5eb7\u8a3a\u65ad\u30bb\u30f3\u30bf\u30fc:    \u5065\u5eb7\u8a3a\u65ad\u30bb\u30f3\u30bf\u30fc\u3067\u306f\u3001\u3042\u308b\u5730\u57df\u306e\u4f4f\u6c11\u306eBMI\uff08\u4f53\u683c\u6307\u6570\uff09\u3068\u5e74\u9f62\u306e\u95a2\u4fc2\u3092\u8abf\u67fb\u3057\u3066\u3044\u307e\u3059\u3002\u8abf\u67fb\u306e\u904e\u7a0b\u3067\u3001\u7279\u5b9a\u306e2\u3064\u306e\u5e74\u9f62\u30b0\u30eb\u30fc\u30d7\uff08A\u3068B\uff09\u306b\u3064\u3044\u3066\u4ee5\u4e0b\u306e\u60c5\u5831\u304c\u5f97\u3089\u308c\u307e\u3057\u305f\uff1a\u30b0\u30eb\u30fc\u30d7A\u3068\u30b0\u30eb\u30fc\u30d7B\u306e\u305d\u308c\u305e\u308c\u306e\u4eba\u6570\u3092\u6c42\u3081\u306a\u3055\u3044\u3002<ul> <li>\u30b0\u30eb\u30fc\u30d7A\u3068\u30b0\u30eb\u30fc\u30d7B\u306e\u5e73\u5747BMI\u306f\u305d\u308c\u305e\u308c23.5\u306826.8\u3067\u3042\u308b</li> <li>\u30b0\u30eb\u30fc\u30d7A\u3068\u30b0\u30eb\u30fc\u30d7B\u3092\u5408\u308f\u305b\u305f\u5168\u4f53\u306e\u5e73\u5747BMI\u306f25.7\u3067\u3042\u308b</li> <li>\u30b0\u30eb\u30fc\u30d7B\u306e\u4eba\u6570\u306f\u30b0\u30eb\u30fc\u30d7A\u306e2\u500d\u3067\u3042\u308b</li> <li>\u5168\u4f53\u306e\u8abf\u67fb\u5bfe\u8c61\u8005\u6570\u306f300\u4eba\u3067\u3042\u308b</li> </ul> </li> </ol>"},{"location":"lectures/LA/10-system-of-linear-equation/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/10-system-of-linear-equation/#q1","title":"Q1: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306b\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u3068\u306f\u3069\u3046\u3044\u3046\u3053\u3068\u3067\u3059\u304b\uff1f","text":"<p>A1: \u5e7e\u4f55\u5b66\u7684\u306b\u306f\u30012\u5143\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u5834\u5408\u30012\u3064\u306e\u76f4\u7dda\u304c\u5e73\u884c\u3067\u4ea4\u308f\u3089\u306a\u3044\u72b6\u614b\u3092\u8868\u3057\u307e\u3059\u3002\u65b9\u7a0b\u5f0f\u306e\u89b3\u70b9\u304b\u3089\u306f\u3001\u77db\u76fe\u3057\u305f\u6761\u4ef6\u304c\u4e0e\u3048\u3089\u308c\u3066\u3044\u308b\u72b6\u614b\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\\(x + y = 2\\) \u3068 \\(x + y = 3\\) \u304c\u540c\u6642\u306b\u6210\u308a\u7acb\u3064\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#q2","title":"Q2: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306b\u7121\u6570\u306e\u89e3\u304c\u3042\u308b\u3068\u306f\u3069\u3046\u3044\u3046\u3053\u3068\u3067\u3059\u304b\uff1f","text":"<p>A2: 2\u5143\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u5834\u5408\u30012\u3064\u306e\u76f4\u7dda\u304c\u5b8c\u5168\u306b\u91cd\u306a\u3063\u3066\u3044\u308b\u72b6\u614b\u3092\u8868\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\\(x + y = 2\\) \u3068 \\(2x + 2y = 4\\) \u306f\u540c\u3058\u76f4\u7dda\u3092\u8868\u3059\u305f\u3081\u3001\u3053\u306e\u76f4\u7dda\u4e0a\u306e\u7121\u6570\u306e\u70b9\u304c\u89e3\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#q3","title":"Q3: \u306a\u305c\u884c\u5217\u3067\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u8868\u73fe\u3059\u308b\u306e\u3067\u3059\u304b\uff1f","text":"<p>A3: \u884c\u5217\u8868\u73fe\u306b\u306f\u4ee5\u4e0b\u306e\u30e1\u30ea\u30c3\u30c8\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u7c21\u6f54\u306b\u8868\u8a18\u3067\u304d\u308b 2. \u884c\u5217\u306e\u6f14\u7b97\u6cd5\u5247\u3092\u7528\u3044\u3066\u52b9\u7387\u7684\u306b\u89e3\u3051\u308b 3. \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3067\u306e\u5b9f\u88c5\u304c\u5bb9\u6613 4. \u7406\u8ad6\u7684\u306a\u89e3\u6790\u304c\u3057\u3084\u3059\u3044</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#q4","title":"Q4: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u6570\u3068\u65b9\u7a0b\u5f0f\u306e\u6570\u3001\u5909\u6570\u306e\u6570\u306e\u95a2\u4fc2\u306f\uff1f","text":"<p>A4: \u4e00\u822c\u7684\u306b\u3001\\(n\\)\u500b\u306e\u5909\u6570\u306b\u5bfe\u3057\u3066\\(n\\)\u500b\u306e\u72ec\u7acb\u3057\u305f\u65b9\u7a0b\u5f0f\u304c\u3042\u308c\u3070\u3001\u552f\u4e00\u89e3\u3092\u6301\u3064\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u65b9\u7a0b\u5f0f\u306e\u6570\u304c\u5909\u6570\u306e\u6570\u3088\u308a\u5c11\u306a\u3044\u5834\u5408\u306f\u3001\u901a\u5e38\u3001\u7121\u6570\u306e\u89e3\u3092\u6301\u3061\u307e\u3059\u3002\u65b9\u7a0b\u5f0f\u306e\u6570\u304c\u5909\u6570\u306e\u6570\u3088\u308a\u591a\u3044\u5834\u5408\u3001\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u3053\u3068\u304c\u591a\u3044\u3067\u3059\u304c\u3001\u65b9\u7a0b\u5f0f\u306b\u5197\u9577\u6027\u304c\u3042\u308b\u5834\u5408\u306f\u89e3\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/10-system-of-linear-equation/#q5","title":"Q5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u6587\u8108\u3067\u306f\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u3069\u306e\u3088\u3046\u306b\u6d3b\u7528\u3055\u308c\u307e\u3059\u304b\uff1f","text":"<p>A7: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5834\u9762\u3067\u9023\u7acb\u65b9\u7a0b\u5f0f\u304c\u6d3b\u7528\u3055\u308c\u307e\u3059\uff1a 1. \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\uff08\u6700\u5c0f\u4e8c\u4e57\u6cd5\uff09 2. \u591a\u5909\u91cf\u89e3\u6790\uff08\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\uff09 3. \u4fe1\u53f7\u51e6\u7406\uff08\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\uff09 4. \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5206\u6790\uff08\u5747\u8861\u72b6\u614b\u306e\u89e3\u6790\uff09 5. \u6700\u9069\u5316\u554f\u984c\uff08\u7dda\u5f62\u8a08\u753b\u6cd5\uff09 \u7279\u306b\u6b21\u56de\u4ee5\u964d\u306e\u8b1b\u7fa9\u3067\u6271\u3046\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u77e5\u8b58\u304c\u76f4\u63a5\u5fdc\u7528\u3055\u308c\u308b\u91cd\u8981\u306a\u4f8b\u3067\u3059\u3002</p>"},{"location":"lectures/LA/11-system-of-linear-equation/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c11\u56de \u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/11-system-of-linear-equation/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c11\u56de \u30c6\u30fc\u30de: \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3068\u89e3\u306e\u63a2\u7d22 \u95a2\u9023\u9805\u76ee: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3001\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9:  - \u7b2c10\u56de\u306e\u5185\u5bb9\uff08\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe\uff09 - \u884c\u5217\u306e\u57fa\u672c\u64cd\u4f5c\u306e\u6982\u5ff5</p>"},{"location":"lectures/LA/11-system-of-linear-equation/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u4ee5\u4e0b\u306e\u80fd\u529b\u3092\u8eab\u306b\u3064\u3051\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3057\u307e\u3059:</p> <ol> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3059\u308b</li> <li>\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u306e\u7a2e\u985e\u3068\u6027\u8cea\u3092\u7406\u89e3\u3059\u308b</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u7406\u89e3\u3057\u5b9f\u884c\u3067\u304d\u308b</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u6d88\u53bb\u6cd5\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3059\u308b</li> <li>Google Colaboratory\u3092\u7528\u3044\u3066\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3053\u3068\u304c\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/11-system-of-linear-equation/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/11-system-of-linear-equation/#31","title":"3.1 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\uff08\u5fa9\u7fd2\uff09","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306f\u884c\u5217\u3068\u4fc2\u6570\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u7c21\u6f54\u306b\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4e00\u822c\u7684\u306a\u5f62\u5f0f\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059:</p> \\[ \\begin{cases} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1 \\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2 \\\\ \\vdots \\\\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n = b_m \\end{cases} \\] <p>\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u884c\u5217\u5f62\u5f0f\u3067\u6b21\u306e\u3088\u3046\u306b\u8868\u73fe\u3067\u304d\u307e\u3059:</p> \\[\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\] <p>\u3053\u3053\u3067\u3001</p> \\[\\mathbf{A} =  \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}, \\quad \\mathbf{x} =  \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\quad \\mathbf{b} =  \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix} \\] <p>\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u969b\u306b\u306f\u3001\u4fc2\u6570\u884c\u5217\u3068\u5b9a\u6570\u9805\u3092\u307e\u3068\u3081\u305f\u62e1\u5927\u4fc2\u6570\u884c\u5217\uff08augmented matrix\uff09\u3092\u8003\u3048\u308b\u3068\u4fbf\u5229\u3067\u3059:</p> \\[\\left(\\mathbf{A} | \\mathbf{b}\\right) =  \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; | &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} &amp; | &amp; b_m \\end{pmatrix} \\]"},{"location":"lectures/LA/11-system-of-linear-equation/#32","title":"3.2 \u884c\u5217\u306e\u57fa\u672c\u5909\u5f62","text":"<p>\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u306f\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u5909\u3048\u305a\u306b\u884c\u5217\u306e\u5f62\u3092\u5909\u3048\u308b\u64cd\u4f5c\u3067\u3059\u3002\u4e3b\u306b3\u7a2e\u985e\u306e\u57fa\u672c\u5909\u5f62\u304c\u3042\u308a\u307e\u3059:</p> <p>\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u306e\u5b9a\u7fa9: 1. \u884c\u306e\u4ea4\u63db\uff08Row Exchange\uff09: 2\u3064\u306e\u884c\u3092\u5165\u308c\u66ff\u3048\u308b 2. \u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d\uff08Row Scaling\uff09: \u3042\u308b\u884c\u306e\u5168\u3066\u306e\u8981\u7d20\u30920\u3067\u306a\u3044\u5b9a\u6570\u500d\u3059\u308b 3. \u884c\u306e\u52a0\u6e1b\uff08Row Addition\uff09: \u3042\u308b\u884c\u306e\u5b9a\u6570\u500d\u3092\u5225\u306e\u884c\u306b\u52a0\u3048\u308b</p> <p>\u3053\u308c\u3089\u306e\u57fa\u672c\u5909\u5f62\u306f\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u5909\u3048\u306a\u3044\u3068\u3044\u3046\u91cd\u8981\u306a\u6027\u8cea\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/11-system-of-linear-equation/#_1","title":"\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u306e\u5177\u4f53\u4f8b","text":"<ol> <li>\u884c\u306e\u4ea4\u63db\uff08Row Exchange\uff09:    \u884c\u5217\u306e2\u3064\u306e\u884c\u3092\u5165\u308c\u66ff\u3048\u308b\u64cd\u4f5c\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u7b2c1\u884c\u3068\u7b2c2\u884c\u3092\u4ea4\u63db\u3059\u308b\u3068:</li> </ol> <p>$$    \\begin{pmatrix}    1 &amp; 2 &amp; 3 \\    4 &amp; 5 &amp; 6 \\    7 &amp; 8 &amp; 9    \\end{pmatrix}    \\rightarrow    \\begin{pmatrix}    4 &amp; 5 &amp; 6 \\    1 &amp; 2 &amp; 3 \\    7 &amp; 8 &amp; 9    \\end{pmatrix}    $$</p> <p>\u3053\u306e\u3088\u3046\u306a\u64cd\u4f5c\u306f\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306b\u306f\u5f71\u97ff\u3057\u307e\u305b\u3093\u3002\u306a\u305c\u306a\u3089\u3001\u65b9\u7a0b\u5f0f\u306e\u9806\u5e8f\u3092\u5165\u308c\u66ff\u3048\u305f\u3060\u3051\u3067\u3001\u65b9\u7a0b\u5f0f\u306e\u5185\u5bb9\u81ea\u4f53\u306f\u5909\u308f\u3063\u3066\u3044\u306a\u3044\u304b\u3089\u3067\u3059\u3002</p> <ol> <li>\u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d\uff08Row Scaling\uff09:    \u884c\u5217\u306e\u3042\u308b\u884c\u306e\u5168\u3066\u306e\u8981\u7d20\u30920\u3067\u306a\u3044\u5b9a\u6570\u500d\u3059\u308b\u64cd\u4f5c\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u7b2c2\u884c\u30922\u500d\u3059\u308b\u3068:</li> </ol> <p>$$    \\begin{pmatrix}    1 &amp; 2 &amp; 3 \\    4 &amp; 5 &amp; 6 \\    7 &amp; 8 &amp; 9    \\end{pmatrix}    \\rightarrow    \\begin{pmatrix}    1 &amp; 2 &amp; 3 \\    8 &amp; 10 &amp; 12 \\    7 &amp; 8 &amp; 9    \\end{pmatrix}    $$</p> <p>\u3053\u308c\u306f\u9023\u7acb\u65b9\u7a0b\u5f0f\u3067\u306f\u30012\u756a\u76ee\u306e\u65b9\u7a0b\u5f0f\u306e\u4e21\u8fba\u30922\u500d\u3059\u308b\u3053\u3068\u306b\u76f8\u5f53\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070:</p> <p>\\(\\(4x + 5y + 6z = 10 \\quad \\rightarrow \\quad 8x + 10y + 12z = 20\\)\\)</p> <p>\u65b9\u7a0b\u5f0f\u306e\u4e21\u8fba\u3092\u540c\u3058\u5b9a\u6570\u500d\u3057\u3066\u3082\u3001\u89e3\u306f\u5909\u308f\u308a\u307e\u305b\u3093\u3002</p> <ol> <li>\u884c\u306e\u52a0\u6e1b\uff08Row Addition\uff09:    \u3042\u308b\u884c\u306e\u5b9a\u6570\u500d\u3092\u5225\u306e\u884c\u306b\u52a0\u3048\u308b\u64cd\u4f5c\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u7b2c1\u884c\u306e(-4)\u500d\u3092\u7b2c2\u884c\u306b\u52a0\u3048\u308b\u3068:</li> </ol> <p>$$    \\begin{pmatrix}    1 &amp; 2 &amp; 3 \\    4 &amp; 5 &amp; 6 \\    7 &amp; 8 &amp; 9    \\end{pmatrix}    \\rightarrow    \\begin{pmatrix}    1 &amp; 2 &amp; 3 \\    0 &amp; -3 &amp; -6 \\    7 &amp; 8 &amp; 9    \\end{pmatrix}    $$</p> <p>\u3053\u308c\u306f\u9023\u7acb\u65b9\u7a0b\u5f0f\u3067\u306f\u3001\u7b2c1\u65b9\u7a0b\u5f0f\u306e(-4)\u500d\u3092\u7b2c2\u65b9\u7a0b\u5f0f\u306b\u52a0\u3048\u308b\u3053\u3068\u306b\u76f8\u5f53\u3057\u307e\u3059:</p> <p>$$    \\begin{cases}    x + 2y + 3z = 10 \\    4x + 5y + 6z = 20    \\end{cases}    \\quad \\rightarrow \\quad    \\begin{cases}    x + 2y + 3z = 10 \\    -3y - 6z = -20    \\end{cases}    $$</p> <p>\u3053\u306e\u64cd\u4f5c\u306b\u3088\u308a\u3001\u7b2c2\u65b9\u7a0b\u5f0f\u304b\u3089x\u5909\u6570\u304c\u6d88\u53bb\u3055\u308c\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u5168\u4f53\u3068\u3057\u3066\u306f\u540c\u3058\u89e3\u3092\u6301\u3061\u307e\u3059\u3002</p> <p>\u3053\u308c\u3089\u306e\u57fa\u672c\u5909\u5f62\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u884c\u5217\u3092\u671b\u307e\u3057\u3044\u5f62\uff08\u4e0a\u4e09\u89d2\u5f62\u3084\u5bfe\u89d2\u5f62\u306a\u3069\uff09\u306b\u5909\u5f62\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u62e1\u5927\u4fc2\u6570\u884c\u5217\u3092\u8003\u3048\u307e\u3059:</p> \\[ \\begin{pmatrix} 1 &amp; 3 &amp; | &amp; 5 \\\\ 2 &amp; 7 &amp; | &amp; 11 \\end{pmatrix} \\] <p>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u5909\u3048\u305a\u306b\u3001\u7b2c1\u884c\u306e(-2)\u500d\u3092\u7b2c2\u884c\u306b\u52a0\u3048\u308b\u3068:</p> \\[ \\begin{pmatrix} 1 &amp; 3 &amp; | &amp; 5 \\\\ 0 &amp; 1 &amp; | &amp; 1 \\end{pmatrix} \\] <p>\u3053\u308c\u306f\u5143\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f:</p> \\[ \\begin{cases} x + 3y = 5 \\\\ 2x + 7y = 11 \\end{cases} \\] <p>\u304c\u4ee5\u4e0b\u306b\u5909\u5f62\u3055\u308c\u305f\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059:</p> \\[ \\begin{cases} x + 3y = 5 \\\\ y = 1 \\end{cases} \\] <p>\u3053\u306e\u5909\u5f62\u306b\u3088\u308a\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u5f8c\u9000\u4ee3\u5165\u3067\u7c21\u5358\u306b\u89e3\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/11-system-of-linear-equation/#33","title":"3.3 \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u6982\u5ff5","text":"<p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306f\u3001\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u62e1\u5927\u4fc2\u6570\u884c\u5217\u3092\u7279\u5b9a\u306e\u5f62\uff08\u4e0a\u4e09\u89d2\u5f62\u307e\u305f\u306f\u968e\u6bb5\u5f62\uff09\u306b\u5909\u5f62\u3059\u308b\u3053\u3068\u3067\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u3059\u3002</p> <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5: \u62e1\u5927\u4fc2\u6570\u884c\u5217\u306b\u57fa\u672c\u5909\u5f62\u3092\u65bd\u3057\u3001\u4fc2\u6570\u884c\u5217\u3092\u4e0a\u4e09\u89d2\u884c\u5217\u307e\u305f\u306f\u968e\u6bb5\u884c\u5217\u306b\u5909\u5f62\u3059\u308b\u65b9\u6cd5\u3002</p> <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u306f\u4e3b\u306b\u4e8c\u3064\u306e\u6bb5\u968e\u304c\u3042\u308a\u307e\u3059: 1. \u524d\u9032\u6d88\u53bb\uff08Forward Elimination\uff09: \u4fc2\u6570\u884c\u5217\u3092\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u5f62\u3059\u308b 2. \u5f8c\u9000\u4ee3\u5165\uff08Back Substitution\uff09: \u4e0a\u4e09\u89d2\u884c\u5217\u306e\u5f62\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f</p>"},{"location":"lectures/LA/11-system-of-linear-equation/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/11-system-of-linear-equation/#41","title":"4.1 \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u624b\u9806","text":"<p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059:</p> <ol> <li>\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u4f5c\u6210: \u9023\u7acb\u65b9\u7a0b\u5f0f\u304b\u3089\u62e1\u5927\u4fc2\u6570\u884c\u5217\u3092\u4f5c\u6210</li> <li>\u524d\u9032\u6d88\u53bb: </li> <li>\u5de6\u4e0a\u304b\u3089\u59cb\u3081\u3001\u305d\u306e\u5217\u306e\u5bfe\u89d2\u6210\u5206\uff08\u30d4\u30dc\u30c3\u30c8\uff09\u3092\u57fa\u6e96\u306b\u3059\u308b</li> <li>\u30d4\u30dc\u30c3\u30c8\u4ee5\u4e0b\u306e\u8981\u7d20\u3092\u3059\u3079\u30660\u306b\u3059\u308b\u3088\u3046\u306b\u57fa\u672c\u5909\u5f62\u3092\u884c\u3046</li> <li>\u6b21\u306e\u5217\u306b\u79fb\u52d5\u3057\u540c\u69d8\u306e\u64cd\u4f5c\u3092\u7e70\u308a\u8fd4\u3059</li> <li>\u5f8c\u9000\u4ee3\u5165: \u5909\u5f62\u3055\u308c\u305f\u65b9\u7a0b\u5f0f\u3092\u6700\u5f8c\u306e\u5909\u6570\u304b\u3089\u9806\u306b\u89e3\u3044\u3066\u3044\u304f</li> </ol>"},{"location":"lectures/LA/11-system-of-linear-equation/#42","title":"4.2 \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u8a73\u7d30\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0","text":"<p>\u3088\u308a\u6570\u5b66\u7684\u306b\u53b3\u5bc6\u306b\u8a18\u8ff0\u3059\u308b\u3068\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u524d\u9032\u6d88\u53bb\u6bb5\u968e\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059:</p> <ol> <li>\\(i = 1\\) \u304b\u3089 \\(n-1\\) \u307e\u3067\u4ee5\u4e0b\u306e\u64cd\u4f5c\u3092\u7e70\u308a\u8fd4\u3059:</li> <li>\u30d4\u30dc\u30c3\u30c8\u8981\u7d20 \\(a_{ii}\\) \u3092\u9078\u3076</li> <li>\\(j = i + 1\\) \u304b\u3089 \\(m\\) \u307e\u3067\u4ee5\u4e0b\u306e\u64cd\u4f5c\u3092\u884c\u3046:<ul> <li>\u4fc2\u6570 \\(\\mu_{ji} = a_{ji}/a_{ii}\\) \u3092\u8a08\u7b97</li> <li>\u884c \\(j\\) \u304b\u3089 \u884c \\(i\\) \u306e \\(\\mu_{ji}\\) \u500d\u3092\u5f15\u304f: \\(\\text{row}_j \\leftarrow \\text{row}_j - \\mu_{ji} \\cdot \\text{row}_i\\)</li> </ul> </li> </ol>"},{"location":"lectures/LA/11-system-of-linear-equation/#43","title":"4.3 \u5f8c\u9000\u4ee3\u5165\u306e\u624b\u9806","text":"<p>\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u5f62\u3055\u308c\u305f\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u305f\u3081\u306e\u5f8c\u9000\u4ee3\u5165\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u884c\u3044\u307e\u3059:</p> <ol> <li>\u6700\u5f8c\u306e\u5909\u6570 \\(x_n\\) \u3092\u8a08\u7b97: \\(x_n = b_n/a_{nn}\\)</li> <li>\\(i = n-1\\) \u304b\u3089 \\(1\\) \u307e\u3067\u9006\u9806\u306b\u4ee5\u4e0b\u3092\u8a08\u7b97:    \\(\\(x_i = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j=i+1}^{n}a_{ij}x_j\\right)\\)\\)</li> </ol>"},{"location":"lectures/LA/11-system-of-linear-equation/#44-3","title":"4.4 \u8a08\u7b97\u4f8b: 3\u5909\u6570\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f","text":"<p>\u4ee5\u4e0b\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u8003\u3048\u307e\u3059:</p> \\[ \\begin{cases} 2x + y - z = 8 \\\\ -3x - y + 2z = -11 \\\\ -2x + y + 2z = -3 \\end{cases} \\] <p>\u30b9\u30c6\u30c3\u30d7 1: \u62e1\u5927\u4fc2\u6570\u884c\u5217\u3092\u4f5c\u6210</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; -1 &amp; | &amp; 8 \\\\ -3 &amp; -1 &amp; 2 &amp; | &amp; -11 \\\\ -2 &amp; 1 &amp; 2 &amp; | &amp; -3 \\end{pmatrix} \\] <p>\u30b9\u30c6\u30c3\u30d7 2: \u524d\u9032\u6d88\u53bb\u3092\u884c\u3046</p> <p>\u307e\u305a\u3001\u7b2c1\u5217\u306e\u7b2c1\u884c\u4ee5\u4e0b\u306e\u8981\u7d20\u30920\u306b\u3057\u307e\u3059:</p> <ul> <li>\u7b2c2\u884c\u306b\u7b2c1\u884c\u306e\\(\\frac{3}{2}\\)\u500d\u3092\u52a0\u3048\u308b:</li> </ul> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; -1 &amp; | &amp; 8 \\\\ 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; | &amp; 1 \\\\ -2 &amp; 1 &amp; 2 &amp; | &amp; -3 \\end{pmatrix} \\] <ul> <li>\u7b2c3\u884c\u306b\u7b2c1\u884c\u306e1\u500d\u3092\u52a0\u3048\u308b:</li> </ul> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; -1 &amp; | &amp; 8 \\\\ 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; | &amp; 1 \\\\ 0 &amp; 2 &amp; 1 &amp; | &amp; 5 \\end{pmatrix} \\] <p>\u6b21\u306b\u3001\u7b2c2\u5217\u306e\u7b2c2\u884c\u4ee5\u4e0b\u306e\u8981\u7d20\u30920\u306b\u3057\u307e\u3059:</p> <ul> <li>\u7b2c3\u884c\u304b\u3089\u7b2c2\u884c\u306e4\u500d\u3092\u5f15\u304f:</li> </ul> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; -1 &amp; | &amp; 8 \\\\ 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; | &amp; 1 \\\\ 0 &amp; 0 &amp; -1 &amp; | &amp; 1 \\end{pmatrix} \\] <p>\u30b9\u30c6\u30c3\u30d7 3: \u5f8c\u9000\u4ee3\u5165\u3092\u884c\u3046</p> <p>\u5909\u5f62\u3055\u308c\u305f\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f:</p> \\[ \\begin{cases} 2x + y - z = 8 \\\\ \\frac{1}{2}y + \\frac{1}{2}z = 1 \\\\ -z = 1 \\end{cases} \\] <p>\u307e\u305a \\(z = -1\\) \u3092\u6c42\u3081\u307e\u3059\u3002 \u6b21\u306b \\(y\\) \u3092\u6c42\u3081\u307e\u3059: \\(\\frac{1}{2}y + \\frac{1}{2} \\cdot (-1) = 1\\) \u3088\u308a \\(y = \\frac{1}{2} \\cdot 2 + \\frac{1}{2} = 1.5\\) \u6700\u5f8c\u306b \\(x\\) \u3092\u6c42\u3081\u307e\u3059: \\(2x + 1.5 - (-1) = 8\\) \u3088\u308a \\(2x = 8 - 1.5 - 1 = 5.5\\) \u3068\u306a\u308b\u306e\u3067 \\(x = 2.75\\)</p> <p>\u5f93\u3063\u3066\u3001\u89e3\u306f \\((x, y, z) = (2.75, 1.5, -1)\\) \u3067\u3059\u3002</p>"},{"location":"lectures/LA/11-system-of-linear-equation/#45-partial-pivoting","title":"4.5 \u90e8\u5206\u30d4\u30dc\u30c3\u30c8\u9078\u629e\uff08Partial Pivoting\uff09","text":"<p>\u8a08\u7b97\u306e\u7cbe\u5ea6\u3092\u9ad8\u3081\u308b\u305f\u3081\u3001\u5b9f\u969b\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u306f\u300c\u30d4\u30dc\u30c3\u30c8\u9078\u629e\u300d\u3068\u547c\u3070\u308c\u308b\u6280\u8853\u3092\u4f7f\u3044\u307e\u3059\u3002\u6700\u3082\u5358\u7d14\u306a\u3082\u306e\u306f\u90e8\u5206\u30d4\u30dc\u30c3\u30c8\u9078\u629e\u3067\u3001\u5404\u30b9\u30c6\u30c3\u30d7\u3067\u5217\u5185\u306e\u6700\u5927\u7d76\u5bfe\u5024\u306e\u8981\u7d20\u3092\u30d4\u30dc\u30c3\u30c8\u3068\u3057\u3066\u9078\u3076\u65b9\u6cd5\u3067\u3059:</p> <ol> <li>\u73fe\u5728\u306e\u5217\u306b\u304a\u3051\u308b\u6700\u5927\u7d76\u5bfe\u5024\u306e\u8981\u7d20\u3092\u898b\u3064\u3051\u308b</li> <li>\u305d\u306e\u8981\u7d20\u304c\u3042\u308b\u884c\u3068\u73fe\u5728\u306e\u884c\u3092\u4ea4\u63db</li> <li>\u901a\u5e38\u306e\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7d9a\u884c</li> </ol> <p>\u3053\u308c\u306b\u3088\u308a\u6570\u5024\u8a08\u7b97\u4e0a\u306e\u8aa4\u5dee\u3092\u5c0f\u3055\u304f\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/11-system-of-linear-equation/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/11-system-of-linear-equation/#51-numpy","title":"5.1 NumPy\u3092\u7528\u3044\u305f\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u5b9f\u88c5","text":"<p>NumPy\u3092\u4f7f\u3063\u3066\u624b\u52d5\u3067\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u3087\u3046:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u9023\u7acb\u65b9\u7a0b\u5f0f Ax = b \u3092\u89e3\u304f\u95a2\u6570\n\n    \u5f15\u6570:\n    A -- \u4fc2\u6570\u884c\u5217 (n x n)\n    b -- \u5b9a\u6570\u30d9\u30af\u30c8\u30eb (n)\n\n    \u623b\u308a\u5024:\n    x -- \u89e3\u30d9\u30af\u30c8\u30eb (n)\n    \"\"\"\n    # \u62e1\u5927\u4fc2\u6570\u884c\u5217\u3092\u4f5c\u6210\n    n = len(b)\n    augmented = np.column_stack((A, b))\n\n    # \u524d\u9032\u6d88\u53bb\n    for i in range(n):\n        # \u90e8\u5206\u30d4\u30dc\u30c3\u30c8\u9078\u629e\n        max_row = i + np.argmax(abs(augmented[i:, i]))\n        if max_row != i:\n            augmented[[i, max_row]] = augmented[[max_row, i]]\n\n        # \u73fe\u5728\u306e\u884c\u4ee5\u4e0b\u306e\u884c\u306b\u3064\u3044\u3066\u6d88\u53bb\u3092\u884c\u3046\n        for j in range(i+1, n):\n            factor = augmented[j, i] / augmented[i, i]\n            augmented[j, i:] -= factor * augmented[i, i:]\n\n    # \u5f8c\u9000\u4ee3\u5165\n    x = np.zeros(n)\n    for i in range(n-1, -1, -1):\n        x[i] = (augmented[i, -1] - np.dot(augmented[i, i+1:n], x[i+1:])) / augmented[i, i]\n\n    return x\n\n# \u4f8b\u984c\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\nA = np.array([[2, 1, -1],\n              [-3, -1, 2],\n              [-2, 1, 2]])\nb = np.array([8, -11, -3])\n\nsolution = gaussian_elimination(A, b)\nprint(\"\u89e3: \", solution)\n\n# NumPy\u306e\u5185\u8535\u95a2\u6570\u3092\u4f7f\u3063\u305f\u89e3\u3068\u6bd4\u8f03\nnumpy_solution = np.linalg.solve(A, b)\nprint(\"NumPy\u306e\u89e3: \", numpy_solution)\n</code></pre>"},{"location":"lectures/LA/11-system-of-linear-equation/#52","title":"5.2 \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u53ef\u8996\u5316","text":"<p>3\u5909\u6570\u306e\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306f3\u6b21\u5143\u7a7a\u9593\u306e\u5e73\u9762\u306e\u4ea4\u70b9\u3068\u3057\u3066\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002\u305d\u308c\u305e\u308c\u306e\u65b9\u7a0b\u5f0f\u306f3\u6b21\u5143\u7a7a\u9593\u306e\u5e73\u9762\u3092\u8868\u3057\u3001\u305d\u306e\u4ea4\u70b9\u304c\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u3068\u306a\u308a\u307e\u3059\u3002</p> <pre><code>def plot_linear_system_3d(A, b, solution):\n    \"\"\"\n    3\u5909\u6570\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u5e73\u9762\u3068\u89e3\u30923D\u30d7\u30ed\u30c3\u30c8\u3059\u308b\u95a2\u6570\n    \"\"\"\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # \u30b0\u30ea\u30c3\u30c9\u30dd\u30a4\u30f3\u30c8\u306e\u751f\u6210\n    x = np.linspace(-5, 5, 10)\n    y = np.linspace(-5, 5, 10)\n    X, Y = np.meshgrid(x, y)\n\n    # \u5404\u5e73\u9762\u3092\u30d7\u30ed\u30c3\u30c8\n    colors = ['r', 'g', 'b']\n    for i in range(len(b)):\n        # Z = (b - A[0]*X - A[1]*Y) / A[2] \u306e\u5f62\u3067\u5e73\u9762\u306e\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\n        Z = (b[i] - A[i, 0] * X - A[i, 1] * Y) / A[i, 2]\n        ax.plot_surface(X, Y, Z, alpha=0.3, color=colors[i])\n\n    # \u89e3\u3092\u30d7\u30ed\u30c3\u30c8\n    ax.scatter([solution[0]], [solution[1]], [solution[2]], \n               color='black', s=100, label='Solution')\n\n    # \u8ef8\u30e9\u30d9\u30eb\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('z')\n    ax.set_title('3\u5909\u6570\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u8868\u73fe')\n    ax.legend()\n\n    plt.show()\n\n# \u4f8b\u984c\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u5e73\u9762\u3068\u89e3\u3092\u53ef\u8996\u5316\nplot_linear_system_3d(A, b, solution)\n</code></pre>"},{"location":"lectures/LA/11-system-of-linear-equation/#53","title":"5.3 \u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u5909\u5f62\u904e\u7a0b\u306e\u53ef\u8996\u5316","text":"<p>\u62e1\u5927\u4fc2\u6570\u884c\u5217\u304c\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u3088\u3063\u3066\u3069\u306e\u3088\u3046\u306b\u5909\u5f62\u3055\u308c\u3066\u3044\u304f\u304b\u3092\u53ef\u8996\u5316\u3057\u307e\u3059:</p> <pre><code>def visualize_gaussian_elimination(A, b):\n    \"\"\"\n    \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u5404\u30b9\u30c6\u30c3\u30d7\u3092\u53ef\u8996\u5316\u3059\u308b\u95a2\u6570\n    \"\"\"\n    n = len(b)\n    augmented = np.column_stack((A, b))\n    steps = [augmented.copy()]\n\n    # \u524d\u9032\u6d88\u53bb\n    for i in range(n):\n        # \u90e8\u5206\u30d4\u30dc\u30c3\u30c8\u9078\u629e\n        max_row = i + np.argmax(abs(augmented[i:, i]))\n        if max_row != i:\n            augmented[[i, max_row]] = augmented[[max_row, i]]\n            steps.append(augmented.copy())\n\n        # \u73fe\u5728\u306e\u884c\u4ee5\u4e0b\u306e\u884c\u306b\u3064\u3044\u3066\u6d88\u53bb\u3092\u884c\u3046\n        for j in range(i+1, n):\n            factor = augmented[j, i] / augmented[i, i]\n            augmented[j, i:] -= factor * augmented[i, i:]\n            steps.append(augmented.copy())\n\n    # \u5404\u30b9\u30c6\u30c3\u30d7\u3092\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3067\u8868\u793a\n    fig, axes = plt.subplots(1, len(steps), figsize=(4*len(steps), 4))\n    if len(steps) == 1:\n        axes = [axes]\n\n    for i, step in enumerate(steps):\n        im = axes[i].imshow(step, cmap='coolwarm')\n        axes[i].set_title(f'Step {i}')\n\n        # \u884c\u3068\u5217\u306e\u30e9\u30d9\u30eb\n        row_labels = [f'Equation {j+1}' for j in range(n)]\n        col_labels = [f'x{j+1}' for j in range(n)] + ['b']\n\n        # \u30e9\u30d9\u30eb\u3092\u8868\u793a\n        axes[i].set_xticks(np.arange(len(col_labels)))\n        axes[i].set_yticks(np.arange(len(row_labels)))\n        axes[i].set_xticklabels(col_labels)\n        axes[i].set_yticklabels(row_labels)\n\n        # \u30de\u30b9\u76ee\u306e\u4e2d\u306b\u5024\u3092\u8868\u793a\n        for ii in range(n):\n            for jj in range(n+1):\n                axes[i].text(jj, ii, f'{step[ii, jj]:.2f}',\n                           ha=\"center\", va=\"center\", color=\"black\")\n\n    plt.tight_layout()\n    plt.show()\n\n# \u4f8b\u984c\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u53ef\u8996\u5316\nvisualize_gaussian_elimination(A, b)\n</code></pre>"},{"location":"lectures/LA/11-system-of-linear-equation/#54-numpy","title":"5.4 NumPy\u306e\u7dda\u5f62\u4ee3\u6570\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3057\u305f\u89e3\u6cd5","text":"<p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u901a\u5e38NumPy\u306e\u5185\u8535\u95a2\u6570\u3092\u4f7f\u3046\u3053\u3068\u304c\u591a\u3044\u3067\u3059:</p> <pre><code>def solve_with_numpy(A, b):\n    \"\"\"\n    NumPy\u3092\u4f7f\u3063\u3066\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u95a2\u6570\n    \"\"\"\n    # numpy.linalg.solve \u3092\u4f7f\u7528\n    solution = np.linalg.solve(A, b)\n    print(\"NumPy\u306b\u3088\u308b\u89e3: \", solution)\n\n    # \u89e3\u306e\u691c\u8a3c: Ax = b \u3092\u6e80\u305f\u3059\u304b\u3069\u3046\u304b\n    verification = np.allclose(np.dot(A, solution), b)\n    print(\"\u89e3\u306e\u691c\u8a3c: \", \"\u6b63\u3057\u3044\" if verification else \"\u8aa4\u5dee\u3042\u308a\")\n\n    return solution\n\n# \u4f8b\u984c\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092NumPy\u3067\u89e3\u304f\nsolve_with_numpy(A, b)\n</code></pre>"},{"location":"lectures/LA/11-system-of-linear-equation/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/11-system-of-linear-equation/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c1: \u4ee5\u4e0b\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u624b\u8a08\u7b97\u3067\u89e3\u304d\u306a\u3055\u3044: $$ \\begin{cases} 3x + 2y - z = 10 \\ -x + 3y + 2z = 5 \\ x - y + z = 0 \\end{cases} $$</p> <p>\u554f\u984c2: \u4ee5\u4e0b\u306e\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306b\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u3001\u4e0a\u4e09\u89d2\u884c\u5217\u306e\u5f62\u306b\u5909\u5f62\u3057\u306a\u3055\u3044: $$ \\begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; | &amp; 4 \\ 2 &amp; 5 &amp; 3 &amp; | &amp; 7 \\ 1 &amp; 0 &amp; 8 &amp; | &amp; 9 \\end{pmatrix} $$</p> <p>\u554f\u984c3: \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3067\u89e3\u304d\u3001\u89e3\u304c \\((x,y,z) = (1,2,3)\\) \u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044: $$ \\begin{cases} 2x - y + z = 3 \\ x + y + z = 6 \\ x - y + 2z = 8 \\end{cases} $$</p>"},{"location":"lectures/LA/11-system-of-linear-equation/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c4: \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3067\u89e3\u3044\u3066\u307f\u306a\u3055\u3044: $$ \\begin{cases} 0.003x + 59.14y = 59.17 \\ 5.291x - 6.130y = 46.78 \\end{cases} $$</p> <p>\u6570\u5024\u7684\u306b\u554f\u984c\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</p> <p>\u554f\u984c5: \u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306e\u6587\u8108\u3067\u3001\u60a3\u8005\u306e\u4f53\u91cd\uff08kg\uff09\u3001\u8eab\u9577\uff08cm\uff09\u3001\u5e74\u9f62\uff08\u6b73\uff09\u304b\u3089\u80ba\u6d3b\u91cf\uff08L\uff09\u3092\u4e88\u6e2c\u3059\u308b\u7dda\u5f62\u30e2\u30c7\u30eb\u3092\u8003\u3048\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u304b\u3089\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> \u4f53\u91cd(kg) \u8eab\u9577(cm) \u5e74\u9f62(\u6b73) \u80ba\u6d3b\u91cf(L) 70 175 30 4.2 60 165 45 3.5 80 180 35 4.7 <p>\u30e2\u30c7\u30eb\u306e\u5f62\u5f0f\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059: \\(\\(\\text{\u80ba\u6d3b\u91cf} = \\beta_0 + \\beta_1 \\cdot \\text{\u4f53\u91cd} + \\beta_2 \\cdot \\text{\u8eab\u9577} + \\beta_3 \\cdot \\text{\u5e74\u9f62}\\)\\)</p> <p>\u3053\u306e\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3\\) \u3092\u6c42\u3081\u308b\u305f\u3081\u306b\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3067\u89e3\u304d\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/11-system-of-linear-equation/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":"<p>Q1: \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3068\u3001\u4e2d\u5b66\u6821\u3067\u7fd2\u3063\u305f\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\u306f\u4f55\u304c\u9055\u3046\u306e\u3067\u3057\u3087\u3046\u304b\uff1f</p> <p>A1: \u4e2d\u5b66\u6821\u3067\u7fd2\u3063\u305f\u6d88\u53bb\u6cd5\u306f\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u7279\u5225\u306a\u5834\u5408\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4e2d\u5b66\u6821\u3067\u306f\u4e3b\u306b2\u5143\u307e\u305f\u306f3\u5143\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u6271\u3044\u3001\u76f4\u611f\u7684\u306b\u5909\u6570\u3092\u6d88\u53bb\u3057\u3066\u3044\u304d\u307e\u3059\u304c\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306f\u3088\u308a\u4f53\u7cfb\u7684\u3067\u3001\u4efb\u610f\u306e\u6b21\u5143\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306b\u9069\u7528\u3067\u304d\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u3059\u3002\u307e\u305f\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3067\u306e\u5b9f\u88c5\u306b\u9069\u3057\u305f\u5f62\u5f0f\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p> <p>Q2: \u62e1\u5927\u4fc2\u6570\u884c\u5217\u306f\u3069\u306e\u3088\u3046\u306a\u3068\u304d\u306b\u4fbf\u5229\u3067\u3059\u304b\uff1f</p> <p>A2: \u62e1\u5927\u4fc2\u6570\u884c\u5217\u306f\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u3092\u884c\u3046\u969b\u306b\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u5168\u4f53\u3092\u4e00\u3064\u306e\u884c\u5217\u3068\u3057\u3066\u6271\u3048\u308b\u305f\u3081\u4fbf\u5229\u3067\u3059\u3002\u7279\u306b\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306a\u3069\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9069\u7528\u3059\u308b\u969b\u3001\u4fc2\u6570\u3068\u5b9a\u6570\u9805\u3092\u4e00\u7dd2\u306b\u5909\u5f62\u3067\u304d\u308b\u305f\u3081\u3001\u8a08\u7b97\u30df\u30b9\u3092\u6e1b\u3089\u3057\u52b9\u7387\u7684\u306b\u89e3\u3092\u6c42\u3081\u3089\u308c\u307e\u3059\u3002</p> <p>Q3: \u30d4\u30dc\u30c3\u30c8\u8981\u7d20\u304c0\u306b\u306a\u3063\u305f\u5834\u5408\u306f\u3069\u3046\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f</p> <p>A3: \u30d4\u30dc\u30c3\u30c8\u8981\u7d20\u304c0\u306e\u5834\u5408\u3001\u305d\u306e\u5217\u30670\u3067\u306a\u3044\u8981\u7d20\u3092\u6301\u3064\u4e0b\u306e\u884c\u3068\u4ea4\u63db\u3057\u307e\u3059\uff08\u90e8\u5206\u30d4\u30dc\u30c3\u30c8\u9078\u629e\uff09\u3002\u3059\u3079\u3066\u306e\u884c\u306e\u540c\u3058\u5217\u8981\u7d20\u304c0\u306e\u5834\u5408\u3001\u6b21\u306e\u5217\u306b\u79fb\u52d5\u3057\u3066\u30d4\u30dc\u30c3\u30c8\u9078\u629e\u3092\u884c\u3044\u307e\u3059\u3002\u3053\u306e\u5834\u5408\u3001\u89e3\u304c\u4e00\u610f\u306b\u5b9a\u307e\u3089\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>Q4: \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u8a08\u7b97\u91cf\u306f\u3069\u306e\u304f\u3089\u3044\u3067\u3059\u304b\uff1f</p> <p>A4: n\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306b\u5bfe\u3057\u3066\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306e\u8a08\u7b97\u91cf\u306f \\(O(n^3)\\) \u3067\u3059\u3002\u3053\u308c\u306f3\u91cd\u306e\u30eb\u30fc\u30d7\uff08i,j,k\uff09\u304c\u3042\u308b\u305f\u3081\u3067\u3001\u5927\u898f\u6a21\u306a\u9023\u7acb\u65b9\u7a0b\u5f0f\u3067\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u304f\u306a\u308a\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u591a\u304f\u306e\u5834\u5408\u3001\u3053\u306e\u8a08\u7b97\u91cf\u306f\u907f\u3051\u3089\u308c\u305a\u3001\u6700\u9069\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p> <p>Q5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306f\u3069\u306e\u3088\u3046\u306b\u4f7f\u308f\u308c\u307e\u3059\u304b\uff1f</p> <p>A5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u66f2\u7dda\u5f53\u3066\u306f\u3081\u3001\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u591a\u304f\u306e\u624b\u6cd5\u306e\u8a08\u7b97\u57fa\u76e4\u3068\u3057\u3066\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u304c\u4f7f\u308f\u308c\u307e\u3059\u3002\u4f8b\u3048\u3070\u7dda\u5f62\u56de\u5e30\u306e\u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u969b\u306b\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u5fc5\u8981\u304c\u3042\u308a\u3001\u305d\u306e\u80cc\u5f8c\u3067\u306f\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u304c\u5229\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>Q6: \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2\u306f\uff1f</p> <p>A6: \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u304a\u3051\u308b\u884c\u306e\u57fa\u672c\u5909\u5f62\u306f\u884c\u5217\u5f0f\u306e\u5024\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u307e\u3059\u304c\u3001\u7279\u5b9a\u306e\u30d1\u30bf\u30fc\u30f3\u3067\u5f71\u97ff\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u884c\u306e\u4ea4\u63db\u306f\u884c\u5217\u5f0f\u306e\u7b26\u53f7\u3092\u53cd\u8ee2\u3055\u305b\u3001\u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u306f\u305d\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u3060\u3051\u884c\u5217\u5f0f\u3092\u5909\u5316\u3055\u305b\u307e\u3059\u3002\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u5f62\u3059\u308c\u3070\u3001\u884c\u5217\u5f0f\u306f\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3068\u3057\u3066\u7c21\u5358\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I \u7b2c12\u56de \u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/12-system-of-linear-equation/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c12\u56de \u30c6\u30fc\u30de: \u30e9\u30f3\u30af\u306e\u6982\u5ff5\u3068\u305d\u306e\u8a08\u7b97 \u95a2\u9023\u9805\u76ee: \u968e\u6bb5\u884c\u5217\u3001\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u3001\u884c\u57fa\u672c\u5909\u5f62\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe\u3001\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u968e\u6bb5\u884c\u5217\u3068\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u5b9a\u7fa9\u3068\u6027\u8cea\u3092\u7406\u89e3\u3059\u308b</li> <li>\u884c\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u968e\u6bb5\u884c\u5217\u3068\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u3092\u5c0e\u51fa\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3057\u3001\u305d\u306e\u610f\u5473\u3092\u8aac\u660e\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u30e9\u30f3\u30af\u3068\u30d9\u30af\u30c8\u30eb\u306e\u4e00\u6b21\u72ec\u7acb\u6027\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/12-system-of-linear-equation/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/12-system-of-linear-equation/#31","title":"3.1 \u968e\u6bb5\u884c\u5217\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \u968e\u6bb5\u884c\u5217\uff08echelon form\uff09\u3068\u306f\u3001\u4ee5\u4e0b\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u884c\u5217\u306e\u3053\u3068\u3067\u3059\u3002 1. \u3059\u3079\u3066\u306e\u30bc\u30ed\u884c\uff08\u8981\u7d20\u304c\u3059\u3079\u30660\u306e\u884c\uff09\u306f\u884c\u5217\u306e\u4e0b\u90e8\u306b\u96c6\u3081\u3089\u308c\u3066\u3044\u308b 2. \u5404\u884c\u306e\u5148\u982d\u306e\u975e\u30bc\u30ed\u8981\u7d20\uff08\u5148\u982d\u4fc2\u6570\uff09\u306f\u3001\u305d\u306e\u4e0a\u306e\u884c\u306e\u5148\u982d\u975e\u30bc\u30ed\u8981\u7d20\u3088\u308a\u3082\u53f3\u306b\u3042\u308b 3. \u5148\u982d\u4fc2\u6570\u306e\u5217\u3088\u308a\u5de6\u306e\u5217\u306f\u3059\u3079\u30660\u3067\u3042\u308b</p> <p>\u968e\u6bb5\u884c\u5217\u306f\u300c\u968e\u6bb5\u306e\u3088\u3046\u306a\u5f62\u300d\u3092\u3057\u3066\u3044\u307e\u3059\u3002\u6b21\u306e\u884c\u5217\u306f\u968e\u6bb5\u884c\u5217\u306e\u4f8b\u3067\u3059\uff1a</p> \\[ \\begin{pmatrix} 2 &amp; 3 &amp; 1 &amp; 7 \\\\ 0 &amp; 4 &amp; -3 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; 5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\] <p>\u3053\u306e\u884c\u5217\u3067\u306f\uff1a - 1\u884c\u76ee\u306e\u5148\u982d\u4fc2\u6570\u306f\u7b2c1\u5217\u306e2 - 2\u884c\u76ee\u306e\u5148\u982d\u4fc2\u6570\u306f\u7b2c2\u5217\u306e4 - 3\u884c\u76ee\u306e\u5148\u982d\u4fc2\u6570\u306f\u7b2c3\u5217\u306e1 - 4\u884c\u76ee\u306f\u30bc\u30ed\u884c</p> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u5148\u982d\u4fc2\u6570\u304c\u53f3\u4e0b\u3078\u3068\u968e\u6bb5\u72b6\u306b\u4e26\u3093\u3067\u3044\u308b\u305f\u3081\u300c\u968e\u6bb5\u884c\u5217\u300d\u3068\u547c\u3070\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#32","title":"3.2 \u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \u7c21\u7d04\u968e\u6bb5\u884c\u5217\uff08reduced echelon form\uff09\u3068\u306f\u3001\u4ee5\u4e0b\u306e\u8ffd\u52a0\u6761\u4ef6\u3092\u6e80\u305f\u3059\u968e\u6bb5\u884c\u5217\u306e\u3053\u3068\u3067\u3059\u3002 1. \u5404\u884c\u306e\u5148\u982d\u4fc2\u6570\u306f1\u3067\u3042\u308b 2. \u5148\u982d\u4fc2\u6570\u306e\u3042\u308b\u5217\u306b\u304a\u3044\u3066\u3001\u305d\u306e\u5148\u982d\u4fc2\u6570\u4ee5\u5916\u306e\u8981\u7d20\u306f\u3059\u3079\u30660\u3067\u3042\u308b</p> <p>\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u4f8b\uff1a</p> \\[ \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\] <p>\u3053\u306e\u884c\u5217\u3067\u306f\uff1a - \u3059\u3079\u3066\u306e\u5148\u982d\u4fc2\u6570\u304c1 - \u5148\u982d\u4fc2\u6570\u306e\u3042\u308b\u5217\uff08\u7b2c1, 2, 3\u5217\uff09\u3067\u306f\u3001\u5148\u982d\u4fc2\u6570\u4ee5\u5916\u306e\u8981\u7d20\u304c\u3059\u3079\u30660</p> <p>\u7279\u306b\u3001\u5404\u884c\u306e\u5148\u982d\u4fc2\u6570\u304c\u5358\u4f4d\u884c\u5217\u306e\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u90e8\u5206\u304c\u3042\u308b\u3053\u3068\u306b\u6ce8\u76ee\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#33","title":"3.3 \u884c\u5217\u306e\u30e9\u30f3\u30af\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \u884c\u5217A\u306e\u30e9\u30f3\u30af\uff08rank\uff09\u3068\u306f\u3001A\u3092\u968e\u6bb5\u884c\u5217\uff08\u307e\u305f\u306f\u7c21\u7d04\u968e\u6bb5\u884c\u5217\uff09\u306b\u5909\u5f62\u3057\u305f\u3068\u304d\u306e\u975e\u30bc\u30ed\u884c\u306e\u6570\u3067\u3059\u3002\u3053\u308c\u306f\u3001\uff08\u5f8c\u3067\u5b66\u3076\uff09\u884c\u5217\u5185\u306e\u7dda\u5f62\u72ec\u7acb\u306a\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u6570\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u30e9\u30f3\u30af\u306f\u6b21\u306e\u3088\u3046\u306a\u8a18\u53f7\u3067\u8868\u3057\u307e\u3059\uff1a \\(\\text{rank}(A)\\) \u307e\u305f\u306f \\(\\text{r}(A)\\)</p> <p>\u91cd\u8981\u306a\u6027\u8cea\uff1a - m\u00d7n\u884c\u5217\u306e\u30e9\u30f3\u30afr\u306f\u3001r \u2264 min(m, n) \u3092\u6e80\u305f\u3059 - \u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u3001\u884c\u57fa\u672c\u5909\u5f62\u306b\u3088\u3063\u3066\u5909\u308f\u3089\u306a\u3044 - \u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u3001\u305d\u306e\u884c\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u7dda\u5f62\u72ec\u7acb\u6570\u3068\u5217\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u7dda\u5f62\u72ec\u7acb\u6570\u306b\u7b49\u3057\u3044</p> <p>\u4f8b\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u968e\u6bb5\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f3\u3067\u3059\uff08\u975e\u30bc\u30ed\u884c\u304c3\u884c\u3042\u308b\u305f\u3081\uff09\uff1a</p> \\[ \\begin{pmatrix} 1 &amp; 3 &amp; 2 &amp; 7 \\\\ 0 &amp; 1 &amp; -3 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; 5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\]"},{"location":"lectures/LA/12-system-of-linear-equation/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/12-system-of-linear-equation/#41","title":"4.1 \u884c\u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u968e\u6bb5\u884c\u5217\u306e\u5c0e\u51fa\u65b9\u6cd5","text":"<p>\u884c\u57fa\u672c\u5909\u5f62\u306b\u3088\u3063\u3066\u4efb\u610f\u306e\u884c\u5217\u3092\u968e\u6bb5\u884c\u5217\u306b\u5909\u5f62\u3067\u304d\u307e\u3059\u3002\u884c\u57fa\u672c\u5909\u5f62\u306b\u306f\u4ee5\u4e0b\u306e3\u7a2e\u985e\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u884c\u306e\u5165\u308c\u66ff\u3048</li> <li>\u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d</li> <li>\u3042\u308b\u884c\u306e\u5b9a\u6570\u500d\u3092\u5225\u306e\u884c\u306b\u52a0\u3048\u308b</li> </ol> <p>\u968e\u6bb5\u884c\u5217\u3078\u306e\u5909\u5f62\u624b\u9806\uff08\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\uff09\uff1a</p> <ol> <li>\u5de6\u7aef\u306e\u5217\u304b\u3089\u9806\u306b\u51e6\u7406\u3059\u308b</li> <li>\u51e6\u7406\u4e2d\u306e\u5217\u3067\u3001\u307e\u3060\u51e6\u7406\u3057\u3066\u3044\u306a\u3044\u884c\u306e\u4e2d\u304b\u3089\u5148\u982d\u4fc2\u6570\u3092\u9078\u3076</li> <li>\u5fc5\u8981\u306a\u3089\u884c\u3092\u5165\u308c\u66ff\u3048\u3066\u3001\u305d\u306e\u5148\u982d\u4fc2\u6570\u3092\u4e0a\u306b\u79fb\u52d5\u3055\u305b\u308b</li> <li>\u305d\u306e\u5148\u982d\u4fc2\u6570\u3092\u542b\u3080\u884c\u3092\u4f7f\u3063\u3066\u3001\u540c\u3058\u5217\u306e\u4ed6\u306e\u8981\u7d20\u3092\u3059\u3079\u30660\u306b\u3059\u308b</li> <li>\u6b21\u306e\u5217\u306b\u79fb\u308a\u3001\u540c\u69d8\u306e\u51e6\u7406\u3092\u7e70\u308a\u8fd4\u3059</li> </ol>"},{"location":"lectures/LA/12-system-of-linear-equation/#42","title":"4.2 \u7c21\u7d04\u968e\u6bb5\u884c\u5217\u3078\u306e\u5909\u5f62\u65b9\u6cd5","text":"<p>\u968e\u6bb5\u884c\u5217\u304b\u3089\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u3078\u306e\u5909\u5f62\u624b\u9806\uff1a</p> <ol> <li>\u6700\u4e0b\u884c\u304b\u3089\u9806\u306b\u4e0a\u3078\u51e6\u7406\u3059\u308b</li> <li>\u5404\u884c\u306e\u5148\u982d\u4fc2\u6570\u30921\u306b\u3059\u308b\u305f\u3081\u3001\u884c\u5168\u4f53\u3092\u305d\u306e\u4fc2\u6570\u3067\u5272\u308b</li> <li>\u305d\u306e\u5148\u982d\u4fc2\u6570\u306e\u5217\u306b\u304a\u3044\u3066\u3001\u4ed6\u306e\u3059\u3079\u3066\u306e\u8981\u7d20\u30920\u306b\u3059\u308b</li> <li>\u524d\u306e\u884c\u306b\u79fb\u308a\u3001\u540c\u69d8\u306e\u51e6\u7406\u3092\u7e70\u308a\u8fd4\u3059</li> </ol>"},{"location":"lectures/LA/12-system-of-linear-equation/#43","title":"4.3 \u30e9\u30f3\u30af\u306e\u8a08\u7b97\u65b9\u6cd5","text":"<p>\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\u3059\u308b\u624b\u9806\uff1a</p> <ol> <li>\u884c\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u3092\u968e\u6bb5\u884c\u5217\u306b\u5909\u5f62\u3059\u308b</li> <li>\u975e\u30bc\u30ed\u884c\u306e\u6570\u3092\u6570\u3048\u308b</li> </ol> <p>\u307e\u305f\u306f\uff1a</p> <ol> <li>\u884c\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u3092\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306b\u5909\u5f62\u3059\u308b</li> <li>\u975e\u30bc\u30ed\u884c\u306e\u6570\u3092\u6570\u3048\u308b</li> </ol> <p>\u3069\u3061\u3089\u306e\u65b9\u6cd5\u3067\u3082\u540c\u3058\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#44","title":"4.4 \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u4e00\u6b21\u72ec\u7acb\u6027\u306e\u95a2\u4fc2","text":"<p>\u30e9\u30f3\u30af\u306b\u306f\u4ee5\u4e0b\u306e\u91cd\u8981\u306a\u89e3\u91c8\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ul> <li>\u884c\u5217A\u306e\u30e9\u30f3\u30af\u306f\u3001A\u306e\u884c\u30d9\u30af\u30c8\u30eb\u306e\u4e2d\u3067\u4e00\u6b21\u72ec\u7acb\u306a\u3082\u306e\u306e\u6700\u5927\u6570</li> <li>\u540c\u69d8\u306b\u3001A\u306e\u30e9\u30f3\u30af\u306f\u3001A\u306e\u5217\u30d9\u30af\u30c8\u30eb\u306e\u4e2d\u3067\u4e00\u6b21\u72ec\u7acb\u306a\u3082\u306e\u306e\u6700\u5927\u6570</li> </ul> <p>\u3053\u308c\u306b\u3088\u308a\u3001\u30e9\u30f3\u30af\u306f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u300c\u6b21\u5143\u300d\u306e\u6982\u5ff5\u3068\u5bc6\u63a5\u306b\u95a2\u4fc2\u3057\u307e\u3059\u3002\u30e9\u30f3\u30af\u304c\u6301\u3064\u91cd\u8981\u306a\u6027\u8cea\uff1a</p> <ul> <li>\\(\\text{rank}(A) = \\text{rank}(A^T)\\)\uff08\u8ee2\u7f6e\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u5143\u306e\u884c\u5217\u3068\u540c\u3058\uff09</li> <li>\\(\\text{rank}(A) = \\text{rank}(AA^T) = \\text{rank}(A^TA)\\)</li> <li>\\(\\text{rank}(AB) \\leq \\min(\\text{rank}(A), \\text{rank}(B))\\)\uff08\u884c\u5217\u306e\u7a4d\u306e\u30e9\u30f3\u30af\uff09</li> </ul>"},{"location":"lectures/LA/12-system-of-linear-equation/#5","title":"5. \u5177\u4f53\u7684\u306a\u8a08\u7b97\u4f8b","text":""},{"location":"lectures/LA/12-system-of-linear-equation/#1_1","title":"\u4f8b\u984c1: \u968e\u6bb5\u884c\u5217\u3078\u306e\u5909\u5f62","text":"<p>\u884c\u5217A\u3092\u968e\u6bb5\u884c\u5217\u306b\u5909\u5f62\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[A =  \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 4 &amp; -1 &amp; 2 &amp; 10 \\\\ 6 &amp; 3 &amp; 9 &amp; 18 \\end{pmatrix} \\] <p>\u89e3\u7b54:</p> <p>\u30b9\u30c6\u30c3\u30d71: \u7b2c1\u5217\u3092\u51e6\u7406\u3057\u307e\u3059\u3002\u5148\u982d\u4fc2\u6570\u3068\u3057\u3066\u7b2c1\u884c\u306e2\u3092\u9078\u3073\u307e\u3059\u3002</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 4 &amp; -1 &amp; 2 &amp; 10 \\\\ 6 &amp; 3 &amp; 9 &amp; 18 \\end{pmatrix} \\] <p>\u7b2c2\u884c\u306e\u8981\u7d20\u30920\u306b\u3059\u308b\u305f\u3081\u3001\u7b2c1\u884c\u306e(-2)\u500d\u3092\u7b2c2\u884c\u306b\u52a0\u3048\u307e\u3059\u3002 \\(R_2 \\leftarrow R_2 + (-2)R_1\\)</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 0 &amp; -3 &amp; -4 &amp; -4 \\\\ 6 &amp; 3 &amp; 9 &amp; 18 \\end{pmatrix} \\] <p>\u7b2c3\u884c\u306e\u8981\u7d20\u30920\u306b\u3059\u308b\u305f\u3081\u3001\u7b2c1\u884c\u306e(-3)\u500d\u3092\u7b2c3\u884c\u306b\u52a0\u3048\u307e\u3059\u3002 \\(R_3 \\leftarrow R_3 + (-3)R_1\\)</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 0 &amp; -3 &amp; -4 &amp; -4 \\\\ 0 &amp; 0 &amp; 0 &amp; -3 \\end{pmatrix} \\] <p>\u30b9\u30c6\u30c3\u30d72: \u7b2c2\u5217\u3092\u51e6\u7406\u3057\u307e\u3059\u3002\u5148\u982d\u4fc2\u6570\u3068\u3057\u3066\u7b2c2\u884c\u306e-3\u3092\u9078\u3073\u307e\u3059\u3002</p> <p>\u7b2c3\u884c\u306f\u65e2\u306b\u7b2c2\u5217\u304c0\u306a\u306e\u3067\u51e6\u7406\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> <p>\u30b9\u30c6\u30c3\u30d73: \u7b2c3\u5217\u3092\u51e6\u7406\u3057\u307e\u3059\u3002\u7b2c3\u884c\u306f\u5168\u30660\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u305f\u3081\u3001\u7b2c3\u5217\u306e\u51e6\u7406\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\u3002</p> <p>\u7d50\u679c\u3068\u3057\u3066\u3001\u968e\u6bb5\u884c\u5217\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 0 &amp; -3 &amp; -4 &amp; -4 \\\\ 0 &amp; 0 &amp; 0 &amp; -3 \\end{pmatrix} \\] <p>\u3053\u306e\u884c\u5217\u306f\u975e\u30bc\u30ed\u884c\u304c3\u884c\u3042\u308b\u305f\u3081\u3001\u5143\u306e\u884c\u5217A\u306e\u30e9\u30f3\u30af\u306f3\u3067\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#2_1","title":"\u4f8b\u984c2: \u7c21\u7d04\u968e\u6bb5\u884c\u5217\u3078\u306e\u5909\u5f62","text":"<p>\u5148\u307b\u3069\u306e\u968e\u6bb5\u884c\u5217\u3092\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306b\u5909\u5f62\u3057\u307e\u3057\u3087\u3046\u3002</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 0 &amp; -3 &amp; -4 &amp; -4 \\\\ 0 &amp; 0 &amp; 0 &amp; -3 \\end{pmatrix} \\] <p>\u89e3\u7b54:</p> <p>\u30b9\u30c6\u30c3\u30d71: \u6700\u4e0b\u884c\u304b\u3089\u51e6\u7406\u3057\u307e\u3059\u3002\u7b2c3\u884c\u306e\u5148\u982d\u4fc2\u6570\u30921\u306b\u3059\u308b\u305f\u3081\u3001\u884c\u5168\u4f53\u3092-3\u3067\u5272\u308a\u307e\u3059\u3002 \\(R_3 \\leftarrow R_3 / (-3)\\)</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 0 &amp; -3 &amp; -4 &amp; -4 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>\u30b9\u30c6\u30c3\u30d72: \u7b2c2\u884c\u306b\u79fb\u308a\u307e\u3059\u3002\u5148\u982d\u4fc2\u6570\u30921\u306b\u3059\u308b\u305f\u3081\u3001\u884c\u5168\u4f53\u3092-3\u3067\u5272\u308a\u307e\u3059\u3002 \\(R_2 \\leftarrow R_2 / (-3)\\)</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 0 &amp; 1 &amp; 4/3 &amp; 4/3 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>\u7b2c4\u5217\u306e\u7b2c2\u884c\u306e\u8981\u7d20\u30920\u306b\u3059\u308b\u305f\u3081\u3001\u7b2c3\u884c\u306e(-4/3)\u500d\u3092\u7b2c2\u884c\u306b\u52a0\u3048\u307e\u3059\u3002 \\(R_2 \\leftarrow R_2 + (-4/3)R_3\\)</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 0 &amp; 1 &amp; 4/3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>\u30b9\u30c6\u30c3\u30d73: \u7b2c1\u884c\u306b\u79fb\u308a\u307e\u3059\u3002\u5148\u982d\u4fc2\u6570\u30921\u306b\u3059\u308b\u305f\u3081\u3001\u884c\u5168\u4f53\u30922\u3067\u5272\u308a\u307e\u3059\u3002 \\(R_1 \\leftarrow R_1 / 2\\)</p> \\[ \\begin{pmatrix} 1 &amp; 1/2 &amp; 3/2 &amp; 7/2 \\\\ 0 &amp; 1 &amp; 4/3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>\u7b2c2\u5217\u306e\u7b2c1\u884c\u306e\u8981\u7d20\u30920\u306b\u3059\u308b\u305f\u3081\u3001\u7b2c2\u884c\u306e(-1/2)\u500d\u3092\u7b2c1\u884c\u306b\u52a0\u3048\u307e\u3059\u3002 \\(R_1 \\leftarrow R_1 + (-1/2)R_2\\)</p> \\[ \\begin{pmatrix} 1 &amp; 0 &amp; 5/6 &amp; 7/2 \\\\ 0 &amp; 1 &amp; 4/3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>\u7b2c4\u5217\u306e\u7b2c1\u884c\u306e\u8981\u7d20\u30920\u306b\u3059\u308b\u305f\u3081\u3001\u7b2c3\u884c\u306e(-7/2)\u500d\u3092\u7b2c1\u884c\u306b\u52a0\u3048\u307e\u3059\u3002 \\(R_1 \\leftarrow R_1 + (-7/2)R_3\\)</p> \\[ \\begin{pmatrix} 1 &amp; 0 &amp; 5/6 &amp; 0 \\\\ 0 &amp; 1 &amp; 4/3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>\u3053\u308c\u304c\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u3067\u3042\u308a\u3001\u975e\u30bc\u30ed\u884c\u304c3\u884c\u3042\u308b\u306e\u3067\u3001\u30e9\u30f3\u30af\u306f3\u3067\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#3_1","title":"\u4f8b\u984c3: \u30e9\u30f3\u30af\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u95a2\u4fc2","text":"<p>\u9023\u7acb\u65b9\u7a0b\u5f0f\u304c\u6b21\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u3066\u3044\u307e\u3059\uff1a</p> \\[ \\begin{align} 2x + y + 3z &amp;= 7 \\\\ 4x - y + 2z &amp;= 10 \\\\ 6x + 3y + 9z &amp;= 18 \\end{align} \\] <p>\u4fc2\u6570\u884c\u5217\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u6c42\u3081\u3001\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>\u89e3\u7b54:</p> <p>\u4fc2\u6570\u884c\u5217A\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217[A|b]\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[A =  \\begin{pmatrix} 2 &amp; 1 &amp; 3 \\\\ 4 &amp; -1 &amp; 2 \\\\ 6 &amp; 3 &amp; 9 \\end{pmatrix}, \\quad [A|b] =  \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 4 &amp; -1 &amp; 2 &amp; 10 \\\\ 6 &amp; 3 &amp; 9 &amp; 18 \\end{pmatrix} \\] <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066[A|b]\u3092\u968e\u6bb5\u884c\u5217\u306b\u5909\u5f62\u3057\u307e\u3059\uff08\u4e0a\u306e\u4f8b\u984c1\u306e\u8a08\u7b97\u3068\u540c\u3058\u3067\u3059\uff09\uff1a</p> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; 3 &amp; 7 \\\\ 0 &amp; -3 &amp; -4 &amp; -4 \\\\ 0 &amp; 0 &amp; 0 &amp; -3 \\end{pmatrix} \\] <p>\u3053\u306e\u884c\u5217\u304b\u3089\u3001\u4fc2\u6570\u884c\u5217A\u306e\u30e9\u30f3\u30af\u306f2\u3001\u62e1\u5927\u4fc2\u6570\u884c\u5217[A|b]\u306e\u30e9\u30f3\u30af\u306f3\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p> <p>\\(\\text{rank}(A) = 2 \\neq \\text{rank}([A|b]) = 3\\)</p> <p>\u30e9\u30f3\u30af\u306e\u4e0d\u4e00\u81f4\u304b\u3089\u3001\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u89e3\u3092\u6301\u305f\u306a\u3044\uff08\u4e0d\u80fd\uff09\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u7b2c3\u884c\u304c \\(0x + 0y + 0z = -3\\) \u3068\u3044\u3046\u77db\u76fe\u3057\u305f\u5f0f\u306b\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3067\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#6-python","title":"6. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/12-system-of-linear-equation/#61","title":"6.1 \u968e\u6bb5\u884c\u5217\u3068\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u8a08\u7b97","text":"<p>\u307e\u305a\u3001NumPy\u3092\u4f7f\u3063\u3066\u884c\u5217\u3092\u968e\u6bb5\u884c\u5217\u3068\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306b\u5909\u63db\u3059\u308b\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nfrom fractions import Fraction\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u884c\u5217\u3092\u898b\u3084\u3059\u304f\u8868\u793a\u3059\u308b\u95a2\u6570\ndef print_matrix(A, precision=4):\n    for row in A:\n        print('[', end=' ')\n        for elem in row:\n            if isinstance(elem, Fraction):\n                print(f\"{float(elem):.{precision}f}\", end=' ')\n            else:\n                print(f\"{elem:.{precision}f}\", end=' ')\n        print(']')\n    print()\n\n# \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u3088\u308a\u968e\u6bb5\u884c\u5217\u3092\u6c42\u3081\u308b\u95a2\u6570\ndef to_echelon_form(A):\n    A = A.copy().astype(float)\n    m, n = A.shape\n    r = 0  # \u73fe\u5728\u306e\u884c\n\n    for c in range(n):  # \u5217\u306b\u3064\u3044\u3066\u51e6\u7406\n        # \u73fe\u5728\u306e\u5217\u3067\u30d4\u30dc\u30c3\u30c8\u3068\u306a\u308b\u975e\u30bc\u30ed\u8981\u7d20\u3092\u63a2\u3059\n        for i in range(r, m):\n            if abs(A[i, c]) &gt; 1e-10:  # \u6570\u5024\u8a08\u7b97\u306e\u8aa4\u5dee\u3092\u8003\u616e\n                # \u884c\u3092\u5165\u308c\u66ff\u3048\n                A[[r, i]] = A[[i, r]]\n                # \u3053\u306e\u884c\u3092\u30d4\u30dc\u30c3\u30c8\u3068\u3057\u3066\u4f7f\u7528\n                pivot = A[r, c]\n                # \u4ed6\u306e\u884c\u304b\u3089\u3053\u306e\u884c\u306e\u500d\u6570\u3092\u5f15\u3044\u30660\u306b\u3059\u308b\n                for j in range(r+1, m):\n                    factor = A[j, c] / pivot\n                    A[j] = A[j] - factor * A[r]\n                r += 1\n                break\n\n        if r == m:  # \u3059\u3079\u3066\u306e\u884c\u3092\u51e6\u7406\u3057\u305f\u3089\u7d42\u4e86\n            break\n\n    return A\n\n# \u7c21\u7d04\u968e\u6bb5\u884c\u5217\u3092\u6c42\u3081\u308b\u95a2\u6570\ndef to_reduced_echelon_form(A):\n    A = to_echelon_form(A)\n    m, n = A.shape\n\n    # \u975e\u30bc\u30ed\u884c\u3092\u7279\u5b9a\n    nonzero_rows = []\n    for i in range(m):\n        for j in range(n):\n            if abs(A[i, j]) &gt; 1e-10:\n                nonzero_rows.append(i)\n                break\n\n    # \u4e0b\u304b\u3089\u4e0a\u306b\u51e6\u7406\n    for i in reversed(nonzero_rows):\n        # \u5148\u982d\u306e\u975e\u30bc\u30ed\u8981\u7d20\uff08\u30d4\u30dc\u30c3\u30c8\uff09\u3092\u898b\u3064\u3051\u308b\n        pivot_col = -1\n        for j in range(n):\n            if abs(A[i, j]) &gt; 1e-10:\n                pivot_col = j\n                break\n\n        if pivot_col == -1:\n            continue\n\n        # \u30d4\u30dc\u30c3\u30c8\u30921\u306b\u3059\u308b\n        A[i] = A[i] / A[i, pivot_col]\n\n        # \u4ed6\u306e\u884c\u306e\u30d4\u30dc\u30c3\u30c8\u5217\u306e\u8981\u7d20\u30920\u306b\u3059\u308b\n        for k in range(i):\n            factor = A[k, pivot_col]\n            A[k] = A[k] - factor * A[i]\n\n    return A\n\n# \u30e9\u30f3\u30af\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\ndef rank(A):\n    echelon = to_echelon_form(A)\n    r = 0\n    for i in range(echelon.shape[0]):\n        if any(abs(echelon[i, j]) &gt; 1e-10 for j in range(echelon.shape[1])):\n            r += 1\n    return r\n\n# \u30b5\u30f3\u30d7\u30eb\u884c\u5217\nA = np.array([\n    [2, 1, 3, 7],\n    [4, -1, 2, 10],\n    [6, 3, 9, 18]\n], dtype=float)\n\nprint(\"\u5143\u306e\u884c\u5217:\")\nprint_matrix(A)\n\nechelon = to_echelon_form(A)\nprint(\"\u968e\u6bb5\u884c\u5217:\")\nprint_matrix(echelon)\n\nreduced_echelon = to_reduced_echelon_form(A)\nprint(\"\u7c21\u7d04\u968e\u6bb5\u884c\u5217:\")\nprint_matrix(reduced_echelon)\n\nprint(f\"\u884c\u5217\u306e\u30e9\u30f3\u30af: {rank(A)}\")\n</code></pre>"},{"location":"lectures/LA/12-system-of-linear-equation/#62","title":"6.2 \u30e9\u30f3\u30af\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u306e\u53ef\u8996\u5316","text":"<p>\u30e9\u30f3\u30af\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u53ef\u8996\u5316\u3059\u308b\u305f\u3081\u306b\u30013\u6b21\u5143\u7a7a\u9593\u3067\u306e\u5217\u30d9\u30af\u30c8\u30eb\u3068\u3001\u305d\u308c\u3089\u306e\u306a\u3059\u90e8\u5206\u7a7a\u9593\u3092\u63cf\u753b\u3057\u307e\u3059\u3002</p> <pre><code>def visualize_rank(A):\n    # 3\u6b21\u5143\u307e\u3067\u306e\u884c\u5217\u306e\u307f\u5bfe\u5fdc\n    if A.shape[1] &gt; 3:\n        A = A[:, :3]\n\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # \u539f\u70b9\n    ax.scatter([0], [0], [0], color='black', s=100, label='Origin')\n\n    # \u5217\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\n    colors = ['r', 'g', 'b']\n    for i in range(A.shape[1]):\n        vec = A[:, i]\n        if len(vec) &lt; 3:\n            vec = np.append(vec, [0] * (3 - len(vec)))\n        ax.quiver(0, 0, 0, vec[0], vec[1], vec[2], color=colors[i], arrow_length_ratio=0.1, \n                  label=f'Column {i+1}')\n\n    # \u30e9\u30f3\u30af\u306b\u57fa\u3065\u3044\u3066\u6b21\u5143\u3092\u8868\u793a\n    r = rank(A)\n\n    if r == 1:  # \u7dda\u3092\u63cf\u753b\n        # \u6700\u521d\u306e\u975e\u30bc\u30ed\u5217\u30d9\u30af\u30c8\u30eb\u3092\u53d6\u5f97\n        col_idx = 0\n        for i in range(A.shape[1]):\n            if np.linalg.norm(A[:, i]) &gt; 1e-10:\n                col_idx = i\n                break\n\n        vec = A[:, col_idx]\n        if len(vec) &lt; 3:\n            vec = np.append(vec, [0] * (3 - len(vec)))\n\n        t = np.linspace(-2, 2, 100)\n        x = t * vec[0]\n        y = t * vec[1]\n        z = t * vec[2]\n        ax.plot(x, y, z, 'k--', alpha=0.5)\n        plt.title(f'Rank = {r}: Vectors span a line')\n\n    elif r == 2:  # \u5e73\u9762\u3092\u63cf\u753b\n        # \u7dda\u5f62\u72ec\u7acb\u306a2\u3064\u306e\u5217\u30d9\u30af\u30c8\u30eb\u3092\u898b\u3064\u3051\u308b\n        cols = []\n        for i in range(A.shape[1]):\n            if len(cols) == 0:\n                if np.linalg.norm(A[:, i]) &gt; 1e-10:\n                    cols.append(i)\n            elif len(cols) == 1:\n                temp = A[:, [cols[0], i]]\n                if rank(temp) == 2:\n                    cols.append(i)\n                    break\n\n        if len(cols) &gt;= 2:\n            vec1 = A[:, cols[0]]\n            vec2 = A[:, cols[1]]\n\n            if len(vec1) &lt; 3:\n                vec1 = np.append(vec1, [0] * (3 - len(vec1)))\n            if len(vec2) &lt; 3:\n                vec2 = np.append(vec2, [0] * (3 - len(vec2)))\n\n            # \u5e73\u9762\u3092\u63cf\u753b\n            xx, yy = np.meshgrid(np.linspace(-2, 2, 10), np.linspace(-2, 2, 10))\n\n            # \u5e73\u9762\u306e\u6cd5\u7dda\u30d9\u30af\u30c8\u30eb\n            normal = np.cross(vec1, vec2)\n\n            if np.linalg.norm(normal) &gt; 1e-10:\n                d = 0  # \u5e73\u9762\u306e\u65b9\u7a0b\u5f0f ax + by + cz + d = 0\n                z = (-normal[0] * xx - normal[1] * yy - d) / max(normal[2], 1e-10)\n                ax.plot_surface(xx, yy, z, alpha=0.2, color='cyan')\n                plt.title(f'Rank = {r}: Vectors span a plane')\n\n    elif r == 3:\n        plt.title(f'Rank = {r}: Vectors span the full 3D space')\n\n    # \u30b0\u30e9\u30d5\u306e\u8a2d\u5b9a\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_xlim([-3, 3])\n    ax.set_ylim([-3, 3])\n    ax.set_zlim([-3, 3])\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# \u7570\u306a\u308b\u30e9\u30f3\u30af\u306e\u884c\u5217\u3092\u53ef\u8996\u5316\n# \u30e9\u30f3\u30af1\u306e\u4f8b\uff08\u5217\u30d9\u30af\u30c8\u30eb\u304c\u7dda\u5f62\u5f93\u5c5e\uff09\nA1 = np.array([\n    [1, 2],\n    [2, 4],\n    [3, 6]\n])\n\n# \u30e9\u30f3\u30af2\u306e\u4f8b\uff08\u5217\u30d9\u30af\u30c8\u30eb\u304c\u5e73\u9762\u3092\u5f35\u308b\uff09\nA2 = np.array([\n    [1, 0],\n    [0, 1],\n    [1, 1]\n])\n\n# \u30e9\u30f3\u30af3\u306e\u4f8b\uff08\u5217\u30d9\u30af\u30c8\u30eb\u304c3\u6b21\u5143\u7a7a\u9593\u3092\u5f35\u308b\uff09\nA3 = np.array([\n    [1, 0, 0],\n    [0, 1, 0],\n    [0, 0, 1]\n])\n\nprint(\"\u30e9\u30f3\u30af1\u306e\u884c\u5217:\")\nprint_matrix(A1)\nvisualize_rank(A1)\n\nprint(\"\u30e9\u30f3\u30af2\u306e\u884c\u5217:\")\nprint_matrix(A2)\nvisualize_rank(A2)\n\nprint(\"\u30e9\u30f3\u30af3\u306e\u884c\u5217:\")\nprint_matrix(A3)\nvisualize_rank(A3)\n</code></pre>"},{"location":"lectures/LA/12-system-of-linear-equation/#63","title":"6.3 \u30e9\u30f3\u30af\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u95a2\u4fc2","text":"<p>\u30e9\u30f3\u30af\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u306e\u95a2\u4fc2\u3092\u8996\u899a\u5316\u3057\u307e\u3059\u3002</p> <pre><code>def analyze_system(A, b):\n    # \u4fc2\u6570\u884c\u5217\n    coef_matrix = A\n    # \u62e1\u5927\u4fc2\u6570\u884c\u5217\n    augmented_matrix = np.column_stack((A, b))\n\n    # \u30e9\u30f3\u30af\u3092\u8a08\u7b97\n    r_A = rank(coef_matrix)\n    r_Aug = rank(augmented_matrix)\n\n    print(f\"\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af: {r_A}\")\n    print(f\"\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af: {r_Aug}\")\n\n    # \u89e3\u306e\u5b58\u5728\u3068\u7a2e\u985e\u3092\u5224\u5b9a\n    if r_A != r_Aug:\n        print(\"\u89e3\u306a\u3057\uff08\u4e0d\u80fd\uff09\")\n        solution_type = \"no solution\"\n    elif r_A &lt; A.shape[1]:\n        print(f\"\u7121\u6570\u306e\u89e3\uff08\u4e0d\u5b9a\uff09- \u81ea\u7531\u5909\u6570\u306e\u6570: {A.shape[1] - r_A}\")\n        solution_type = \"infinitely many solutions\"\n    else:\n        print(\"\u552f\u4e00\u89e3\")\n        solution_type = \"unique solution\"\n\n    # \u7c21\u7d04\u968e\u6bb5\u884c\u5217\u3092\u8868\u793a\n    reduced = to_reduced_echelon_form(augmented_matrix)\n    print(\"\u7c21\u7d04\u968e\u6bb5\u884c\u5217:\")\n    print_matrix(reduced)\n\n    return r_A, r_Aug, solution_type\n\n# \u4f8b1: \u552f\u4e00\u89e3\u3092\u6301\u3064\u7cfb\nA1 = np.array([\n    [2, 1],\n    [1, 1]\n])\nb1 = np.array([[4], [3]])\n\n# \u4f8b2: \u7121\u6570\u306e\u89e3\u3092\u6301\u3064\u7cfb\nA2 = np.array([\n    [1, 2, 3],\n    [2, 4, 6]\n])\nb2 = np.array([[6], [12]])\n\n# \u4f8b3: \u89e3\u3092\u6301\u305f\u306a\u3044\u7cfb\nA3 = np.array([\n    [1, 1],\n    [2, 2]\n])\nb3 = np.array([[2], [5]])\n\nprint(\"\u4f8b1: \")\nanalyze_system(A1, b1)\nprint(\"\\n\u4f8b2: \")\nanalyze_system(A2, b2)\nprint(\"\\n\u4f8b3: \")\nanalyze_system(A3, b3)\n</code></pre>"},{"location":"lectures/LA/12-system-of-linear-equation/#64","title":"6.4 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u304a\u3051\u308b\u30e9\u30f3\u30af\u3068\u6b21\u5143\u524a\u6e1b","text":"<p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u30e9\u30f3\u30af\u306e\u6982\u5ff5\u306f\u6b21\u5143\u524a\u6e1b\u3068\u5bc6\u63a5\u306b\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u7c21\u5358\u306a\u4f8b\u3068\u3057\u3066\u3001\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u524d\u51e6\u7406\u3068\u3057\u3066\u306e\u30e9\u30f3\u30af\u8a08\u7b97\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n# \u30a2\u30e4\u30e1\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080\niris = load_iris()\nX = iris.data\nfeature_names = iris.feature_names\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3068\u3057\u3066\u8868\u793a\ndf = pd.DataFrame(X, columns=feature_names)\nprint(\"Iris \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08\u6700\u521d\u306e5\u884c\uff09:\")\nprint(df.head())\n\n# \u30c7\u30fc\u30bf\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\nr = rank(X)\nprint(f\"\\n\u30c7\u30fc\u30bf\u884c\u5217\u306e\u30e9\u30f3\u30af: {r}\")\n\n# \u76f8\u95a2\u884c\u5217\u3092\u8a08\u7b97\nX_centered = X - X.mean(axis=0)\ncorr_matrix = np.corrcoef(X.T)\nprint(\"\\n\u7279\u5fb4\u91cf\u9593\u306e\u76f8\u95a2\u884c\u5217:\")\nprint_matrix(corr_matrix)\n\n# \u76f8\u95a2\u884c\u5217\u306e\u30e9\u30f3\u30af\nr_corr = rank(corr_matrix)\nprint(f\"\u76f8\u95a2\u884c\u5217\u306e\u30e9\u30f3\u30af: {r_corr}\")\n\n# \u4e3b\u6210\u5206\u5206\u6790\u3092\u5b9f\u884c\npca = PCA()\npca.fit(X)\n\n# \u7d2f\u7a4d\u5bc4\u4e0e\u7387\u3092\u8a08\u7b97\ncumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n\n# \u7d50\u679c\u3092\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, alpha=0.6, label='Individual explained variance')\nplt.step(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, where='mid', label='Cumulative explained variance')\nplt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\nplt.xlabel('Principal Components')\nplt.ylabel('Explained Variance Ratio')\nplt.title('PCA: Explained Variance by Components')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(\"\\n\u5404\u4e3b\u6210\u5206\u306e\u5bc4\u4e0e\u7387:\")\nfor i, ratio in enumerate(pca.explained_variance_ratio_):\n    print(f\"\u4e3b\u6210\u5206{i+1}: {ratio:.4f} ({ratio*100:.22f}%)\")\nprint(\"\\n\u7d2f\u7a4d\u5bc4\u4e0e\u7387:\")\nfor i, ratio in enumerate(cumulative_variance_ratio):\nprint(f\"\u4e3b\u6210\u5206{i+1}\u307e\u3067: {ratio:.4f} ({ratio*100:.2f}%)\")\n</code></pre> <p>\u5b9f\u884c\u3059\u308b\u3068\u3001Iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e4\u3064\u306e\u7279\u5fb4\u91cf\u306e\u9593\u306e\u76f8\u95a2\u95a2\u4fc2\u3092\u5206\u6790\u3057\u3001\u30c7\u30fc\u30bf\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u4e3b\u6210\u5206\u5206\u6790\u306e\u7d50\u679c\u3092\u8868\u793a\u3057\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306b\u3001\u30e9\u30f3\u30af\u306e\u6982\u5ff5\u306f\u30c7\u30fc\u30bf\u306e\u672c\u8cea\u7684\u306a\u6b21\u5143\u6570\u3092\u628a\u63e1\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/12-system-of-linear-equation/#71","title":"7.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u3092\u968e\u6bb5\u884c\u5217\u306b\u5909\u5f62\u3057\u3001\u30e9\u30f3\u30af\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    \\(\\(A = \\begin{pmatrix}    1 &amp; 2 &amp; 3 &amp; 4 \\\\    2 &amp; 4 &amp; 6 &amp; 8 \\\\    3 &amp; 6 &amp; 8 &amp; 10    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u3092\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306b\u5909\u5f62\u3057\u3001\u30e9\u30f3\u30af\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    \\(\\(B = \\begin{pmatrix}    1 &amp; 2 &amp; 3 \\\\    4 &amp; 5 &amp; 6 \\\\    7 &amp; 8 &amp; 9    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u6c42\u3081\u3001\u5217\u30d9\u30af\u30c8\u30eb\u306e\u7dda\u5f62\u72ec\u7acb\u6027\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002    \\(\\(C = \\begin{pmatrix}    2 &amp; 4 &amp; 2 \\\\    1 &amp; 2 &amp; 1 \\\\    3 &amp; 6 &amp; 3    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u3001\u305d\u306e\u8ee2\u7f6e\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u6c42\u3081\u3001\u4e21\u8005\u304c\u7b49\u3057\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002    \\(\\(D = \\begin{pmatrix}    1 &amp; 0 &amp; 2 &amp; -1 \\\\    0 &amp; 1 &amp; 3 &amp; 2 \\\\    2 &amp; 1 &amp; 7 &amp; 0    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u884c\u5217\u3067\u8868\u3057\u3001\u4fc2\u6570\u884c\u5217\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u304b\u3089\u3001\u89e3\u306e\u5b58\u5728\u3068\u7a2e\u985e\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3057\u306a\u3055\u3044\u3002    $$    \\begin{align}    x + 2y - z &amp;= 5 \\    2x + 4y - 2z &amp;= 10 \\    3x + 6y - 3z &amp;= 20    \\end{align}    $$</p> </li> </ol>"},{"location":"lectures/LA/12-system-of-linear-equation/#72","title":"7.2 \u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u884c\u5217 \\(A\\) \u3068 \\(B\\) \u306b\u3064\u3044\u3066\u3001\\(\\text{rank}(AB) \\leq \\min(\\text{rank}(A), \\text{rank}(B))\\) \u3068\u306a\u308b\u3053\u3068\u3092\u3001\u6b21\u306e\u4f8b\u3092\u7528\u3044\u3066\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002    \\(\\(A = \\begin{pmatrix}    1 &amp; 2 \\\\    3 &amp; 4 \\\\    5 &amp; 6    \\end{pmatrix}, B = \\begin{pmatrix}    1 &amp; 0 &amp; 2 \\\\    0 &amp; 1 &amp; 1    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u884c\u5217 \\(A\\) \u304c\u6b21\u306e\u3068\u304d\u3001\\(\\text{rank}(A^T A)\\) \u3068 \\(\\text{rank}(A A^T)\\) \u3092\u8a08\u7b97\u3057\u3001\u4e21\u8005\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002    \\(\\(A = \\begin{pmatrix}    1 &amp; 0 &amp; 2 \\\\    0 &amp; 1 &amp; 1 \\\\    0 &amp; 0 &amp; 0    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b63\u65b9\u884c\u5217 \\(A\\) \u306b\u3064\u3044\u3066\u3001\\(\\text{rank}(A) = n\\) \u3067\u3042\u308b\u3053\u3068\u3068\u3001\\(A\\) \u304c\u6b63\u5247\uff08\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\uff09\u3067\u3042\u308b\u3053\u3068\u304c\u540c\u5024\u3067\u3042\u308b\u3053\u3068\u3092\u3001\u6b21\u306e\u4f8b\u3092\u7528\u3044\u3066\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002    \\(\\(A_1 = \\begin{pmatrix}    1 &amp; 2 \\\\    3 &amp; 4    \\end{pmatrix}, A_2 = \\begin{pmatrix}    1 &amp; 2 \\\\    2 &amp; 4    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u8907\u6570\u306e\u751f\u4f53\u6307\u6a19\uff08\u8840\u5727\u3001\u5fc3\u62cd\u6570\u3001\u4f53\u6e29\u306a\u3069\uff09\u3092\u542b\u3080\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u3042\u308b\u3068\u304d\u3001\u3053\u308c\u3089\u306e\u6307\u6a19\u9593\u306b\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3001\u30c7\u30fc\u30bf\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u6570\u3088\u308a\u3082\u5c0f\u3055\u304f\u306a\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u72b6\u6cc1\u3092\u4ee5\u4e0b\u306e\u884c\u5217\u3092\u4f8b\u306b\u3057\u3066\u8aac\u660e\u3057\u306a\u3055\u3044\u3002    $$    X = \\begin{pmatrix}    120 &amp; 72 &amp; 36.5 &amp; 96 \\    115 &amp; 70 &amp; 36.7 &amp; 94 \\    125 &amp; 75 &amp; 36.3 &amp; 98 \\    130 &amp; 78 &amp; 36.2 &amp; 100 \\    118 &amp; 71 &amp; 36.6 &amp; 95    \\end{pmatrix}    $$</p> </li> </ol> <p>\u3053\u3053\u3067\u5404\u5217\u306f\u3001\u53ce\u7e2e\u671f\u8840\u5727\u3001\u62e1\u5f35\u671f\u8840\u5727\u3001\u4f53\u6e29\u3001\u5fc3\u62cd\u6570\u3092\u8868\u3057\u307e\u3059\u3002\u3082\u3057\u53ce\u7e2e\u671f\u8840\u5727\u3068\u62e1\u5f35\u671f\u8840\u5727\u306b\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3001\u30c7\u30fc\u30bf\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u3069\u306e\u3088\u3046\u306b\u306a\u308b\u304b\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/12-system-of-linear-equation/#q1","title":"Q1: \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u6b21\u5143\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A1: \u884c\u5217\u306e\u300c\u6b21\u5143\u300d\u306f\u901a\u5e38\u3001\u884c\u6570\u00d7\u5217\u6570\u306e\u5f62\u5f0f\u3067\u8868\u3055\u308c\u308b\u884c\u5217\u306e\u30b5\u30a4\u30ba\u3092\u6307\u3057\u307e\u3059\uff08\u4f8b\uff1a3\u00d74\u884c\u5217\uff09\u3002\u4e00\u65b9\u3001\u300c\u30e9\u30f3\u30af\u300d\u306f\u884c\u5217\u5185\u306e\u7dda\u5f62\u72ec\u7acb\u306a\u884c\u307e\u305f\u306f\u5217\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u6570\u3067\u3059\u3002\u30e9\u30f3\u30af\u306f\u884c\u5217\u304c\u8868\u73fe\u3067\u304d\u308b\u90e8\u5206\u7a7a\u9593\u306e\u6b21\u5143\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>m\u00d7n\u884c\u5217\u306e\u30e9\u30f3\u30afr\u306f\u5e38\u306b r \u2264 min(m, n) \u3092\u6e80\u305f\u3057\u307e\u3059\u3002\u30e9\u30f3\u30af\u304c\u6700\u5927\uff08r = min(m, n)\uff09\u306e\u3068\u304d\u3001\u30d5\u30eb\u30e9\u30f3\u30af\u884c\u5217\u3068\u547c\u3073\u307e\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#q2","title":"Q2: \u968e\u6bb5\u884c\u5217\u3068\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A2: \u968e\u6bb5\u884c\u5217\u3068\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u4e3b\u306a\u9055\u3044\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ul> <li>\u968e\u6bb5\u884c\u5217\uff1a</li> <li>\u975e\u30bc\u30ed\u884c\u306f\u4e0a\u90e8\u306b\u96c6\u3081\u3089\u308c\u308b</li> <li> <p>\u5404\u884c\u306e\u5148\u982d\u306e\u975e\u30bc\u30ed\u8981\u7d20\uff08\u5148\u982d\u4fc2\u6570\uff09\u306f\u3001\u524d\u306e\u884c\u306e\u5148\u982d\u4fc2\u6570\u3088\u308a\u53f3\u306b\u3042\u308b</p> </li> <li> <p>\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306f\u968e\u6bb5\u884c\u5217\u306e\u6761\u4ef6\u306b\u52a0\u3048\u3066\uff1a</p> </li> <li>\u5404\u884c\u306e\u5148\u982d\u4fc2\u6570\u306f1\u3067\u3042\u308b</li> <li>\u5148\u982d\u4fc2\u6570\u306e\u3042\u308b\u5217\u3067\u306f\u3001\u305d\u306e\u4ed6\u306e\u8981\u7d20\u306f\u3059\u3079\u30660\u3067\u3042\u308b</li> </ul> <p>\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306f\u3088\u308a\u53b3\u3057\u3044\u6761\u4ef6\u3092\u6301\u3061\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u76f4\u63a5\u8aad\u307f\u53d6\u308a\u3084\u3059\u3044\u5f62\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#q3","title":"Q3: \u306a\u305c\u884c\u57fa\u672c\u5909\u5f62\u306f\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u5909\u3048\u306a\u3044\u306e\u3067\u3059\u304b\uff1f","text":"<p>A3: \u884c\u57fa\u672c\u5909\u5f62\uff08\u884c\u306e\u5165\u308c\u66ff\u3048\u3001\u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u3001\u3042\u308b\u884c\u306e\u5b9a\u6570\u500d\u3092\u5225\u306e\u884c\u306b\u52a0\u3048\u308b\uff09\u306f\u3001\u884c\u30d9\u30af\u30c8\u30eb\u306e\u7dda\u5f62\u7d50\u5408\u3092\u53d6\u308b\u64cd\u4f5c\u3067\u3059\u3002\u3053\u308c\u3089\u306e\u64cd\u4f5c\u306f\u884c\u30d9\u30af\u30c8\u30eb\u306e\u7dda\u5f62\u5f93\u5c5e\u6027\u3092\u4fdd\u5b58\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u5909\u5f62\u524d\u306b\u7dda\u5f62\u5f93\u5c5e\u3060\u3063\u305f\u884c\u30d9\u30af\u30c8\u30eb\u306e\u96c6\u5408\u306f\u3001\u5909\u5f62\u5f8c\u3082\u7dda\u5f62\u5f93\u5c5e\u3067\u3059\u3002\u540c\u69d8\u306b\u3001\u7dda\u5f62\u72ec\u7acb\u3060\u3063\u305f\u884c\u30d9\u30af\u30c8\u30eb\u306e\u96c6\u5408\u3082\u7dda\u5f62\u72ec\u7acb\u6027\u3092\u4fdd\u3061\u307e\u3059\u3002</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u6700\u5927\u7dda\u5f62\u72ec\u7acb\u30d9\u30af\u30c8\u30eb\u6570\uff08\u30e9\u30f3\u30af\uff09\u306f\u884c\u57fa\u672c\u5909\u5f62\u306b\u3088\u3063\u3066\u5909\u308f\u308a\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#q4-rank-deficient","title":"Q4: \u30e9\u30f3\u30af\u843d\u3061\uff08rank deficient\uff09\u884c\u5217\u3068\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A4: \u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\u3068\u306f\u3001\u305d\u306e\u30e9\u30f3\u30af\u304c\u884c\u6570\u307e\u305f\u306f\u5217\u6570\uff08\u5c0f\u3055\u3044\u65b9\uff09\u3088\u308a\u5c0f\u3055\u3044\u884c\u5217\u306e\u3053\u3068\u3067\u3059\u3002\u3064\u307e\u308a\u3001m\u00d7n\u884c\u5217\u306b\u304a\u3044\u3066\u3001rank(A) &lt; min(m, n) \u3092\u6e80\u305f\u3059\u884c\u5217\u3067\u3059\u3002</p> <p>\u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\u306f\u3001\u884c\u30d9\u30af\u30c8\u30eb\u307e\u305f\u306f\u5217\u30d9\u30af\u30c8\u30eb\u306e\u9593\u306b\u7dda\u5f62\u5f93\u5c5e\u95a2\u4fc2\u304c\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u6587\u8108\u3067\u306f\u3001\u65b9\u7a0b\u5f0f\u306e\u9593\u306b\u5197\u9577\u6027\u304c\u3042\u308b\u304b\u3001\u307e\u305f\u306f\u77db\u76fe\u304c\u3042\u308b\u3053\u3068\u3092\u793a\u5506\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#q5","title":"Q5: \u30e9\u30f3\u30af\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u95a2\u4fc2\u3092\u7c21\u5358\u306b\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A5: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f Ax = b \u306b\u304a\u3044\u3066\u3001\u4fc2\u6570\u884c\u5217A\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217[A|b]\u306e\u30e9\u30f3\u30af\u306e\u95a2\u4fc2\u304b\u3089\u3001\u89e3\u306e\u5b58\u5728\u3068\u4e00\u610f\u6027\u306b\u3064\u3044\u3066\u6b21\u306e\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\uff1a</p> <ol> <li>rank(A) = rank([A|b]) &lt; n\uff08n\u306f\u672a\u77e5\u6570\u306e\u6570\uff09\uff1a</li> <li>\u89e3\u306f\u7121\u6570\u306b\u5b58\u5728\u3059\u308b\uff08\u4e0d\u5b9a\uff09</li> <li> <p>\u81ea\u7531\u5909\u6570\u306e\u6570\u306f n - rank(A)</p> </li> <li> <p>rank(A) = rank([A|b]) = n\uff1a</p> </li> <li> <p>\u552f\u4e00\u306e\u89e3\u304c\u5b58\u5728\u3059\u308b</p> </li> <li> <p>rank(A) &lt; rank([A|b])\uff1a</p> </li> <li>\u89e3\u306f\u5b58\u5728\u3057\u306a\u3044\uff08\u4e0d\u80fd\uff09</li> </ol> <p>\u3053\u306e\u95a2\u4fc2\u306f\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5206\u985e\u306b\u304a\u3044\u3066\u975e\u5e38\u306b\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/12-system-of-linear-equation/#q6-pca","title":"Q6: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306b\u304a\u3044\u3066\u30e9\u30f3\u30af\u306f\u3069\u306e\u3088\u3046\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u304b\uff1f","text":"<p>A6: \u4e3b\u6210\u5206\u5206\u6790\u3067\u306f\u3001\u30c7\u30fc\u30bf\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u672c\u8cea\u7684\u306b\u5fc5\u8981\u306a\u4e3b\u6210\u5206\u306e\u6570\u3092\u793a\u5506\u3057\u307e\u3059\u3002\u30c7\u30fc\u30bf\u884c\u5217X\u306e\u30e9\u30f3\u30af\u304cr\u3067\u3042\u308b\u5834\u5408\u3001r\u500b\u306e\u4e3b\u6210\u5206\u3067\u30c7\u30fc\u30bf\u306e\u5168\u5206\u6563\u3092\u8aac\u660e\u3067\u304d\u307e\u3059\u3002</p> <p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u30ce\u30a4\u30ba\u3084\u6e2c\u5b9a\u8aa4\u5dee\u306b\u3088\u3063\u3066\u5c0f\u3055\u306a\u56fa\u6709\u5024\u304c\u751f\u3058\u308b\u305f\u3081\u3001\u7406\u8ad6\u4e0a\u306e\u30e9\u30f3\u30af\u3088\u308a\u3082\u5c11\u306a\u3044\u4e3b\u6210\u5206\u6570\u3092\u9078\u3076\u3053\u3068\u304c\u4e00\u822c\u7684\u3067\u3059\u3002\u3057\u304b\u3057\u3001\u30e9\u30f3\u30af\u306e\u6982\u5ff5\u306f\u3001\u30c7\u30fc\u30bf\u306b\u5185\u5728\u3059\u308b\u672c\u8cea\u7684\u306a\u6b21\u5143\u6570\u3092\u628a\u63e1\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c13\u56de\uff1a\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c13\u56de \u95a2\u9023\u9805\u76ee: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5206\u985e\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af \u4e88\u7fd2\u5185\u5bb9:  - \u7b2c11\u56de\u300c\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3068\u89e3\u306e\u63a2\u7d22\u300d\u306e\u5185\u5bb9\u3092\u5fa9\u7fd2\u3057\u3066\u304a\u304f\u3053\u3068 - \u7b2c12\u56de\u300c\u30e9\u30f3\u30af\u306e\u6982\u5ff5\u3068\u305d\u306e\u8a08\u7b97\u300d\u306e\u5185\u5bb9\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u70b9\u3067\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\uff1a</p> <ol> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e\uff08\u552f\u4e00\u89e3\u3001\u7121\u6570\u306e\u89e3\u3001\u89e3\u306a\u3057\uff09\u3092\u7406\u89e3\u3057\u3001\u533a\u5225\u3067\u304d\u308b</li> <li>\u5177\u4f53\u7684\u306a\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3092\u5224\u5b9a\u3067\u304d\u308b</li> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u304c\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u95a2\u4fc2\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>Google Colab\u3092\u7528\u3044\u3066\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u8996\u899a\u5316\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/13-system-of-linear-equation/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#31","title":"3.1 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u5fa9\u7fd2","text":"<p>\\(n\\)\u500b\u306e\u672a\u77e5\u6570 \\(x_1, x_2, \\ldots, x_n\\) \u306b\u95a2\u3059\u308b \\(m\\)\u500b\u306e\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u96c6\u307e\u308a\u3092\u8003\u3048\u307e\u3059\uff1a</p> \\[ \\begin{align} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &amp;= b_1\\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &amp;= b_2\\\\ \\vdots\\\\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &amp;= b_m \\end{align} \\] <p>\u3053\u308c\u3092\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u8868\u73fe\u3059\u308b\u3068\uff1a</p> \\[A\\mathbf{x} = \\mathbf{b}\\] <p>\u3053\u3053\u3067\u3001\\(A\\)\u306f\\(m \\times n\\)\u306e\u4fc2\u6570\u884c\u5217\u3001\\(\\mathbf{x}\\)\u306f\\(n\\)\u6b21\u5143\u306e\u672a\u77e5\u6570\u30d9\u30af\u30c8\u30eb\u3001\\(\\mathbf{b}\\)\u306f\\(m\\)\u6b21\u5143\u306e\u53f3\u8fba\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#32","title":"3.2 \u89e3\u306e\u5b9a\u7fa9\u3068\u7a2e\u985e","text":"<p>\u5b9a\u7fa9: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f \\(A\\mathbf{x} = \\mathbf{b}\\) \u306e\u300c\u89e3\u300d\u3068\u306f\u3001\u65b9\u7a0b\u5f0f\u3092\u6e80\u305f\u3059\u672a\u77e5\u6570\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u306e\u5024\u306e\u3053\u3068\u3067\u3059\u3002</p> <p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306f\u3001\u4ee5\u4e0b\u306e3\u3064\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u306b\u5206\u985e\u3055\u308c\u307e\u3059\uff1a</p> <ol> <li>\u552f\u4e00\u89e3 (Unique solution): \u89e3\u304c\u305f\u30601\u3064\u3060\u3051\u5b58\u5728\u3059\u308b\u5834\u5408</li> <li>\u7121\u6570\u306e\u89e3 (Infinitely many solutions): \u89e3\u304c\u7121\u9650\u306b\u5b58\u5728\u3059\u308b\u5834\u5408</li> <li>\u89e3\u306a\u3057 (No solution): \u65b9\u7a0b\u5f0f\u3092\u6e80\u305f\u3059\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408</li> </ol>"},{"location":"lectures/LA/13-system-of-linear-equation/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#41","title":"4.1 \u65b9\u7a0b\u5f0f\u3068\u672a\u77e5\u6570\u306e\u6570\u306b\u3088\u308b\u89e3\u306e\u95a2\u4fc2","text":"<p>\\(m\\)\u500b\u306e\u65b9\u7a0b\u5f0f\u3068\\(n\\)\u500b\u306e\u672a\u77e5\u6570\u304c\u3042\u308b\u5834\u5408\uff1a</p> <ol> <li>\\(m &lt; n\\): \u65b9\u7a0b\u5f0f\u306e\u6570\u3088\u308a\u672a\u77e5\u6570\u306e\u6570\u304c\u591a\u3044\u5834\u5408\uff08\u4e0d\u8db3\u6c7a\u5b9a\u7cfb\uff09</li> <li> <p>\u901a\u5e38\u3001\u89e3\u306f\u7121\u6570\u306b\u5b58\u5728\u3059\u308b\uff08\u305f\u3060\u3057\u89e3\u304c\u306a\u3044\u5834\u5408\u3082\u3042\u308b\uff09</p> </li> <li> <p>\\(m = n\\): \u65b9\u7a0b\u5f0f\u306e\u6570\u3068\u672a\u77e5\u6570\u306e\u6570\u304c\u7b49\u3057\u3044\u5834\u5408\uff08\u5b8c\u5168\u6c7a\u5b9a\u7cfb\uff09</p> </li> <li>\u4fc2\u6570\u884c\u5217\\(A\\)\u304c\u6b63\u5247\uff08\u884c\u5217\u5f0f\\(\\det(A) \\neq 0\\)\uff09\u306a\u3089\u552f\u4e00\u89e3\u304c\u5b58\u5728</li> <li> <p>\u4fc2\u6570\u884c\u5217\\(A\\)\u304c\u7279\u7570\uff08\u884c\u5217\u5f0f\\(\\det(A) = 0\\)\uff09\u306a\u3089\u89e3\u306a\u3057\u307e\u305f\u306f\u7121\u6570\u306e\u89e3</p> </li> <li> <p>\\(m &gt; n\\): \u65b9\u7a0b\u5f0f\u306e\u6570\u304c\u672a\u77e5\u6570\u306e\u6570\u3088\u308a\u591a\u3044\u5834\u5408\uff08\u904e\u5270\u6c7a\u5b9a\u7cfb\uff09</p> </li> <li>\u901a\u5e38\u3001\u89e3\u306f\u5b58\u5728\u3057\u306a\u3044\uff08\u305f\u3060\u3057\u7279\u6b8a\u306a\u5834\u5408\u306f\u89e3\u304c\u3042\u308b\u3053\u3068\u3082\u3042\u308b\uff09</li> </ol>"},{"location":"lectures/LA/13-system-of-linear-equation/#42","title":"4.2 \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u89e3\u306e\u95a2\u4fc2","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f \\(A\\mathbf{x} = \\mathbf{b}\\) \u306e\u89e3\u306e\u5b58\u5728\u3068\u7a2e\u985e\u306f\u3001\u4fc2\u6570\u884c\u5217 \\(A\\) \u306e\u30e9\u30f3\u30af \\(\\text{rank}(A)\\) \u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217 \\([A|\\mathbf{b}]\\) \u306e\u30e9\u30f3\u30af \\(\\text{rank}([A|\\mathbf{b}])\\) \u306b\u3088\u3063\u3066\u6c7a\u5b9a\u3055\u308c\u307e\u3059\u3002</p> <p>\u5b9a\u7406: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f \\(A\\mathbf{x} = \\mathbf{b}\\) \uff08\\(A\\)\u306f\\(m \\times n\\)\u884c\u5217\uff09\u306b\u3064\u3044\u3066\uff1a</p> <ol> <li>\\(\\text{rank}(A) = \\text{rank}([A|\\mathbf{b}]) &lt; n\\) \u306e\u5834\u5408\uff1a\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3059\u308b</li> <li>\\(\\text{rank}(A) = \\text{rank}([A|\\mathbf{b}]) = n\\) \u306e\u5834\u5408\uff1a\u552f\u4e00\u89e3\u304c\u5b58\u5728\u3059\u308b</li> <li>\\(\\text{rank}(A) &lt; \\text{rank}([A|\\mathbf{b}])\\) \u306e\u5834\u5408\uff1a\u89e3\u306a\u3057</li> </ol>"},{"location":"lectures/LA/13-system-of-linear-equation/#43","title":"4.3 \u7c21\u7d04\u968e\u6bb5\u884c\u5217\u3092\u7528\u3044\u305f\u89e3\u306e\u5224\u5b9a","text":"<p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u3088\u3063\u3066\u5f97\u3089\u308c\u305f\u7c21\u7d04\u968e\u6bb5\u884c\u5217\u306e\u5f62\u304b\u3089\u3001\u89e3\u306e\u7a2e\u985e\u3092\u5224\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <ol> <li>\u552f\u4e00\u89e3\uff1a\u5404\u5909\u6570\u306b\u5bfe\u5fdc\u3059\u308b\u4e3b\u6210\u5206\uff08leading entry\uff09\u304c\u3042\u308a\u3001\u77db\u76fe\u3059\u308b\u65b9\u7a0b\u5f0f\u304c\u306a\u3044</li> <li>\u7121\u6570\u306e\u89e3\uff1a\u30d5\u30ea\u30fc\u5909\u6570\uff08\u81ea\u7531\u5909\u6570\uff09\u304c\u5b58\u5728\u3057\u3001\u77db\u76fe\u3059\u308b\u65b9\u7a0b\u5f0f\u304c\u306a\u3044</li> <li>\u89e3\u306a\u3057\uff1a<code>0 = \u975e\u30bc\u30ed\u5b9a\u6570</code>\u3068\u3044\u3046\u77db\u76fe\u3059\u308b\u884c\u304c\u5b58\u5728\u3059\u308b</li> </ol>"},{"location":"lectures/LA/13-system-of-linear-equation/#5","title":"5. \u5177\u4f53\u4f8b\u3068\u89e3\u8aac","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#51","title":"5.1 \u552f\u4e00\u89e3\u3092\u6301\u3064\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u4f8b","text":"\\[ \\begin{align} x + 2y &amp;= 5\\\\ 2x - y &amp;= 0 \\end{align} \\] <p>\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u884c\u5217\u5f62\u5f0f\u3067\u8868\u3059\u3068\uff1a</p> \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix} \\] <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3067\u89e3\u304f\u3068\uff1a</p> \\[ \\begin{bmatrix} 1 &amp; 2 &amp; 5 \\\\ 2 &amp; -1 &amp; 0 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 2 &amp; 5 \\\\ 0 &amp; -5 &amp; -10 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 2 &amp; 5 \\\\ 0 &amp; 1 &amp; 2 \\end{bmatrix} \\] <p>\u5f8c\u9000\u4ee3\u5165\u306b\u3088\u308a\u3001\\(y = 2\\), \\(x + 2(2) = 5\\) \u3088\u308a \\(x = 1\\) \u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u89e3\u306f\\(x = 1\\), \\(y = 2\\)\u306e\u552f\u4e00\u89e3\u3067\u3059\u3002</p> <p>\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\([A|b]\\)\u306e\u30e9\u30f3\u30af\u3092\u8003\u3048\u308b\u3068\uff1a - \\(\\text{rank}(A) = 2\\) - \\(\\text{rank}([A|b]) = 2\\) - \u672a\u77e5\u6570\u306e\u6570\\(n = 2\\)</p> <p>\\(\\text{rank}(A) = \\text{rank}([A|b]) = n\\)\u306a\u306e\u3067\u3001\u552f\u4e00\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#52","title":"5.2 \u7121\u6570\u306e\u89e3\u3092\u6301\u3064\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u4f8b","text":"\\[ \\begin{align} x + 2y - z &amp;= 5\\\\ 2x - y + z &amp;= 0\\\\ 3x + y &amp;= 5 \\end{align} \\] <p>\u884c\u5217\u5f62\u5f0f\u3067\uff1a</p> \\[ \\begin{bmatrix} 1 &amp; 2 &amp; -1 \\\\ 2 &amp; -1 &amp; 1 \\\\ 3 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 0 \\\\ 5 \\end{bmatrix} \\] <p>\u30ac\u30a6\u30b9\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3059\u308b\u3068\uff1a</p> \\[ \\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; 5 \\\\ 2 &amp; -1 &amp; 1 &amp; 0 \\\\ 3 &amp; 1 &amp; 0 &amp; 5 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; 5 \\\\ 0 &amp; -5 &amp; 3 &amp; -10 \\\\ 0 &amp; -5 &amp; 3 &amp; -10 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; 5 \\\\ 0 &amp; 1 &amp; -\\frac{3}{5} &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\] <p>3\u884c\u76ee\u306f\u3059\u3079\u30660\u3068\u306a\u308a\u3001\u65b9\u7a0b\u5f0f\u306e\u6570\u304c\u5b9f\u8cea\u7684\u306b\u6e1b\u5c11\u3057\u307e\u3057\u305f\u3002\\(z\\)\u3092\u81ea\u7531\u5909\u6570\uff08\u30d1\u30e9\u30e1\u30fc\u30bf\uff09\u3068\u3059\u308b\u3068\uff1a</p> <p>\\(y - \\frac{3}{5}z = 2\\) \u3088\u308a \\(y = 2 + \\frac{3}{5}z\\)</p> <p>\\(x + 2y - z = 5\\) \u3088\u308a \\(x = 5 - 2y + z = 5 - 2(2 + \\frac{3}{5}z) + z = 5 - 4 - \\frac{6}{5}z + z = 1 - \\frac{1}{5}z\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u89e3\u306f \\(z\\)\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066\uff1a \\(x = 1 - \\frac{1}{5}z\\), \\(y = 2 + \\frac{3}{5}z\\), \\(z = z\\)\uff08\u4efb\u610f\u306e\u5024\uff09</p> <p>\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\([A|b]\\)\u306e\u30e9\u30f3\u30af\u3092\u8003\u3048\u308b\u3068\uff1a - \\(\\text{rank}(A) = 2\\) - \\(\\text{rank}([A|b]) = 2\\) - \u672a\u77e5\u6570\u306e\u6570\\(n = 3\\)</p> <p>\\(\\text{rank}(A) = \\text{rank}([A|b]) &lt; n\\)\u306a\u306e\u3067\u3001\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#53","title":"5.3 \u89e3\u304c\u306a\u3044\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u4f8b","text":"\\[ \\begin{align} x + 2y &amp;= 5\\\\ 2x + 4y &amp;= 6 \\end{align} \\] <p>\u884c\u5217\u5f62\u5f0f\u3067\uff1a</p> \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 4 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix} \\] <p>\u30ac\u30a6\u30b9\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3059\u308b\u3068\uff1a</p> \\[ \\begin{bmatrix} 1 &amp; 2 &amp; 5 \\\\ 2 &amp; 4 &amp; 6 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 2 &amp; 5 \\\\ 0 &amp; 0 &amp; -4 \\end{bmatrix} \\] <p>2\u884c\u76ee\u304c \\(0 = -4\\) \u3068\u306a\u308a\u3001\u3053\u308c\u306f\u77db\u76fe\u3057\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u89e3\u3092\u6301\u3061\u307e\u305b\u3093\u3002</p> <p>\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\([A|b]\\)\u306e\u30e9\u30f3\u30af\u3092\u8003\u3048\u308b\u3068\uff1a - \\(\\text{rank}(A) = 1\\) - \\(\\text{rank}([A|b]) = 2\\)</p> <p>\\(\\text{rank}(A) &lt; \\text{rank}([A|b])\\)\u306a\u306e\u3067\u3001\u89e3\u306f\u5b58\u5728\u3057\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#6","title":"6. \u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#61-2","title":"6.1 2\u6b21\u5143\u3067\u306e\u89e3\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473","text":"<p>2\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u5834\u5408\u3001\u5404\u65b9\u7a0b\u5f0f\u306f\u5e73\u9762\u4e0a\u306e\u76f4\u7dda\u3092\u8868\u3057\u307e\u3059\u3002</p> <ol> <li>\u552f\u4e00\u89e3: 2\u3064\u306e\u76f4\u7dda\u304c1\u70b9\u3067\u4ea4\u308f\u308b\u5834\u5408</li> <li>\u7121\u6570\u306e\u89e3: 2\u3064\u306e\u76f4\u7dda\u304c\u91cd\u306a\u308b\u5834\u5408\uff08\u540c\u4e00\u76f4\u7dda\uff09</li> <li>\u89e3\u306a\u3057: 2\u3064\u306e\u76f4\u7dda\u304c\u5e73\u884c\u3067\u4ea4\u308f\u3089\u306a\u3044\u5834\u5408</li> </ol>"},{"location":"lectures/LA/13-system-of-linear-equation/#62-3","title":"6.2 3\u6b21\u5143\u3067\u306e\u89e3\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473","text":"<p>3\u5143\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u5834\u5408\u3001\u5404\u65b9\u7a0b\u5f0f\u306f3\u6b21\u5143\u7a7a\u9593\u5185\u306e\u5e73\u9762\u3092\u8868\u3057\u307e\u3059\u3002</p> <ol> <li>\u552f\u4e00\u89e3: 3\u3064\u306e\u5e73\u9762\u304c1\u70b9\u3067\u4ea4\u308f\u308b\u5834\u5408</li> <li>\u7121\u6570\u306e\u89e3: </li> <li>3\u3064\u306e\u5e73\u9762\u304c1\u672c\u306e\u76f4\u7dda\u3067\u4ea4\u308f\u308b\u5834\u5408\uff08\u7121\u9650\u306b\u591a\u304f\u306e\u70b9\uff09</li> <li>2\u3064\u4ee5\u4e0a\u306e\u5e73\u9762\u304c\u91cd\u306a\u308a\u3001\u6b8b\u308a\u306e\u5e73\u9762\u3068\u305d\u306e\u91cd\u306a\u3063\u305f\u5e73\u9762\u304c\u4ea4\u308f\u308b\u5834\u5408</li> <li>\u89e3\u306a\u3057: </li> <li>2\u3064\u4ee5\u4e0a\u306e\u5e73\u9762\u304c\u5e73\u884c\u3067\u4ea4\u308f\u3089\u306a\u3044\u5834\u5408</li> <li>3\u3064\u306e\u5e73\u9762\u304c\u5171\u901a\u70b9\u3092\u6301\u305f\u305a\u306b\u4ea4\u308f\u308b\u5834\u5408\uff083\u3064\u306e\u5e73\u9762\u304c\u4e09\u89d2\u67f1\u306e\u3088\u3046\u306a\u5f62\u3067\u4ea4\u308f\u308b\uff09</li> </ol>"},{"location":"lectures/LA/13-system-of-linear-equation/#7-python","title":"7. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#71","title":"7.1 \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u89e3\u306e\u5224\u5b9a","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e\u3092\u5224\u5b9a\u3059\u308b\u95a2\u6570\ndef check_solution_type(A, b):\n    A_matrix = np.array(A, dtype=float)\n    b_vector = np.array(b, dtype=float)\n\n    # \u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\n    rank_A = np.linalg.matrix_rank(A_matrix)\n\n    # \u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\n    augmented = np.column_stack((A_matrix, b_vector))\n    rank_augmented = np.linalg.matrix_rank(augmented)\n\n    # \u672a\u77e5\u6570\u306e\u6570\n    n = A_matrix.shape[1]\n\n    # \u89e3\u306e\u7a2e\u985e\u3092\u5224\u5b9a\n    if rank_A &lt; rank_augmented:\n        return \"\u89e3\u306a\u3057\"\n    elif rank_A == rank_augmented and rank_A &lt; n:\n        return \"\u7121\u6570\u306e\u89e3\"\n    else:  # rank_A == rank_augmented == n\n        return \"\u552f\u4e00\u89e3\"\n\n# \u4f8b1: \u552f\u4e00\u89e3\u306e\u5834\u5408\nA1 = [[1, 2], [2, -1]]\nb1 = [5, 0]\nprint(\"\u4f8b1\u306e\u89e3\u306e\u7a2e\u985e:\", check_solution_type(A1, b1))\n\n# \u4f8b2: \u7121\u6570\u306e\u89e3\u306e\u5834\u5408\nA2 = [[1, 2, -1], [2, -1, 1], [3, 1, 0]]\nb2 = [5, 0, 5]\nprint(\"\u4f8b2\u306e\u89e3\u306e\u7a2e\u985e:\", check_solution_type(A2, b2))\n\n# \u4f8b3: \u89e3\u306a\u3057\u306e\u5834\u5408\nA3 = [[1, 2], [2, 4]]\nb3 = [5, 6]\nprint(\"\u4f8b3\u306e\u89e3\u306e\u7a2e\u985e:\", check_solution_type(A3, b3))\n</code></pre>"},{"location":"lectures/LA/13-system-of-linear-equation/#72-2","title":"7.2 2\u6b21\u5143\u3067\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u53ef\u8996\u5316","text":"<pre><code>def plot_2d_system(A, b, x_range=(-10, 10)):\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    x = np.linspace(x_range[0], x_range[1], 1000)\n    colors = ['b', 'g', 'r', 'c', 'm']\n\n    for i in range(len(b)):\n        if A[i][1] != 0:  # y\u4fc2\u6570\u304c0\u3067\u306a\u3044\u5834\u5408\n            y = (b[i] - A[i][0] * x) / A[i][1]\n            ax.plot(x, y, colors[i % len(colors)], label=f'\u5f0f{i+1}: {A[i][0]}x + {A[i][1]}y = {b[i]}')\n        else:  # y\u4fc2\u6570\u304c0\u306e\u5834\u5408\uff08\u5782\u76f4\u7dda\uff09\n            if A[i][0] != 0:\n                x_val = b[i] / A[i][0]\n                ax.axvline(x=x_val, color=colors[i % len(colors)], label=f'\u5f0f{i+1}: {A[i][0]}x = {b[i]}')\n\n    # \u30b0\u30e9\u30d5\u306e\u8a2d\u5b9a\n    ax.grid(True)\n    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n    ax.set_xlim(x_range)\n    ax.set_ylim(x_range)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_aspect('equal')\n    ax.legend()\n    ax.set_title('\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8')\n\n    plt.show()\n\n# \u4f8b1: \u552f\u4e00\u89e3 (x=1, y=2)\nplot_2d_system(A1, b1)\n\n# \u4f8b3: \u89e3\u306a\u3057 (\u5e73\u884c\u7dda)\nplot_2d_system(A3, b3)\n</code></pre>"},{"location":"lectures/LA/13-system-of-linear-equation/#73-3","title":"7.3 3\u6b21\u5143\u3067\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u53ef\u8996\u5316","text":"<pre><code>def plot_3d_system(A, b, ranges=(-5, 5)):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # \u5e73\u9762\u4e0a\u306e\u70b9\u3092\u30b0\u30ea\u30c3\u30c9\u3067\u751f\u6210\n    xx, yy = np.meshgrid(np.linspace(ranges[0], ranges[1], 10),\n                         np.linspace(ranges[0], ranges[1], 10))\n\n    # \u5404\u65b9\u7a0b\u5f0f\u306e\u5e73\u9762\u3092\u30d7\u30ed\u30c3\u30c8\n    colors = ['b', 'g', 'r']\n    for i in range(min(len(b), 3)):  # \u6700\u59273\u3064\u306e\u5e73\u9762\u307e\u3067\u8868\u793a\n        if A[i][2] != 0:  # z\u4fc2\u6570\u304c0\u3067\u306a\u3044\u5834\u5408\n            z = (b[i] - A[i][0] * xx - A[i][1] * yy) / A[i][2]\n            surf = ax.plot_surface(xx, yy, z, alpha=0.6, color=colors[i], \n                                  label=f'\u5e73\u9762{i+1}')\n            surf._facecolors2d = surf._facecolor3d\n            surf._edgecolors2d = surf._edgecolor3d\n\n    # \u30b0\u30e9\u30d5\u306e\u8a2d\u5b9a\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_xlim(ranges)\n    ax.set_ylim(ranges)\n    ax.set_zlim(ranges)\n    ax.set_title('\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e3D\u8868\u73fe')\n    ax.legend()\n\n    plt.show()\n\n# \u4f8b2: \u7121\u6570\u306e\u89e3\u306e3D\u53ef\u8996\u5316\nplot_3d_system(A2, b2)\n</code></pre>"},{"location":"lectures/LA/13-system-of-linear-equation/#8","title":"8. \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306e\u5fdc\u7528\u4f8b","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#81","title":"8.1 \u7dda\u5f62\u56de\u5e30\u306b\u304a\u3051\u308b\u89e3\u306e\u5b58\u5728","text":"<p>\u7dda\u5f62\u56de\u5e30\u3067\u306f\u3001\u30c7\u30fc\u30bf\u70b9\\((x_i, y_i)\\)\u3092\u6700\u3082\u3088\u304f\u8868\u3059\u76f4\u7dda\\(y = \\beta_0 + \\beta_1 x\\)\u3092\u6c42\u3081\u308b\u554f\u984c\u3092\u8003\u3048\u307e\u3059\u3002\u4e00\u822c\u306b\u3001\u30c7\u30fc\u30bf\u70b9\u306e\u6570\\(n\\)\u304c2\u3088\u308a\u5927\u304d\u3044\u5834\u5408\u3001\u901a\u5e38\u306f\u5b8c\u5168\u306a\u89e3\u306f\u5b58\u5728\u305b\u305a\uff08\u904e\u5270\u6c7a\u5b9a\u7cfb\uff09\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u3063\u3066\u8fd1\u4f3c\u89e3\u3092\u6c42\u3081\u307e\u3059\u3002</p> <pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u751f\u6210\nnp.random.seed(42)\nx = np.random.rand(20) * 10\ny = 2 * x + 1 + np.random.randn(20) * 2\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u4f5c\u6210\ndata = pd.DataFrame({'x': x, 'y': y})\n\n# \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\nmodel = LinearRegression()\nmodel.fit(data[['x']], data['y'])\n\n# \u56de\u5e30\u76f4\u7dda\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\nbeta_0 = model.intercept_\nbeta_1 = model.coef_[0]\n\nprint(f'\u63a8\u5b9a\u3055\u308c\u305f\u76f4\u7dda: y = {beta_0:.4f} + {beta_1:.4f}x')\n\n# \u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.scatter(data['x'], data['y'], color='blue', label='\u30c7\u30fc\u30bf\u70b9')\nplt.plot(data['x'], model.predict(data[['x']]), color='red', label='\u56de\u5e30\u76f4\u7dda')\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('\u7dda\u5f62\u56de\u5e30 - \u904e\u5270\u6c7a\u5b9a\u7cfb\u306e\u8fd1\u4f3c\u89e3')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/13-system-of-linear-equation/#82","title":"8.2 \u5065\u5eb7\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308b\u4f53\u7d44\u6210\u306e\u63a8\u5b9a","text":"<p>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u4f8b\u3068\u3057\u3066\u3001\u4f53\u7d44\u6210\uff08\u4f53\u8102\u80aa\u7387\u3001\u7b4b\u8089\u91cf\u306a\u3069\uff09\u3092\u8eab\u9577\u3001\u4f53\u91cd\u3001\u5e74\u9f62\u304b\u3089\u63a8\u5b9a\u3059\u308b\u30e2\u30c7\u30eb\u3092\u8003\u3048\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\u306f\u901a\u5e38\u3001\u904e\u5270\u6c7a\u5b9a\u7cfb\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3068\u306a\u308a\u3001\u5b8c\u5168\u306a\u89e3\u3067\u306f\u306a\u304f\u6700\u9069\u306a\u8fd1\u4f3c\u89e3\u3092\u6c42\u3081\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# \u4eee\u60f3\u7684\u306a\u5065\u5eb7\u30c7\u30fc\u30bf\u3092\u751f\u6210\nnp.random.seed(42)\nn_samples = 100\n\n# \u8aac\u660e\u5909\u6570: \u8eab\u9577(cm)\u3001\u4f53\u91cd(kg)\u3001\u5e74\u9f62(years)\nheight = np.random.normal(170, 10, n_samples)\nweight = np.random.normal(70, 15, n_samples)\nage = np.random.randint(20, 70, n_samples)\n\n# \u5fdc\u7b54\u5909\u6570: \u4f53\u8102\u80aa\u7387(%) - \u8eab\u9577\u3001\u4f53\u91cd\u3001\u5e74\u9f62\u306e\u95a2\u6570\u3068\u3057\u3066\u751f\u6210\uff08\u30ce\u30a4\u30ba\u4ed8\u304d\uff09\nbody_fat = 10 + 0.3 * (weight - 70) - 0.1 * (height - 170) + 0.2 * (age - 40) + np.random.normal(0, 3, n_samples)\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u4f5c\u6210\nhealth_data = pd.DataFrame({\n    'height': height,\n    'weight': weight,\n    'age': age,\n    'body_fat': body_fat\n})\n\n# \u8aac\u660e\u5909\u6570\u3068\u5fdc\u7b54\u5909\u6570\nX = health_data[['height', 'weight', 'age']]\ny = health_data['body_fat']\n\n# \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# \u30e2\u30c7\u30eb\u306e\u4fc2\u6570\nprint(f'\u5207\u7247: {model.intercept_:.4f}')\nprint(f'\u8eab\u9577\u306e\u4fc2\u6570: {model.coef_[0]:.4f}')\nprint(f'\u4f53\u91cd\u306e\u4fc2\u6570: {model.coef_[1]:.4f}')\nprint(f'\u5e74\u9f62\u306e\u4fc2\u6570: {model.coef_[2]:.4f}')\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u4e88\u6e2c\ny_pred = model.predict(X_test)\n\n# \u4e88\u6e2c\u7cbe\u5ea6\u306e\u8a55\u4fa1\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee (MSE): {mse:.4f}')\nprint(f'\u6c7a\u5b9a\u4fc2\u6570 (R\u00b2): {r2:.4f}')\n\n# \u5b9f\u6e2c\u5024\u3068\u4e88\u6e2c\u5024\u306e\u6563\u5e03\u56f3\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, c='blue', alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('\u5b9f\u6e2c\u4f53\u8102\u80aa\u7387 (%)')\nplt.ylabel('\u4e88\u6e2c\u4f53\u8102\u80aa\u7387 (%)')\nplt.title('\u4f53\u8102\u80aa\u7387\u306e\u5b9f\u6e2c\u5024\u3068\u4e88\u6e2c\u5024\u306e\u6bd4\u8f03')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/13-system-of-linear-equation/#9","title":"9. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u6c42\u3081\u3001\u89e3\u306e\u7a2e\u985e\uff08\u552f\u4e00\u89e3\u3001\u7121\u6570\u306e\u89e3\u3001\u89e3\u306a\u3057\uff09\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002</p> <p>(a) \\(\\begin{cases} 2x + 3y = 8 \\\\ -4x - 6y = -16 \\end{cases}\\)</p> <p>(b) \\(\\begin{cases} 2x + y - z = 3 \\\\ x - y + z = 2 \\\\ 3x + 2y = 4 \\end{cases}\\)</p> <p>(c) \\(\\begin{cases} x + 2y - z = 5 \\\\ 2x + 4y - 2z = 6 \\\\ 3x + 6y - 3z = 15 \\end{cases}\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\\(A\\)\u3068\u53f3\u8fba\u30d9\u30af\u30c8\u30eb\\(\\mathbf{b}\\)\u306b\u3064\u3044\u3066\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\\(A\\mathbf{x} = \\mathbf{b}\\)\u306e\u89e3\u306e\u7a2e\u985e\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002</p> <p>(a) \\(A = \\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\ 2 &amp; 6 &amp; 4 \\\\ 3 &amp; 9 &amp; 6 \\end{bmatrix}\\), \\(\\mathbf{b} = \\begin{bmatrix} 4 \\\\ 8 \\\\ 12 \\end{bmatrix}\\)</p> <p>(b) \\(A = \\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\ 2 &amp; 6 &amp; 4 \\\\ 3 &amp; 9 &amp; 6 \\end{bmatrix}\\), \\(\\mathbf{b} = \\begin{bmatrix} 4 \\\\ 8 \\\\ 10 \\end{bmatrix}\\)</p> </li> <li> <p>\u884c\u5217\\(A\\)\u306e\u30e9\u30f3\u30af\u304c2\u3001\u62e1\u5927\u4fc2\u6570\u884c\u5217\\([A|\\mathbf{b}]\\)\u306e\u30e9\u30f3\u30af\u304c3\u3001\u672a\u77e5\u6570\u306e\u6570\u304c4\u306e\u5834\u5408\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\\(A\\mathbf{x} = \\mathbf{b}\\)\u306e\u89e3\u306e\u7a2e\u985e\u306f\u3069\u3046\u306a\u308b\u304b\u3002\u7406\u7531\u3082\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/13-system-of-linear-equation/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\\(\\lambda\\)\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3059\u308b\u9023\u7acb\u65b9\u7a0b\u5f0f     \\(\\begin{cases}     x + y + z = 6 \\\\     x + 2y + \\lambda z = 10 \\\\     x + 3y + \\lambda^2 z = \\lambda + 12     \\end{cases}\\)</p> <p>\u306b\u304a\u3044\u3066\u3001\\(\\lambda\\)\u306e\u5024\u306b\u3088\u3063\u3066\u89e3\u306e\u7a2e\u985e\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u304b\u8abf\u3079\u306a\u3055\u3044\u3002</p> </li> <li> <p>3\u6b21\u5143\u5e73\u9762\u4e0a\u306e3\u70b9 \\(P_1(1, 2, 3)\\), \\(P_2(2, 3, 1)\\), \\(P_3(3, 1, 2)\\) \u3092\u901a\u308b\u5e73\u9762\u306e\u65b9\u7a0b\u5f0f\u3092\u6c42\u3081\u306a\u3055\u3044\u3002\u3053\u306e\u554f\u984c\u3092\u3069\u306e\u3088\u3046\u306b\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3067\u304d\u308b\u304b\u8aac\u660e\u3057\u3001\u89e3\u304d\u65b9\u3092\u793a\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u5fdc\u7528\u3068\u3057\u3066\u3001\u3042\u308b\u7814\u7a76\u3067\u306f5\u540d\u306e\u88ab\u9a13\u8005\u306e\u5e74\u9f62\uff08\u5e74\uff09\u3001\u4f53\u91cd\uff08kg\uff09\u3001\u904b\u52d5\u6642\u9593\uff08\u6642\u9593/\u9031\uff09\u3001\u304a\u3088\u3073\u8840\u5727\u5024\uff08mmHg\uff09\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a18\u9332\u3055\u308c\u3066\u3044\u307e\u3059\uff1a</p> </li> </ol> \u88ab\u9a13\u8005 \u5e74\u9f62 \u4f53\u91cd \u904b\u52d5\u6642\u9593 \u8840\u5727 1 25 70 3 120 2 45 85 1 140 3 35 65 5 115 4 55 75 2 145 5 30 80 4 125 <p>\u8840\u5727\uff08\\(y\\)\uff09\u3092\u5e74\u9f62\uff08\\(x_1\\)\uff09\u3001\u4f53\u91cd\uff08\\(x_2\\)\uff09\u3001\u904b\u52d5\u6642\u9593\uff08\\(x_3\\)\uff09\u306e\u7dda\u5f62\u95a2\u6570 \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) \u3068\u3057\u3066\u8868\u305d\u3046\u3068\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>a. \u3053\u306e\u554f\u984c\u3092\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3057\u306a\u3055\u3044\u3002    b. \u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002    c. \u6700\u5c0f\u4e8c\u4e57\u6cd5\u3092\u7528\u3044\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\\(\\beta_0, \\beta_1, \\beta_2, \\beta_3\\)\uff09\u3092\u63a8\u5b9a\u3057\u306a\u3055\u3044\u3002    d. 40\u6b73\u300175kg\u3001\u9031\u306b3\u6642\u9593\u904b\u52d5\u3059\u308b\u4eba\u306e\u4e88\u60f3\u8840\u5727\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#10","title":"10. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#q1","title":"Q1: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u306a\u3044\u3068\u3044\u3046\u306e\u306f\u3001\u6570\u5b66\u7684\u306b\u3069\u3046\u3044\u3046\u610f\u5473\u3067\u3059\u304b\uff1f","text":"<p>A1: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u306f\u3001\u3059\u3079\u3066\u306e\u65b9\u7a0b\u5f0f\u3092\u540c\u6642\u306b\u6e80\u305f\u3059\u5909\u6570\u306e\u5024\u304c\u5b58\u5728\u3057\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u4f8b\u3048\u30702\u6b21\u5143\u306e\u5834\u5408\u30012\u3064\u306e\u76f4\u7dda\u304c\u5e73\u884c\u3067\u4ea4\u308f\u3089\u306a\u3044\u72b6\u6cc1\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u65b9\u7a0b\u5f0f\u9593\u306b\u77db\u76fe\u304c\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#10_1","title":"10. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54\uff08\u7d9a\u304d\uff09","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#q2","title":"Q2: \u89e3\u304c\u7121\u6570\u306b\u3042\u308b\u3068\u304d\u3001\u4e00\u822c\u89e3\u306f\u3069\u306e\u3088\u3046\u306b\u8868\u73fe\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A2: \u89e3\u304c\u7121\u6570\u306b\u3042\u308b\u5834\u5408\u3001\u4e00\u822c\u89e3\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u81ea\u7531\u5909\u6570\uff09\u3092\u7528\u3044\u3066\u8868\u73fe\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u30013\u5143\u9023\u7acb\u65b9\u7a0b\u5f0f\u3067\u89e3\u304c\u7121\u6570\u306b\u3042\u308b\u5834\u5408\u30011\u3064\u307e\u305f\u306f\u8907\u6570\u306e\u5909\u6570\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066\u9078\u3073\u3001\u6b8b\u308a\u306e\u5909\u6570\u3092\u305d\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u95a2\u6570\u3068\u3057\u3066\u8868\u3057\u307e\u3059\u3002\u4f8b\uff1a\\(z\\)\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066\u3001\\(x = 1 - \\frac{1}{5}z\\), \\(y = 2 + \\frac{3}{5}z\\)\u306e\u3088\u3046\u306b\u8868\u73fe\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#q3","title":"Q3: \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u89e3\u306e\u95a2\u4fc2\u3092\u3069\u306e\u3088\u3046\u306b\u899a\u3048\u3066\u304a\u3051\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A3: \u6b21\u306e\u95a2\u4fc2\u3092\u899a\u3048\u3066\u304a\u304f\u3068\u4fbf\u5229\u3067\u3059\uff1a 1. \\(\\text{rank}(A) &lt; \\text{rank}([A|\\mathbf{b}])\\) \u2192 \u89e3\u306a\u3057 2. \\(\\text{rank}(A) = \\text{rank}([A|\\mathbf{b}]) = n\\) \u2192 \u552f\u4e00\u89e3 3. \\(\\text{rank}(A) = \\text{rank}([A|\\mathbf{b}]) &lt; n\\) \u2192 \u7121\u6570\u306e\u89e3 \u3053\u3053\u3067\u3001\\(n\\)\u306f\u672a\u77e5\u6570\u306e\u6570\u3001\\(A\\)\u306f\u4fc2\u6570\u884c\u5217\u3001\\([A|\\mathbf{b}]\\)\u306f\u62e1\u5927\u4fc2\u6570\u884c\u5217\u3067\u3059\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#q4","title":"Q4: \u904e\u5270\u6c7a\u5b9a\u7cfb\uff08\u65b9\u7a0b\u5f0f\u306e\u6570 &gt; \u672a\u77e5\u6570\u306e\u6570\uff09\u306e\u5834\u5408\u3001\u306a\u305c\u901a\u5e38\u306f\u89e3\u304c\u306a\u3044\u306e\u3067\u3059\u304b\uff1f","text":"<p>A4: \u904e\u5270\u6c7a\u5b9a\u7cfb\u3067\u306f\u3001\u65b9\u7a0b\u5f0f\u306e\u6570\u304c\u672a\u77e5\u6570\u306e\u6570\u3088\u308a\u591a\u3044\u305f\u3081\u3001\u5404\u65b9\u7a0b\u5f0f\u304c\u8ab2\u3059\u6761\u4ef6\u3092\u3059\u3079\u3066\u540c\u6642\u306b\u6e80\u305f\u3059\u3053\u3068\u306f\u4e00\u822c\u7684\u306b\u56f0\u96e3\u3067\u3059\u3002\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u4f8b\u3048\u30703\u3064\u306e\u5e73\u9762\u304c1\u70b9\u3067\u4ea4\u308f\u308b\u78ba\u7387\u306f\u4f4e\u304f\u3001\u901a\u5e38\u306f\u5171\u901a\u70b9\u3092\u6301\u3061\u307e\u305b\u3093\u3002\u305f\u3060\u3057\u3001\u65b9\u7a0b\u5f0f\u9593\u306b\u7dda\u5f62\u5f93\u5c5e\u95a2\u4fc2\u304c\u3042\u308b\u5834\u5408\uff08\u4f8b\uff1a1\u3064\u306e\u65b9\u7a0b\u5f0f\u304c\u4ed6\u306e\u65b9\u7a0b\u5f0f\u306e\u7dda\u5f62\u7d50\u5408\u3067\u8868\u305b\u308b\u5834\u5408\uff09\u306f\u89e3\u304c\u5b58\u5728\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#q5","title":"Q5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u89e3\u304c\u306a\u3044\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u3069\u306e\u3088\u3046\u306b\u6271\u308f\u308c\u307e\u3059\u304b\uff1f","text":"<p>A5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u7279\u306b\u56de\u5e30\u5206\u6790\u306a\u3069\u306b\u304a\u3044\u3066\u3001\u5b8c\u5168\u306a\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u904e\u5270\u6c7a\u5b9a\u7cfb\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u304c\u3088\u304f\u73fe\u308c\u307e\u3059\u3002\u305d\u306e\u3088\u3046\u306a\u5834\u5408\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3092\u7528\u3044\u3066\u300c\u6700\u826f\u306e\u8fd1\u4f3c\u89e3\u300d\u3092\u6c42\u3081\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u3059\u3079\u3066\u306e\u65b9\u7a0b\u5f0f\u3092\u5b8c\u5168\u306b\u6e80\u305f\u3059\u306e\u3067\u306f\u306a\u304f\u3001\u8aa4\u5dee\u306e\u4e8c\u4e57\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u89e3\u3092\u6c42\u3081\u308b\u65b9\u6cd5\u3067\u3059\u3002\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u306b\u306f\u8aa4\u5dee\u3084\u30ce\u30a4\u30ba\u304c\u542b\u307e\u308c\u308b\u305f\u3081\u3001\u3053\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u73fe\u5b9f\u7684\u304b\u3064\u6709\u7528\u3067\u3059\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#11","title":"11. \u6f14\u7fd2\u554f\u984c\u306e\u89e3\u7b54\u4f8b","text":""},{"location":"lectures/LA/13-system-of-linear-equation/#_3","title":"\u57fa\u672c\u554f\u984c\u306e\u89e3\u7b54","text":"<p>\u554f\u984c1(a) \\(\\begin{cases} 2x + 3y = 8 \\\\ -4x - 6y = -16 \\end{cases}\\)</p> <p>\u7b2c2\u5f0f\u3092\\(-1/2\\)\u500d\u3059\u308b\u3068\uff1a \\(\\begin{cases} 2x + 3y = 8 \\\\ 2x + 3y = 8 \\end{cases}\\)</p> <p>\u4e21\u65b9\u306e\u5f0f\u304c\u540c\u3058\u306b\u306a\u308b\u305f\u3081\u30012\u3064\u306e\u65b9\u7a0b\u5f0f\u306f\u7dda\u5f62\u5f93\u5c5e\u3067\u3042\u308a\u3001\u7b2c2\u5f0f\u306f\u7b2c1\u5f0f\u304b\u3089\u5c0e\u51fa\u3067\u304d\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001\u89e3\u306f\u7121\u6570\u306b\u5b58\u5728\u3057\u307e\u3059\u3002</p> <p>\\(2x + 3y = 8\\)\u3092\u89e3\u304f\u3068\uff1a\\(x = (8 - 3y)/2 = 4 - 3y/2\\)</p> <p>\u3088\u3063\u3066\u4e00\u822c\u89e3\u306f\u3001\\(y\\)\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066\uff1a\\(x = 4 - \\frac{3}{2}y\\), \\(y\\)\u306f\u4efb\u610f\u306e\u5b9f\u6570</p> <p>\u554f\u984c1(b) \\(\\begin{cases} 2x + y - z = 3 \\\\ x - y + z = 2 \\\\ 3x + 2y = 4 \\end{cases}\\)</p> <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3059\u308b\u3068\uff1a \\(\\begin{bmatrix} 2 &amp; 1 &amp; -1 &amp; 3 \\\\ 1 &amp; -1 &amp; 1 &amp; 2 \\\\ 3 &amp; 2 &amp; 0 &amp; 4 \\end{bmatrix}\\)</p> <p>\u7b2c1\u884c\u3068\u7b2c2\u884c\u3092\u5165\u308c\u66ff\u3048\uff1a \\(\\begin{bmatrix} 1 &amp; -1 &amp; 1 &amp; 2 \\\\ 2 &amp; 1 &amp; -1 &amp; 3 \\\\ 3 &amp; 2 &amp; 0 &amp; 4 \\end{bmatrix}\\)</p> <p>\u7b2c2\u884c\u304b\u3089\u7b2c1\u884c\u306e2\u500d\u3092\u5f15\u304f\uff1a \\(\\begin{bmatrix} 1 &amp; -1 &amp; 1 &amp; 2 \\\\ 0 &amp; 3 &amp; -3 &amp; -1 \\\\ 3 &amp; 2 &amp; 0 &amp; 4 \\end{bmatrix}\\)</p> <p>\u7b2c3\u884c\u304b\u3089\u7b2c1\u884c\u306e3\u500d\u3092\u5f15\u304f\uff1a \\(\\begin{bmatrix} 1 &amp; -1 &amp; 1 &amp; 2 \\\\ 0 &amp; 3 &amp; -3 &amp; -1 \\\\ 0 &amp; 5 &amp; -3 &amp; -2 \\end{bmatrix}\\)</p> <p>\u7b2c2\u884c\u30923\u3067\u5272\u308b\uff1a \\(\\begin{bmatrix} 1 &amp; -1 &amp; 1 &amp; 2 \\\\ 0 &amp; 1 &amp; -1 &amp; -\\frac{1}{3} \\\\ 0 &amp; 5 &amp; -3 &amp; -2 \\end{bmatrix}\\)</p> <p>\u7b2c3\u884c\u304b\u3089\u7b2c2\u884c\u306e5\u500d\u3092\u5f15\u304f\uff1a \\(\\begin{bmatrix} 1 &amp; -1 &amp; 1 &amp; 2 \\\\ 0 &amp; 1 &amp; -1 &amp; -\\frac{1}{3} \\\\ 0 &amp; 0 &amp; 2 &amp; -\\frac{1}{3} \\end{bmatrix}\\)</p> <p>\u7b2c3\u884c\u30922\u3067\u5272\u308b\uff1a \\(\\begin{bmatrix} 1 &amp; -1 &amp; 1 &amp; 2 \\\\ 0 &amp; 1 &amp; -1 &amp; -\\frac{1}{3} \\\\ 0 &amp; 0 &amp; 1 &amp; -\\frac{1}{6} \\end{bmatrix}\\)</p> <p>\u5f8c\u9000\u4ee3\u5165\u3059\u308b\u3068\uff1a \\(z = -\\frac{1}{6}\\) \\(y - z = -\\frac{1}{3}\\) \u3088\u308a \\(y = -\\frac{1}{3} + z = -\\frac{1}{3} - \\frac{1}{6} = -\\frac{1}{2}\\) \\(x - y + z = 2\\) \u3088\u308a \\(x = 2 + y - z = 2 - \\frac{1}{2} + \\frac{1}{6} = \\frac{10}{6} = \\frac{5}{3}\\)</p> <p>\u3088\u3063\u3066\u3001\u552f\u4e00\u89e3 \\(x = \\frac{5}{3}\\), \\(y = -\\frac{1}{2}\\), \\(z = -\\frac{1}{6}\\) \u304c\u5b58\u5728\u3057\u307e\u3059\u3002</p> <p>\u554f\u984c1(c) \\(\\begin{cases} x + 2y - z = 5 \\\\ 2x + 4y - 2z = 6 \\\\ 3x + 6y - 3z = 15 \\end{cases}\\)</p> <p>\u7b2c2\u5f0f\u306f\u7b2c1\u5f0f\u306e2\u500d\u3001\u7b2c3\u5f0f\u306f\u7b2c1\u5f0f\u306e3\u500d\u306a\u306e\u3067\uff1a \\(\\begin{cases} x + 2y - z = 5 \\\\ 2(x + 2y - z) = 6 \\\\ 3(x + 2y - z) = 15 \\end{cases}\\)</p> <p>\u7b2c2\u5f0f\uff1a\\(2 \\cdot 5 = 10 \\neq 6\\) \u3068\u306a\u308a\u77db\u76fe \u7b2c3\u5f0f\uff1a\\(3 \\cdot 5 = 15\\) \u306f\u6574\u5408</p> <p>\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u77db\u76fe\u3092\u542b\u3080\u305f\u3081\u89e3\u3092\u6301\u3061\u307e\u305b\u3093\u3002</p> <p>\u554f\u984c2(a) \\(A = \\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\ 2 &amp; 6 &amp; 4 \\\\ 3 &amp; 9 &amp; 6 \\end{bmatrix}\\), \\(\\mathbf{b} = \\begin{bmatrix} 4 \\\\ 8 \\\\ 12 \\end{bmatrix}\\)</p> <p>\u307e\u305a\u3001\u4fc2\u6570\u884c\u5217\\(A\\)\u306e\u30e9\u30f3\u30af\u3092\u8abf\u3079\u307e\u3059\u3002\u884c\u57fa\u672c\u5909\u5f62\u3092\u9069\u7528\u3059\u308b\u3068\uff1a \\(\\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\ 2 &amp; 6 &amp; 4 \\\\ 3 &amp; 9 &amp; 6 \\end{bmatrix}\\) \u2192 \\(\\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\)</p> <p>\u3088\u3063\u3066 \\(\\text{rank}(A) = 1\\)</p> <p>\u6b21\u306b\u3001\u62e1\u5927\u4fc2\u6570\u884c\u5217 \\([A|\\mathbf{b}]\\) \u306e\u30e9\u30f3\u30af\u3092\u8abf\u3079\u307e\u3059\uff1a \\(\\begin{bmatrix} 1 &amp; 3 &amp; 2 &amp; 4 \\\\ 2 &amp; 6 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 6 &amp; 12 \\end{bmatrix}\\) \u2192 \\(\\begin{bmatrix} 1 &amp; 3 &amp; 2 &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\)</p> <p>\u3088\u3063\u3066 \\(\\text{rank}([A|\\mathbf{b}]) = 1\\)</p> <p>\u672a\u77e5\u6570\u306e\u6570 \\(n = 3\\) \u3067\u3001\\(\\text{rank}(A) = \\text{rank}([A|\\mathbf{b}]) &lt; n\\) \u306a\u306e\u3067\u3001\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u7121\u6570\u306e\u89e3\u3092\u6301\u3061\u307e\u3059\u3002</p> <p>\u554f\u984c2(b) \\(A = \\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\ 2 &amp; 6 &amp; 4 \\\\ 3 &amp; 9 &amp; 6 \\end{bmatrix}\\), \\(\\mathbf{b} = \\begin{bmatrix} 4 \\\\ 8 \\\\ 10 \\end{bmatrix}\\)</p> <p>\\(\\text{rank}(A) = 1\\) \u3067\u3042\u308b\u3053\u3068\u306f\u524d\u554f\u3067\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002</p> <p>\u62e1\u5927\u4fc2\u6570\u884c\u5217 \\([A|\\mathbf{b}]\\) \u306e\u30e9\u30f3\u30af\u3092\u8abf\u3079\u307e\u3059\uff1a \\(\\begin{bmatrix} 1 &amp; 3 &amp; 2 &amp; 4 \\\\ 2 &amp; 6 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 6 &amp; 10 \\end{bmatrix}\\)</p> <p>\u7b2c1\u884c\u3092\u7528\u3044\u3066\u4ed6\u306e\u884c\u3092\u6d88\u53bb\u3059\u308b\u3068\uff1a \\(\\begin{bmatrix} 1 &amp; 3 &amp; 2 &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -2 \\end{bmatrix}\\)</p> <p>\u7b2c3\u884c\u306f \\(0 = -2\\) \u3068\u3044\u3046\u77db\u76fe\u3092\u542b\u3080\u305f\u3081\u3001\\(\\text{rank}([A|\\mathbf{b}]) = 2\\)</p> <p>\\(\\text{rank}(A) &lt; \\text{rank}([A|\\mathbf{b}])\\) \u306a\u306e\u3067\u3001\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u89e3\u3092\u6301\u3061\u307e\u305b\u3093\u3002</p> <p>\u554f\u984c3 \u884c\u5217\\(A\\)\u306e\u30e9\u30f3\u30af\u304c2\u3001\u62e1\u5927\u4fc2\u6570\u884c\u5217\\([A|\\mathbf{b}]\\)\u306e\u30e9\u30f3\u30af\u304c3\u3001\u672a\u77e5\u6570\u306e\u6570\u304c4\u306e\u5834\u5408\uff1a</p> <p>\\(\\text{rank}(A) = 2 &lt; \\text{rank}([A|\\mathbf{b}]) = 3\\) \u306a\u306e\u3067\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\\(A\\mathbf{x} = \\mathbf{b}\\)\u306f\u89e3\u3092\u6301\u3061\u307e\u305b\u3093\u3002\u3053\u308c\u306f\u3001\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u3088\u308a\u5927\u304d\u3044\u3068\u3044\u3046\u3053\u3068\u306f\u3001\u53f3\u8fba\u30d9\u30af\u30c8\u30eb\\(\\mathbf{b}\\)\u304c\u4fc2\u6570\u884c\u5217\\(A\\)\u306e\u5217\u7a7a\u9593\u306b\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u610f\u5473\u3059\u308b\u305f\u3081\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u3069\u306e\u3088\u3046\u306a\u672a\u77e5\u6570\u306e\u5024\u3092\u9078\u3093\u3067\u3082\u3001\u53f3\u8fba\u30d9\u30af\u30c8\u30eb\\(\\mathbf{b}\\)\u3092\u4fc2\u6570\u884c\u5217\\(A\\)\u306e\u5217\u30d9\u30af\u30c8\u30eb\u306e\u7dda\u5f62\u7d50\u5408\u3068\u3057\u3066\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#_4","title":"\u5fdc\u7528\u554f\u984c\u306e\u4e00\u90e8\u89e3\u7b54\u4f8b","text":"<p>\u554f\u984c4 \\(\\begin{cases} x + y + z = 6 \\\\ x + 2y + \\lambda z = 10 \\\\ x + 3y + \\lambda^2 z = \\lambda + 12 \\end{cases}\\)</p> <p>\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\([A|\\mathbf{b}]\\)\u3092\u66f8\u304d\u51fa\u3057\u307e\u3059\uff1a</p> <p>\\(A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; \\lambda \\\\ 1 &amp; 3 &amp; \\lambda^2 \\end{bmatrix}\\), \\([A|\\mathbf{b}] = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 1 &amp; 2 &amp; \\lambda &amp; 10 \\\\ 1 &amp; 3 &amp; \\lambda^2 &amp; \\lambda + 12 \\end{bmatrix}\\)</p> <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u307e\u3059\uff1a</p> <p>\\(\\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 0 &amp; 1 &amp; \\lambda-1 &amp; 4 \\\\ 0 &amp; 2 &amp; \\lambda^2-1 &amp; \\lambda + 6 \\end{bmatrix}\\)</p> <p>\u7b2c3\u884c\u304b\u3089\u7b2c2\u884c\u306e2\u500d\u3092\u5f15\u304f\u3068\uff1a</p> <p>\\(\\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 0 &amp; 1 &amp; \\lambda-1 &amp; 4 \\\\ 0 &amp; 0 &amp; \\lambda^2-1-2(\\lambda-1) &amp; \\lambda + 6 - 2 \\cdot 4 \\end{bmatrix}\\)</p> <p>\\(\\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 0 &amp; 1 &amp; \\lambda-1 &amp; 4 \\\\ 0 &amp; 0 &amp; \\lambda^2-2\\lambda+1 &amp; \\lambda - 2 \\end{bmatrix}\\)</p> <p>\\(\\lambda^2-2\\lambda+1 = (\\lambda-1)^2\\)</p> <p>\\(\\lambda\\)\u306e\u5024\u306b\u3088\u3063\u3066\u89e3\u306e\u7a2e\u985e\u304c\u5909\u308f\u308a\u307e\u3059\uff1a</p> <ol> <li> <p>\\(\\lambda \\neq 1\\) \u306e\u5834\u5408\uff1a    \\((\\lambda-1)^2 \\neq 0\\) \u306a\u306e\u3067\u3001\u7b2c3\u884c\u306f \\((\\lambda-1)^2 z = \\lambda - 2\\) \u3068\u306a\u308a\u3001\\(z = \\frac{\\lambda-2}{(\\lambda-1)^2}\\) \u3068\u6c7a\u307e\u308a\u307e\u3059\u3002\u5f8c\u9000\u4ee3\u5165\u306b\u3088\u308a\u552f\u4e00\u89e3\u304c\u6c42\u307e\u308a\u307e\u3059\u3002</p> </li> <li> <p>\\(\\lambda = 1\\) \u306e\u5834\u5408\uff1a    \\((\\lambda-1)^2 = 0\\) \u3068\u306a\u308a\u3001\u7b2c3\u884c\u306f \\(0 \\cdot z = \\lambda - 2 = 1 - 2 = -1\\) \u3068\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306f \\(0 = -1\\) \u3068\u3044\u3046\u77db\u76fe\u3092\u542b\u3080\u305f\u3081\u3001\\(\\lambda = 1\\) \u306e\u3068\u304d\u306f\u89e3\u304c\u5b58\u5728\u3057\u307e\u305b\u3093\u3002</p> </li> </ol> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\lambda = 1\\) \u306e\u3068\u304d\u306f\u89e3\u306a\u3057\u3001\\(\\lambda \\neq 1\\) \u306e\u3068\u304d\u306f\u552f\u4e00\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\u3002</p> <p>\u554f\u984c6(a) \u8840\u5727\uff08\\(y\\)\uff09\u3092\u5e74\u9f62\uff08\\(x_1\\)\uff09\u3001\u4f53\u91cd\uff08\\(x_2\\)\uff09\u3001\u904b\u52d5\u6642\u9593\uff08\\(x_3\\)\uff09\u306e\u7dda\u5f62\u95a2\u6570 \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) \u3068\u3057\u3066\u8868\u3059\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\uff1a</p> <p>\\(\\begin{cases} \\beta_0 + 25\\beta_1 + 70\\beta_2 + 3\\beta_3 = 120 \\\\ \\beta_0 + 45\\beta_1 + 85\\beta_2 + 1\\beta_3 = 140 \\\\ \\beta_0 + 35\\beta_1 + 65\\beta_2 + 5\\beta_3 = 115 \\\\ \\beta_0 + 55\\beta_1 + 75\\beta_2 + 2\\beta_3 = 145 \\\\ \\beta_0 + 30\\beta_1 + 80\\beta_2 + 4\\beta_3 = 125 \\end{cases}\\)</p> <p>\u3053\u308c\u306f\u30014\u3064\u306e\u672a\u77e5\u6570\uff08\\(\\beta_0, \\beta_1, \\beta_2, \\beta_3\\)\uff09\u30685\u3064\u306e\u65b9\u7a0b\u5f0f\u304b\u3089\u306a\u308b\u904e\u5270\u6c7a\u5b9a\u7cfb\u3067\u3059\u3002</p> <p>\u554f\u984c6(b) \u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u904e\u5270\u6c7a\u5b9a\u7cfb\uff08\u65b9\u7a0b\u5f0f\u306e\u6570 &gt; \u672a\u77e5\u6570\u306e\u6570\uff09\u306a\u306e\u3067\u3001\u4e00\u822c\u7684\u306b\u306f\u89e3\u3092\u6301\u3061\u307e\u305b\u3093\u3002\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u306b\u5b8c\u5168\u306b\u9069\u5408\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u5024\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u306f\u901a\u5e38\u3067\u304d\u306a\u3044\u305f\u3081\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306a\u3069\u306e\u8fd1\u4f3c\u624b\u6cd5\u3092\u7528\u3044\u3066\u6700\u9069\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u5024\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> <p>\uff08\u6b8b\u308a\u306e\u554f\u984c\u306e\u89e3\u7b54\u7701\u7565\uff09</p>"},{"location":"lectures/LA/13-system-of-linear-equation/#12","title":"12. \u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e\u3068\u305d\u306e\u5224\u5b9a\u65b9\u6cd5\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\u3002\u4e3b\u306a\u30dd\u30a4\u30f3\u30c8\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306f\u300c\u552f\u4e00\u89e3\u300d\u300c\u7121\u6570\u306e\u89e3\u300d\u300c\u89e3\u306a\u3057\u300d\u306e3\u7a2e\u985e\u306b\u5206\u985e\u3055\u308c\u308b</li> <li>\u89e3\u306e\u7a2e\u985e\u306f\u4fc2\u6570\u884c\u5217\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u306b\u3088\u3063\u3066\u5224\u5b9a\u3067\u304d\u308b</li> <li>\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u89e3\u306e\u7a2e\u985e\u306f\u76f4\u7dda\u3084\u5e73\u9762\u306e\u4ea4\u308f\u308a\u65b9\u306b\u5bfe\u5fdc\u3059\u308b</li> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u904e\u5270\u6c7a\u5b9a\u7cfb\u306e\u65b9\u7a0b\u5f0f\u304c\u3088\u304f\u73fe\u308c\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306a\u3069\u3067\u8fd1\u4f3c\u89e3\u3092\u6c42\u3081\u308b</li> </ol> <p>\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u306b\u3064\u3044\u3066\u3055\u3089\u306b\u8a73\u3057\u304f\u5b66\u3073\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u89e3\u306e\u95a2\u4fc2\u6027\u306b\u3064\u3044\u3066\u306e\u7406\u89e3\u3092\u6df1\u3081\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c14\u56de\uff1a\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6","text":""},{"location":"lectures/LA/14-system-of-linear-equation/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<ul> <li>\u8b1b\u7fa9\u56de: \u7b2c14\u56de</li> <li>\u95a2\u9023\u9805\u76ee: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af\u3001\u62e1\u5927\u4fc2\u6570\u884c\u5217</li> <li>\u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u7b2c12\u56de\u300c\u30e9\u30f3\u30af\u306e\u6982\u5ff5\u3068\u305d\u306e\u8a08\u7b97\u300d\u304a\u3088\u3073\u7b2c13\u56de\u300c\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e\u300d\u306e\u5fa9\u7fd2</li> </ul>"},{"location":"lectures/LA/14-system-of-linear-equation/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3059\u308b</li> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3092\u7406\u8ad6\u7684\u306b\u8aac\u660e\u3067\u304d\u308b</li> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5224\u5225\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u6c42\u3081\u65b9\u3092\u5b9f\u4f8b\u3092\u901a\u3057\u3066\u7406\u89e3\u3059\u308b</li> <li>Google Colab\u3067\u306e\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> </ol>"},{"location":"lectures/LA/14-system-of-linear-equation/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/14-system-of-linear-equation/#31","title":"3.1 \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u5fa9\u7fd2","text":"<p>\u524d\u56de\u306e\u8b1b\u7fa9\u3067\u5b66\u3093\u3060\u3088\u3046\u306b\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306f\u884c\u5217\u3092\u7528\u3044\u3066\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\\(n\\)\u500b\u306e\u672a\u77e5\u6570\u3068\\(m\\)\u500b\u306e\u65b9\u7a0b\u5f0f\u304b\u3089\u306a\u308b\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306f\u6b21\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[ \\begin{cases} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1 \\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2 \\\\ \\vdots \\\\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n = b_m \\end{cases} \\] <p>\u3053\u308c\u306f\u884c\u5217\u8868\u8a18\u3067\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[AX = B\\] <p>\u3053\u3053\u3067\u3001\\(A\\)\u306f\u4fc2\u6570\u884c\u5217\u3001\\(X\\)\u306f\u672a\u77e5\u6570\u30d9\u30af\u30c8\u30eb\u3001\\(B\\)\u306f\u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30eb\u3067\u3059\uff1a</p> \\[ A =  \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}, \\quad X =  \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\quad B =  \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix} \\] <p>\u307e\u305f\u3001\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306f\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30eb\\(B\\)\u3092\u6a2a\u306b\u4e26\u3079\u305f\u884c\u5217\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[ (A|B) =  \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; b_1 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; b_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} &amp; b_m \\end{pmatrix} \\] <p>\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u3001\u305d\u306e\u884c\u5217\u306e\u6700\u5927\u7dda\u5f62\u72ec\u7acb\u306a\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u6570\u3067\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002\u7b2c12\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u3066\u5f97\u3089\u308c\u308b\u968e\u6bb5\u884c\u5217\u306e\u975e\u30bc\u30ed\u884c\u306e\u6570\u3068\u3057\u3066\u8a08\u7b97\u3067\u304d\u308b\u3053\u3068\u3092\u5b66\u3073\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/14-system-of-linear-equation/#41","title":"4.1 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\\(AX = B\\)\u306e\u89e3\u306e\u5b58\u5728\u3068\u4e00\u610f\u6027\u306f\u3001\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\((A|B)\\)\u306e\u30e9\u30f3\u30af\u306b\u3088\u3063\u3066\u6c7a\u5b9a\u3055\u308c\u307e\u3059\u3002</p> <p>\u5b9a\u7406\uff08\u89e3\u306e\u5b58\u5728\u6761\u4ef6\uff09</p> <p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\\(AX = B\\)\u306b\u304a\u3044\u3066\uff1a</p> <ol> <li>\u89e3\u304c\u5b58\u5728\u3059\u308b\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f \\(\\text{rank}(A) = \\text{rank}(A|B)\\) \u3067\u3042\u308b</li> <li>\u89e3\u304c\u4e00\u610f\u306b\u5b9a\u307e\u308b\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f \\(\\text{rank}(A) = \\text{rank}(A|B) = n\\) \u3067\u3042\u308b\uff08\u3053\u3053\u3067\\(n\\)\u306f\u672a\u77e5\u6570\u306e\u6570\uff09</li> </ol> <p>\u3053\u306e\u5b9a\u7406\u304b\u3089\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u72b6\u6cc1\u30923\u3064\u306e\u30b1\u30fc\u30b9\u306b\u5206\u985e\u3067\u304d\u307e\u3059\uff1a</p> <ol> <li>\\(\\text{rank}(A) \\neq \\text{rank}(A|B)\\) \u306e\u5834\u5408\uff1a\u89e3\u306f\u5b58\u5728\u3057\u306a\u3044</li> <li>\\(\\text{rank}(A) = \\text{rank}(A|B) = n\\) \u306e\u5834\u5408\uff1a\u89e3\u306f\u552f\u4e00\u5b58\u5728\u3059\u308b</li> <li>\\(\\text{rank}(A) = \\text{rank}(A|B) &lt; n\\) \u306e\u5834\u5408\uff1a\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3059\u308b</li> </ol>"},{"location":"lectures/LA/14-system-of-linear-equation/#42","title":"4.2 \u5b9a\u7406\u306e\u76f4\u611f\u7684\u7406\u89e3","text":"<p>\u5b9a\u7406\u306e\u76f4\u611f\u7684\u306a\u7406\u89e3\u3092\u6df1\u3081\u308b\u305f\u3081\u306b\u3001\u6b21\u306e\u3088\u3046\u306b\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> <ul> <li>\u4fc2\u6570\u884c\u5217\\(A\\)\u306e\u30e9\u30f3\u30af\u306f\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u300c\u5236\u7d04\u306e\u52b9\u679c\u7684\u306a\u6570\u300d\u3092\u8868\u3057\u307e\u3059</li> <li>\u62e1\u5927\u4fc2\u6570\u884c\u5217\\((A|B)\\)\u306e\u30e9\u30f3\u30af\u304c\\(A\\)\u306e\u30e9\u30f3\u30af\u3088\u308a\u5927\u304d\u3044\u5834\u5408\u3001\u3053\u308c\u306f\\(B\\)\u304c\\(A\\)\u306e\u5217\u7a7a\u9593\u306b\u542b\u307e\u308c\u306a\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u3001\u65b9\u7a0b\u5f0f\u306f\u77db\u76fe\u3057\u3066\u3044\u307e\u3059\uff08\u89e3\u306a\u3057\uff09</li> <li>\\(A\\)\u306e\u30e9\u30f3\u30af\u304c\u672a\u77e5\u6570\u306e\u6570\\(n\\)\u306b\u7b49\u3057\u3044\u5834\u5408\u3001\u3059\u3079\u3066\u306e\u672a\u77e5\u6570\u3092\u4e00\u610f\u306b\u6c7a\u5b9a\u3059\u308b\u306e\u306b\u5341\u5206\u306a\u72ec\u7acb\u3057\u305f\u5236\u7d04\u304c\u3042\u308a\u307e\u3059\uff08\u552f\u4e00\u89e3\uff09</li> <li>\\(A\\)\u306e\u30e9\u30f3\u30af\u304c\\(n\\)\u672a\u6e80\u306e\u5834\u5408\u3001\u81ea\u7531\u5ea6\uff08\u81ea\u7531\u306b\u5024\u3092\u9078\u3079\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\uff09\u304c\u3042\u308a\u3001\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059</li> </ul>"},{"location":"lectures/LA/14-system-of-linear-equation/#43","title":"4.3 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\u306e\u30b9\u30c6\u30c3\u30d7","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u6709\u7121\u3068\u7a2e\u985e\u3092\u5224\u65ad\u3057\u3001\u89e3\u3092\u6c42\u3081\u308b\u305f\u3081\u306e\u4e00\u822c\u7684\u306a\u624b\u9806\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\((A|B)\\)\u3092\u4f5c\u6210\u3059\u308b</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u3066\u3001\u968e\u6bb5\u884c\u5217\uff08\u7c21\u7d04\u884c\u5217\uff09\u306e\u5f62\u306b\u5909\u63db\u3059\u308b</li> <li>\\(A\\)\u3068\\((A|B)\\)\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\u3059\u308b</li> <li>\u30e9\u30f3\u30af\u3092\u6bd4\u8f03\u3057\u3066\u3001\u89e3\u306e\u5b58\u5728\u3068\u4e00\u610f\u6027\u3092\u5224\u65ad\u3059\u308b</li> <li>\u89e3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u7c21\u7d04\u3055\u308c\u305f\u62e1\u5927\u4fc2\u6570\u884c\u5217\u304b\u3089\u89e3\u3092\u6c42\u3081\u308b</li> </ol>"},{"location":"lectures/LA/14-system-of-linear-equation/#5","title":"5. \u4f8b\u984c\u3068\u89e3\u6cd5","text":""},{"location":"lectures/LA/14-system-of-linear-equation/#1_1","title":"\u4f8b\u984c1: \u552f\u4e00\u89e3\u3092\u6301\u3064\u9023\u7acb\u65b9\u7a0b\u5f0f","text":"<p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u6c42\u3081\u307e\u3057\u3087\u3046\u3002</p> \\[ \\begin{cases} 2x + y - z = 5 \\\\ x - y + 2z = 0 \\\\ 3x + 2y + z = 7 \\end{cases} \\] <p>\u89e3\u6cd5:</p> <ol> <li>\u307e\u305a\u3001\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\((A|B)\\)\u3092\u66f8\u304d\u4e0b\u3057\u307e\u3059\uff1a</li> </ol> \\[ A =  \\begin{pmatrix} 2 &amp; 1 &amp; -1 \\\\ 1 &amp; -1 &amp; 2 \\\\ 3 &amp; 2 &amp; 1 \\end{pmatrix}, \\quad (A|B) =  \\begin{pmatrix} 2 &amp; 1 &amp; -1 &amp; 5 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 &amp; 7 \\end{pmatrix} \\] <ol> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u3066\u968e\u6bb5\u884c\u5217\u3092\u6c42\u3081\u307e\u3059\uff1a</li> </ol> \\[ \\begin{pmatrix} 2 &amp; 1 &amp; -1 &amp; 5 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 &amp; 7 \\end{pmatrix} \\xrightarrow{R_2 = R_2} \\begin{pmatrix} 2 &amp; 1 &amp; -1 &amp; 5 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 &amp; 7 \\end{pmatrix} \\] \\[ \\xrightarrow{R_1 = R_1 - 2R_2} \\begin{pmatrix} 0 &amp; 3 &amp; -5 &amp; 5 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 &amp; 7 \\end{pmatrix} \\] \\[ \\xrightarrow{R_3 = R_3 - 3R_2} \\begin{pmatrix} 0 &amp; 3 &amp; -5 &amp; 5 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 0 &amp; 5 &amp; -5 &amp; 7 \\end{pmatrix} \\] \\[ \\xrightarrow{R_1 = R_1/3} \\begin{pmatrix} 0 &amp; 1 &amp; -5/3 &amp; 5/3 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 0 &amp; 5 &amp; -5 &amp; 7 \\end{pmatrix} \\] \\[ \\xrightarrow{R_3 = R_3 - 5R_1} \\begin{pmatrix} 0 &amp; 1 &amp; -5/3 &amp; 5/3 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 25/3 &amp; -1/3 \\end{pmatrix} \\] \\[ \\xrightarrow{R_3 = 3R_3/25} \\begin{pmatrix} 0 &amp; 1 &amp; -5/3 &amp; 5/3 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1/25 \\end{pmatrix} \\] \\[ \\xrightarrow{R_1 = R_1 + (5/3)R_3} \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 5/3 - 1/15 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1/25 \\end{pmatrix} \\] \\[ \\xrightarrow{R_1 = R_1} \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 24/15 \\\\ 1 &amp; -1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1/25 \\end{pmatrix} \\] \\[ \\xrightarrow{R_2 = R_2 - 2R_3} \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 24/15 \\\\ 1 &amp; -1 &amp; 0 &amp; 2/25 \\\\ 0 &amp; 0 &amp; 1 &amp; -1/25 \\end{pmatrix} \\] \\[ \\xrightarrow{R_2 = R_2 + R_1} \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 24/15 \\\\ 1 &amp; 0 &amp; 0 &amp; 2/25 + 24/15 \\\\ 0 &amp; 0 &amp; 1 &amp; -1/25 \\end{pmatrix} \\] <p>\u7c21\u5358\u306b\u8a08\u7b97\u3059\u308b\u3068 \\(2/25 + 24/15 = (2 \\cdot 3 + 24 \\cdot 5) / 75 = (6 + 120) / 75 = 126/75 = 42/25\\)</p> \\[ \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 24/15 = 8/5 \\\\ 1 &amp; 0 &amp; 0 &amp; 42/25 \\\\ 0 &amp; 0 &amp; 1 &amp; -1/25 \\end{pmatrix} \\] <ol> <li> <p>\u968e\u6bb5\u884c\u5217\u304b\u3089\u3001\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\((A|B)\\)\u306e\u30e9\u30f3\u30af\u306f\u3069\u3061\u3089\u30823\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u672a\u77e5\u6570\u306e\u6570\u30823\u3067\u3042\u308b\u305f\u3081\u3001\\(\\text{rank}(A) = \\text{rank}(A|B) = n = 3\\)\u3068\u306a\u308a\u3001\u552f\u4e00\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u968e\u6bb5\u884c\u5217\u304b\u3089\u89e3\u3092\u8aad\u307f\u53d6\u308a\u307e\u3059\uff1a</p> </li> <li>\\(x = 42/25 = 1.68\\)</li> <li>\\(y = 8/5 = 1.6\\)</li> <li>\\(z = -1/25 = -0.04\\)</li> </ol> <p>\u7d50\u679c\u3092\u691c\u7b97\u3059\u308b\u3068\uff1a - \u7b2c1\u65b9\u7a0b\u5f0f: \\(2(42/25) + (8/5) - (-1/25) = 84/25 + 8/5 + 1/25 = (84 + 40 + 1)/25 = 125/25 = 5\\) \u2713 - \u7b2c2\u65b9\u7a0b\u5f0f: \\((42/25) - (8/5) + 2(-1/25) = 42/25 - 8/5 - 2/25 = (42 - 40 - 2)/25 = 0\\) \u2713 - \u7b2c3\u65b9\u7a0b\u5f0f: \\(3(42/25) + 2(8/5) + (-1/25) = 126/25 + 16/5 - 1/25 = (126 + 80 - 1)/25 = 205/25 = 7\\) \u2713</p> <p>\u5f93\u3063\u3066\u3001\u89e3\u306f \\((x, y, z) = (42/25, 8/5, -1/25)\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#2_1","title":"\u4f8b\u984c2: \u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u9023\u7acb\u65b9\u7a0b\u5f0f","text":"<p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u6c42\u3081\u307e\u3057\u3087\u3046\u3002</p> \\[ \\begin{cases} x + y + z = 6 \\\\ 2x - y + z = 1 \\\\ x + 2y + 2z = 9 \\\\ 4x + 2y + 4z = 14 \\end{cases} \\] <p>\u89e3\u6cd5:</p> <ol> <li>\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\((A|B)\\)\u3092\u4f5c\u6210\u3057\u307e\u3059\uff1a</li> </ol> \\[ A =  \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; -1 &amp; 1 \\\\ 1 &amp; 2 &amp; 2 \\\\ 4 &amp; 2 &amp; 4 \\end{pmatrix}, \\quad (A|B) =  \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 2 &amp; -1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 2 &amp; 9 \\\\ 4 &amp; 2 &amp; 4 &amp; 14 \\end{pmatrix} \\] <ol> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u307e\u3059\uff1a</li> </ol> \\[ \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 2 &amp; -1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 2 &amp; 9 \\\\ 4 &amp; 2 &amp; 4 &amp; 14 \\end{pmatrix} \\xrightarrow{R_2 = R_2 - 2R_1} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 0 &amp; -3 &amp; -1 &amp; -11 \\\\ 1 &amp; 2 &amp; 2 &amp; 9 \\\\ 4 &amp; 2 &amp; 4 &amp; 14 \\end{pmatrix} \\] \\[ \\xrightarrow{R_3 = R_3 - R_1} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 0 &amp; -3 &amp; -1 &amp; -11 \\\\ 0 &amp; 1 &amp; 1 &amp; 3 \\\\ 4 &amp; 2 &amp; 4 &amp; 14 \\end{pmatrix} \\] \\[ \\xrightarrow{R_4 = R_4 - 4R_1} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 0 &amp; -3 &amp; -1 &amp; -11 \\\\ 0 &amp; 1 &amp; 1 &amp; 3 \\\\ 0 &amp; -2 &amp; 0 &amp; -10 \\end{pmatrix} \\] \\[ \\xrightarrow{R_2 = -R_2/3} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 0 &amp; 1 &amp; 1/3 &amp; 11/3 \\\\ 0 &amp; 1 &amp; 1 &amp; 3 \\\\ 0 &amp; -2 &amp; 0 &amp; -10 \\end{pmatrix} \\] \\[ \\xrightarrow{R_3 = R_3 - R_2} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 0 &amp; 1 &amp; 1/3 &amp; 11/3 \\\\ 0 &amp; 0 &amp; 2/3 &amp; -2/3 \\\\ 0 &amp; -2 &amp; 0 &amp; -10 \\end{pmatrix} \\] \\[ \\xrightarrow{R_4 = R_4 + 2R_2} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 6 \\\\ 0 &amp; 1 &amp; 1/3 &amp; 11/3 \\\\ 0 &amp; 0 &amp; 2/3 &amp; -2/3 \\\\ 0 &amp; 0 &amp; 2/3 &amp; 22/3 - 10 = 22/3 - 30/3 = -8/3 \\end{pmatrix} \\] <ol> <li> <p>\u6700\u5f8c\u306e\u884c\u3092\u898b\u308b\u3068\u3001\\(0x + 0y + (2/3)z = -8/3\\)\u3068\u3044\u3046\u5f0f\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u5f0f\u306f\\(z = -4\\)\u3092\u610f\u5473\u3057\u307e\u3059\u304c\u3001\u3053\u308c\u3092\u7b2c3\u884c\u306e\\(0x + 0y + (2/3)z = -2/3\\)\u306b\u4ee3\u5165\u3059\u308b\u3068\u3001\\(0x + 0y + (2/3)(-4) = -8/3 \\neq -2/3\\)\u3068\u306a\u308a\u3001\u77db\u76fe\u304c\u751f\u3058\u307e\u3059\u3002</p> </li> <li> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\text{rank}(A) = 3\\)\uff08\u975e\u30bc\u30ed\u884c\u304c3\u884c\uff09\u3001\\(\\text{rank}(A|B) = 4\\)\uff08\u77db\u76fe\u3059\u308b\u884c\u3092\u542b\u3080\uff09\u3068\u306a\u308a\u3001\\(\\text{rank}(A) \\neq \\text{rank}(A|B)\\)\u3067\u3059\u3002</p> </li> <li> <p>\u3088\u3063\u3066\u3001\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306b\u306f\u89e3\u304c\u5b58\u5728\u3057\u307e\u305b\u3093\u3002</p> </li> </ol>"},{"location":"lectures/LA/14-system-of-linear-equation/#3_1","title":"\u4f8b\u984c3: \u7121\u6570\u306e\u89e3\u3092\u6301\u3064\u9023\u7acb\u65b9\u7a0b\u5f0f","text":"<p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u6c42\u3081\u307e\u3057\u3087\u3046\u3002</p> \\[ \\begin{cases} x + 2y - z = 3 \\\\ 2x + 4y - 2z = 6 \\\\ 3x + 6y - 3z = 9 \\end{cases} \\] <p>\u89e3\u6cd5:</p> <ol> <li>\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\((A|B)\\)\u3092\u4f5c\u6210\u3057\u307e\u3059\uff1a</li> </ol> \\[ A =  \\begin{pmatrix} 1 &amp; 2 &amp; -1 \\\\ 2 &amp; 4 &amp; -2 \\\\ 3 &amp; 6 &amp; -3 \\end{pmatrix}, \\quad (A|B) =  \\begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 3 \\\\ 2 &amp; 4 &amp; -2 &amp; 6 \\\\ 3 &amp; 6 &amp; -3 &amp; 9 \\end{pmatrix} \\] <ol> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u307e\u3059\uff1a</li> </ol> \\[ \\begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 3 \\\\ 2 &amp; 4 &amp; -2 &amp; 6 \\\\ 3 &amp; 6 &amp; -3 &amp; 9 \\end{pmatrix} \\xrightarrow{R_2 = R_2 - 2R_1} \\begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 3 &amp; 6 &amp; -3 &amp; 9 \\end{pmatrix} \\] \\[ \\xrightarrow{R_3 = R_3 - 3R_1} \\begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\] <ol> <li> <p>\u968e\u6bb5\u884c\u5217\u3088\u308a\u3001\\(\\text{rank}(A) = \\text{rank}(A|B) = 1 &lt; 3 = n\\)\uff08\u672a\u77e5\u6570\u306e\u6570\uff09\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u7121\u6570\u306e\u89e3\u3092\u6301\u3061\u307e\u3059\u3002</p> </li> <li> <p>\u7c21\u7d04\u3055\u308c\u305f\u5f0f\u306f\\(x + 2y - z = 3\\)\u306e\u307f\u3067\u3059\u3002\\(y\\)\u3068\\(z\\)\u3092\u81ea\u7531\u5909\u6570\u3068\u3059\u308b\u3068\u3001\\(x = 3 - 2y + z\\)\u3068\u8868\u305b\u307e\u3059\u3002\u4efb\u610f\u306e\\(y\\)\u3068\\(z\\)\u306e\u5024\u306b\u5bfe\u3057\u3066\u89e3\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u3001\u7121\u6570\u306e\u89e3\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> </ol> <p>\u305f\u3068\u3048\u3070\u3001\\(y = 0\\), \\(z = 0\\)\u3068\u3059\u308b\u3068\u3001\\(x = 3\\)\u3068\u3044\u3046\u89e3\u304c\u5f97\u3089\u308c\u307e\u3059\u3002 \u307e\u305f\u3001\\(y = 1\\), \\(z = 0\\)\u3068\u3059\u308b\u3068\u3001\\(x = 3 - 2 \\cdot 1 + 0 = 1\\)\u3068\u3044\u3046\u89e3\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <p>\u4e00\u822c\u7684\u306a\u89e3\u306f\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\\(s\\)\u3068\\(t\\)\u3092\u7528\u3044\u3066 \\((x, y, z) = (3 - 2s + t, s, t)\\) \u3068\u8868\u305b\u307e\u3059\u3002\u3053\u3053\u3067\\(s\\)\u3068\\(t\\)\u306f\u4efb\u610f\u306e\u5b9f\u6570\u3067\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#6-python","title":"6. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3084\u89e3\u306e\u7a2e\u985e\u3092Python\u3067\u691c\u8a3c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002NumPy\u3092\u4f7f\u3063\u3066\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\u3057\u3001\u89e3\u3092\u6c42\u3081\u308b\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u4f8b\u984c1: \u552f\u4e00\u89e3\u3092\u6301\u3064\u9023\u7acb\u65b9\u7a0b\u5f0f\ndef example1():\n    # \u4fc2\u6570\u884c\u5217A\n    A = np.array([\n        [2, 1, -1],\n        [1, -1, 2],\n        [3, 2, 1]\n    ])\n\n    # \u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30ebB\n    B = np.array([5, 0, 7])\n\n    # \u62e1\u5927\u4fc2\u6570\u884c\u5217(A|B)\n    AB = np.column_stack((A, B))\n\n    # \u30e9\u30f3\u30af\u306e\u8a08\u7b97\n    rank_A = np.linalg.matrix_rank(A)\n    rank_AB = np.linalg.matrix_rank(AB)\n\n    print(\"\u4f8b\u984c1:\")\n    print(f\"rank(A) = {rank_A}\")\n    print(f\"rank(A|B) = {rank_AB}\")\n\n    # \u89e3\u306e\u5224\u5b9a\n    n = A.shape[1]  # \u672a\u77e5\u6570\u306e\u6570\n    if rank_A != rank_AB:\n        print(\"\u89e3\u306f\u5b58\u5728\u3057\u307e\u305b\u3093\")\n    elif rank_A == rank_AB and rank_A == n:\n        print(\"\u552f\u4e00\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\")\n        solution = np.linalg.solve(A, B)\n        print(f\"\u89e3: x = {solution[0]:.2f}, y = {solution[1]:.2f}, z = {solution[2]:.2f}\")\n    else:\n        print(\"\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\")\n\n# \u4f8b\u984c2: \u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u9023\u7acb\u65b9\u7a0b\u5f0f\ndef example2():\n    # \u4fc2\u6570\u884c\u5217A\n    A = np.array([\n        [1, 1, 1],\n        [2, -1, 1],\n        [1, 2, 2],\n        [4, 2, 4]\n    ])\n\n    # \u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30ebB\n    B = np.array([6, 1, 9, 14])\n\n    # \u62e1\u5927\u4fc2\u6570\u884c\u5217(A|B)\n    AB = np.column_stack((A, B))\n\n    # \u30e9\u30f3\u30af\u306e\u8a08\u7b97\n    rank_A = np.linalg.matrix_rank(A)\n    rank_AB = np.linalg.matrix_rank(AB)\n\n    print(\"\\n\u4f8b\u984c2:\")\n    print(f\"rank(A) = {rank_A}\")\n    print(f\"rank(A|B) = {rank_AB}\")\n\n    # \u89e3\u306e\u5224\u5b9a\n    n = A.shape[1]  # \u672a\u77e5\u6570\u306e\u6570\n    if rank_A != rank_AB:\n        print(\"\u89e3\u306f\u5b58\u5728\u3057\u307e\u305b\u3093\")\n    elif rank_A == rank_AB and rank_A == n:\n        print(\"\u552f\u4e00\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\")\n        # \u89e3\u306a\u3057\u306e\u5834\u5408\u3001np.linalg.solve\u306f\u30a8\u30e9\u30fc\u306b\u306a\u308b\u306e\u3067\u4f8b\u5916\u51e6\u7406\n        try:\n            solution = np.linalg.lstsq(A, B, rcond=None)[0]\n            print(f\"\u6700\u5c0f\u4e8c\u4e57\u89e3: x = {solution[0]:.2f}, y = {solution[1]:.2f}, z = {solution[2]:.2f}\")\n        except np.linalg.LinAlgError:\n            print(\"np.linalg.solve\u306b\u3088\u308b\u89e3\u6cd5\u306b\u5931\u6557\u3057\u307e\u3057\u305f\")\n    else:\n        print(\"\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\")\n\n# \u4f8b\u984c3: \u7121\u6570\u306e\u89e3\u3092\u6301\u3064\u9023\u7acb\u65b9\u7a0b\u5f0f\ndef example3():\n    # \u4fc2\u6570\u884c\u5217A\n    A = np.array([\n        [1, 2, -1],\n        [2, 4, -2],\n        [3, 6, -3]\n    ])\n\n    # \u5b9a\u6570\u9805\u30d9\u30af\u30c8\u30ebB\n    B = np.array([3, 6, 9])\n\n    # \u62e1\u5927\u4fc2\u6570\u884c\u5217(A|B)\n    AB = np.column_stack((A, B))\n\n    # \u30e9\u30f3\u30af\u306e\u8a08\u7b97\n    rank_A = np.linalg.matrix_rank(A)\n    rank_AB = np.linalg.matrix_rank(AB)\n\n    print(\"\\n\u4f8b\u984c3:\")\n    print(f\"rank(A) = {rank_A}\")\n    print(f\"rank(A|B) = {rank_AB}\")\n\n    # \u89e3\u306e\u5224\u5b9a\n    n = A.shape[1]  # \u672a\u77e5\u6570\u306e\u6570\n    if rank_A != rank_AB:\n        print(\"\u89e3\u306f\u5b58\u5728\u3057\u307e\u305b\u3093\")\n    elif rank_A == rank_AB and rank_A == n:\n        print(\"\u552f\u4e00\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\")\n        solution = np.linalg.solve(A, B)\n        print(f\"\u89e3: x = {solution[0]:.2f}, y = {solution[1]:.2f}, z = {solution[2]:.2f}\")\n    else:\n        print(\"\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\")\n\n        # \u4e00\u822c\u89e3\u3092\u30d1\u30e9\u30e1\u30c8\u30ea\u30c3\u30af\u306b\u8868\u793a\n        print(\"\u4e00\u822c\u89e3: x = 3 - 2s + t, y = s, z = t (s\u3068t\u306f\u4efb\u610f\u306e\u5b9f\u6570)\")\n\n        # \u53ef\u8996\u5316\u306e\u305f\u3081\u306b\u89e3\u7a7a\u9593\u3092\u63cf\u753b\n        fig = plt.figure(figsize=(10, 8))\n        ax = fig.add_subplot(111, projection='3d')\n\n        # \u30d1\u30e9\u30e1\u30fc\u30bfs\u3068t\u306e\u7bc4\u56f2\n        s = np.linspace(-2, 2, 10)\n        t = np.linspace(-2, 2, 10)\n        S, T = np.meshgrid(s, t)\n        X = 3 - 2*S + T\n\n        # \u89e3\u7a7a\u9593\u306e\u5e73\u9762\u3092\u63cf\u753b\n        ax.plot_surface(X, S, T, alpha=0.5, rstride=1, cstride=1, color='b')\n\n        # \u5ea7\u6a19\u8ef8\u306e\u30e9\u30d9\u30eb\n        ax.set_xlabel('x')\n        ax.set_ylabel('y')\n        ax.set_zlabel('z')\n        ax.set_title('\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u7a7a\u9593\uff08\u5e73\u9762\uff09')\n\n        plt.tight_layout()\n        plt.show()\n\n# 3\u3064\u306e\u4f8b\u3092\u5b9f\u884c\nexample1()\nexample2()\nexample3()\n</code></pre> <p>\u3053\u306e\u5b9f\u88c5\u306b\u3088\u308a\u3001\u4f8b\u984c1\u301c3\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5224\u5225\u3068\u8a08\u7b97\u304c\u884c\u3048\u307e\u3059\u3002\u4f8b\u984c3\u3067\u306f\u3001\u89e3\u7a7a\u9593\u304c\u5e73\u9762\u3068\u306a\u308b\u3053\u3068\u3092\u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u306b\u53ef\u8996\u5316\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#7","title":"7. \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u610f\u7fa9","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u591a\u304f\u306e\u554f\u984c\u306b\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002\u7279\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5fdc\u7528\u304c\u3042\u308a\u307e\u3059\uff1a</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#71","title":"7.1 \u56de\u5e30\u5206\u6790\u3068\u6700\u5c0f\u4e8c\u4e57\u6cd5","text":"<p>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u3001\u901a\u5e38\u3001\u65b9\u7a0b\u5f0f\u306e\u6570\uff08\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u6570\uff09\u304c\u672a\u77e5\u6570\u306e\u6570\uff08\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\uff09\u3088\u308a\u591a\u304f\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u304c\u904e\u5270\u6c7a\u5b9a\u7cfb\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u5834\u5408\u3001\u53b3\u5bc6\u306a\u89e3\u306f\u901a\u5e38\u5b58\u5728\u305b\u305a\u3001\u4ee3\u308f\u308a\u306b\u300c\u6700\u5c0f\u4e8c\u4e57\u89e3\u300d\u304c\u7528\u3044\u3089\u308c\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\\(n\\)\u500b\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\\((x_i, y_i)\\)\u304b\u3089\\(y = ax + b\\)\u306e\u5f62\u306e\u56de\u5e30\u76f4\u7dda\u3092\u6c42\u3081\u308b\u5834\u5408\u3001\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3053\u3068\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[ \\begin{cases} ax_1 + b = y_1 \\\\ ax_2 + b = y_2 \\\\ \\vdots \\\\ ax_n + b = y_n \\end{cases} \\] <p>\u4e00\u822c\u306b\\(n &gt; 2\\)\u306e\u5834\u5408\u3001\u3053\u306e\u65b9\u7a0b\u5f0f\u306f\u53b3\u5bc6\u306a\u89e3\u3092\u6301\u3061\u307e\u305b\u3093\u304c\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308a\u6700\u9069\u306a\\(a\\)\u3068\\(b\\)\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#7_1","title":"7. \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u610f\u7fa9\uff08\u7d9a\u304d\uff09","text":""},{"location":"lectures/LA/14-system-of-linear-equation/#72","title":"7.2 \u7279\u5fb4\u9078\u629e\u3068\u591a\u91cd\u5171\u7dda\u6027\uff08\u7d9a\u304d\uff09","text":"<p>\u4f8b\u3048\u3070\u3001\u3042\u308b\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u4f53\u91cd\u3001BMI\u3001\u30a6\u30a8\u30b9\u30c8\u5468\u56f2\u9577\u306a\u3069\u306e\u5f37\u304f\u76f8\u95a2\u3059\u308b\u7279\u5fb4\u3092\u5168\u3066\u8aac\u660e\u5909\u6570\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u3068\u3001\u30e2\u30c7\u30eb\u306e\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u4f4e\u4e0b\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u63a8\u5b9a\u3055\u308c\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4e0d\u5b89\u5b9a\u306b\u306a\u308a\u3001\u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u3084\u4e88\u6e2c\u6027\u80fd\u304c\u4f4e\u4e0b\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u5b9f\u969b\u3001\u591a\u91cd\u5171\u7dda\u6027\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u3001\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u5217\u6570\uff08\u7279\u5fb4\u6570\uff09\u3088\u308a\u5c0f\u3055\u304f\u306a\u308a\u3001\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3059\u308b\u72b6\u6cc1\u304c\u751f\u3058\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u5834\u5408\u3001\u6b63\u5247\u5316\u624b\u6cd5\uff08\u30ea\u30c3\u30b8\u56de\u5e30\u3084LASSO\uff09\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u6570\u5024\u7684\u306b\u5b89\u5b9a\u3057\u305f\u89e3\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#73","title":"7.3 \u5b9f\u9a13\u8a08\u753b\u6cd5\u3068\u5909\u6570\u306e\u8b58\u5225\u53ef\u80fd\u6027","text":"<p>\u5b9f\u9a13\u8a08\u753b\u6cd5\u306b\u304a\u3044\u3066\u3001\u5b9f\u9a13\u6761\u4ef6\u306e\u8a2d\u5b9a\u304c\u4e0d\u9069\u5207\u306a\u5834\u5408\u3001\u5168\u3066\u306e\u5909\u6570\u306e\u52b9\u679c\u3092\u8b58\u5225\u3067\u304d\u306a\u3044\uff08\u5206\u96e2\u3067\u304d\u306a\u3044\uff09\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u4e00\u610f\u6027\u306e\u554f\u984c\u3068\u76f4\u63a5\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u5065\u5eb7\u95a2\u9023\u306e\u81e8\u5e8a\u8a66\u9a13\u3067\u306f\u3001\u8907\u6570\u306e\u6cbb\u7642\u6cd5\u3084\u4ecb\u5165\u306e\u52b9\u679c\u3092\u540c\u6642\u306b\u691c\u8a3c\u3059\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u305d\u306e\u3088\u3046\u306a\u5b9f\u9a13\u8a2d\u8a08\u306b\u304a\u3044\u3066\u3001\u5404\u4ecb\u5165\u306e\u52b9\u679c\u3092\u660e\u78ba\u306b\u533a\u5225\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u4fc2\u6570\u884c\u5217\u304c\u884c\u30d5\u30eb\u30e9\u30f3\u30af\uff08\u5168\u3066\u306e\u884c\u304c\u7dda\u5f62\u72ec\u7acb\uff09\u3067\u3042\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u305d\u3046\u3067\u306a\u3051\u308c\u3070\u3001\u500b\u3005\u306e\u4ecb\u5165\u52b9\u679c\u3092\u4e00\u610f\u306b\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#8","title":"8. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/14-system-of-linear-equation/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c1. \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u6709\u7121\u3092\u5224\u5b9a\u3057\u3001\u89e3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u6c42\u3081\u306a\u3055\u3044\u3002</p> \\[ \\begin{cases} 2x + y = 5 \\\\ 4x + 2y = 10 \\end{cases} \\] <p>\u554f\u984c2. \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u6709\u7121\u3092\u5224\u5b9a\u3057\u3001\u89e3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u6c42\u3081\u306a\u3055\u3044\u3002</p> \\[ \\begin{cases} x + 2y - z = 3 \\\\ 2x + y + z = 8 \\\\ 3x + 4y + 0z = 14 \\end{cases} \\] <p>\u554f\u984c3. \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u6709\u7121\u3092\u5224\u5b9a\u3057\u3001\u89e3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u6c42\u3081\u306a\u3055\u3044\u3002</p> \\[ \\begin{cases} x + y + z = 3 \\\\ 2x + 2y + 2z = 5 \\\\ 3x + 3y + 3z = 9 \\end{cases} \\] <p>\u554f\u984c4. \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u6709\u7121\u3092\u5224\u5b9a\u3057\u3001\u89e3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u6c42\u3081\u306a\u3055\u3044\u3002</p> \\[ \\begin{cases} x + 2y - z = 4 \\\\ 2x + 4y - 2z = 8 \\\\ 3x + 6y - 3z = 10 \\end{cases} \\]"},{"location":"lectures/LA/14-system-of-linear-equation/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c5. \u30d1\u30e9\u30e1\u30fc\u30bf\\(a\\)\u306e\u5024\u306b\u3088\u3063\u3066\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7a2e\u985e\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u304b\u8abf\u3079\u306a\u3055\u3044\u3002</p> \\[ \\begin{cases} x + y = 1 \\\\ 2x + 2y = a \\end{cases} \\] <p>\u554f\u984c6. \u4ee5\u4e0b\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u6761\u4ef6\u3092\\(a\\)\u3092\u7528\u3044\u3066\u8868\u3057\u306a\u3055\u3044\u3002</p> \\[ \\begin{cases} x + y + z = 1 \\\\ 2x + 3y + 4z = 2 \\\\ 3x + 4y + 5z = a \\end{cases} \\] <p>\u554f\u984c7. \uff08\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5fdc\u7528\u554f\u984c\uff09 \u3042\u308b\u7814\u7a76\u8005\u304c\u3001BMI\uff08\u4f53\u683c\u6307\u6570\uff09\u3001\u904b\u52d5\u91cf\uff08\u9031\u3042\u305f\u308a\u306e\u904b\u52d5\u6642\u9593\uff09\u3001\u98df\u4e8b\u306e\u8cea\uff080-10\u306e\u70b9\u6570\uff09\u306e3\u3064\u306e\u8981\u56e0\u304c\u8840\u5727\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u8abf\u3079\u308b\u305f\u3081\u306b\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u307e\u3059\u3002\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3057\u305f\u3068\u3053\u308d\u3001\u4ee5\u4e0b\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u304c\u5f97\u3089\u308c\u307e\u3057\u305f\u3002</p> \\[ \\begin{cases} \\beta_1 + 25\\beta_2 + 3\\beta_3 + 7\\beta_4 = 120 \\\\ \\beta_1 + 30\\beta_2 + 5\\beta_3 + 6\\beta_4 = 130 \\\\ \\beta_1 + 27\\beta_2 + 4\\beta_3 + 8\\beta_4 = 125 \\\\ \\beta_1 + 29\\beta_2 + 2\\beta_3 + 5\\beta_4 = 128 \\end{cases} \\] <p>\u3053\u3053\u3067\u3001\\(\\beta_1\\)\u306f\u5207\u7247\u3001\\(\\beta_2\\)\u306fBMI\u306e\u4fc2\u6570\u3001\\(\\beta_3\\)\u306f\u904b\u52d5\u91cf\u306e\u4fc2\u6570\u3001\\(\\beta_4\\)\u306f\u98df\u4e8b\u306e\u8cea\u306e\u4fc2\u6570\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>(1) \u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u6709\u7121\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002 (2) \u89e3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u6c42\u3081\u3001\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u7406\u7531\u3092\u8aac\u660e\u3057\u306a\u3055\u3044\u3002 (3) \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u89b3\u70b9\u304b\u3089\u3001\u3053\u306e\u7d50\u679c\u306f\u4f55\u3092\u610f\u5473\u3059\u308b\u306e\u304b\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#9","title":"9. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/14-system-of-linear-equation/#q1","title":"Q1: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u6709\u7121\u3092\u3069\u306e\u3088\u3046\u306b\u5224\u65ad\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A1: \u9023\u7acb\u65b9\u7a0b\u5f0f\\(AX = B\\)\u306e\u89e3\u306e\u6709\u7121\u306f\u3001\u4fc2\u6570\u884c\u5217\\(A\\)\u306e\u30e9\u30f3\u30af\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\((A|B)\\)\u306e\u30e9\u30f3\u30af\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u3067\u5224\u65ad\u3067\u304d\u307e\u3059\u3002</p> <ul> <li>\u3082\u3057\\(\\text{rank}(A) \\neq \\text{rank}(A|B)\\)\u3067\u3042\u308c\u3070\u3001\u89e3\u306f\u5b58\u5728\u3057\u307e\u305b\u3093\u3002</li> <li>\u3082\u3057\\(\\text{rank}(A) = \\text{rank}(A|B)\\)\u3067\u3042\u308c\u3070\u3001\u89e3\u306f\u5b58\u5728\u3057\u307e\u3059\u3002</li> </ul> <p>\u30e9\u30f3\u30af\u306f\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u3066\u5f97\u3089\u308c\u308b\u968e\u6bb5\u884c\u5217\u306e\u975e\u30bc\u30ed\u884c\u306e\u6570\u3068\u3057\u3066\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002\u307e\u305f\u3001Python\uff08NumPy\uff09\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f\u3001<code>np.linalg.matrix_rank()</code>\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u7c21\u5358\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#q2","title":"Q2: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u7121\u6570\u306b\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306b\u8868\u73fe\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A2: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u7121\u6570\u306b\u5b58\u5728\u3059\u308b\u5834\u5408\uff08\\(\\text{rank}(A) = \\text{rank}(A|B) &lt; n\\)\u306e\u5834\u5408\uff09\u3001\u89e3\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u7528\u3044\u3066\u8868\u73fe\u3057\u307e\u3059\u3002\u307e\u305a\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u3066\u968e\u6bb5\u884c\u5217\u3092\u5f97\u307e\u3059\u3002\u6b21\u306b\u3001\u57fa\u672c\u5909\u6570\uff08\u5148\u982d\u306e\u975e\u30bc\u30ed\u8981\u7d20\u3092\u6301\u3064\u5217\u306b\u5bfe\u5fdc\u3059\u308b\u5909\u6570\uff09\u3068\u81ea\u7531\u5909\u6570\uff08\u305d\u306e\u4ed6\u306e\u5909\u6570\uff09\u306b\u5206\u3051\u307e\u3059\u3002\u81ea\u7531\u5909\u6570\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5272\u308a\u5f53\u3066\u3001\u57fa\u672c\u5909\u6570\u3092\u305d\u308c\u3089\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u8868\u73fe\u3057\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\\(x + 2y - z = 3\\)\u3068\u3044\u3046\u65b9\u7a0b\u5f0f\u3067\u306f\u3001\\(y\\)\u3068\\(z\\)\u3092\u81ea\u7531\u5909\u6570\u3068\u3059\u308b\u3068\u3001\\(x = 3 - 2y + z\\)\u3068\u8868\u305b\u307e\u3059\u3002\u4e00\u822c\u7684\u306b\u306f\u3001\\((x, y, z) = (3 - 2s + t, s, t)\\)\u306e\u3088\u3046\u306b\u8868\u73fe\u3057\u307e\u3059\u3002\u3053\u3053\u3067\\(s\\)\u3068\\(t\\)\u306f\u4efb\u610f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#q3","title":"Q3: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u306f\u3069\u306e\u3088\u3046\u306a\u610f\u5473\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A3: \u4fc2\u6570\u884c\u5217\\(A\\)\u306e\u30e9\u30f3\u30af\u306f\u3001\u305d\u306e\u884c\u5217\u306e\u7dda\u5f62\u72ec\u7acb\u306a\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u6700\u5927\u6570\u3067\u3059\u3002\u3053\u308c\u306f\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u65b9\u7a0b\u5f0f\u304c\u8868\u3059\u8d85\u5e73\u9762\u306e\u300c\u6709\u52b9\u306a\u5236\u7d04\u306e\u6570\u300d\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u30013\u3064\u306e\u672a\u77e5\u6570\u3092\u6301\u3064\u9023\u7acb\u65b9\u7a0b\u5f0f\u3067\uff1a - \u30e9\u30f3\u30af\u304c3\u306e\u5834\u5408\uff1a3\u3064\u306e\u72ec\u7acb\u3057\u305f\u5236\u7d04\u304c\u3042\u308a\u3001\u89e3\u306f\u4e00\u610f\u306b\u5b9a\u307e\u308a\u307e\u3059\uff08\u70b9\uff09 - \u30e9\u30f3\u30af\u304c2\u306e\u5834\u5408\uff1a2\u3064\u306e\u72ec\u7acb\u3057\u305f\u5236\u7d04\u304c\u3042\u308a\u3001\u89e3\u306f\u76f4\u7dda\u306b\u306a\u308a\u307e\u3059 - \u30e9\u30f3\u30af\u304c1\u306e\u5834\u5408\uff1a1\u3064\u306e\u72ec\u7acb\u3057\u305f\u5236\u7d04\u304c\u3042\u308a\u3001\u89e3\u306f\u5e73\u9762\u306b\u306a\u308a\u307e\u3059</p> <p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u6587\u8108\u3067\u306f\u3001\u30e9\u30f3\u30af\u306f\u8aac\u660e\u5909\u6570\u9593\u306e\u72ec\u7acb\u6027\u306e\u6307\u6a19\u3068\u3057\u3066\u3082\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002\u30e9\u30f3\u30af\u304c\u4f4e\u3044\u5834\u5408\u3001\u8aac\u660e\u5909\u6570\u9593\u306b\u591a\u91cd\u5171\u7dda\u6027\u304c\u5b58\u5728\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#q4","title":"Q4: \u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A4: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u3001\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u9023\u7acb\u65b9\u7a0b\u5f0f\u306f\u3001\u30c7\u30fc\u30bf\u306b\u77db\u76fe\u3084\u4e0d\u6574\u5408\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u56de\u5e30\u5206\u6790\u3067\u306f\u3001\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u6570\u304c\u672a\u77e5\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u3088\u308a\u591a\u3044\u5834\u5408\u3001\u53b3\u5bc6\u306a\u89e3\u306f\u901a\u5e38\u5b58\u5728\u3057\u307e\u305b\u3093\u3002</p> <p>\u3053\u306e\u5834\u5408\u3001\u4e00\u822c\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3092\u7528\u3044\u3066\u3001\u65b9\u7a0b\u5f0f\u3092\u5b8c\u5168\u306b\u6e80\u305f\u3059\u89e3\u3067\u306f\u306a\u304f\u3001\u8aa4\u5dee\u306e\u4e8c\u4e57\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u89e3\u3092\u6c42\u3081\u308b\u3053\u3068\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30c7\u30fc\u30bf\u306e\u5168\u4f53\u7684\u306a\u30c8\u30ec\u30f3\u30c9\u3092\u6349\u3048\u305f\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3067\u304d\u307e\u3059\u3002</p> <p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u306f\u3001\u30ce\u30a4\u30ba\u3084\u6e2c\u5b9a\u8aa4\u5dee\u306e\u5b58\u5728\u3092\u8003\u616e\u3057\u3001\u53b3\u5bc6\u306a\u89e3\u3088\u308a\u3082\u4e00\u822c\u5316\u6027\u80fd\u306e\u9ad8\u3044\u89e3\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#q5","title":"Q5: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\u306b\u304a\u3051\u308b\u6570\u5024\u7684\u306a\u5b89\u5b9a\u6027\u306e\u554f\u984c\u306f\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A5: \u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u969b\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u6570\u5024\u7684\u306a\u5b89\u5b9a\u6027\u306e\u554f\u984c\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li> <p>\u6761\u4ef6\u6570\u306e\u5927\u304d\u3044\u884c\u5217\uff1a\u4fc2\u6570\u884c\u5217\u306e\u6761\u4ef6\u6570\u304c\u5927\u304d\u3044\u5834\u5408\u3001\u5c0f\u3055\u306a\u5165\u529b\u5909\u5316\u304c\u5927\u304d\u306a\u51fa\u529b\u5909\u5316\u3092\u5f15\u304d\u8d77\u3053\u3059\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u7279\u306b\u591a\u91cd\u5171\u7dda\u6027\u306e\u3042\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u554f\u984c\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u6841\u843d\u3061\uff1a\u7570\u306a\u308b\u30b9\u30b1\u30fc\u30eb\u306e\u5024\u304c\u6df7\u5728\u3059\u308b\u5834\u5408\u3001\u8a08\u7b97\u4e2d\u306b\u6709\u52b9\u6841\u6570\u304c\u5931\u308f\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u975e\u5e38\u306b\u5927\u304d\u306a\u6570\u3068\u975e\u5e38\u306b\u5c0f\u3055\u306a\u6570\u3092\u5408\u8a08\u3059\u308b\u5834\u5408\u306a\u3069\u3002</p> </li> <li> <p>\u4e38\u3081\u8aa4\u5dee\u306e\u84c4\u7a4d\uff1a\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\uff08\u7279\u306b\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\uff09\u3067\u306f\u3001\u591a\u304f\u306e\u6f14\u7b97\u304c\u9806\u6b21\u5b9f\u884c\u3055\u308c\u308b\u305f\u3081\u3001\u4e38\u3081\u8aa4\u5dee\u304c\u84c4\u7a4d\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> </ol> <p>\u3053\u308c\u3089\u306e\u554f\u984c\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u306b\u3001\u5909\u6570\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3001\u30d4\u30dc\u30c3\u30c8\u9078\u629e\u3092\u4f34\u3046\u30ac\u30a6\u30b9\u6d88\u53bb\u6cd5\u3001\u6b63\u5247\u5316\u6280\u8853\uff08\u30ea\u30c3\u30b8\u56de\u5e30\u3084LASSO\uff09\u306a\u3069\u306e\u624b\u6cd5\u304c\u7528\u3044\u3089\u308c\u307e\u3059\u3002\u7279\u306b\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u7570\u306a\u308b\u6e2c\u5b9a\u5358\u4f4d\u3084\u30b9\u30b1\u30fc\u30eb\u306e\u5909\u6570\u304c\u6df7\u5728\u3059\u308b\u3053\u3068\u304c\u591a\u3044\u305f\u3081\u3001\u3053\u308c\u3089\u306e\u5b89\u5b9a\u6027\u554f\u984c\u306b\u6ce8\u610f\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/14-system-of-linear-equation/#10","title":"10. \u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\u3002\u4e3b\u8981\u306a\u30dd\u30a4\u30f3\u30c8\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\\(AX = B\\)\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u306f\u3001\u4fc2\u6570\u884c\u5217\\(A\\)\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\\((A|B)\\)\u306e\u30e9\u30f3\u30af\u306e\u95a2\u4fc2\u306b\u3088\u3063\u3066\u6c7a\u307e\u308a\u307e\u3059\u3002</li> <li>\\(\\text{rank}(A) \\neq \\text{rank}(A|B)\\)\u306e\u5834\u5408\uff1a\u89e3\u306f\u5b58\u5728\u3057\u306a\u3044</li> <li>\\(\\text{rank}(A) = \\text{rank}(A|B) = n\\)\u306e\u5834\u5408\uff1a\u89e3\u306f\u552f\u4e00\u5b58\u5728\u3059\u308b</li> <li> <p>\\(\\text{rank}(A) = \\text{rank}(A|B) &lt; n\\)\u306e\u5834\u5408\uff1a\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3059\u308b</p> </li> <li> <p>\u89e3\u306e\u5224\u5225\u3068\u8a08\u7b97\u306f\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u4fc2\u6570\u884c\u5217\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u3092\u968e\u6bb5\u884c\u5217\u306b\u5909\u63db\u3057\u3001\u30e9\u30f3\u30af\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u7406\u8ad6\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u591a\u304f\u306e\u5206\u91ce\uff08\u56de\u5e30\u5206\u6790\u3001\u7279\u5fb4\u9078\u629e\u3001\u5b9f\u9a13\u8a08\u753b\u6cd5\u306a\u3069\uff09\u306b\u5fdc\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u7279\u306b\u3001\u30e9\u30f3\u30af\u306e\u6982\u5ff5\u306f\u3001\u591a\u91cd\u5171\u7dda\u6027\u306e\u691c\u51fa\u3084\u5909\u6570\u306e\u8b58\u5225\u53ef\u80fd\u6027\u306e\u8a55\u4fa1\u306b\u91cd\u8981\u3067\u3059\u3002</p> </li> <li> <p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u53b3\u5bc6\u306a\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u3053\u3068\u304c\u591a\u3044\u305f\u3081\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306a\u3069\u306e\u8fd1\u4f3c\u89e3\u6cd5\u304c\u91cd\u8981\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> </ol> <p>\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u9006\u884c\u5217\u306e\u6982\u5ff5\u3068\u5b58\u5728\u6761\u4ef6\u306b\u3064\u3044\u3066\u5b66\u3073\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\u3068\u306e\u95a2\u9023\u3092\u6df1\u3081\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c15\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8\uff1a\u9006\u884c\u5217\u306e\u6982\u5ff5\u3068\u5b58\u5728\u6761\u4ef6","text":""},{"location":"lectures/LA/15-system-of-linear-equation/#_1","title":"\u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<ul> <li>\u8b1b\u7fa9\u56de: \u7b2c15\u56de</li> <li>\u30c6\u30fc\u30de: \u9006\u884c\u5217\u306e\u6982\u5ff5\u3068\u5b58\u5728\u6761\u4ef6</li> <li>\u95a2\u9023\u9805\u76ee: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af</li> <li>\u4e88\u7fd2\u4e8b\u9805: \u7b2c11\u56de\u301c\u7b2c14\u56de\u306e\u5185\u5bb9\uff08\u7279\u306b\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\uff09\u3092\u5fa9\u7fd2\u3057\u3066\u304a\u304f\u3053\u3068</li> </ul>"},{"location":"lectures/LA/15-system-of-linear-equation/#_2","title":"\u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u4e8b\u9805\u306b\u3064\u3044\u3066\u7406\u89e3\u3057\u3001\u8aac\u660e\u30fb\u8a08\u7b97\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\u3002</p> <ol> <li>\u9006\u884c\u5217\u306e\u5b9a\u7fa9\u3068\u57fa\u672c\u7684\u306a\u6027\u8cea\u3092\u7406\u89e3\u3057\u3001\u8aac\u660e\u3067\u304d\u308b</li> <li>\u9006\u884c\u5217\u3068\u884c\u5217\u306e\u30e9\u30f3\u30af\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3057\u3001\u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u9006\u884c\u5217\u3092\u5177\u4f53\u7684\u306b\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u9006\u884c\u5217\u3068\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3057\u3001\u5fdc\u7528\u3067\u304d\u308b</li> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u9006\u884c\u5217\u306e\u91cd\u8981\u6027\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/15-system-of-linear-equation/#1","title":"1. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/15-system-of-linear-equation/#11","title":"1.1 \u9006\u884c\u5217\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \\(A\\) \u3092 \\(n \\times n\\) \u306e\u6b63\u65b9\u884c\u5217\u3068\u3059\u308b\u3068\u304d\u3001\\(AB = BA = I_n\\) \u3068\u306a\u308b\u884c\u5217 \\(B\\) \u304c\u5b58\u5728\u3059\u308c\u3070\u3001\\(B\\) \u3092 \\(A\\) \u306e\u9006\u884c\u5217\u3068\u3044\u3044\u3001\\(A^{-1}\\) \u3068\u8868\u3059\u3002\u3053\u3053\u3067 \\(I_n\\) \u306f \\(n\\) \u6b21\u306e\u5358\u4f4d\u884c\u5217\u3067\u3042\u308b\u3002</p> <p>\u9006\u884c\u5217\u306f\u3001\u3082\u3068\u306e\u884c\u5217\u306b\u5bfe\u3057\u3066\u300c\u639b\u3051\u308b\u3068\u5358\u4f4d\u884c\u5217\u306b\u306a\u308b\u884c\u5217\u300d\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u9006\u884c\u5217 \\(A^{-1}\\) \u304c\u5b58\u5728\u3059\u308b\u3068\u304d\u3001\u4ee5\u4e0b\u306e\u95a2\u4fc2\u304c\u6210\u308a\u7acb\u3061\u307e\u3059\u3002</p> \\[A \\cdot A^{-1} = A^{-1} \\cdot A = I_n\\] <p>\u3053\u308c\u306f\u6570\u306e\u4e16\u754c\u3067\u306e\u300c\u9006\u6570\u300d\u306b\u76f8\u5f53\u3059\u308b\u6982\u5ff5\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u6570 \\(a\\) \u306e\u9006\u6570\u306f \\(\\frac{1}{a}\\) \u3067\u3042\u308a\u3001\\(a \\cdot \\frac{1}{a} = 1\\) \u3068\u306a\u308a\u307e\u3059\u3002\u884c\u5217\u306e\u4e16\u754c\u3067\u306f\u3001\u5358\u4f4d\u884c\u5217 \\(I\\) \u304c\u6570\u306e\u300c1\u300d\u306b\u5bfe\u5fdc\u3057\u3001\u9006\u884c\u5217 \\(A^{-1}\\) \u304c\u300c\\(\\frac{1}{a}\\)\u300d\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#12","title":"1.2 \u9006\u884c\u5217\u306e\u57fa\u672c\u7684\u6027\u8cea","text":"<p>\u9006\u884c\u5217\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u91cd\u8981\u306a\u6027\u8cea\u304c\u3042\u308a\u307e\u3059\u3002</p> <ol> <li>\u6b63\u65b9\u884c\u5217\u3067\u306a\u3051\u308c\u3070\u9006\u884c\u5217\u306f\u5b58\u5728\u3057\u306a\u3044</li> <li>\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u3068\u304d\u3001\u305d\u306e\u9006\u884c\u5217\u306f\u552f\u4e00\u3064\u306b\u5b9a\u307e\u308b\uff08\u4e00\u610f\u6027\uff09</li> <li>\u9006\u884c\u5217\u3092\u6301\u3064\u884c\u5217\u306f\u6b63\u5247\u884c\u5217\u307e\u305f\u306f\u53ef\u9006\u884c\u5217\u3068\u547c\u3070\u308c\u308b</li> <li>\u884c\u5217 \\(A\\) \u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f \\(\\det(A) \\neq 0\\) \u3067\u3042\u308b</li> <li>\u9006\u884c\u5217\u306e\u9006\u884c\u5217\u306f\u3082\u3068\u306e\u884c\u5217\u306b\u7b49\u3057\u3044\uff1a\\((A^{-1})^{-1} = A\\)</li> <li>\u8ee2\u7f6e\u884c\u5217\u306e\u9006\u884c\u5217\u306f\u3001\u9006\u884c\u5217\u306e\u8ee2\u7f6e\u884c\u5217\u306b\u7b49\u3057\u3044\uff1a\\((A^T)^{-1} = (A^{-1})^T\\)</li> <li>\u7a4d\u306e\u9006\u884c\u5217\u306f\u3001\u9006\u306e\u9806\u5e8f\u3067\u306e\u9006\u884c\u5217\u306e\u7a4d\u306b\u7b49\u3057\u3044\uff1a\\((AB)^{-1} = B^{-1}A^{-1}\\)</li> </ol>"},{"location":"lectures/LA/15-system-of-linear-equation/#2","title":"2. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/15-system-of-linear-equation/#21","title":"2.1 \u9006\u884c\u5217\u3068\u884c\u5217\u306e\u30e9\u30f3\u30af","text":"<p>\u524d\u56de\u307e\u3067\u306e\u8b1b\u7fa9\u3067\u5b66\u3093\u3060\u3088\u3046\u306b\u3001\\(n \\times n\\) \u306e\u6b63\u65b9\u884c\u5217 \\(A\\) \u306e\u30e9\u30f3\u30af\u306f\u3001\u884c\u307e\u305f\u306f\u5217\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u7dda\u5f62\u72ec\u7acb\u306a\u6570\u3092\u8868\u3057\u307e\u3059\u3002\u884c\u5217 \\(A\\) \u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> <p>\u5b9a\u7406: \\(n \\times n\\) \u306e\u6b63\u65b9\u884c\u5217 \\(A\\) \u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\u3001\\(\\operatorname{rank}(A) = n\\) \u3067\u3042\u308b\u3002</p> <p>\u3064\u307e\u308a\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u6570\u306b\u7b49\u3057\u3044\u3068\u304d\u3001\u304b\u3064\u305d\u306e\u3068\u304d\u306b\u9650\u308a\u3001\u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u884c\u5217 \\(A\\) \u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u30d9\u30af\u30c8\u30eb\u304c\u3059\u3079\u3066\u7dda\u5f62\u72ec\u7acb\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#22","title":"2.2 \u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6\u3068\u884c\u5217\u5f0f","text":"<p>\u5225\u306e\u89b3\u70b9\u304b\u3089\u898b\u308b\u3068\u3001\u884c\u5217 \\(A\\) \u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u6761\u4ef6\u306f\u884c\u5217\u5f0f\u3092\u7528\u3044\u3066\u8868\u3059\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002</p> <p>\u5b9a\u7406: \\(n \\times n\\) \u306e\u6b63\u65b9\u884c\u5217 \\(A\\) \u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\u3001\\(\\det(A) \\neq 0\\) \u3067\u3042\u308b\u3002</p> <p>\u3053\u306e\u6761\u4ef6\u306f\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af\u306b\u95a2\u3059\u308b\u6761\u4ef6\u3068\u540c\u5024\u3067\u3059\u3002\u306a\u305c\u306a\u3089\u3001\\(\\det(A) \\neq 0\\) \u3067\u3042\u308b\u3068\u304d\u3001\u304b\u3064\u305d\u306e\u3068\u304d\u306b\u9650\u308a\u3001\\(\\operatorname{rank}(A) = n\\) \u3068\u306a\u308b\u304b\u3089\u3067\u3059\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#23","title":"2.3 \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u3088\u308b\u9006\u884c\u5217\u306e\u8a08\u7b97","text":"<p>\u9006\u884c\u5217\u3092\u6c42\u3081\u308b\u306b\u306f\u3001\u3044\u304f\u3064\u304b\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u3053\u3053\u3067\u306f\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u305f\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002\u3053\u306e\u65b9\u6cd5\u306f\u3001\u62e1\u5927\u884c\u5217\u3092\u7528\u3044\u3066\u7cfb\u7d71\u7684\u306b\u8a08\u7b97\u3092\u884c\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#_3","title":"\u624b\u9806:","text":"<ol> <li>\u884c\u5217 \\(A\\) \u3068\u5358\u4f4d\u884c\u5217 \\(I\\) \u3092\u6a2a\u306b\u4e26\u3079\u305f\u62e1\u5927\u884c\u5217 \\([A|I]\\) \u3092\u4f5c\u6210\u3059\u308b</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066 \\(A\\) \u306e\u90e8\u5206\u3092\u5358\u4f4d\u884c\u5217 \\(I\\) \u306b\u5909\u5f62\u3059\u308b</li> <li>\u305d\u306e\u7d50\u679c\u5f97\u3089\u308c\u308b\u62e1\u5927\u884c\u5217\u306f \\([I|A^{-1}]\\) \u306e\u5f62\u3068\u306a\u308a\u3001\u53f3\u5074\u306e\u884c\u5217\u304c\u6c42\u3081\u308b\u9006\u884c\u5217 \\(A^{-1}\\) \u3067\u3042\u308b</li> </ol> <p>\u3053\u306e\u65b9\u6cd5\u306f\u3001\u8907\u6570\u306e\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u540c\u6642\u306b\u89e3\u304f\u3053\u3068\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\\(AX = I\\) \u3068\u3044\u3046\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3053\u3068\u3067\u3001\\(X = A^{-1}\\) \u3092\u6c42\u3081\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#1-22","title":"\u4f8b\u984c1: 2\u00d72\u884c\u5217\u306e\u9006\u884c\u5217","text":"<p>\u4f8b\u3068\u3057\u3066\u3001\u4ee5\u4e0b\u306e2\u00d72\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[A = \\begin{pmatrix} 2 &amp; 1 \\\\ 3 &amp; 2 \\end{pmatrix}\\] <p>\u307e\u305a\u3001\u62e1\u5927\u884c\u5217 \\([A|I]\\) \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 2 &amp; 1 &amp; | &amp; 1 &amp; 0 \\\\ 3 &amp; 2 &amp; | &amp; 0 &amp; 1 \\end{pmatrix}\\] <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u3001\u5de6\u5074\u3092\u5358\u4f4d\u884c\u5217\u306b\u5909\u5f62\u3057\u307e\u3059\u3002</p> <p>\u7b2c1\u884c\u3092\u7528\u3044\u3066\u3001\u7b2c2\u884c\u304b\u3089\u7b2c1\u884c\u306e \\(\\frac{3}{2}\\) \u500d\u3092\u5f15\u304d\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 2 &amp; 1 &amp; | &amp; 1 &amp; 0 \\\\ 0 &amp; \\frac{1}{2} &amp; | &amp; -\\frac{3}{2} &amp; 1 \\end{pmatrix}\\] <p>\u7b2c2\u884c\u3092 \\(2\\) \u500d\u3057\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 2 &amp; 1 &amp; | &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; | &amp; -3 &amp; 2 \\end{pmatrix}\\] <p>\u7b2c2\u884c\u3092\u7528\u3044\u3066\u3001\u7b2c1\u884c\u304b\u3089\u7b2c2\u884c\u306e \\(1\\) \u500d\u3092\u5f15\u304d\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 2 &amp; 0 &amp; | &amp; 4 &amp; -2 \\\\ 0 &amp; 1 &amp; | &amp; -3 &amp; 2 \\end{pmatrix}\\] <p>\u7b2c1\u884c\u3092 \\(\\frac{1}{2}\\) \u500d\u3057\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 1 &amp; 0 &amp; | &amp; 2 &amp; -1 \\\\ 0 &amp; 1 &amp; | &amp; -3 &amp; 2 \\end{pmatrix}\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\u6c42\u3081\u308b\u9006\u884c\u5217\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\u3002</p> \\[A^{-1} = \\begin{pmatrix} 2 &amp; -1 \\\\ -3 &amp; 2 \\end{pmatrix}\\] <p>\u5b9f\u969b\u306b\u3001\\(A \\cdot A^{-1}\\) \u3092\u8a08\u7b97\u3057\u3066\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[A \\cdot A^{-1} = \\begin{pmatrix} 2 &amp; 1 \\\\ 3 &amp; 2 \\end{pmatrix} \\begin{pmatrix} 2 &amp; -1 \\\\ -3 &amp; 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 2 + 1 \\cdot (-3) &amp; 2 \\cdot (-1) + 1 \\cdot 2 \\\\ 3 \\cdot 2 + 2 \\cdot (-3) &amp; 3 \\cdot (-1) + 2 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\] <p>\u78ba\u304b\u306b\u5358\u4f4d\u884c\u5217\u306b\u306a\u308a\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#2-33","title":"\u4f8b\u984c2: 3\u00d73\u884c\u5217\u306e\u9006\u884c\u5217","text":"<p>\u6b21\u306b\u3001\u4ee5\u4e0b\u306e3\u00d73\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[A = \\begin{pmatrix} 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 5 &amp; 3 \\\\ 1 &amp; 0 &amp; 2 \\end{pmatrix}\\] <p>\u62e1\u5927\u884c\u5217 \\([A|I]\\) \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 1 &amp; 2 &amp; 1 &amp; | &amp; 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 5 &amp; 3 &amp; | &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 2 &amp; | &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u3001\u5de6\u5074\u3092\u5358\u4f4d\u884c\u5217\u306b\u5909\u5f62\u3057\u307e\u3059\u3002</p> <p>\u7b2c1\u884c\u3092\u7528\u3044\u3066\u3001\u7b2c2\u884c\u304b\u3089\u7b2c1\u884c\u306e \\(2\\) \u500d\u3092\u5f15\u304d\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 1 &amp; 2 &amp; 1 &amp; | &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; | &amp; -2 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 2 &amp; | &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] <p>\u7b2c1\u884c\u3092\u7528\u3044\u3066\u3001\u7b2c3\u884c\u304b\u3089\u7b2c1\u884c\u306e \\(1\\) \u500d\u3092\u5f15\u304d\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 1 &amp; 2 &amp; 1 &amp; | &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; | &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; -2 &amp; 1 &amp; | &amp; -1 &amp; 0 &amp; 1 \\end{pmatrix}\\] <p>\u7b2c2\u884c\u3092\u7528\u3044\u3066\u3001\u7b2c3\u884c\u306b\u7b2c2\u884c\u306e \\(2\\) \u500d\u3092\u8db3\u3057\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 1 &amp; 2 &amp; 1 &amp; | &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; | &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 &amp; | &amp; -5 &amp; 2 &amp; 1 \\end{pmatrix}\\] <p>\u7b2c3\u884c\u3092 \\(\\frac{1}{3}\\) \u500d\u3057\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 1 &amp; 2 &amp; 1 &amp; | &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; | &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; -\\frac{5}{3} &amp; \\frac{2}{3} &amp; \\frac{1}{3} \\end{pmatrix}\\] <p>\u7b2c3\u884c\u3092\u7528\u3044\u3066\u3001\u7b2c2\u884c\u304b\u3089\u7b2c3\u884c\u306e \\(1\\) \u500d\u3092\u5f15\u304d\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 1 &amp; 2 &amp; 1 &amp; | &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; | &amp; -\\frac{1}{3} &amp; \\frac{1}{3} &amp; -\\frac{1}{3} \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; -\\frac{5}{3} &amp; \\frac{2}{3} &amp; \\frac{1}{3} \\end{pmatrix}\\] <p>\u7b2c3\u884c\u3092\u7528\u3044\u3066\u3001\u7b2c1\u884c\u304b\u3089\u7b2c3\u884c\u306e \\(1\\) \u500d\u3092\u5f15\u304d\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 1 &amp; 2 &amp; 0 &amp; | &amp; \\frac{8}{3} &amp; -\\frac{2}{3} &amp; -\\frac{1}{3} \\\\ 0 &amp; 1 &amp; 0 &amp; | &amp; -\\frac{1}{3} &amp; \\frac{1}{3} &amp; -\\frac{1}{3} \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; -\\frac{5}{3} &amp; \\frac{2}{3} &amp; \\frac{1}{3} \\end{pmatrix}\\] <p>\u7b2c2\u884c\u3092\u7528\u3044\u3066\u3001\u7b2c1\u884c\u304b\u3089\u7b2c2\u884c\u306e \\(2\\) \u500d\u3092\u5f15\u304d\u307e\u3059\u3002</p> \\[\\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; | &amp; \\frac{10}{3} &amp; -\\frac{4}{3} &amp; \\frac{1}{3} \\\\ 0 &amp; 1 &amp; 0 &amp; | &amp; -\\frac{1}{3} &amp; \\frac{1}{3} &amp; -\\frac{1}{3} \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; -\\frac{5}{3} &amp; \\frac{2}{3} &amp; \\frac{1}{3} \\end{pmatrix}\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\u6c42\u3081\u308b\u9006\u884c\u5217\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\u3002</p> \\[A^{-1} = \\begin{pmatrix} \\frac{10}{3} &amp; -\\frac{4}{3} &amp; \\frac{1}{3} \\\\ -\\frac{1}{3} &amp; \\frac{1}{3} &amp; -\\frac{1}{3} \\\\ -\\frac{5}{3} &amp; \\frac{2}{3} &amp; \\frac{1}{3} \\end{pmatrix}\\]"},{"location":"lectures/LA/15-system-of-linear-equation/#24","title":"2.4 \u9006\u884c\u5217\u3068\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f","text":"<p>\u9006\u884c\u5217\u306f\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u969b\u306b\u975e\u5e38\u306b\u4fbf\u5229\u306a\u30c4\u30fc\u30eb\u3068\u306a\u308a\u307e\u3059\u3002\\(AX = B\\) \u3068\u3044\u3046\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u304c\u3042\u308b\u3068\u304d\u3001\u884c\u5217 \\(A\\) \u306e\u9006\u884c\u5217 \\(A^{-1}\\) \u304c\u5b58\u5728\u3059\u308c\u3070\u3001\u4e21\u8fba\u306b\u5de6\u304b\u3089 \\(A^{-1}\\) \u3092\u304b\u3051\u308b\u3053\u3068\u3067\u89e3\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> \\[AX = B$$ $$A^{-1}AX = A^{-1}B$$ $$IX = A^{-1}B$$ $$X = A^{-1}B\\] <p>\u3053\u306e\u3088\u3046\u306b\u3001\u9006\u884c\u5217\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u76f4\u63a5\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u3057\u304b\u3057\u3001\u5b9f\u969b\u306e\u8a08\u7b97\u3067\u306f\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u76f4\u63a5\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u307b\u3046\u304c\u52b9\u7387\u7684\u306a\u30b1\u30fc\u30b9\u304c\u591a\u3044\u3067\u3059\u3002\u7279\u306b\u3001\\(A^{-1}\\) \u3092\u6c42\u3081\u308b\u306b\u306f \\(n^2\\) \u500b\u306e\u65b9\u7a0b\u5f0f\uff08\\(n\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3092 \\(n\\) \u672c\uff09\u3092\u89e3\u304f\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304c\u3001\\(AX = B\\) \u3092\u76f4\u63a5\u89e3\u304f\u5834\u5408\u306f \\(n\\) \u500b\u306e\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3060\u3051\u3067\u6e08\u307f\u307e\u3059\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#3-python","title":"3. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u3053\u3053\u3067\u306f\u3001NumPy\u3092\u7528\u3044\u3066\u9006\u884c\u5217\u306e\u8a08\u7b97\u3068\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\u3092Python\u3067\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#31","title":"3.1 \u9006\u884c\u5217\u306e\u8a08\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# 2\u00d72\u884c\u5217\u306e\u4f8b\nA = np.array([[2, 1], [3, 2]])\nprint(\"\u884c\u5217 A:\")\nprint(A)\n\n# numpy.linalg.inv() \u3092\u4f7f\u3063\u3066\u9006\u884c\u5217\u3092\u8a08\u7b97\nA_inv = np.linalg.inv(A)\nprint(\"\\nA \u306e\u9006\u884c\u5217:\")\nprint(A_inv)\n\n# A\u30fbA^-1 = I \u3092\u78ba\u8a8d\nI = np.dot(A, A_inv)\nprint(\"\\nA\u30fbA^-1:\")\nprint(np.round(I, decimals=10))  # \u4e38\u3081\u8aa4\u5dee\u3092\u8003\u616e\n\n# 3\u00d73\u884c\u5217\u306e\u4f8b\nB = np.array([[1, 2, 1], [2, 5, 3], [1, 0, 2]])\nprint(\"\\n\u884c\u5217 B:\")\nprint(B)\n\n# \u9006\u884c\u5217\u3092\u8a08\u7b97\nB_inv = np.linalg.inv(B)\nprint(\"\\nB \u306e\u9006\u884c\u5217:\")\nprint(B_inv)\n\n# B\u30fbB^-1 = I \u3092\u78ba\u8a8d\nI_B = np.dot(B, B_inv)\nprint(\"\\nB\u30fbB^-1:\")\nprint(np.round(I_B, decimals=10))\n</code></pre>"},{"location":"lectures/LA/15-system-of-linear-equation/#32","title":"3.2 \u7279\u7570\u884c\u5217\u3068\u9006\u884c\u5217\u306e\u95a2\u4fc2","text":"<p>\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u6570\u306b\u6e80\u305f\u306a\u3044\u5834\u5408\u3001\u305d\u306e\u884c\u5217\u306f\u7279\u7570\u884c\u5217\u3068\u306a\u308a\u3001\u901a\u5e38\u306e\u9006\u884c\u5217\u306f\u5b58\u5728\u3057\u307e\u305b\u3093\u3002</p> <pre><code># \u7279\u7570\u884c\u5217\u306e\u4f8b\nC = np.array([[1, 2], [2, 4]])  # 2\u3064\u306e\u884c\u30d9\u30af\u30c8\u30eb\u306f\u7dda\u5f62\u5f93\u5c5e\nprint(\"\u7279\u7570\u884c\u5217 C:\")\nprint(C)\n\n# \u30e9\u30f3\u30af\u3092\u78ba\u8a8d\nrank_C = np.linalg.matrix_rank(C)\nprint(\"\\nC \u306e\u30e9\u30f3\u30af:\", rank_C)\n\n# \u884c\u5217\u5f0f\u3092\u78ba\u8a8d\ndet_C = np.linalg.det(C)\nprint(\"C \u306e\u884c\u5217\u5f0f:\", det_C)\n\ntry:\n    # \u9006\u884c\u5217\u3092\u8a08\u7b97\u3057\u3088\u3046\u3068\u3059\u308b\u3068\u4f8b\u5916\u304c\u767a\u751f\n    C_inv = np.linalg.inv(C)\n    print(\"C \u306e\u9006\u884c\u5217:\", C_inv)\nexcept np.linalg.LinAlgError as e:\n    print(\"\u30a8\u30e9\u30fc:\", e)\n</code></pre>"},{"location":"lectures/LA/15-system-of-linear-equation/#33","title":"3.3 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5","text":"<p>\u9006\u884c\u5217\u3092\u7528\u3044\u3066\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u3044\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code># \u9023\u7acb\u65b9\u7a0b\u5f0f: Ax = b \u306e\u4f8b\nA = np.array([[2, 1], [3, 2]])\nb = np.array([5, 8])\n\nprint(\"\u884c\u5217 A:\")\nprint(A)\nprint(\"\\n\u30d9\u30af\u30c8\u30eb b:\")\nprint(b)\n\n# \u9006\u884c\u5217\u3092\u7528\u3044\u305f\u89e3\u6cd5\nx_inv = np.dot(np.linalg.inv(A), b)\nprint(\"\\n\u9006\u884c\u5217\u306b\u3088\u308b\u89e3:\")\nprint(x_inv)\n\n# numpy.linalg.solve() \u3092\u4f7f\u3063\u305f\u76f4\u63a5\u89e3\u6cd5\nx_solve = np.linalg.solve(A, b)\nprint(\"\\nnp.linalg.solve() \u306b\u3088\u308b\u89e3:\")\nprint(x_solve)\n\n# \u89e3\u306e\u78ba\u8a8d\nprint(\"\\nA\u30fbx:\")\nprint(np.dot(A, x_solve))\n</code></pre>"},{"location":"lectures/LA/15-system-of-linear-equation/#34","title":"3.4 \u7dda\u5f62\u5909\u63db\u3068\u3057\u3066\u306e\u884c\u5217\u3068\u9006\u884c\u5217\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":"<p>\u884c\u5217\u306f\u7dda\u5f62\u5909\u63db\u3068\u3057\u3066\u89e3\u91c8\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u9006\u884c\u5217\u306f\u3001\u305d\u306e\u5909\u63db\u3092\u300c\u6253\u3061\u6d88\u3059\u300d\u5909\u63db\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <pre><code># \u7dda\u5f62\u5909\u63db\u306e\u8996\u899a\u5316\ndef plot_transformation(A, title=\"Linear Transformation\"):\n    # \u5358\u4f4d\u6b63\u65b9\u5f62\u306e\u9802\u70b9\n    square = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n\n    # \u5909\u63db\u5f8c\u306e\u6b63\u65b9\u5f62\n    transformed_square = np.dot(square, A.T)\n\n    # \u30d7\u30ed\u30c3\u30c8\n    plt.figure(figsize=(10, 10))\n    plt.grid(True)\n    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n    # \u5143\u306e\u6b63\u65b9\u5f62\n    plt.plot(square[:, 0], square[:, 1], 'b-', label=\"Original\")\n\n    # \u5909\u63db\u5f8c\u306e\u6b63\u65b9\u5f62\n    plt.plot(transformed_square[:, 0], transformed_square[:, 1], 'r-', label=\"Transformed\")\n\n    # \u539f\u70b9\u3068\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\n    plt.arrow(0, 0, 1, 0, head_width=0.05, head_length=0.1, fc='b', ec='b', label=\"i\")\n    plt.arrow(0, 0, 0, 1, head_width=0.05, head_length=0.1, fc='b', ec='b', label=\"j\")\n\n    plt.arrow(0, 0, A[0, 0], A[0, 1], head_width=0.05, head_length=0.1, fc='r', ec='r', label=\"A\u00b7i\")\n    plt.arrow(0, 0, A[1, 0], A[1, 1], head_width=0.05, head_length=0.1, fc='r', ec='r', label=\"A\u00b7j\")\n\n    plt.title(title)\n    plt.xlim(-2, 3)\n    plt.ylim(-2, 3)\n    plt.axis('equal')\n    plt.legend()\n    plt.show()\n\n# \u5909\u63db\u884c\u5217\u3068\u9006\u884c\u5217\nA = np.array([[2, 1], [1, 2]])\nA_inv = np.linalg.inv(A)\n\n# \u5909\u63db\u306e\u53ef\u8996\u5316\nplot_transformation(A, \"Linear Transformation: A\")\nplot_transformation(A_inv, \"Inverse Transformation: A^(-1)\")\n</code></pre>"},{"location":"lectures/LA/15-system-of-linear-equation/#5","title":"5. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/15-system-of-linear-equation/#_4","title":"\u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c1: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u306a\u3055\u3044\u3002 \\(\\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 1 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c2: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u304b\u5224\u5b9a\u3057\u3001\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u9006\u884c\u5217\u3092\u6c42\u3081\u306a\u3055\u3044\u3002 \\(\\(B = \\begin{pmatrix} 2 &amp; 4 \\\\ 1 &amp; 2 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c3: \u4ee5\u4e0b\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u9006\u884c\u5217\u3092\u7528\u3044\u3066\u89e3\u304d\u306a\u3055\u3044\u3002 \\(\\(\\begin{cases} 2x + y = 7 \\\\ x + y = 4 \\end{cases}\\)\\)</p> <p>\u554f\u984c4: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u306a\u3055\u3044\u3002 \\(\\(C = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 2 &amp; 1 \\\\ 1 &amp; 0 &amp; 2 \\end{pmatrix}\\)\\)</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#_5","title":"\u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c5: \\(A\\) \u3068 \\(B\\) \u304c\u6b63\u65b9\u884c\u5217\u3067\u3001\\(AB\\) \u304c\u6b63\u5247\u884c\u5217\u306a\u3089\u3070\u3001\\(A\\) \u3068 \\(B\\) \u306f\u5171\u306b\u6b63\u5247\u884c\u5217\u3067\u3042\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u306a\u3055\u3044\u3002</p> <p>\u554f\u984c6: 2\u6b21\u6b63\u65b9\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(A^2 = O\\) \uff08\\(O\\) \u306f\u96f6\u884c\u5217\uff09\u304c\u6210\u308a\u7acb\u3064\u3068\u304d\u3001\\(I - A\\) \u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> <p>\u554f\u984c7 (\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u95a2\u9023): \u5065\u5eb7\u4fdd\u967a\u4f1a\u793e\u306e\u30c7\u30fc\u30bf\u3067\u306f\u3001\u3042\u308b\u6cbb\u7642\u6cd5\u3092\u53d7\u3051\u305f\u60a3\u8005\u306e\u72b6\u614b\u304c\u6539\u5584\u3059\u308b\u78ba\u7387\u3092\u8868\u3059\u9077\u79fb\u884c\u5217\u304c\u3042\u308a\u307e\u3059\u3002\u73fe\u5728\u306e\u5065\u5eb7\u72b6\u614b\uff08\u826f\u597d\u3001\u4e2d\u7a0b\u5ea6\u3001\u60aa\u3044\uff09\u304b\u30896\u30f6\u6708\u5f8c\u306e\u5065\u5eb7\u72b6\u614b\u3078\u306e\u9077\u79fb\u3092\u8868\u3059\u884c\u5217 \\(P\\) \u304c\u6b21\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u3066\u3044\u307e\u3059\u3002</p> \\[P = \\begin{pmatrix}  0.7 &amp; 0.2 &amp; 0.1 \\\\ 0.3 &amp; 0.5 &amp; 0.2 \\\\ 0.1 &amp; 0.3 &amp; 0.6 \\end{pmatrix}\\] <p>\u3053\u3053\u3067\u3001\\(P_{ij}\\) \u306f\u72b6\u614b \\(i\\) \u304b\u3089\u72b6\u614b \\(j\\) \u3078\u306e\u9077\u79fb\u78ba\u7387\u3092\u8868\u3057\u307e\u3059\uff08\\(i, j = 1,2,3\\) \u306f\u300c\u826f\u597d\u300d\u300c\u4e2d\u7a0b\u5ea6\u300d\u300c\u60aa\u3044\u300d\u306b\u305d\u308c\u305e\u308c\u5bfe\u5fdc\uff09\u3002</p> <p>(a) \u3053\u306e\u9077\u79fb\u884c\u5217\u306e\u9006\u884c\u5217 \\(P^{-1}\\) \u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> <p>(b) \u9006\u884c\u5217 \\(P^{-1}\\) \u306e\u5404\u8981\u7d20 \\((P^{-1})_{ij}\\) \u306e\u89e3\u91c8\u306b\u3064\u3044\u3066\u3001\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u6587\u8108\u3067\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002\u7279\u306b\u3001\u9006\u884c\u5217\u306e\u8ca0\u306e\u5024\u306f\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3067\u304d\u308b\u304b\uff1f</p> <p>(c) 6\u30f6\u6708\u5f8c\u306e\u60a3\u8005\u306e\u5065\u5eb7\u72b6\u614b\u5206\u5e03\u304c \\([0.4, 0.35, 0.25]\\) \u3067\u3042\u3063\u305f\u3068\u304d\u3001\u5143\u306e\u5065\u5eb7\u72b6\u614b\u5206\u5e03\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#6","title":"6. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":"<p>Q1: \u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u6761\u4ef6\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A1: \\(n \\times n\\) \u306e\u6b63\u65b9\u884c\u5217 \\(A\\) \u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\u4ee5\u4e0b\u306e\u3044\u305a\u308c\u304b\u3067\u3059\u3002 - \\(\\operatorname{rank}(A) = n\\) \uff08\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u884c\u6570\u306b\u7b49\u3057\u3044\uff09 - \\(\\det(A) \\neq 0\\) \uff08\u884c\u5217\u5f0f\u304c\u30bc\u30ed\u3067\u306a\u3044\uff09 - \u884c\u5217 \\(A\\) \u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u30d9\u30af\u30c8\u30eb\u304c\u7dda\u5f62\u72ec\u7acb\u3067\u3042\u308b</p> <p>Q2: \u9006\u884c\u5217\u306f\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u5b58\u5728\u3057\u307e\u305b\u3093\u304b\uff1f</p> <p>A2: \u4ee5\u4e0b\u306e\u5834\u5408\u306b\u9006\u884c\u5217\u306f\u5b58\u5728\u3057\u307e\u305b\u3093\u3002 - \u6b63\u65b9\u884c\u5217\u3067\u306a\u3044\u5834\u5408 - \u6b63\u65b9\u884c\u5217\u3060\u304c\u7279\u7570\u884c\u5217\uff08\u884c\u5217\u5f0f\u304c\u30bc\u30ed\uff09\u306e\u5834\u5408 - \u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u884c\u6570\u3088\u308a\u5c0f\u3055\u3044\u5834\u5408 - \u884c\u5217\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u30d9\u30af\u30c8\u30eb\u304c\u7dda\u5f62\u5f93\u5c5e\u95a2\u4fc2\u306b\u3042\u308b\u5834\u5408</p> <p>Q3: \u9006\u884c\u5217\u3068\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u95a2\u4fc2\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>A3: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f \\(AX = B\\) \u306b\u304a\u3044\u3066\u3001\\(A\\) \u306e\u9006\u884c\u5217 \\(A^{-1}\\) \u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u65b9\u7a0b\u5f0f\u306e\u89e3\u306f \\(X = A^{-1}B\\) \u3067\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\u3053\u308c\u306f\u3001\\(A\\) \u304c\u6b63\u5247\uff08\u53ef\u9006\uff09\u3067\u3042\u308c\u3070\u3001\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u304c\u552f\u4e00\u3064\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> <p>Q4: \u9006\u884c\u5217\u306e\u8a08\u7b97\u65b9\u6cd5\u306b\u306f\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u304b\uff1f</p> <p>A4: \u4e3b\u306a\u8a08\u7b97\u65b9\u6cd5\u306b\u306f\u4ee5\u4e0b\u304c\u3042\u308a\u307e\u3059\u3002 - \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\uff08\u62e1\u5927\u884c\u5217\u3092\u7528\u3044\u308b\u65b9\u6cd5\uff09 - \u4f59\u56e0\u5b50\u884c\u5217\u3092\u7528\u3044\u308b\u65b9\u6cd5\uff08\u4f59\u56e0\u5b50\u884c\u5217\u306e\u8ee2\u7f6e\u3092\u884c\u5217\u5f0f\u3067\u5272\u308b\uff09 - \u6570\u5024\u8a08\u7b97\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3059\u308b\u65b9\u6cd5\uff08NumPy\u306e <code>linalg.inv()</code> \u306a\u3069\uff09</p> <p>Q5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u9006\u884c\u5217\u304c\u91cd\u8981\u306a\u306e\u306f\u306a\u305c\u3067\u3059\u304b\uff1f</p> <p>A5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5834\u9762\u3067\u9006\u884c\u5217\u304c\u91cd\u8981\u3067\u3059\u3002 - \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u4fc2\u6570\u63a8\u5b9a\uff08\u6b63\u898f\u65b9\u7a0b\u5f0f\uff09 - \u6b21\u5143\u524a\u6e1b\u624b\u6cd5\uff08PCA\u306a\u3069\uff09\u306b\u304a\u3051\u308b\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97 - \u591a\u5909\u91cf\u89e3\u6790\u306b\u304a\u3051\u308b\u5171\u5206\u6563\u884c\u5217\u306e\u51e6\u7406 - \u753b\u50cf\u51e6\u7406\u3084\u4fe1\u53f7\u51e6\u7406\u306b\u304a\u3051\u308b\u5909\u63db\u884c\u5217\u306e\u64cd\u4f5c</p> <p>Q6: \u9006\u884c\u5217\u306e\u8a08\u7b97\u3067\u6c17\u3092\u3064\u3051\u308b\u3079\u304d\u70b9\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A6: \u4e3b\u306a\u6ce8\u610f\u70b9\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002 - \u884c\u5217\u304c\u7279\u7570\u884c\u5217\u306b\u8fd1\u3044\u5834\u5408\uff08\u6761\u4ef6\u6570\u304c\u5927\u304d\u3044\u5834\u5408\uff09\u3001\u6570\u5024\u7684\u306b\u4e0d\u5b89\u5b9a\u306b\u306a\u308b - \u5927\u304d\u306a\u884c\u5217\u3067\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u304f\u306a\u308b\uff08\\(n \\times n\\) \u884c\u5217\u306e\u5834\u5408\u3001\u8a08\u7b97\u91cf\u306f \\(O(n^3)\\)\uff09 - \u4e38\u3081\u8aa4\u5dee\u304c\u84c4\u7a4d\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b - \u9006\u884c\u5217\u3092\u76f4\u63a5\u8a08\u7b97\u305b\u305a\u306b\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u65b9\u304c\u52b9\u7387\u7684\u306a\u5834\u5408\u304c\u591a\u3044</p>"},{"location":"lectures/LA/15-system-of-linear-equation/#7","title":"7. \u53c2\u8003\u6587\u732e","text":"<ol> <li>Gilbert Strang. \"\u7dda\u5f62\u4ee3\u6570\u3068\u305d\u306e\u5fdc\u7528\"</li> <li>David C. Lay. \"\u7dda\u5f62\u4ee3\u6570\u3068\u305d\u306e\u5fdc\u7528\"</li> <li>\u6751\u4e0a\u6b63\u5eb7. \"\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u305f\u3081\u306e\u7dda\u5f62\u4ee3\u6570\"</li> <li>\u9ad8\u6a4b\u6e09. \"\u5de5\u5b66\u306e\u305f\u3081\u306e\u7dda\u5f62\u4ee3\u6570\u5b66\"</li> <li>Sheldon Axler. \"\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u4e16\u754c -\u62bd\u8c61\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u5165\u9580-\"</li> </ol>"},{"location":"lectures/LA/16-system-of-linear-equation/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c16\u56de \u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/16-system-of-linear-equation/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c16\u56de \u30c6\u30fc\u30de: \u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u3068\u5358\u56de\u5e30\u30e2\u30c7\u30eb \u95a2\u9023\u9805\u76ee: \u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3001\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3001\u6b63\u898f\u65b9\u7a0b\u5f0f \u4e88\u7fd2\u5185\u5bb9:  - \u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe\uff08\u7b2c10\u56de\u301c\u7b2c15\u56de\uff09\u306e\u5fa9\u7fd2 - \u57fa\u672c\u7684\u306a\u7d71\u8a08\u91cf\uff08\u5e73\u5747\u3001\u5206\u6563\uff09\u306e\u6982\u5ff5</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u7d71\u8a08\u30e2\u30c7\u30eb\u306e\u57fa\u672c\u6982\u5ff5\u3068\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u7406\u89e3</li> <li>\u8aac\u660e\u5909\u6570\u3068\u53cd\u5fdc\u5909\u6570\u306e\u95a2\u4fc2\u6027\u306e\u7406\u89e3</li> <li>\u884c\u5217\u30fb\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u8868\u73fe\u65b9\u6cd5\u306e\u7fd2\u5f97</li> <li>\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3068\u6b63\u898f\u65b9\u7a0b\u5f0f\u306b\u3088\u308b\u56de\u5e30\u4fc2\u6570\u306e\u63a8\u5b9a\u65b9\u6cd5\u306e\u7fd2\u5f97</li> <li>\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u9069\u7528\u3068\u89e3\u91c8\u306e\u65b9\u6cd5\u306e\u7406\u89e3</li> </ol>"},{"location":"lectures/LA/16-system-of-linear-equation/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/16-system-of-linear-equation/#31","title":"3.1 \u7d71\u8a08\u30e2\u30c7\u30eb\u3068\u306f","text":"<p>\u5b9a\u7fa9: \u7d71\u8a08\u30e2\u30c7\u30eb\u3068\u306f\u3001\u89b3\u6e2c\u30c7\u30fc\u30bf\u306e\u80cc\u5f8c\u306b\u3042\u308b\u78ba\u7387\u7684\u306a\u69cb\u9020\u3092\u6570\u5b66\u7684\u306b\u8868\u73fe\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002\u30c7\u30fc\u30bf\u306e\u751f\u6210\u904e\u7a0b\u3092\u78ba\u7387\u7684\u306b\u8a18\u8ff0\u3057\u3001\u672a\u77e5\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u542b\u3080\u6570\u5b66\u7684\u306a\u95a2\u6570\u3068\u3057\u3066\u8868\u3055\u308c\u308b\u3002</p> <p>\u7d71\u8a08\u30e2\u30c7\u30eb\u306f\u3001\u30c7\u30fc\u30bf\u306e\u7279\u6027\u3084\u50be\u5411\u3092\u6349\u3048\u3001\u5c06\u6765\u306e\u4e88\u6e2c\u3084\u73fe\u8c61\u306e\u7406\u89e3\u306b\u5f79\u7acb\u3066\u308b\u305f\u3081\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002\u30e2\u30c7\u30eb\u306f\u5358\u7d14\u306a\u3082\u306e\u304b\u3089\u8907\u96d1\u306a\u3082\u306e\u307e\u3067\u69d8\u3005\u3042\u308a\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u6700\u3082\u57fa\u672c\u7684\u306a\u300c\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u300d\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#32","title":"3.2 \u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3068\u306f","text":"<p>\u5b9a\u7fa9: \u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3068\u306f\u30011\u3064\u306e\u8aac\u660e\u5909\u6570 \\(x\\) \u30681\u3064\u306e\u53cd\u5fdc\u5909\u6570 \\(y\\) \u306e\u9593\u306e\u95a2\u4fc2\u3092\u3001\u76f4\u7dda\u7684\u306a\u95a2\u6570 \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\) \u3067\u8868\u73fe\u3059\u308b\u30e2\u30c7\u30eb\u3067\u3042\u308b\u3002\u3053\u3053\u3067\u3001\\(\\beta_0\\) \u306f\u5207\u7247\u3001\\(\\beta_1\\) \u306f\u56de\u5e30\u4fc2\u6570\u3001\\(\\varepsilon\\) \u306f\u8aa4\u5dee\u9805\u3092\u8868\u3059\u3002</p> <p>\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u6700\u3082\u57fa\u672c\u7684\u306a\u7d71\u8a08\u30e2\u30c7\u30eb\u306e\u4e00\u3064\u3067\u30012\u3064\u306e\u5909\u6570\u9593\u306e\u7dda\u5f62\u95a2\u4fc2\u3092\u8868\u73fe\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\uff1a - \u52c9\u5f37\u6642\u9593\uff08\\(x\\)\uff09\u3068\u8a66\u9a13\u306e\u70b9\u6570\uff08\\(y\\)\uff09\u306e\u95a2\u4fc2 - \u904b\u52d5\u91cf\uff08\\(x\\)\uff09\u3068\u4f53\u91cd\u6e1b\u5c11\u91cf\uff08\\(y\\)\uff09\u306e\u95a2\u4fc2 - \u85ac\u306e\u6295\u4e0e\u91cf\uff08\\(x\\)\uff09\u3068\u8840\u5727\u4f4e\u4e0b\uff08\\(y\\)\uff09\u306e\u95a2\u4fc2</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#33","title":"3.3 \u8aac\u660e\u5909\u6570\u3068\u53cd\u5fdc\u5909\u6570","text":"<p>\u5b9a\u7fa9: - \u8aac\u660e\u5909\u6570\uff08\u72ec\u7acb\u5909\u6570\u3001predictor\uff09: \u30e2\u30c7\u30eb\u306b\u304a\u3044\u3066\u3001\u4ed6\u306e\u5909\u6570\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u3068\u8003\u3048\u3089\u308c\u308b\u5909\u6570\u3002\\(x\\) \u3067\u8868\u3055\u308c\u308b\u3053\u3068\u304c\u591a\u3044\u3002 - \u53cd\u5fdc\u5909\u6570\uff08\u5f93\u5c5e\u5909\u6570\u3001response\uff09: \u8aac\u660e\u5909\u6570\u306e\u5f71\u97ff\u3092\u53d7\u3051\u308b\u3068\u8003\u3048\u3089\u308c\u308b\u5909\u6570\u3002\\(y\\) \u3067\u8868\u3055\u308c\u308b\u3053\u3068\u304c\u591a\u3044\u3002</p> <p>\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u3001\u8aac\u660e\u5909\u6570 \\(x\\) \u304c\u53cd\u5fdc\u5909\u6570 \\(y\\) \u306b\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u304b\u3092\u8abf\u67fb\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u904b\u52d5\u6642\u9593\uff08\u8aac\u660e\u5909\u6570\uff09\u304c\u6d88\u8cbb\u30ab\u30ed\u30ea\u30fc\uff08\u53cd\u5fdc\u5909\u6570\uff09\u306b\u3069\u3046\u5f71\u97ff\u3059\u308b\u304b\u3092\u5206\u6790\u3059\u308b\u5834\u5408\u306a\u3069\u3067\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#34","title":"3.4 \u5207\u7247\u3068\u56de\u5e30\u4fc2\u6570","text":"<p>\u5b9a\u7fa9: - \u5207\u7247 \\(\\beta_0\\): \\(x = 0\\) \u306e\u3068\u304d\u306e \\(y\\) \u306e\u5024\uff08y\u5207\u7247\uff09 - \u56de\u5e30\u4fc2\u6570 \\(\\beta_1\\): \\(x\\) \u304c1\u5358\u4f4d\u5897\u52a0\u3057\u305f\u3068\u304d\u306e \\(y\\) \u306e\u5e73\u5747\u7684\u306a\u5909\u5316\u91cf\uff08\u76f4\u7dda\u306e\u50be\u304d\uff09</p> <p>\u3053\u308c\u3089\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u901a\u5e38\u3001\u30c7\u30fc\u30bf\u304b\u3089\u63a8\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u30c7\u30fc\u30bf\u304b\u3089\u6700\u3082\u5f53\u3066\u306f\u307e\u308a\u306e\u826f\u3044\u76f4\u7dda\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u304c\u3001\u56de\u5e30\u5206\u6790\u306e\u76ee\u7684\u3067\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#35","title":"3.5 \u30c7\u30fc\u30bf\u3068\u56de\u5e30\u76f4\u7dda","text":"<p>\\(n\\)\u500b\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8 \\((x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\) \u304c\u3042\u308b\u3068\u304d\u3001\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\quad (i = 1, 2, \\ldots, n)\\] <p>\u3053\u3053\u3067\u3001\\(\\varepsilon_i\\) \u306f\u8aa4\u5dee\u9805\u3067\u3001\u30e2\u30c7\u30eb\u3067\u8aac\u660e\u3067\u304d\u306a\u3044\u5909\u52d5\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/16-system-of-linear-equation/#41","title":"4.1 \u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u3092\u7528\u3044\u305f\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u8868\u73fe","text":"<p>\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u3001\u30d9\u30af\u30c8\u30eb\u3068\u884c\u5217\u3092\u7528\u3044\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u73fe\u3067\u304d\u307e\u3059\uff1a</p> \\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\] <p>\u3053\u3053\u3067\uff1a - \\(\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}\\) \u306f \\(n \\times 1\\) \u306e\u53cd\u5fdc\u5909\u6570\u30d9\u30af\u30c8\u30eb - \\(\\mathbf{X} = \\begin{pmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{pmatrix}\\) \u306f \\(n \\times 2\\) \u306e\u30c7\u30b6\u30a4\u30f3\u884c\u5217 - \\(\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\) \u306f \\(2 \\times 1\\) \u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30d9\u30af\u30c8\u30eb - \\(\\boldsymbol{\\varepsilon} = \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix}\\) \u306f \\(n \\times 1\\) \u306e\u8aa4\u5dee\u30d9\u30af\u30c8\u30eb</p> <p>\u3053\u306e\u884c\u5217\u8868\u8a18\u306f\u3001\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe\u3068\u985e\u4f3c\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u91cd\u8981\u306a\u9055\u3044\u304c\u3042\u308a\u307e\u3059\uff1a\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u5834\u5408\u3001\u89e3\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u304c\u671f\u5f85\u3055\u308c\u307e\u3059\u304c\u3001\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u901a\u5e38\u3001\u5b8c\u5168\u306a\u89e3\u306f\u5b58\u5728\u305b\u305a\u3001\u6700\u826f\u306e\u8fd1\u4f3c\u89e3\u3092\u898b\u3064\u3051\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#42","title":"4.2 \u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u63a8\u5b9a","text":"<p>\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306f\u3001\u30e2\u30c7\u30eb\u304c\u4e88\u6e2c\u3059\u308b\u5024\u3068\u5b9f\u969b\u306e\u89b3\u6e2c\u5024\u3068\u306e\u5dee\uff08\u6b8b\u5dee\uff09\u306e\u4e8c\u4e57\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9: \u6700\u5c0f\u4e8c\u4e57\u6cd5\u3067\u306f\u3001\u6b8b\u5dee\u5e73\u65b9\u548c \\(RSS = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\\) \u3092\u6700\u5c0f\u306b\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\beta_0\\) \u3068 \\(\\beta_1\\) \u3092\u6c42\u3081\u308b\u3002</p> <p>\u6b8b\u5dee\u5e73\u65b9\u548c\u3092\u884c\u5217\u5f62\u5f0f\u3067\u8868\u3059\u3068\uff1a</p> \\[RSS = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\] <p>\u3053\u308c\u3092 \\(\\boldsymbol{\\beta}\\) \u3067\u5fae\u5206\u3057\u30660\u3068\u304a\u304f\u3053\u3068\u3067\u3001\u6700\u9069\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> \\[\\frac{\\partial RSS}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}\\]"},{"location":"lectures/LA/16-system-of-linear-equation/#43","title":"4.3 \u6b63\u898f\u65b9\u7a0b\u5f0f","text":"<p>\u4e0a\u8a18\u306e\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306e\u6761\u4ef6\u304b\u3089\u3001\u4ee5\u4e0b\u306e\u65b9\u7a0b\u5f0f\uff08\u6b63\u898f\u65b9\u7a0b\u5f0f\uff09\u304c\u5c0e\u304b\u308c\u307e\u3059\uff1a</p> <p>\u5b9a\u7406: \u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u63a8\u5b9a\u5024 \\(\\hat{\\boldsymbol{\\beta}}\\) \u306f\u3001\u6b63\u898f\u65b9\u7a0b\u5f0f \\(\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}\\) \u306e\u89e3\u3067\u3042\u308b\u3002</p> <p>\u3053\u306e\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3053\u3068\u3067\u3001\u6700\u9069\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u63a8\u5b9a\u5024 \\(\\hat{\\boldsymbol{\\beta}}\\) \u304c\u5f97\u3089\u308c\u307e\u3059\uff1a</p> \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\] <p>\u305f\u3060\u3057\u3001\\(\\mathbf{X}^T\\mathbf{X}\\) \u304c\u53ef\u9006\u3067\u3042\u308b\u3053\u3068\u3092\u524d\u63d0\u3068\u3057\u3066\u3044\u307e\u3059\uff08\u901a\u5e38\u306e\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u3053\u306e\u6761\u4ef6\u306f\u6e80\u305f\u3055\u308c\u307e\u3059\uff09\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#44","title":"4.4 \u5358\u56de\u5e30\u306e\u5834\u5408\u306e\u5177\u4f53\u7684\u306a\u8a08\u7b97\u5f0f","text":"<p>\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5834\u5408\u3001\u6b63\u898f\u65b9\u7a0b\u5f0f\u304b\u3089\u4ee5\u4e0b\u306e\u5177\u4f53\u7684\u306a\u8a08\u7b97\u5f0f\u304c\u5c0e\u3051\u307e\u3059\uff1a</p> \\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}\\] \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] <p>\u3053\u3053\u3067\uff1a - \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\) \uff08\\(x\\)\u306e\u5e73\u5747\uff09 - \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\) \uff08\\(y\\)\u306e\u5e73\u5747\uff09 - \\(S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\) \uff08\\(x\\)\u3068\\(y\\)\u306e\u5171\u5206\u6563\u306b\u6bd4\u4f8b\uff09 - \\(S_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\) \uff08\\(x\\)\u306e\u5206\u6563\u306b\u6bd4\u4f8b\uff09</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#45-r2","title":"4.5 \u6c7a\u5b9a\u4fc2\u6570 \\(R^2\\)","text":"<p>\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5f53\u3066\u306f\u307e\u308a\u306e\u826f\u3055\u3092\u6e2c\u308b\u6307\u6a19\u3068\u3057\u3066\u3001\u6c7a\u5b9a\u4fc2\u6570 \\(R^2\\) \u304c\u7528\u3044\u3089\u308c\u307e\u3059\uff1a</p> <p>\u5b9a\u7fa9: \u6c7a\u5b9a\u4fc2\u6570 \\(R^2\\) \u306f\u3001\u53cd\u5fdc\u5909\u6570\u306e\u5168\u5909\u52d5\u306e\u3046\u3061\u3001\u56de\u5e30\u30e2\u30c7\u30eb\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u5272\u5408\u3092\u8868\u3059\u3002 \\(\\(R^2 = \\frac{\u56de\u5e30\u306b\u3088\u308b\u5909\u52d5}{\u5168\u5909\u52d5} = 1 - \\frac{\u6b8b\u5dee\u5909\u52d5}{\u5168\u5909\u52d5} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\\)\\)</p> <p>\\(R^2\\) \u306f0\u304b\u30891\u306e\u9593\u306e\u5024\u3092\u3068\u308a\u30011\u306b\u8fd1\u3044\u307b\u3069\u30e2\u30c7\u30eb\u306e\u5f53\u3066\u306f\u307e\u308a\u304c\u826f\u3044\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002</p> <p>\u306f\u3044\u3001\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5177\u4f53\u7684\u306a\u8a08\u7b97\u4f8b\u3092\u8ffd\u52a0\u3059\u308b\u306e\u306f\u975e\u5e38\u306b\u6709\u76ca\u3067\u3059\u3002\u7279\u306b\u5b66\u751f\u304c\u7406\u89e3\u3057\u3084\u3059\u3044\u3088\u3046\u306b\u3001\u6570\u5024\u3092\u7528\u3044\u305f\u8a08\u7b97\u306e\u30b9\u30c6\u30c3\u30d7\u30d0\u30a4\u30b9\u30c6\u30c3\u30d7\u306e\u89e3\u8aac\u3092\u542b\u3081\u307e\u3057\u3087\u3046\u3002\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5b9f\u4f8b\u3092\u8b1b\u7fa9\u30ce\u30fc\u30c8\u306b\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\uff1a</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#46","title":"4.6 \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5177\u4f53\u7684\u306a\u8a08\u7b97\u4f8b","text":"<p>\u4ee5\u4e0b\u306b\u3001\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u8a08\u7b97\u30d7\u30ed\u30bb\u30b9\u3092\u5b9f\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u8a73\u7d30\u306b\u89e3\u8aac\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#_1","title":"\u4f8b\u984c\uff1a\u904b\u52d5\u6642\u9593\u3068\u6d88\u8cbb\u30ab\u30ed\u30ea\u30fc\u306e\u95a2\u4fc2","text":"<p>5\u4eba\u306e\u88ab\u9a13\u8005\u304b\u3089\u5f97\u3089\u308c\u305f\u904b\u52d5\u6642\u9593\uff08\u5206\uff09\u3068\u6d88\u8cbb\u30ab\u30ed\u30ea\u30fc\uff08kcal\uff09\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \u88ab\u9a13\u8005 \u904b\u52d5\u6642\u9593 \\(x\\) (\u5206) \u6d88\u8cbb\u30ab\u30ed\u30ea\u30fc \\(y\\) (kcal) 1 20 100 2 30 150 3 40 190 4 50 230 5 60 280"},{"location":"lectures/LA/16-system-of-linear-equation/#1_1","title":"\u30b9\u30c6\u30c3\u30d71\uff1a\u30c7\u30fc\u30bf\u306e\u8981\u7d04\u7d71\u8a08\u91cf\u3092\u8a08\u7b97\u3059\u308b","text":"<p>\u307e\u305a\u3001\\(x\\)\u3068\\(y\\)\u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a - \\(\\bar{x} = \\frac{20 + 30 + 40 + 50 + 60}{5} = \\frac{200}{5} = 40\\) - \\(\\bar{y} = \\frac{100 + 150 + 190 + 230 + 280}{5} = \\frac{950}{5} = 190\\)</p> <p>\u6b21\u306b\u3001\u8a08\u7b97\u306b\u5fc5\u8981\u306a\u5404\u7a2e\u306e\u548c\u3092\u6c42\u3081\u307e\u3059\uff1a - \\(\\sum x_i = 200\\) - \\(\\sum y_i = 950\\) - \\(\\sum x_i^2 = 20^2 + 30^2 + 40^2 + 50^2 + 60^2 = 8,500\\) - \\(\\sum x_i y_i = 20 \\times 100 + 30 \\times 150 + 40 \\times 190 + 50 \\times 230 + 60 \\times 280 = 41,000\\)</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#2-beta_1","title":"\u30b9\u30c6\u30c3\u30d72\uff1a\u56de\u5e30\u4fc2\u6570 \\(\\beta_1\\) \u3092\u8a08\u7b97\u3059\u308b","text":"<p>\u56de\u5e30\u4fc2\u6570 \\(\\beta_1\\) \u306e\u516c\u5f0f\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> \\[\\hat{\\beta}_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}\\] <p>\u3053\u306e\u5f0f\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u66f8\u304d\u76f4\u3059\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff1a</p> \\[\\hat{\\beta}_1 = \\frac{n\\sum x_i y_i - \\sum x_i \\sum y_i}{n\\sum x_i^2 - (\\sum x_i)^2}\\] <p>\u3053\u308c\u306b\u5024\u3092\u4ee3\u5165\u3057\u307e\u3059\uff1a</p> \\[\\hat{\\beta}_1 = \\frac{5 \\times 41,000 - 200 \\times 950}{5 \\times 8,500 - 200^2} = \\frac{205,000 - 190,000}{42,500 - 40,000} = \\frac{15,000}{2,500} = 6\\]"},{"location":"lectures/LA/16-system-of-linear-equation/#3-beta_0","title":"\u30b9\u30c6\u30c3\u30d73\uff1a\u5207\u7247 \\(\\beta_0\\) \u3092\u8a08\u7b97\u3059\u308b","text":"<p>\u5207\u7247 \\(\\beta_0\\) \u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u6c42\u3081\u3089\u308c\u307e\u3059\uff1a</p> \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] <p>\u5024\u3092\u4ee3\u5165\u3057\u307e\u3059\uff1a</p> \\[\\hat{\\beta}_0 = 190 - 6 \\times 40 = 190 - 240 = -50\\]"},{"location":"lectures/LA/16-system-of-linear-equation/#4_1","title":"\u30b9\u30c6\u30c3\u30d74\uff1a\u56de\u5e30\u5f0f\u3092\u5f97\u308b","text":"<p>\u4ee5\u4e0a\u306e\u8a08\u7b97\u304b\u3089\u3001\u56de\u5e30\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[\\hat{y} = -50 + 6x\\] <p>\u3053\u306e\u5f0f\u306f\u3001\u300c\u904b\u52d5\u6642\u9593\u304c1\u5206\u5897\u3048\u308b\u3054\u3068\u306b\u3001\u5e73\u5747\u3057\u30666\u30ab\u30ed\u30ea\u30fc\u4f59\u5206\u306b\u6d88\u8cbb\u3055\u308c\u308b\u300d\u3068\u3044\u3046\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u5207\u7247\u304c-50\u3067\u3042\u308b\u3053\u3068\u306f\u3001\u3053\u306e\u56de\u5e30\u30e2\u30c7\u30eb\u304c\u975e\u5e38\u306b\u77ed\u3044\u904b\u52d5\u6642\u9593\uff08x &lt; 8.33\u5206\uff09\u306e\u5834\u5408\u306b\u73fe\u5b9f\u7684\u306a\u4e88\u6e2c\u3092\u4e0e\u3048\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\u3053\u3068\u3092\u793a\u5506\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#5","title":"\u30b9\u30c6\u30c3\u30d75\uff1a\u4e88\u6e2c\u5024\u3068\u6b8b\u5dee\u3092\u8a08\u7b97\u3059\u308b","text":"<p>\u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306b\u3064\u3044\u3066\u3001\u30e2\u30c7\u30eb\u304b\u3089\u306e\u4e88\u6e2c\u5024\u3068\u5b9f\u969b\u306e\u5024\u3068\u306e\u5dee\uff08\u6b8b\u5dee\uff09\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a</p> \u88ab\u9a13\u8005 \\(x\\) \\(y\\) \\(\\hat{y} = -50 + 6x\\) \u6b8b\u5dee \\(y - \\hat{y}\\) 1 20 100 70 30 2 30 150 130 20 3 40 190 190 0 4 50 230 250 -20 5 60 280 310 -30"},{"location":"lectures/LA/16-system-of-linear-equation/#6-r2","title":"\u30b9\u30c6\u30c3\u30d76\uff1a\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\uff08\u6c7a\u5b9a\u4fc2\u6570 \\(R^2\\) \u306e\u8a08\u7b97\uff09","text":"<p>\u6c7a\u5b9a\u4fc2\u6570 \\(R^2\\) \u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306b\u3001\u4ee5\u4e0b\u306e\u91cf\u3092\u6c42\u3081\u307e\u3059\uff1a</p> <ul> <li>\u5168\u5909\u52d5\uff08TSS\uff09\uff1a\\(\\sum (y_i - \\bar{y})^2 = (100-190)^2 + (150-190)^2 + (190-190)^2 + (230-190)^2 + (280-190)^2 = 17,000\\)</li> <li>\u6b8b\u5dee\u5909\u52d5\uff08RSS\uff09\uff1a\\(\\sum (y_i - \\hat{y}_i)^2 = 30^2 + 20^2 + 0^2 + (-20)^2 + (-30)^2 = 2,600\\)</li> <li>\u56de\u5e30\u306b\u3088\u308b\u5909\u52d5\uff08ESS\uff09\uff1a\\(\\sum (\\hat{y}_i - \\bar{y})^2 = TSS - RSS = 17,000 - 2,600 = 14,400\\)</li> </ul> <p>\u6c7a\u5b9a\u4fc2\u6570 \\(R^2\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3055\u308c\u307e\u3059\uff1a</p> \\[R^2 = \\frac{ESS}{TSS} = \\frac{14,400}{17,000} = 0.847 \\approx 0.85\\] <p>\u3053\u306e\u7d50\u679c\u306f\u3001\u56de\u5e30\u30e2\u30c7\u30eb\u304c\u6d88\u8cbb\u30ab\u30ed\u30ea\u30fc\u306e\u5909\u52d5\u306e\u7d0485%\u3092\u8aac\u660e\u3067\u304d\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#7","title":"\u30b9\u30c6\u30c3\u30d77\uff1a\u6b63\u898f\u65b9\u7a0b\u5f0f\u306b\u3088\u308b\u89e3\u6cd5\u306e\u78ba\u8a8d","text":"<p>\u6b63\u898f\u65b9\u7a0b\u5f0f\u306f\u4ee5\u4e0b\u306e\u5f62\u5f0f\u3067\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[\\begin{pmatrix} n &amp; \\sum x_i \\\\ \\sum x_i &amp; \\sum x_i^2 \\end{pmatrix} \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{pmatrix} = \\begin{pmatrix} \\sum y_i \\\\ \\sum x_i y_i \\end{pmatrix}\\] <p>\u6570\u5024\u3092\u4ee3\u5165\u3059\u308b\u3068\uff1a</p> \\[\\begin{pmatrix} 5 &amp; 200 \\\\ 200 &amp; 8,500 \\end{pmatrix} \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{pmatrix} = \\begin{pmatrix} 950 \\\\ 41,000 \\end{pmatrix}\\] <p>\u3053\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3068\uff1a</p> \\[5\\hat{\\beta}_0 + 200\\hat{\\beta}_1 = 950 \\tag{1}$$ $$200\\hat{\\beta}_0 + 8,500\\hat{\\beta}_1 = 41,000 \\tag{2}\\] <p>\u65b9\u7a0b\u5f0f(1)\u304b\u3089 \\(\\hat{\\beta}_0\\) \u306b\u3064\u3044\u3066\u89e3\u304f\u3068\uff1a</p> \\[\\hat{\\beta}_0 = \\frac{950 - 200\\hat{\\beta}_1}{5} = 190 - 40\\hat{\\beta}_1 \\tag{3}\\] <p>\u65b9\u7a0b\u5f0f(3)\u3092\u65b9\u7a0b\u5f0f(2)\u306b\u4ee3\u5165\uff1a</p> \\[200(190 - 40\\hat{\\beta}_1) + 8,500\\hat{\\beta}_1 = 41,000$$ $$38,000 - 8,000\\hat{\\beta}_1 + 8,500\\hat{\\beta}_1 = 41,000$$ $$38,000 + 500\\hat{\\beta}_1 = 41,000$$ $$500\\hat{\\beta}_1 = 3,000$$ $$\\hat{\\beta}_1 = 6\\] <p>\u305d\u3057\u3066\u3001\u3053\u306e\u5024\u3092\u65b9\u7a0b\u5f0f(3)\u306b\u4ee3\u5165\u3059\u308b\u3053\u3068\u3067 \\(\\hat{\\beta}_0\\) \u3092\u6c42\u3081\u307e\u3059\uff1a</p> \\[\\hat{\\beta}_0 = 190 - 40 \\times 6 = 190 - 240 = -50\\] <p>\u3053\u308c\u306f\u5148\u307b\u3069\u306e\u8a08\u7b97\u3068\u540c\u3058\u7d50\u679c\u306b\u306a\u308a\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#8","title":"\u30b9\u30c6\u30c3\u30d78\uff1a\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u4e88\u6e2c","text":"<p>\u904b\u52d5\u6642\u9593\u304c45\u5206\u306e\u5834\u5408\u306e\u6d88\u8cbb\u30ab\u30ed\u30ea\u30fc\u3092\u4e88\u6e2c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> \\[\\hat{y} = -50 + 6 \\times 45 = -50 + 270 = 220 \\text{ kcal}\\] <p>\u3053\u306e\u3088\u3046\u306b\u3001\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3059\u308c\u3070\u3001\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u3053\u306e\u8a73\u7d30\u306a\u8a08\u7b97\u4f8b\u306f\u3001\u5b66\u751f\u304c\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5404\u30b9\u30c6\u30c3\u30d7\u3092\u5177\u4f53\u7684\u306b\u7406\u89e3\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\u7279\u306b\u3001\u7d71\u8a08\u5b66\u3084\u7dda\u5f62\u4ee3\u6570\u306e\u6982\u5ff5\u304c\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u3069\u306e\u3088\u3046\u306b\u9069\u7528\u3055\u308c\u308b\u304b\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/16-system-of-linear-equation/#51","title":"5.1 \u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u751f\u6210\uff08\u52c9\u5f37\u6642\u9593\u3068\u8a66\u9a13\u6210\u7e3e\u306e\u4f8b\uff09\nnp.random.seed(42)\nhours_studied = np.random.uniform(1, 10, 30)  # 1\u301c10\u6642\u9593\u306e\u52c9\u5f37\u6642\u9593\uff0830\u4eba\u5206\uff09\nnoise = np.random.normal(0, 5, 30)  # \u30e9\u30f3\u30c0\u30e0\u306a\u30ce\u30a4\u30ba\ntest_score = 50 + 5 * hours_studied + noise  # \u52c9\u5f37\u6642\u9593\u3068\u5f97\u70b9\u306e\u95a2\u4fc2 (y = 50 + 5x + \u03b5)\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210\ndata = pd.DataFrame({\n    'hours': hours_studied,\n    'score': test_score\n})\n\n# \u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u624b\u52d5\u5b9f\u88c5\ndef manual_simple_regression(x, y):\n    n = len(x)\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n\n    # \u5206\u6bcd: x \u306e\u504f\u5dee\u5e73\u65b9\u548c\n    ss_xx = np.sum((x - mean_x) ** 2)\n    # \u5206\u5b50: x \u3068 y \u306e\u504f\u5dee\u7a4d\u548c\n    ss_xy = np.sum((x - mean_x) * (y - mean_y))\n\n    # \u56de\u5e30\u4fc2\u6570\u3068\u5207\u7247\u306e\u8a08\u7b97\n    beta_1 = ss_xy / ss_xx\n    beta_0 = mean_y - beta_1 * mean_x\n\n    # \u4e88\u6e2c\u5024\u306e\u8a08\u7b97\n    y_pred = beta_0 + beta_1 * x\n\n    # \u6c7a\u5b9a\u4fc2\u6570 R^2 \u306e\u8a08\u7b97\n    ss_total = np.sum((y - mean_y) ** 2)\n    ss_residual = np.sum((y - y_pred) ** 2)\n    r_squared = 1 - (ss_residual / ss_total)\n\n    return beta_0, beta_1, y_pred, r_squared\n\n# \u624b\u52d5\u3067\u5358\u56de\u5e30\u5206\u6790\u3092\u5b9f\u884c\nbeta_0, beta_1, y_pred, r_squared = manual_simple_regression(data['hours'], data['score'])\n\nprint(f\"\u624b\u52d5\u8a08\u7b97\u306b\u3088\u308b\u7d50\u679c:\")\nprint(f\"\u5207\u7247 (\u03b2\u2080): {beta_0:.4f}\")\nprint(f\"\u56de\u5e30\u4fc2\u6570 (\u03b2\u2081): {beta_1:.4f}\")\nprint(f\"\u6c7a\u5b9a\u4fc2\u6570 (R\u00b2): {r_squared:.4f}\")\n\n# scikit-learn\u3092\u7528\u3044\u305f\u5358\u56de\u5e30\u5206\u6790\nmodel = LinearRegression()\nX = data['hours'].values.reshape(-1, 1)  # \u8aac\u660e\u5909\u6570\u3092numpy\u914d\u5217\u306b\u5909\u63db\ny = data['score'].values  # \u53cd\u5fdc\u5909\u6570\u3092numpy\u914d\u5217\u306b\u5909\u63db\n\nmodel.fit(X, y)\ny_pred_sklearn = model.predict(X)\n\nprint(\"\\nscikit-learn\u306b\u3088\u308b\u7d50\u679c:\")\nprint(f\"\u5207\u7247 (\u03b2\u2080): {model.intercept_:.4f}\")\nprint(f\"\u56de\u5e30\u4fc2\u6570 (\u03b2\u2081): {model.coef_[0]:.4f}\")\nprint(f\"\u6c7a\u5b9a\u4fc2\u6570 (R\u00b2): {r2_score(y, y_pred_sklearn):.4f}\")\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.scatter(data['hours'], data['score'], color='blue', alpha=0.7, label='\u30c7\u30fc\u30bf\u70b9')\nplt.plot(data['hours'], y_pred, color='red', linewidth=2, label='\u56de\u5e30\u76f4\u7dda')\nplt.xlabel('\u52c9\u5f37\u6642\u9593 (\u6642\u9593)')\nplt.ylabel('\u30c6\u30b9\u30c8\u5f97\u70b9')\nplt.title('\u52c9\u5f37\u6642\u9593\u3068\u30c6\u30b9\u30c8\u5f97\u70b9\u306e\u5358\u56de\u5e30\u5206\u6790')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/16-system-of-linear-equation/#52","title":"5.2 \u6b63\u898f\u65b9\u7a0b\u5f0f\u306b\u3088\u308b\u89e3\u6cd5","text":"<pre><code># \u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u4f7f\u7528\u3057\u305f\u89e3\u6cd5\ndef normal_equation(X, y):\n    # \u30c7\u30b6\u30a4\u30f3\u884c\u5217\u306b\u5b9a\u6570\u9805\uff081\uff09\u306e\u5217\u3092\u8ffd\u52a0\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n\n    # \u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f: \u03b2 = (X^T X)^(-1) X^T y\n    beta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n\n    return beta\n\n# \u6b63\u898f\u65b9\u7a0b\u5f0f\u3067\u89e3\u304f\nX_array = data['hours'].values.reshape(-1, 1)\ny_array = data['score'].values\nbeta_norm_eq = normal_equation(X_array, y_array)\n\nprint(\"\\n\u6b63\u898f\u65b9\u7a0b\u5f0f\u306b\u3088\u308b\u7d50\u679c:\")\nprint(f\"\u5207\u7247 (\u03b2\u2080): {beta_norm_eq[0]:.4f}\")\nprint(f\"\u56de\u5e30\u4fc2\u6570 (\u03b2\u2081): {beta_norm_eq[1]:.4f}\")\n</code></pre>"},{"location":"lectures/LA/16-system-of-linear-equation/#53","title":"5.3 \u5065\u5eb7\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u5358\u56de\u5e30\u306e\u4f8b","text":"<pre><code># \u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\uff08\u65e5\u3005\u306e\u904b\u52d5\u6642\u9593\u3068\u5fc3\u62cd\u6570\u306e\u5909\u5316\uff09\nnp.random.seed(123)\nexercise_minutes = np.random.uniform(10, 60, 40)  # 10\u301c60\u5206\u306e\u904b\u52d5\u6642\u9593\nnoise = np.random.normal(0, 3, 40)  # \u30e9\u30f3\u30c0\u30e0\u306a\u30ce\u30a4\u30ba\nheart_rate_decrease = 0.3 * exercise_minutes + noise  # \u904b\u52d5\u6642\u9593\u3068\u5fc3\u62cd\u6570\u4f4e\u4e0b\u306e\u95a2\u4fc2\n\nhealth_data = pd.DataFrame({\n    'exercise_min': exercise_minutes,\n    'heart_rate_decrease': heart_rate_decrease\n})\n\n# \u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u9069\u7528\nbeta_0_health, beta_1_health, y_pred_health, r_squared_health = manual_simple_regression(\n    health_data['exercise_min'], health_data['heart_rate_decrease']\n)\n\nprint(\"\\n\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u5358\u56de\u5e30\u5206\u6790\u7d50\u679c:\")\nprint(f\"\u5207\u7247 (\u03b2\u2080): {beta_0_health:.4f}\")\nprint(f\"\u56de\u5e30\u4fc2\u6570 (\u03b2\u2081): {beta_1_health:.4f}\")\nprint(f\"\u6c7a\u5b9a\u4fc2\u6570 (R\u00b2): {r_squared_health:.4f}\")\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.scatter(health_data['exercise_min'], health_data['heart_rate_decrease'], \n            color='green', alpha=0.7, label='\u30c7\u30fc\u30bf\u70b9')\nplt.plot(health_data['exercise_min'], y_pred_health, \n         color='purple', linewidth=2, label='\u56de\u5e30\u76f4\u7dda')\nplt.xlabel('\u904b\u52d5\u6642\u9593 (\u5206)')\nplt.ylabel('\u5b89\u9759\u6642\u5fc3\u62cd\u6570\u306e\u4f4e\u4e0b (bpm)')\nplt.title('\u904b\u52d5\u6642\u9593\u3068\u5fc3\u62cd\u6570\u4f4e\u4e0b\u306e\u5358\u56de\u5e30\u5206\u6790')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# \u63a8\u5b9a\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u4e88\u6e2c\nnew_exercise_time = 45  # \u65b0\u3057\u3044\u904b\u52d5\u6642\u9593: 45\u5206\npredicted_decrease = beta_0_health + beta_1_health * new_exercise_time\nprint(f\"\\n\u904b\u52d5\u6642\u9593\u304c{new_exercise_time}\u5206\u306e\u5834\u5408\u3001\u4e88\u6e2c\u3055\u308c\u308b\u5fc3\u62cd\u6570\u4f4e\u4e0b: {predicted_decrease:.2f} bpm\")\n</code></pre>"},{"location":"lectures/LA/16-system-of-linear-equation/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/16-system-of-linear-equation/#_2","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li>\u6b21\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u624b\u8a08\u7b97\u3067\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\hat{\\beta}_0\\) \u3068 \\(\\hat{\\beta}_1\\) \u3092\u6c42\u3081\u306a\u3055\u3044\u3002</li> </ol> \\(x\\) 1 2 3 4 5 \\(y\\) 3 5 7 8 11 <ol> <li>\u6b21\u306e\u5f0f\u306f\u3001\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u6b63\u898f\u65b9\u7a0b\u5f0f\u3067\u3059\u3002\u3053\u306e\u5f0f\u3092\u5c55\u958b\u3057\u3066\u3001\\(\\hat{\\beta}_0\\) \u3068 \\(\\hat{\\beta}_1\\) \u3092\u6c42\u3081\u308b\u4e00\u822c\u5f0f\u3092\u5c0e\u51fa\u305b\u3088\u3002</li> </ol> <p>\\(\\(\\begin{pmatrix} n &amp; \\sum x_i \\\\ \\sum x_i &amp; \\sum x_i^2 \\end{pmatrix} \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{pmatrix} = \\begin{pmatrix} \\sum y_i \\\\ \\sum x_i y_i \\end{pmatrix}\\)\\)</p> <ol> <li> <p>\u5358\u56de\u5e30\u30e2\u30c7\u30eb \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\) \u306b\u304a\u3044\u3066\u3001\\(\\hat{\\beta}_1 = 2.5\\) \u3068\u63a8\u5b9a\u3055\u308c\u305f\u3002\u3053\u306e\u6642\u3001\u8aac\u660e\u5909\u6570 \\(x\\) \u304c1\u5358\u4f4d\u5897\u52a0\u3059\u308b\u3068\u3001\u53cd\u5fdc\u5909\u6570 \\(y\\) \u306f\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u3068\u4e88\u6e2c\u3055\u308c\u308b\u304b\u8aac\u660e\u305b\u3088\u3002</p> </li> <li> <p>\u6c7a\u5b9a\u4fc2\u6570 \\(R^2 = 0.85\\) \u3092\u6301\u3064\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u304c\u3042\u308b\u3002\u3053\u306e\u5024\u306f\u3001\u30e2\u30c7\u30eb\u306e\u5f53\u3066\u306f\u307e\u308a\u306e\u826f\u3055\u306b\u3064\u3044\u3066\u4f55\u3092\u610f\u5473\u3059\u308b\u304b\u8aac\u660e\u305b\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/16-system-of-linear-equation/#_3","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li>\u3042\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u3064\u3044\u3066\u3001\u4ee5\u4e0b\u306e\u60c5\u5831\u304c\u5f97\u3089\u308c\u3066\u3044\u308b\uff1a</li> <li>\\(\\sum_{i=1}^{10} x_i = 50\\)</li> <li>\\(\\sum_{i=1}^{10} y_i = 150\\)</li> <li>\\(\\sum_{i=1}^{10} x_i^2 = 300\\)</li> <li>\\(\\sum_{i=1}^{10} x_i y_i = 800\\)</li> </ol> <p>\u3053\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\hat{\\beta}_0\\) \u3068 \\(\\hat{\\beta}_1\\) \u3092\u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u7528\u3044\u3066\u6c42\u3081\u3088\u3002</p> <ol> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u95a2\u9023\u3059\u308b\u5fdc\u7528\u554f\u984c\uff1a</li> </ol> <p>\u3042\u308b\u533b\u5b66\u7814\u7a76\u3067\u306f\u3001\u60a3\u8005\u306e\u8840\u4e2d\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\uff08mg/dL\uff09\u304c\u30011\u65e5\u306e\u904b\u52d5\u6642\u9593\uff08\u5206\uff09\u3068\u3069\u306e\u3088\u3046\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u304b\u3092\u8abf\u67fb\u3057\u3066\u3044\u308b\u300220\u4eba\u306e\u60a3\u8005\u304b\u3089\u5f97\u3089\u308c\u305f\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306b\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u9069\u7528\u3057\u305f\u3068\u3053\u308d\u3001\u4ee5\u4e0b\u306e\u7d50\u679c\u304c\u5f97\u3089\u308c\u305f\uff1a</p> <p>\\(\\hat{\\beta}_0 = 240\\)\u3001\\(\\hat{\\beta}_1 = -0.8\\)\u3001\\(R^2 = 0.65\\)</p> <p>(a) \u3053\u306e\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5f0f\u3092\u66f8\u304d\u3001\u904b\u52d5\u6642\u9593\u3068\u8840\u4e2d\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u89e3\u91c8\u305b\u3088\u3002</p> <p>(b) 1\u65e530\u5206\u306e\u904b\u52d5\u3092\u3057\u3066\u3044\u308b\u60a3\u8005\u306e\u4e88\u6e2c\u8840\u4e2d\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u3092\u6c42\u3081\u3088\u3002</p> <p>(c) \u3053\u306e\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u9650\u754c\u70b9\u30922\u3064\u6319\u3052\u3088\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#7_1","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/16-system-of-linear-equation/#q1-1","title":"Q1: \u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3068\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A1: \u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306f\u3001\u65b9\u7a0b\u5f0f\u306e\u6570\u3068\u672a\u77e5\u6570\u306e\u6570\u304c\u540c\u3058\u5834\u5408\u306b\u53b3\u5bc6\u306a\u89e3\u3092\u6301\u3061\u307e\u3059\u304c\u3001\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u901a\u5e38\u3001\u30c7\u30fc\u30bf\u70b9\u306e\u6570\uff08\u89b3\u6e2c\u6570 \\(n\\)\uff09\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6570\uff082\u3064\u3001\\(\\beta_0\\) \u3068 \\(\\beta_1\\)\uff09\u3088\u308a\u3082\u591a\u3044\u305f\u3081\u3001\u4e00\u822c\u7684\u306b\u53b3\u5bc6\u306a\u89e3\u3092\u6301\u3061\u307e\u305b\u3093\u3002\u305d\u306e\u305f\u3081\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306a\u3069\u3092\u7528\u3044\u3066\u300c\u6700\u3082\u826f\u3044\u8fd1\u4f3c\u89e3\u300d\u3092\u6c42\u3081\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#q2","title":"Q2: \u6700\u5c0f\u4e8c\u4e57\u6cd5\u306f\u306a\u305c\u4e8c\u4e57\u3092\u4f7f\u3046\u306e\u3067\u3059\u304b\uff1f\u7d76\u5bfe\u5024\u3067\u306f\u3060\u3081\u3067\u3059\u304b\uff1f","text":"<p>A2: \u4e8c\u4e57\u3092\u4f7f\u3046\u7406\u7531\u306f\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3059\uff1a 1. \u5fae\u5206\u53ef\u80fd\u3067\u3042\u308b\u305f\u3081\u3001\u89e3\u6790\u7684\u306b\u6700\u9069\u89e3\u3092\u6c42\u3081\u3084\u3059\u3044 2. \u5927\u304d\u306a\u8aa4\u5dee\u3092\u91cd\u304f\u7f70\u5247\u3059\u308b\u305f\u3081\u3001\u5916\u308c\u5024\u306e\u5f71\u97ff\u3092\u5f37\u304f\u53d7\u3051\u308b\uff08\u3053\u308c\u306f\u6642\u306b\u6b20\u70b9\u306b\u3082\u306a\u308a\u307e\u3059\u304c\uff09 3. \u6b63\u3068\u8ca0\u306e\u8aa4\u5dee\u3092\u5bfe\u79f0\u306b\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u308b \u7d76\u5bfe\u5024\u3092\u4f7f\u3046\u624b\u6cd5\uff08\u6700\u5c0f\u7d76\u5bfe\u504f\u5dee\u6cd5\u306a\u3069\uff09\u3082\u3042\u308a\u307e\u3059\u304c\u3001\u5fae\u5206\u53ef\u80fd\u3067\u306a\u3044\u70b9\u304c\u3042\u308b\u305f\u3081\u6700\u9069\u5316\u304c\u6280\u8853\u7684\u306b\u96e3\u3057\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#q3","title":"Q3: \u8aac\u660e\u5909\u6570\u304c\u8907\u6570\u3042\u308b\u5834\u5408\u306f\u3069\u3046\u306a\u308a\u307e\u3059\u304b\uff1f","text":"<p>A3: \u8907\u6570\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u3001\u305d\u308c\u3092\u300c\u91cd\u56de\u5e30\u30e2\u30c7\u30eb\uff08multiple regression model\uff09\u300d\u3068\u547c\u3073\u307e\u3059\u3002\u6570\u5f0f\u306f \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\varepsilon\\) \u3068\u306a\u308a\u3001\u884c\u5217\u5f62\u5f0f\u3067\u306e\u8868\u73fe\u3084\u6b63\u898f\u65b9\u7a0b\u5f0f\u306b\u3088\u308b\u89e3\u6cd5\u306f\u672c\u8cea\u7684\u306b\u540c\u3058\u3067\u3059\u304c\u3001\u8a08\u7b97\u304c\u3088\u308a\u8907\u96d1\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306b\u3064\u3044\u3066\u306f\u7b2c18\u56de\u306e\u8b1b\u7fa9\u3067\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#q4","title":"Q4: \u6b63\u898f\u65b9\u7a0b\u5f0f\u306b\u3088\u308b\u89e3\u6cd5\u3068\u52fe\u914d\u964d\u4e0b\u6cd5\u306b\u3088\u308b\u89e3\u6cd5\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A4: \u6b63\u898f\u65b9\u7a0b\u5f0f\u306f\u89e3\u6790\u7684\u306b\u4e00\u5ea6\u3067\u6700\u9069\u89e3\u3092\u6c42\u3081\u308b\u65b9\u6cd5\u3067\u3042\u308b\u4e00\u65b9\u3001\u52fe\u914d\u964d\u4e0b\u6cd5\u306f\u7e70\u308a\u8fd4\u3057\u8a08\u7b97\u306b\u3088\u3063\u3066\u5f90\u3005\u306b\u89e3\u306b\u8fd1\u3065\u3051\u308b\u6570\u5024\u7684\u306a\u65b9\u6cd5\u3067\u3059\u3002\u6b63\u898f\u65b9\u7a0b\u5f0f\u306f\u5c0f\u301c\u4e2d\u898f\u6a21\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u306f\u52b9\u7387\u7684\u3067\u3059\u304c\u3001\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u3067\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u304f\u306a\u308a\u307e\u3059\u3002\u52fe\u914d\u964d\u4e0b\u6cd5\u306f\u53cd\u5fa9\u8a08\u7b97\u304c\u5fc5\u8981\u3067\u3059\u304c\u3001\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u3067\u3082\u30b9\u30b1\u30fc\u30eb\u3057\u3084\u3059\u3044\u3068\u3044\u3046\u5229\u70b9\u304c\u3042\u308a\u307e\u3059\u3002\u73fe\u4ee3\u306e\u6a5f\u68b0\u5b66\u7fd2\u3067\u306f\u4e21\u65b9\u306e\u624b\u6cd5\u304c\u72b6\u6cc1\u306b\u5fdc\u3058\u3066\u4f7f\u3044\u5206\u3051\u3089\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#q5","title":"Q5: \u56de\u5e30\u30e2\u30c7\u30eb\u306e\u4eee\u5b9a\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A5: \u56de\u5e30\u30e2\u30c7\u30eb\u306e\u4e3b\u306a\u4eee\u5b9a\u306b\u306f\u4ee5\u4e0b\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u7dda\u5f62\u6027: \u8aac\u660e\u5909\u6570\u3068\u53cd\u5fdc\u5909\u6570\u306e\u9593\u306b\u306f\u7dda\u5f62\u306e\u95a2\u4fc2\u304c\u3042\u308b 2. \u72ec\u7acb\u6027: \u8aa4\u5dee\u9805\u306f\u4e92\u3044\u306b\u72ec\u7acb\u3057\u3066\u3044\u308b 3. \u7b49\u5206\u6563\u6027: \u8aa4\u5dee\u306e\u5206\u6563\u306f\u3059\u3079\u3066\u306e\u89b3\u6e2c\u5024\u3067\u4e00\u5b9a\u3067\u3042\u308b 4. \u6b63\u898f\u6027: \u8aa4\u5dee\u9805\u306f\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046 \u3053\u308c\u3089\u306e\u4eee\u5b9a\u304c\u6e80\u305f\u3055\u308c\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306b\u3001\u6b8b\u5dee\u5206\u6790\u306a\u3069\u306e\u8a3a\u65ad\u624b\u6cd5\u304c\u7528\u3044\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/16-system-of-linear-equation/#8_1","title":"8. \u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u77e5\u8b58\u3092\u57fa\u306b\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u672c\u7684\u624b\u6cd5\u3067\u3042\u308b\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\u3002\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u30011\u3064\u306e\u8aac\u660e\u5909\u6570\u304b\u30891\u3064\u306e\u53cd\u5fdc\u5909\u6570\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306e\u7d71\u8a08\u30e2\u30c7\u30eb\u3067\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3092\u7528\u3044\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> <p>\u4e3b\u306a\u5b66\u7fd2\u5185\u5bb9\uff1a - \u7d71\u8a08\u30e2\u30c7\u30eb\u3068\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u6982\u5ff5 - \u8aac\u660e\u5909\u6570\u3068\u53cd\u5fdc\u5909\u6570\u306e\u95a2\u4fc2 - \u884c\u5217\u30fb\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u8868\u73fe - \u6700\u5c0f\u4e8c\u4e57\u6cd5\u3068\u6b63\u898f\u65b9\u7a0b\u5f0f\u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u63a8\u5b9a - Python\u3092\u7528\u3044\u305f\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u3068\u53ef\u8996\u5316 - \u6c7a\u5b9a\u4fc2\u6570 \\(R^2\\) \u306b\u3088\u308b\u30e2\u30c7\u30eb\u8a55\u4fa1</p> <p>\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u69d8\u3005\u306a\u5206\u6790\u624b\u6cd5\u306e\u57fa\u790e\u3068\u306a\u308b\u3082\u306e\u3067\u3059\u3002\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3068\u9006\u884c\u5217\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u5b66\u3073\u3001\u305d\u306e\u5f8c\u3001\u8907\u6570\u306e\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u91cd\u56de\u5e30\u30e2\u30c7\u30eb\u3078\u3068\u767a\u5c55\u3055\u305b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I - \u7b2c17\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":"<p>\u8b1b\u7fa9\u540d: \u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II \u8b1b\u7fa9\u56de: \u7b2c17\u56de \u30c6\u30fc\u30de: \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u3068\u9006\u884c\u5217 \u65e5\u4ed8: 2025\u5e74xx\u6708xx\u65e5</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":""},{"location":"lectures/LA/17-system-of-linear-equation/#_1","title":"\u95a2\u9023\u9805\u76ee","text":"<ul> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\uff08\u7b2c10\u56de\uff5e\u7b2c15\u56de\uff09</li> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\uff08\u7b2c12\u56de\uff09</li> <li>\u9006\u884c\u5217\uff08\u7b2c15\u56de\uff09</li> </ul>"},{"location":"lectures/LA/17-system-of-linear-equation/#_2","title":"\u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9","text":"<ul> <li>\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u6f14\u7b97\uff08\u52a0\u6cd5\u3001\u30b9\u30ab\u30e9\u30fc\u500d\uff09</li> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\u306e\u5b9a\u7fa9\u3068\u8a08\u7b97\u65b9\u6cd5</li> <li>\u9006\u884c\u5217\u306e\u5b9a\u7fa9\u3068\u8a08\u7b97\u65b9\u6cd5</li> </ul>"},{"location":"lectures/LA/17-system-of-linear-equation/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a 1. \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u3092\u7406\u89e3\u3057\u3001\u5177\u4f53\u4f8b\u3067\u8a08\u7b97\u3067\u304d\u308b 2. \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3092\u7406\u89e3\u3057\u3001\u5224\u5b9a\u3067\u304d\u308b 3. \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u3068\u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6\u306e\u95a2\u4fc2\u3092\u8aac\u660e\u3067\u304d\u308b 4. \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3092\u5224\u5b9a\u3067\u304d\u308b 5. 1\u6b21\u72ec\u7acb\u3067\u306a\u3044\u30d9\u30af\u30c8\u30eb\u304c\u542b\u307e\u308c\u308b\u5834\u5408\u306e\u9006\u884c\u5217\u306e\u4e0d\u5b89\u5b9a\u6027\u3092\u7406\u89e3\u3067\u304d\u308b</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/17-system-of-linear-equation/#31-1","title":"3.1 \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u3068\u306f","text":"<p>\u5b9a\u7fa9\uff1a\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408 \u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) \u306b\u5bfe\u3057\u3066\u3001\u30b9\u30ab\u30e9\u30fc \\(c_1, c_2, \\ldots, c_n\\) \u3092\u7528\u3044\u3066\u8868\u3055\u308c\u308b\u548c\uff1a \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n\\) \u3092\u3053\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u3068\u3044\u3046\u3002</p> <p>1\u6b21\u7d50\u5408\u306f\u3001\u4e0e\u3048\u3089\u308c\u305f\u30d9\u30af\u30c8\u30eb\u306e\u300c\u91cd\u307f\u4ed8\u304d\u548c\u300d\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u5404\u30d9\u30af\u30c8\u30eb\u306b\u5bfe\u3057\u3066\u3001\u4fc2\u6570\uff08\u91cd\u307f\uff09\u3092\u304b\u3051\u3066\u8db3\u3057\u5408\u308f\u305b\u305f\u7d50\u679c\u3067\u3059\u3002</p> <p>\u4f8b\u984c 3.1.1\uff1a \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\\(\\mathbf{v}_1 = (1, 0, 2)^T\\), \\(\\mathbf{v}_2 = (0, 1, -1)^T\\) \u306e\u3068\u304d\u3001\\(2\\mathbf{v}_1 + 3\\mathbf{v}_2\\) \u3092\u8a08\u7b97\u305b\u3088\u3002</p> <p>\u89e3\u7b54\uff1a \\(2\\mathbf{v}_1 + 3\\mathbf{v}_2 = 2(1, 0, 2)^T + 3(0, 1, -1)^T = (2, 0, 4)^T + (0, 3, -3)^T = (2, 3, 1)^T\\)</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#32-1","title":"3.2 \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u3068\u306f","text":"<p>\u5b9a\u7fa9\uff1a\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb \u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) \u306b\u5bfe\u3057\u3066\u3001 \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n = \\mathbf{0}\\) \u3068\u306a\u308b\u30b9\u30ab\u30e9\u30fc \\(c_1, c_2, \\ldots, c_n\\) \u304c\u3001\u3059\u3079\u3066 \\(0\\) \u306e\u5834\u5408\u306e\u307f\u3067\u3042\u308b\u3068\u304d\u3001\u3053\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u306f1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3068\u3044\u3046\u3002</p> <p>\u4e00\u65b9\u3001\u5c11\u306a\u304f\u3068\u30821\u3064\u306e \\(c_i \\neq 0\\) \u304c\u5b58\u5728\u3057\u3066\u4e0a\u5f0f\u304c\u6210\u308a\u7acb\u3064\u3068\u304d\u3001\u3053\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u306f1\u6b21\u5f93\u5c5e\u3067\u3042\u308b\u3068\u3044\u3046\u3002</p> <p>1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u306f\u3001\u3069\u306e\u30d9\u30af\u30c8\u30eb\u3082\u4ed6\u306e\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u3067\u306f\u8868\u305b\u306a\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u5404\u30d9\u30af\u30c8\u30eb\u306f\u4ed6\u306e\u30d9\u30af\u30c8\u30eb\u3067\u306f\u4ee3\u7528\u3067\u304d\u306a\u3044\u300c\u72ec\u81ea\u306e\u60c5\u5831\u300d\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002</p> <p>\u4f8b\u984c 3.2.1\uff1a \\(\\mathbb{R}^2\\) \u306b\u304a\u3044\u3066\u3001\\(\\mathbf{v}_1 = (1, 0)^T\\), \\(\\mathbf{v}_2 = (0, 1)^T\\) \u304c1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u305b\u3002</p> <p>\u89e3\u7b54\uff1a \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 = \\mathbf{0}\\) \u3068\u3059\u308b\u3068\u3001 \\(c_1(1, 0)^T + c_2(0, 1)^T = (0, 0)^T\\) \\((c_1, c_2)^T = (0, 0)^T\\) \u3088\u3063\u3066 \\(c_1 = c_2 = 0\\) \u3068\u306a\u308b\u305f\u3081\u3001\\(\\mathbf{v}_1\\) \u3068 \\(\\mathbf{v}_2\\) \u306f1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3002</p> <p>\u4f8b\u984c 3.2.2\uff1a \\(\\mathbb{R}^2\\) \u306b\u304a\u3044\u3066\u3001\\(\\mathbf{v}_1 = (2, 1)^T\\), \\(\\mathbf{v}_2 = (4, 2)^T\\) \u304c1\u6b21\u5f93\u5c5e\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u305b\u3002</p> <p>\u89e3\u7b54\uff1a \\(\\mathbf{v}_2 = 2\\mathbf{v}_1\\) \u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u76ee\u3059\u308b\u3068\u3001 \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 = \\mathbf{0}\\) \u3068\u3044\u3046\u5f0f\u306b \\(c_1 = -2, c_2 = 1\\) \u3092\u4ee3\u5165\u3059\u308b\u3068\u3001 \\(-2\\mathbf{v}_1 + \\mathbf{v}_2 = -2(2, 1)^T + (4, 2)^T = (-4, -2)^T + (4, 2)^T = (0, 0)^T = \\mathbf{0}\\) \u3068\u306a\u308b\u3002\\(c_1 \\neq 0, c_2 \\neq 0\\) \u3067\u3042\u308b\u306b\u3082\u304b\u304b\u308f\u3089\u305a\u4e0a\u5f0f\u304c\u6210\u308a\u7acb\u3064\u305f\u3081\u3001\\(\\mathbf{v}_1\\) \u3068 \\(\\mathbf{v}_2\\) \u306f1\u6b21\u5f93\u5c5e\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#33-1","title":"3.3 \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3068\u884c\u5217\u306e\u30e9\u30f3\u30af\u306e\u95a2\u4fc2","text":"<p>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3068\u884c\u5217\u306e\u30e9\u30f3\u30af\u306b\u306f\u5bc6\u63a5\u306a\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u5b9a\u7406\uff1a\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027 \u884c\u5217 \\(A\\) \u306e\u30e9\u30f3\u30af \\(\\text{rank}(A)\\) \u306f\u3001\\(A\\) \u306e\u5217\u30d9\u30af\u30c8\u30eb\u306e\u3046\u3061\u30011\u6b21\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u500b\u6570\u306b\u7b49\u3057\u3044\u3002</p> <p>\u3053\u306e\u5b9a\u7406\u306b\u3088\u308a\u3001\u5217\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\) \u304b\u3089\u306a\u308b\u884c\u5217 \\(A = [\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n]\\) \u306b\u3064\u3044\u3066\uff1a</p> <ul> <li>\\(\\text{rank}(A) = n\\) \u3067\u3042\u308c\u3070\u3001\u3059\u3079\u3066\u306e\u5217\u30d9\u30af\u30c8\u30eb\u306f1\u6b21\u72ec\u7acb</li> <li>\\(\\text{rank}(A) &lt; n\\) \u3067\u3042\u308c\u3070\u3001\u5217\u30d9\u30af\u30c8\u30eb\u306e\u4e2d\u306b1\u6b21\u5f93\u5c5e\u306a\u3082\u306e\u304c\u5b58\u5728\u3059\u308b</li> </ul>"},{"location":"lectures/LA/17-system-of-linear-equation/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/17-system-of-linear-equation/#41-1","title":"4.1 \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u3088\u308b\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u306e\u5224\u5b9a","text":"<p>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u306f\u3001\u305d\u308c\u3089\u3092\u5217\u30d9\u30af\u30c8\u30eb\u3068\u3059\u308b\u884c\u5217\u3092\u4f5c\u308a\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u3067\u5224\u5b9a\u3067\u304d\u307e\u3059\u3002</p> <p>\u65b9\u6cd5\uff1a\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306b\u3088\u308b\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u306e\u5224\u5b9a 1. \u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) \u3092\u5217\u30d9\u30af\u30c8\u30eb\u3068\u3059\u308b\u884c\u5217 \\(A = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n]\\) \u3092\u4f5c\u308b 2. \u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u3001\u968e\u6bb5\u884c\u5217\uff08\u307e\u305f\u306f\u7c21\u7d04\u968e\u6bb5\u884c\u5217\uff09\u306b\u5909\u5f62\u3059\u308b 3. \u5909\u5f62\u5f8c\u306e\u884c\u5217\u306e\u30e9\u30f3\u30af\uff08\u30bc\u30ed\u3067\u306a\u3044\u884c\u306e\u6570\uff09\u3092\u6c42\u3081\u308b 4. \u30e9\u30f3\u30af\u304c \\(n\\) \uff08\u5217\u30d9\u30af\u30c8\u30eb\u306e\u6570\uff09\u306b\u7b49\u3057\u3051\u308c\u3070\u3001\u30d9\u30af\u30c8\u30eb\u306f1\u6b21\u72ec\u7acb 5. \u30e9\u30f3\u30af\u304c \\(n\\) \u3088\u308a\u5c0f\u3055\u3051\u308c\u3070\u3001\u30d9\u30af\u30c8\u30eb\u306f1\u6b21\u5f93\u5c5e</p> <p>\u4f8b\u984c 4.1.1\uff1a \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\\(\\mathbf{v}_1 = (1, 2, 3)^T\\), \\(\\mathbf{v}_2 = (2, 3, 4)^T\\), \\(\\mathbf{v}_3 = (3, 5, 7)^T\\) \u304c1\u6b21\u72ec\u7acb\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002</p> <p>\u89e3\u7b54\uff1a \u3053\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u3092\u5217\u30d9\u30af\u30c8\u30eb\u3068\u3059\u308b\u884c\u5217\u3092\u4f5c\u308a\u307e\u3059\uff1a \\(A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 3 &amp; 5 \\\\ 3 &amp; 4 &amp; 7 \\end{bmatrix}\\)</p> <p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u307e\u3059\uff1a 1. \u7b2c1\u5217\u3092\u57fa\u6e96\u306b\u7b2c2\u5217\u3092\u5909\u5f62\uff1a\u7b2c2\u5217 \\(-\\) 2\u00d7\u7b2c1\u5217 = \\((0, -1, -2)^T\\) 2. \u7b2c1\u5217\u3092\u57fa\u6e96\u306b\u7b2c3\u5217\u3092\u5909\u5f62\uff1a\u7b2c3\u5217 \\(-\\) 3\u00d7\u7b2c1\u5217 = \\((0, -1, -2)^T\\)</p> <p>\u5909\u5f62\u5f8c\u306e\u884c\u5217\uff1a \\(A' = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; -1 &amp; -1 \\\\ 3 &amp; -2 &amp; -2 \\end{bmatrix}\\)</p> <p>\u3055\u3089\u306b\u5909\u5f62\u3092\u7d9a\u3051\u308b\u3068\uff1a \\(A'' = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\)</p> <p>\u5909\u5f62\u5f8c\u306e\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f2\u3067\u3042\u308a\u3001\u3053\u308c\u306f\u5217\u30d9\u30af\u30c8\u30eb\u306e\u65703\u3088\u308a\u5c0f\u3055\u3044\u305f\u3081\u3001\\(\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\) \u306f1\u6b21\u5f93\u5c5e\u3067\u3042\u308b\u3002</p> <p>\u5b9f\u969b\u3001\\(\\mathbf{v}_3 = \\mathbf{v}_1 + \\mathbf{v}_2\\) \u3067\u3042\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u308b\uff1a \\((3, 5, 7)^T = (1, 2, 3)^T + (2, 3, 4)^T\\)</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#42-1","title":"4.2 \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u3068\u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6","text":"<p>\u6b63\u65b9\u884c\u5217\u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u6761\u4ef6\u306f\u3001\u305d\u306e\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u884c\u6570\uff08\u307e\u305f\u306f\u5217\u6570\uff09\u306b\u7b49\u3057\u3044\u3053\u3068\u3067\u3059\u3002\u3053\u308c\u306f\u3059\u306a\u308f\u3061\u3001\u305d\u306e\u884c\u5217\u306e\u5217\u30d9\u30af\u30c8\u30eb\u304c\u3059\u3079\u30661\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> <p>\u5b9a\u7406\uff1a\u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6 \\(n\\) \u6b21\u6b63\u65b9\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u306f\u540c\u5024\u3067\u3042\u308b\uff1a 1. \\(A\\) \u306f\u9006\u884c\u5217\u3092\u6301\u3064\uff08\\(A\\) \u306f\u6b63\u5247\u3067\u3042\u308b\uff09 2. \\(\\text{rank}(A) = n\\) 3. \\(A\\) \u306e\u5217\u30d9\u30af\u30c8\u30eb\u306f\u3059\u3079\u30661\u6b21\u72ec\u7acb\u3067\u3042\u308b 4. \\(\\det(A) \\neq 0\\) 5. \\(Ax = 0\\) \u306e\u89e3\u306f \\(x = 0\\) \u306e\u307f\u3067\u3042\u308b</p> <p>\u4f8b\u984c 4.2.1\uff1a \\(A = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 6 \\end{bmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\\(A\\) \u304c\u9006\u884c\u5217\u3092\u6301\u3064\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002</p> <p>\u89e3\u7b54\uff1a \u884c\u5217 \\(A\\) \u306e\u5217\u30d9\u30af\u30c8\u30eb\u306f \\(\\mathbf{a}_1 = (1, 3)^T\\) \u3068 \\(\\mathbf{a}_2 = (2, 6)^T\\) \u3067\u3042\u308b\u3002 \\(\\mathbf{a}_2 = 2\\mathbf{a}_1\\) \u3067\u3042\u308b\u305f\u3081\u3001\u5217\u30d9\u30af\u30c8\u30eb\u306f1\u6b21\u5f93\u5c5e\u3067\u3042\u308b\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\\(\\text{rank}(A) &lt; 2\\) \u3067\u3042\u308a\u3001\\(A\\) \u306f\u9006\u884c\u5217\u3092\u6301\u305f\u306a\u3044\u3002</p> <p>\u3053\u308c\u306f\u884c\u5217\u5f0f \\(\\det(A) = 1 \\cdot 6 - 2 \\cdot 3 = 6 - 6 = 0\\) \u304b\u3089\u3082\u78ba\u8a8d\u3067\u304d\u308b\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#43-1","title":"4.3 1\u6b21\u72ec\u7acb\u3067\u306a\u3044\u30d9\u30af\u30c8\u30eb\u304c\u3042\u308b\u5834\u5408\u306e\u9006\u884c\u5217\u306e\u4e0d\u5b89\u5b9a\u6027","text":"<p>\u5217\u30d9\u30af\u30c8\u30eb\u304c\u300c\u307b\u307c1\u6b21\u5f93\u5c5e\u300d\u3067\u3042\u308b\u5834\u5408\uff08\u3064\u307e\u308a\u3001\u3042\u308b\u5217\u30d9\u30af\u30c8\u30eb\u304c\u4ed6\u306e\u5217\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u3067\u8fd1\u4f3c\u3067\u304d\u308b\u5834\u5408\uff09\u3001\u305d\u306e\u884c\u5217\u306e\u9006\u884c\u5217\u306f\u6570\u5024\u7684\u306b\u4e0d\u5b89\u5b9a\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u6ce8\u610f\uff1a\u6761\u4ef6\u6570\u3068\u9006\u884c\u5217\u306e\u5b89\u5b9a\u6027 \u884c\u5217 \\(A\\) \u306e\u6761\u4ef6\u6570 \\(\\kappa(A)\\) \u306f\u3001\\(A\\) \u306e\u6700\u5927\u7279\u7570\u5024\u3068\u6700\u5c0f\u7279\u7570\u5024\u306e\u6bd4\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u3001\u9006\u884c\u5217\u306e\u6570\u5024\u7684\u5b89\u5b9a\u6027\u306e\u6307\u6a19\u3068\u306a\u308b\u3002\u6761\u4ef6\u6570\u304c\u5927\u304d\u3044\u307b\u3069\u3001\u9006\u884c\u5217\u306f\u4e0d\u5b89\u5b9a\u306b\u306a\u308b\u3002</p> <p>1\u6b21\u72ec\u7acb\u6027\u304c\u4f4e\u3044\uff08\u5217\u30d9\u30af\u30c8\u30eb\u304c\u4e92\u3044\u306b\u8fd1\u3044\u65b9\u5411\u3092\u5411\u3044\u3066\u3044\u308b\uff09\u5834\u5408\u3001\u5c0f\u3055\u306a\u8aa4\u5dee\u304c\u5927\u304d\u304f\u5897\u5e45\u3055\u308c\u3001\u6570\u5024\u8a08\u7b97\u306b\u304a\u3044\u3066\u554f\u984c\u3092\u5f15\u304d\u8d77\u3053\u3059\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u73fe\u8c61\u306f\u3001\u7279\u306b\u7dda\u5f62\u56de\u5e30\u306b\u304a\u3051\u308b\u591a\u91cd\u5171\u7dda\u6027\u554f\u984c\u3068\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u4f8b\u984c 4.3.1\uff1a \u884c\u5217 \\(A = \\begin{bmatrix} 1 &amp; 1.001 \\\\ 2 &amp; 2.001 \\end{bmatrix}\\) \u306b\u3064\u3044\u3066\u8003\u3048\u308b\u3002\u3053\u306e\u884c\u5217\u306e\u5217\u30d9\u30af\u30c8\u30eb\u306f\u307b\u307c1\u6b21\u5f93\u5c5e\u3067\u3042\u308a\u3001\u305d\u306e\u5f71\u97ff\u3092\u8abf\u3079\u3088\u3002</p> <p>\u89e3\u7b54\uff1a \\(A\\) \u306e\u5217\u30d9\u30af\u30c8\u30eb\u306f \\(\\mathbf{a}_1 = (1, 2)^T\\) \u3068 \\(\\mathbf{a}_2 = (1.001, 2.002)^T\\) \u3067\u3042\u308b\u3002 \\(\\mathbf{a}_2 \\approx 1.001 \\cdot \\mathbf{a}_1\\) \u3067\u3042\u308a\u3001\u307b\u307c1\u6b21\u5f93\u5c5e\u3068\u8a00\u3048\u308b\u3002</p> <p>\u884c\u5217\u5f0f\u306f \\(\\det(A) = 1 \\cdot 2.001 - 1.001 \\cdot 2 = 2.001 - 2.002 = -0.001\\) \u3068\u307b\u307c0\u3067\u3042\u308b\u3002</p> <p>\u9006\u884c\u5217\u3092\u8a08\u7b97\u3059\u308b\u3068\uff1a \\(A^{-1} = \\frac{1}{\\det(A)} \\begin{bmatrix} 2.002 &amp; -1.001 \\\\ -2 &amp; 1 \\end{bmatrix} = -1000 \\cdot \\begin{bmatrix} 2.002 &amp; -1.001 \\\\ -2 &amp; 1 \\end{bmatrix}\\)</p> <p>\u975e\u5e38\u306b\u5927\u304d\u306a\u5024\u304c\u73fe\u308c\u3001\u6570\u5024\u7684\u306b\u4e0d\u5b89\u5b9a\u306a\u7d50\u679c\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u3053\u306e\u3088\u3046\u306a\u884c\u5217\u306b\u5bfe\u3057\u3066\u9006\u884c\u5217\u3092\u7528\u3044\u305f\u8a08\u7b97\u3092\u884c\u3046\u3068\u3001\u5c0f\u3055\u306a\u8aa4\u5dee\u304c\u5927\u304d\u304f\u5897\u5e45\u3055\u308c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/17-system-of-linear-equation/#51-1python","title":"5.1 \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3092Python\u3067\u78ba\u8a8d\u3059\u308b","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# 1\u6b21\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u306e\u4f8b\nv1 = np.array([1, 0, 0])\nv2 = np.array([0, 1, 0])\nv3 = np.array([0, 0, 1])\n\n# \u884c\u5217\u3092\u4f5c\u6210\nA_independent = np.column_stack((v1, v2, v3))\nprint(\"1\u6b21\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u304b\u3089\u306a\u308b\u884c\u5217:\")\nprint(A_independent)\n\n# \u30e9\u30f3\u30af\u3092\u8a08\u7b97\nrank_independent = np.linalg.matrix_rank(A_independent)\nprint(f\"\u884c\u5217\u306e\u30e9\u30f3\u30af: {rank_independent}\")\nprint(f\"\u30d9\u30af\u30c8\u30eb\u306f1\u6b21\u72ec\u7acb\u304b? {rank_independent == 3}\")\n\n# 1\u6b21\u5f93\u5c5e\u306a\u30d9\u30af\u30c8\u30eb\u306e\u4f8b\nv1 = np.array([1, 2, 3])\nv2 = np.array([2, 4, 6])\nv3 = np.array([3, 6, 9])\n\n# \u884c\u5217\u3092\u4f5c\u6210\nA_dependent = np.column_stack((v1, v2, v3))\nprint(\"\\n1\u6b21\u5f93\u5c5e\u306a\u30d9\u30af\u30c8\u30eb\u304b\u3089\u306a\u308b\u884c\u5217:\")\nprint(A_dependent)\n\n# \u30e9\u30f3\u30af\u3092\u8a08\u7b97\nrank_dependent = np.linalg.matrix_rank(A_dependent)\nprint(f\"\u884c\u5217\u306e\u30e9\u30f3\u30af: {rank_dependent}\")\nprint(f\"\u30d9\u30af\u30c8\u30eb\u306f1\u6b21\u72ec\u7acb\u304b? {rank_dependent == 3}\")\n</code></pre>"},{"location":"lectures/LA/17-system-of-linear-equation/#52-1","title":"5.2 \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3092\u8996\u899a\u5316\u3059\u308b","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_vectors(vectors, title, dependency=None):\n    \"\"\"\u30d9\u30af\u30c8\u30eb\u30923D\u7a7a\u9593\u306b\u63cf\u753b\u3059\u308b\u95a2\u6570\"\"\"\n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # \u539f\u70b9\n    origin = np.zeros(3)\n\n    # \u5404\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\n    colors = ['r', 'g', 'b', 'c', 'm', 'y']\n    for i, v in enumerate(vectors):\n        ax.quiver(origin[0], origin[1], origin[2], \n                  v[0], v[1], v[2], \n                  color=colors[i % len(colors)], \n                  label=f'v{i+1}', arrow_length_ratio=0.1)\n\n    # \u4f9d\u5b58\u95a2\u4fc2\u306e\u3042\u308b\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\n    if dependency is not None:\n        v_dep = np.zeros(3)\n        for i, coef in enumerate(dependency):\n            v_dep += coef * vectors[i]\n        ax.quiver(origin[0], origin[1], origin[2], \n                  v_dep[0], v_dep[1], v_dep[2], \n                  color='k', linestyle='dashed', \n                  label='linear combination', arrow_length_ratio=0.1)\n\n    # \u30b0\u30e9\u30d5\u306e\u8a2d\u5b9a\n    ax.set_xlim([-1, 3])\n    ax.set_ylim([-1, 3])\n    ax.set_zlim([-1, 3])\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_title(title)\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n# 1\u6b21\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u306e\u4f8b\nv1 = np.array([1, 0, 0])\nv2 = np.array([0, 1, 0])\nv3 = np.array([0, 0, 1])\nplot_vectors([v1, v2, v3], \"1\u6b21\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\")\n\n# 1\u6b21\u5f93\u5c5e\u306a\u30d9\u30af\u30c8\u30eb\u306e\u4f8b\nv1 = np.array([1, 0, 0])\nv2 = np.array([0, 1, 0])\nv3 = np.array([1, 1, 0])  # v3 = v1 + v2\nplot_vectors([v1, v2, v3], \"1\u6b21\u5f93\u5c5e\u306a\u30d9\u30af\u30c8\u30eb\", [1, 1, -1])  # v1 + v2 - v3 = 0\n</code></pre>"},{"location":"lectures/LA/17-system-of-linear-equation/#53-python","title":"5.3 \u9006\u884c\u5217\u306e\u4e0d\u5b89\u5b9a\u6027\u3092Python\u3067\u78ba\u8a8d\u3059\u308b","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u307b\u307c1\u6b21\u5f93\u5c5e\u306a\u30d9\u30af\u30c8\u30eb\u304b\u3089\u306a\u308b\u884c\u5217\u306e\u4f8b\ndef create_nearly_dependent_matrix(epsilon):\n    v1 = np.array([1, 2])\n    v2 = np.array([1 + epsilon, 2 + 2*epsilon])  # v2 \u2248 (1+\u03b5)v1\n    return np.column_stack((v1, v2))\n\n# \u7570\u306a\u308b\u03b5\u306b\u5bfe\u3059\u308b\u6761\u4ef6\u6570\u3068\u9006\u884c\u5217\u306e\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\nepsilons = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\ncondition_numbers = []\ninverse_norms = []\n\nfor eps in epsilons:\n    A = create_nearly_dependent_matrix(eps)\n    cond = np.linalg.cond(A)\n    inv_norm = np.linalg.norm(np.linalg.inv(A))\n\n    condition_numbers.append(cond)\n    inverse_norms.append(inv_norm)\n\n    print(f\"\u03b5 = {eps}:\")\n    print(f\"\u884c\u5217 A =\\n{A}\")\n    print(f\"\u6761\u4ef6\u6570 \u03ba(A) = {cond}\")\n    print(f\"\u9006\u884c\u5217 A^(-1) =\\n{np.linalg.inv(A)}\")\n    print(f\"\u9006\u884c\u5217\u306e\u30ce\u30eb\u30e0 ||A^(-1)|| = {inv_norm}\\n\")\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.subplot(1, 2, 1)\nplt.loglog(epsilons, condition_numbers, 'o-')\nplt.xlabel('\u03b5 (epsilon)')\nplt.ylabel('\u6761\u4ef6\u6570 \u03ba(A)')\nplt.title('\u884c\u5217\u306e\u6761\u4ef6\u6570')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.loglog(epsilons, inverse_norms, 'o-')\nplt.xlabel('\u03b5 (epsilon)')\nplt.ylabel('\u9006\u884c\u5217\u306e\u30ce\u30eb\u30e0 ||A^(-1)||')\nplt.title('\u9006\u884c\u5217\u306e\u30ce\u30eb\u30e0')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/17-system-of-linear-equation/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/17-system-of-linear-equation/#_3","title":"\u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c 6.1\uff1a \u6b21\u306e\u30d9\u30af\u30c8\u30eb\u304c1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u304b\u5224\u5b9a\u305b\u3088\u3002 (a) \\(\\mathbf{v}_1 = (1, 2, 3)^T\\), \\(\\mathbf{v}_2 = (2, 4, 6)^T\\) (b) \\(\\mathbf{v}_1 = (1, 2, 3)^T\\), \\(\\mathbf{v}_2 = (4, 5, 6)^T\\) (c) \\(\\mathbf{v}_1 = (1, 2, 3)^T\\), \\(\\mathbf{v}_2 = (2, 3, 4)^T\\), \\(\\mathbf{v}_3 = (3, 5, 7)^T\\)</p> <p>\u554f\u984c 6.2\uff1a \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_1 = (1, 0, 1)^T\\), \\(\\mathbf{v}_2 = (0, 1, 1)^T\\), \\(\\mathbf{v}_3 = (1, 1, 0)^T\\) \u304c1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u305b\u3002</p> <p>\u554f\u984c 6.3\uff1a \u6b21\u306e\u884c\u5217\u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u304b\u5224\u5b9a\u305b\u3088\u3002 (a) \\(A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix}\\) (b) \\(B = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 10 \\end{bmatrix}\\)</p> <p>\u554f\u984c 6.4\uff1a \\(\\mathbf{v}_1 = (2, 0, -1)^T\\), \\(\\mathbf{v}_2 = (1, 1, 1)^T\\), \\(\\mathbf{v}_3 = (0, 1, 2)^T\\) \u306e\u3068\u304d\u3001\u30d9\u30af\u30c8\u30eb \\(\\mathbf{w} = (5, 3, 2)^T\\) \u3092 \\(\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\) \u306e1\u6b21\u7d50\u5408\u3067\u8868\u305b\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#_4","title":"\u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c 6.5\uff1a \\(\\mathbb{R}^n\\) \u306b\u304a\u3051\u308b \\(n\\) \u500b\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u3001\u3053\u308c\u3089\u304c1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3053\u3068\u3068\u3001\u884c\u5217 \\(V = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n]\\) \u304c\u6b63\u5247\u3067\u3042\u308b\u3053\u3068\u304c\u540c\u5024\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u305b\u3002</p> <p>\u554f\u984c 6.6\uff1a \\(n\\) \u6b21\u6b63\u65b9\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\text{rank}(A) &lt; n\\) \u3067\u3042\u308b\u3068\u304d\u3001\\(A\\mathbf{x} = \\mathbf{0}\\) \u306e\u81ea\u660e\u3067\u306a\u3044\u89e3\uff08\u3064\u307e\u308a \\(\\mathbf{x} \\neq \\mathbf{0}\\)\uff09\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u793a\u305b\u3002</p> <p>\u554f\u984c 6.7\uff1a \\(A = \\begin{bmatrix} 1 &amp; 1+\\epsilon \\\\ 2 &amp; 2+\\epsilon \\end{bmatrix}\\) \u3068\u3059\u308b\u3002\\(\\epsilon\\) \u304c\u975e\u5e38\u306b\u5c0f\u3055\u3044\u3068\u304d\u3001\\(A^{-1}\\) \u306e\u5404\u6210\u5206\u304c\u3069\u306e\u3088\u3046\u306b\u632f\u308b\u821e\u3046\u304b\u3092\u8abf\u3079\u3088\u3002</p> <p>\u554f\u984c 6.2.4\uff08\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3078\u306e\u5fdc\u7528\uff09\uff1a</p> <p>\u6700\u8fd1\u306e\u7814\u7a76\u3067\u306f\u3001\u7570\u306a\u308b\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u304c\u7279\u5b9a\u306e\u5065\u5eb7\u72b6\u614b\u3068\u95a2\u9023\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3042\u308b\u7814\u7a76\u8005\u304c\u30013\u3064\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\uff08X\u2081, X\u2082, X\u2083\uff09\u3068\u5fc3\u8840\u7ba1\u75be\u60a3\u30ea\u30b9\u30af\u306e\u95a2\u4fc2\u3092\u8abf\u67fb\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u6e2c\u5b9a\u5024\u306f\u4ee5\u4e0b\u306e\u884c\u5217\u3067\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[B = \\begin{bmatrix}  X_1 &amp; X_2 &amp; X_3 \\\\ \\hline 2.5 &amp; 1.2 &amp; 5.0 \\\\ 3.0 &amp; 1.8 &amp; 6.6 \\\\ 3.5 &amp; 2.4 &amp; 8.2 \\\\ 4.0 &amp; 3.0 &amp; 9.8 \\\\ 4.5 &amp; 3.6 &amp; 11.4 \\end{bmatrix}\\] <ol> <li> <p>\\(X_3\\)\u3068\\(X_1 + 2X_2\\)\u3092\u8003\u3048\u308b\u3053\u3068\u306b\u3088\u308a\u3001\u3053\u308c\u3089\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u30d9\u30af\u30c8\u30eb\u304c1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u304b\u3092\u5224\u5b9a\u3057\u3001\u305d\u306e\u7406\u7531\u3092\u8aac\u660e\u305b\u3088\u3002</p> </li> <li> <p>\u3053\u306e\u3088\u3046\u306a\u95a2\u4fc2\u304c\u7dda\u5f62\u56de\u5e30\u5206\u6790\u306b\u304a\u3044\u3066\u3069\u306e\u3088\u3046\u306a\u554f\u984c\u3092\u5f15\u304d\u8d77\u3053\u3059\u53ef\u80fd\u6027\u304c\u3042\u308b\u304b\u8aac\u660e\u305b\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/17-system-of-linear-equation/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/17-system-of-linear-equation/#q1-1","title":"Q1: \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3068\u9006\u884c\u5217\u306e\u95a2\u4fc2\u3092\u76f4\u611f\u7684\u306b\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A1: \u884c\u5217\u306e\u5217\u30d9\u30af\u30c8\u30eb\u304c1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u306f\u3001\u305d\u308c\u305e\u308c\u306e\u30d9\u30af\u30c8\u30eb\u304c\u300c\u72ec\u81ea\u306e\u65b9\u5411\u300d\u3092\u6301\u3063\u3066\u304a\u308a\u3001\u4ed6\u306e\u30d9\u30af\u30c8\u30eb\u3067\u306f\u4ee3\u7528\u3067\u304d\u306a\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u884c\u5217\u5909\u63db\u306b\u3088\u3063\u3066\u7a7a\u9593\u306e\u3059\u3079\u3066\u306e\u65b9\u5411\u306b\u5bfe\u5fdc\u3067\u304d\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u5217\u30d9\u30af\u30c8\u30eb\u306e\u3069\u308c\u304b\u304c\u4ed6\u306e\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u3067\u8868\u305b\u308b\uff081\u6b21\u5f93\u5c5e\u3067\u3042\u308b\uff09\u5834\u5408\u3001\u305d\u306e\u65b9\u5411\u3078\u306e\u5909\u63db\u60c5\u5831\u304c\u5931\u308f\u308c\u3001\u9006\u5909\u63db\uff08\u9006\u884c\u5217\uff09\u3092\u4e00\u610f\u306b\u5b9a\u7fa9\u3067\u304d\u306a\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#q2-1","title":"Q2: \u306a\u305c1\u6b21\u72ec\u7acb\u3067\u306a\u3044\u30d9\u30af\u30c8\u30eb\u304c\u3042\u308b\u3068\u9006\u884c\u5217\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u308b\u306e\u3067\u3059\u304b\uff1f","text":"<p>A2: \u5217\u30d9\u30af\u30c8\u30eb\u304c\u300c\u307b\u307c1\u6b21\u5f93\u5c5e\u300d\u3067\u3042\u308b\u5834\u5408\u3001\u884c\u5217\u306f\u300c\u307b\u307c\u7279\u7570\uff08\u9006\u884c\u5217\u3092\u6301\u305f\u306a\u3044\uff09\u300d\u306a\u72b6\u614b\u306b\u8fd1\u3065\u304d\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f0\u306b\u8fd1\u304f\u3001\u9006\u884c\u5217\u306e\u8a08\u7b97\u3067\u306f\u884c\u5217\u5f0f\u3067\u5272\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u975e\u5e38\u306b\u5927\u304d\u306a\u5024\u304c\u73fe\u308c\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u5165\u529b\u30c7\u30fc\u30bf\u306e\u5c0f\u3055\u306a\u8aa4\u5dee\u304c\u51fa\u529b\u7d50\u679c\u306b\u5927\u304d\u304f\u5897\u5e45\u3055\u308c\u3066\u3057\u307e\u3044\u307e\u3059\u3002\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u3001\u3053\u308c\u306f\u4fc2\u6570\u306e\u63a8\u5b9a\u5024\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u308a\u3001\u4fe1\u983c\u6027\u304c\u4f4e\u4e0b\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#q3-1","title":"Q3: 1\u6b21\u72ec\u7acb\u6027\u306e\u5224\u5b9a\u306b\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u4f7f\u3046\u7406\u7531\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A3: \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306f\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u6c42\u3081\u308b\u52b9\u7387\u7684\u306a\u65b9\u6cd5\u3067\u3042\u308a\u3001\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u306f\u30e9\u30f3\u30af\u3068\u76f4\u63a5\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\\(n\\)\u500b\u306e\u30d9\u30af\u30c8\u30eb\u304c1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\u3001\u305d\u308c\u3089\u3092\u5217\u30d9\u30af\u30c8\u30eb\u3068\u3059\u308b\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\\(n\\)\u3067\u3042\u308b\u3053\u3068\u3067\u3059\u3002\u307e\u305f\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306f\u6570\u5024\u7684\u306b\u3082\u5b89\u5b9a\u3057\u305f\u65b9\u6cd5\u3067\u3042\u308a\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3067\u306e\u5b9f\u88c5\u3082\u5bb9\u6613\u3067\u3059\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#q4-1","title":"Q4: \u884c\u5217\u306e\u30e9\u30f3\u30af\u30681\u6b21\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u500b\u6570\u304c\u7b49\u3057\u3044\u3053\u3068\u3092\u3069\u306e\u3088\u3046\u306b\u7406\u89e3\u3059\u308c\u3070\u826f\u3044\u3067\u3059\u304b\uff1f","text":"<p>A4: \u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u3001\u884c\u57fa\u672c\u5909\u5f62\u3092\u9069\u7528\u3057\u305f\u5f8c\u306b\u5f97\u3089\u308c\u308b\u968e\u6bb5\u884c\u5217\u306e\u30bc\u30ed\u3067\u306a\u3044\u884c\u306e\u6570\u3068\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u968e\u6bb5\u884c\u5217\u306e\u5404\u884c\u306f\u3001\u5143\u306e\u884c\u5217\u306e\u5217\u30d9\u30af\u30c8\u30eb\u306e\u9593\u306b\u5b58\u5728\u3059\u308b\u7dda\u5f62\u95a2\u4fc2\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001\u30e9\u30f3\u30af\u304c \\(r\\) \u3067\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u306f\u3001\\(r\\) \u500b\u306e1\u6b21\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u304c\u5b58\u5728\u3057\u3001\u6b8b\u308a\u306e\u30d9\u30af\u30c8\u30eb\u306f\u305d\u308c\u3089\u306e1\u6b21\u7d50\u5408\u3067\u8868\u305b\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#q5-1","title":"Q5: 1\u6b21\u72ec\u7acb\u6027\u3068\u591a\u91cd\u5171\u7dda\u6027\u306f\u3069\u306e\u3088\u3046\u306a\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A5: \u591a\u91cd\u5171\u7dda\u6027\u3068\u306f\u3001\u7d71\u8a08\u5b66\u3067\u8aac\u660e\u5909\u6570\u9593\u306b\u5f37\u3044\u76f8\u95a2\u95a2\u4fc2\u304c\u3042\u308b\u72b6\u614b\u3092\u6307\u3057\u307e\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u306e\u89b3\u70b9\u3067\u306f\u3001\u3053\u308c\u306f\u8aac\u660e\u5909\u6570\u3092\u8868\u3059\u30d9\u30af\u30c8\u30eb\u304c\u300c\u307b\u307c1\u6b21\u5f93\u5c5e\u300d\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u5b8c\u5168\u306b1\u6b21\u5f93\u5c5e\u3067\u3042\u308c\u3070\u3001\u3042\u308b\u8aac\u660e\u5909\u6570\u304c\u4ed6\u306e\u8aac\u660e\u5909\u6570\u306e\u7dda\u5f62\u7d50\u5408\u3067\u5b8c\u5168\u306b\u8868\u73fe\u3067\u304d\u307e\u3059\u304c\u3001\u591a\u91cd\u5171\u7dda\u6027\u306e\u5834\u5408\u306f\u8fd1\u4f3c\u7684\u306b\u8868\u73fe\u3067\u304d\u308b\u72b6\u614b\u3067\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u72b6\u614b\u3067\u306f\u3001\u56de\u5e30\u4fc2\u6570\u306e\u63a8\u5b9a\u5024\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u308a\u3001\u6a19\u6e96\u8aa4\u5dee\u304c\u5927\u304d\u304f\u306a\u308b\u305f\u3081\u3001\u7d71\u8a08\u7684\u63a8\u8ad6\u306e\u4fe1\u983c\u6027\u304c\u4f4e\u4e0b\u3057\u307e\u3059\u3002\u591a\u91cd\u5171\u7dda\u6027\u3092\u6570\u5024\u7684\u306b\u8a55\u4fa1\u3059\u308b\u6307\u6a19\u3068\u3057\u3066\u3001\u884c\u5217\u306e\u6761\u4ef6\u6570\u3084\u5206\u6563\u62e1\u5927\u8981\u56e0\uff08VIF\uff09\u306a\u3069\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#q6-1","title":"Q6: \u30d9\u30af\u30c8\u30eb\u304c1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u969b\u3001\u6570\u5024\u8a08\u7b97\u306e\u8aa4\u5dee\u306f\u3069\u306e\u3088\u3046\u306b\u51e6\u7406\u3059\u3079\u304d\u3067\u3059\u304b\uff1f","text":"<p>A6: \u6570\u5024\u8a08\u7b97\u3067\u306f\u6d6e\u52d5\u5c0f\u6570\u70b9\u306e\u8aa4\u5dee\u304c\u907f\u3051\u3089\u308c\u306a\u3044\u305f\u3081\u3001\u53b3\u5bc6\u306a0\u304b\u3069\u3046\u304b\u306e\u5224\u5b9a\u306f\u56f0\u96e3\u3067\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u300c\u5341\u5206\u306b\u5c0f\u3055\u3044\u5024\u300d\u30920\u3068\u307f\u306a\u3059\u8a31\u5bb9\u8aa4\u5dee\uff08\u95be\u5024\uff09\u3092\u8a2d\u5b9a\u3059\u308b\u306e\u304c\u4e00\u822c\u7684\u3067\u3059\u3002NumPy\u306e\u3088\u3046\u306a\u79d1\u5b66\u8a08\u7b97\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306f\u3001<code>numpy.linalg.matrix_rank</code>\u95a2\u6570\u304c\u81ea\u52d5\u7684\u306b\u9069\u5207\u306a\u95be\u5024\u3092\u8a2d\u5b9a\u3057\u3066\u30e9\u30f3\u30af\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u5b9f\u7528\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3068\u3057\u3066\u306f\u3001\u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u3092\u7528\u3044\u3066\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u63a8\u5b9a\u3059\u308b\u65b9\u6cd5\u304c\u3042\u308a\u3001\u3053\u308c\u306f\u6570\u5024\u7684\u306b\u5b89\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#q7","title":"Q7: \u884c\u5217\u306e\u6761\u4ef6\u6570\u3068\u9006\u884c\u5217\u306e\u4e0d\u5b89\u5b9a\u6027\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A7: \u884c\u5217\u306e\u6761\u4ef6\u6570 \\(\\kappa(A)\\) \u306f\u6700\u5927\u7279\u7570\u5024\u3068\u6700\u5c0f\u7279\u7570\u5024\u306e\u6bd4\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u3001\u884c\u5217\u306e\u300c\u4e0d\u5b89\u5b9a\u3055\u300d\u3092\u6570\u5024\u5316\u3057\u305f\u3082\u306e\u3067\u3059\u3002\u6761\u4ef6\u6570\u304c\u5927\u304d\u3044\u307b\u3069\u3001\u9006\u884c\u5217\u3092\u4f7f\u3063\u305f\u8a08\u7b97\uff08\u4f8b\u3048\u3070\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\uff09\u306b\u304a\u3044\u3066\u5165\u529b\u306e\u5c0f\u3055\u306a\u8aa4\u5dee\u304c\u51fa\u529b\u306b\u5927\u304d\u304f\u5f71\u97ff\u3057\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u3001\u5165\u529b\u306e\u76f8\u5bfe\u8aa4\u5dee\u304c \\(\\epsilon\\) \u306e\u3068\u304d\u3001\u51fa\u529b\u306e\u76f8\u5bfe\u8aa4\u5dee\u306f\u6700\u5927\u3067 \\(\\kappa(A) \\cdot \\epsilon\\) \u306b\u306a\u308a\u5f97\u307e\u3059\u3002\u6761\u4ef6\u6570\u304c\u5927\u304d\u3044\u884c\u5217\u306f\u300c\u4e0d\u826f\u6761\u4ef6\uff08ill-conditioned\uff09\u300d\u3068\u547c\u3070\u308c\u3001\u6570\u5024\u8a08\u7b97\u306b\u304a\u3044\u3066\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u7279\u306b\u7dda\u5f62\u56de\u5e30\u306b\u304a\u3044\u3066\u8aac\u660e\u5909\u6570\u9593\u306b\u591a\u91cd\u5171\u7dda\u6027\u304c\u3042\u308b\u5834\u5408\u3001\u30c7\u30b6\u30a4\u30f3\u884c\u5217\u306e\u6761\u4ef6\u6570\u304c\u5927\u304d\u304f\u306a\u308a\u3001\u4fc2\u6570\u63a8\u5b9a\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#q8-1","title":"Q8: 1\u6b21\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u304c\u6301\u3064\u300c\u72ec\u81ea\u306e\u65b9\u5411\u6027\u300d\u3092\u5e7e\u4f55\u5b66\u7684\u306b\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A8: 2\u6b21\u5143\u5e73\u9762\u3067\u8003\u3048\u308b\u3068\u30011\u6b21\u72ec\u7acb\u306a2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u5e73\u884c\u3067\u306a\u3044\u4efb\u610f\u306e2\u3064\u306e\u65b9\u5411\u3092\u8868\u3057\u307e\u3059\u30023\u6b21\u5143\u7a7a\u9593\u3067\u306f\u30011\u6b21\u72ec\u7acb\u306a3\u3064\u306e\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u5171\u5e73\u9762\u4e0a\u306b\u306a\u3044\u3001\u3064\u307e\u308a\u7a7a\u9593\u306e3\u3064\u306e\u72ec\u7acb\u3057\u305f\u65b9\u5411\u3092\u8868\u3057\u307e\u3059\u3002\u4e00\u822c\u306b\u3001\\(n\\)\u6b21\u5143\u7a7a\u9593\u306b\u304a\u3044\u30661\u6b21\u72ec\u7acb\u306a\\(n\\)\u500b\u306e\u30d9\u30af\u30c8\u30eb\u306f\u3001\u305d\u306e\u7a7a\u9593\u5185\u306e\u3059\u3079\u3066\u306e\u65b9\u5411\u3092\u300c\u30ab\u30d0\u30fc\u300d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u306b\u5bfe\u3057\u30011\u6b21\u5f93\u5c5e\u306a\u30d9\u30af\u30c8\u30eb\u7fa4\u306f\u3001\u5b9f\u969b\u306b\u306f\u7a7a\u9593\u306e\u4e00\u90e8\uff08\u90e8\u5206\u7a7a\u9593\uff09\u3057\u304b\u30ab\u30d0\u30fc\u3067\u304d\u307e\u305b\u3093\u3002\u4f8b\u3048\u3070\u30013\u6b21\u5143\u7a7a\u9593\u5185\u306e1\u6b21\u5f93\u5c5e\u306a3\u3064\u306e\u30d9\u30af\u30c8\u30eb\u306f\u3001\u5b9f\u8cea\u7684\u306b\u5e73\u9762\u4e0a\u307e\u305f\u306f\u76f4\u7dda\u4e0a\u306b\u3057\u304b\u5b58\u5728\u3067\u304d\u306a\u3044\u305f\u3081\u30013\u6b21\u5143\u7a7a\u9593\u5168\u4f53\u3092\u30ab\u30d0\u30fc\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#8","title":"8. \u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u30681\u6b21\u72ec\u7acb\u6027\u3001\u305d\u3057\u3066\u305d\u308c\u3089\u3068\u9006\u884c\u5217\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\u3002\u4e3b\u8981\u306a\u5b66\u7fd2\u5185\u5bb9\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u306f\u3001\u8907\u6570\u306e\u30d9\u30af\u30c8\u30eb\u306b\u4fc2\u6570\u3092\u639b\u3051\u3066\u8db3\u3057\u5408\u308f\u305b\u305f\u3082\u306e\u3067\u3059\u3002</li> <li>\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u306f\u3001\u3042\u308b\u30d9\u30af\u30c8\u30eb\u7fa4\u304c\u300c\u72ec\u81ea\u306e\u65b9\u5411\u6027\u300d\u3092\u6301\u3061\u3001\u4e92\u3044\u306b\u4ed6\u306e\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u3067\u8868\u305b\u306a\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3092\u5224\u5b9a\u3067\u304d\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u30d9\u30af\u30c8\u30eb\u3092\u5217\u3068\u3059\u308b\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u5217\u6570\u306b\u7b49\u3057\u3051\u308c\u30701\u6b21\u72ec\u7acb\u3067\u3059\u3002</li> <li>\u6b63\u65b9\u884c\u5217\u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\u3001\u305d\u306e\u5217\u30d9\u30af\u30c8\u30eb\u304c1\u6b21\u72ec\u7acb\u3067\u3042\u308b\u3053\u3068\u3067\u3059\u3002</li> <li>\u5217\u30d9\u30af\u30c8\u30eb\u304c\u300c\u307b\u307c1\u6b21\u5f93\u5c5e\u300d\u3067\u3042\u308b\u5834\u5408\u3001\u9006\u884c\u5217\u306e\u8a08\u7b97\u306f\u6570\u5024\u7684\u306b\u4e0d\u5b89\u5b9a\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u884c\u5217\u306e\u6761\u4ef6\u6570\u304c\u5927\u304d\u3044\u3053\u3068\u3068\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002</li> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u591a\u91cd\u5171\u7dda\u6027\u554f\u984c\u306f\u3001\u8aac\u660e\u5909\u6570\u306e\u30d9\u30af\u30c8\u30eb\u304c\u8fd1\u4f3c\u7684\u306b1\u6b21\u5f93\u5c5e\u3067\u3042\u308b\u3053\u3068\u306b\u8d77\u56e0\u3057\u307e\u3059\u3002</li> </ol> <p>\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u6982\u5ff5\u3092\u5fdc\u7528\u3057\u3066\u3001\u8907\u6570\u306e\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002\u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u306f\u3001\u9069\u5207\u306a\u30e2\u30c7\u30eb\u69cb\u7bc9\u3068\u5b89\u5b9a\u3057\u305f\u4fc2\u6570\u63a8\u5b9a\u306e\u305f\u3081\u306b\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/17-system-of-linear-equation/#9","title":"9. \u6b21\u56de\u306e\u4e88\u7fd2\u30dd\u30a4\u30f3\u30c8","text":"<ul> <li>\u591a\u5909\u91cf\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u6570\u5b66\u7684\u8868\u73fe</li> <li>\u6b63\u898f\u65b9\u7a0b\u5f0f\u3068\u6700\u5c0f\u4e8c\u4e57\u6cd5</li> <li>\u591a\u91cd\u5171\u7dda\u6027\u306e\u691c\u51fa\u65b9\u6cd5\u3068\u5bfe\u7b56</li> <li>\u5e73\u9762\u3068\u8d85\u5e73\u9762\u306e\u65b9\u7a0b\u5f0f\u3068\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8</li> </ul> <p>\u5f15\u304d\u7d9a\u304d\u7dda\u5f62\u4ee3\u6570\u306e\u57fa\u672c\u6982\u5ff5\u3092\u78ba\u5b9f\u306b\u7406\u89e3\u3057\u3001\u305d\u308c\u3089\u304c\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u3069\u306e\u3088\u3046\u306b\u5fdc\u7528\u3055\u308c\u308b\u304b\u3092\u8003\u3048\u306a\u304c\u3089\u5b66\u7fd2\u3092\u9032\u3081\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/","title":"\u7b2c18\u56de\uff1a\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u30682\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb","text":""},{"location":"lectures/LA/18-system-of-linear-equation/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c18\u56de \u95a2\u9023\u9805\u76ee: \u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u3001\u8907\u6570\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u7b2c16\u56de\u3067\u5b66\u3093\u3060\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u6982\u5ff5\u3001\u7b2c10\u56de\u301c\u7b2c15\u56de\u3067\u306e\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u8907\u6570\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u6570\u5b66\u7684\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b</li> <li>\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u8868\u73fe\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u63a8\u5b9a\u306e\u539f\u7406\u3068\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u6b63\u898f\u65b9\u7a0b\u5f0f\u306e\u5c0e\u51fa\u3068\u89e3\u6cd5\u3092\u7406\u89e3\u3059\u308b</li> <li>\u8907\u6570\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> </ol>"},{"location":"lectures/LA/18-system-of-linear-equation/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/18-system-of-linear-equation/#31","title":"3.1 \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5fa9\u7fd2","text":"<p>\u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u30011\u3064\u306e\u8aac\u660e\u5909\u6570\u3092\u7528\u3044\u3066\u53cd\u5fdc\u5909\u6570\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002\u3053\u308c\u3092\u6570\u5f0f\u3067\u8868\u3059\u3068\uff1a</p> <p>\u5358\u56de\u5e30\u30e2\u30c7\u30eb: \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\)</p> <p>\u3053\u3053\u3067\uff1a \\(y\\): \u53cd\u5fdc\u5909\u6570\uff08\u4e88\u6e2c\u3057\u305f\u3044\u5909\u6570\uff09 \\(x\\): \u8aac\u660e\u5909\u6570 \\(\\beta_0\\): \u5207\u7247 \\(\\beta_1\\): \u50be\u304d\uff08\u56de\u5e30\u4fc2\u6570\uff09 \\(\\varepsilon\\): \u8aa4\u5dee\u9805  </p> <p>\u3053\u306e\u30e2\u30c7\u30eb\u306f2\u6b21\u5143\u5e73\u9762\u4e0a\u3067\u306f\u300c\u76f4\u7dda\u300d\u3068\u3057\u3066\u8868\u73fe\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#32","title":"3.2 \u8907\u6570\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb","text":"<p>2\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u3001\u30e2\u30c7\u30eb\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u62e1\u5f35\u3055\u308c\u307e\u3059\uff1a</p> <p>2\u5909\u6570\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\)</p> <p>\u3053\u3053\u3067\uff1a \\(y\\): \u53cd\u5fdc\u5909\u6570 \\(x_1, x_2\\): 2\u3064\u306e\u8aac\u660e\u5909\u6570 \\(\\beta_0\\): \u5207\u7247 \\(\\beta_1, \\beta_2\\): \u5404\u8aac\u660e\u5909\u6570\u306e\u56de\u5e30\u4fc2\u6570 \\(\\varepsilon\\): \u8aa4\u5dee\u9805  </p> <p>\u3053\u306e\u30e2\u30c7\u30eb\u306f3\u6b21\u5143\u7a7a\u9593\u3067\u306f\u300c\u5e73\u9762\u300d\u3068\u3057\u3066\u8868\u73fe\u3055\u308c\u307e\u3059\u3002</p> <p>\u3088\u308a\u4e00\u822c\u7684\u306b\u3001\\(p\\)\u500b\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\uff1a</p> <p>\u591a\u5909\u91cf\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\varepsilon\\)</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#33","title":"3.3 \u884c\u5217\u8868\u8a18\u306b\u3088\u308b\u8868\u73fe","text":"<p>\u30c7\u30fc\u30bf\u304c\\(n\\)\u500b\u306e\u89b3\u6e2c\u5024\u3092\u6301\u3061\u3001\u5404\u89b3\u6e2c\u5024\u306b\u5bfe\u3057\u3066\\(p\\)\u500b\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u3001\u884c\u5217\u8868\u8a18\u3092\u7528\u3044\u308b\u3068\uff1a</p> <p>\u884c\u5217\u5f62\u5f0f\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb: \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\)</p> <p>\u3053\u3053\u3067\uff1a \\(\\mathbf{y}\\): \\(n \\times 1\\)\u306e\u53cd\u5fdc\u5909\u6570\u30d9\u30af\u30c8\u30eb \\(\\mathbf{X}\\): \\(n \\times (p+1)\\)\u306e\u30c7\u30b6\u30a4\u30f3\u884c\u5217\uff08\u8aac\u660e\u5909\u6570\u884c\u5217\uff09 \\(\\boldsymbol{\\beta}\\): \\((p+1) \\times 1\\)\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{\\varepsilon}\\): \\(n \\times 1\\)\u306e\u8aa4\u5dee\u30d9\u30af\u30c8\u30eb  </p> <p>\u5177\u4f53\u7684\u306b\u30012\u3064\u306e\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u30e2\u30c7\u30eb\u306e\u5834\u5408\uff1a</p> \\[\\mathbf{X} =  \\begin{bmatrix}  1 &amp; x_{11} &amp; x_{12} \\\\ 1 &amp; x_{21} &amp; x_{22} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} \\end{bmatrix}\\] \\[\\boldsymbol{\\beta} =  \\begin{bmatrix}  \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\\] <p>\u6700\u521d\u306e\u5217\u306e1\u306f\u5207\u7247\\(\\beta_0\\)\u306b\u5bfe\u5fdc\u3059\u308b\u5217\u3067\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/18-system-of-linear-equation/#41","title":"4.1 \u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u63a8\u5b9a","text":"<p>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u63a8\u5b9a\u306b\u306f\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3092\u7528\u3044\u307e\u3059\u3002\u8aa4\u5dee\u306e\u4e8c\u4e57\u548c\uff1a</p> \\[S(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}))^2 = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2\\] <p>\u3092\u6700\u5c0f\u5316\u3059\u308b\\(\\boldsymbol{\\beta}\\)\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\u884c\u5217\u8868\u8a18\u3067\u8868\u3059\u3068\uff1a</p> \\[S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\]"},{"location":"lectures/LA/18-system-of-linear-equation/#42","title":"4.2 \u6b63\u898f\u65b9\u7a0b\u5f0f\u306e\u5c0e\u51fa","text":"<p>\\(S(\\boldsymbol{\\beta})\\)\u3092\u6700\u5c0f\u5316\u3059\u308b\u305f\u3081\u306b\u3001\\(\\boldsymbol{\\beta}\\)\u306b\u95a2\u3059\u308b\u504f\u5fae\u5206\u30920\u3068\u304a\u304d\u307e\u3059\uff1a</p> \\[\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}\\] <p>\u3053\u308c\u3092\u6574\u7406\u3059\u308b\u3068\uff1a</p> \\[\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\\] <p>\u3053\u308c\u304c\u6b63\u898f\u65b9\u7a0b\u5f0f\u3067\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#43","title":"4.3 \u6b63\u898f\u65b9\u7a0b\u5f0f\u306e\u89e3","text":"<p>\u6b63\u898f\u65b9\u7a0b\u5f0f\u304b\u3089\\(\\boldsymbol{\\beta}\\)\u306e\u63a8\u5b9a\u5024\u3092\u6c42\u3081\u308b\u3068\uff1a</p> <p>\u6700\u5c0f\u4e8c\u4e57\u63a8\u5b9a\u91cf: \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)</p> <p>\u305f\u3060\u3057\u3001\\(\\mathbf{X}^T\\mathbf{X}\\)\u304c\u6b63\u5247\uff08\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\uff09\u3053\u3068\u304c\u5fc5\u8981\u3067\u3059\u3002\u3053\u308c\u306f\u8aac\u660e\u5909\u6570\u9593\u306b\u5b8c\u5168\u306a\u7dda\u5f62\u95a2\u4fc2\u304c\u306a\u3044\uff08\u591a\u91cd\u5171\u7dda\u6027\u304c\u306a\u3044\uff09\u5834\u5408\u306b\u6210\u7acb\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#44","title":"4.4 \u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":"<p>\u884c\u5217\\(\\mathbf{P} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\u306f\u5c04\u5f71\u884c\u5217\u3068\u547c\u3070\u308c\u3001\\(\\mathbf{y}\\)\u3092\\(\\mathbf{X}\\)\u306e\u5217\u7a7a\u9593\u306b\u5c04\u5f71\u3059\u308b\u5f79\u5272\u3092\u6301\u3061\u307e\u3059\u3002</p> <p>\u4e88\u6e2c\u5024\\(\\hat{\\mathbf{y}}\\)\u306f\uff1a</p> \\[\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} = \\mathbf{P}\\mathbf{y}\\] <p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\\(\\mathbf{y}\\)\u3068\u4e88\u6e2c\u5024\\(\\hat{\\mathbf{y}}\\)\u306e\u5dee\uff08\u6b8b\u5dee\uff09\\(\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}\\)\u306f\\(\\mathbf{X}\\)\u306e\u5217\u7a7a\u9593\u306b\u76f4\u4ea4\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#45-2","title":"4.5 2\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u5e7e\u4f55\u5b66\u7684\u30a4\u30e1\u30fc\u30b8","text":"<p>2\u3064\u306e\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306f3\u6b21\u5143\u7a7a\u9593\u306b\u304a\u3051\u308b\u5e73\u9762\u3092\u8868\u3057\u307e\u3059\uff1a</p> \\[z = \\beta_0 + \\beta_1 x + \\beta_2 y\\] <p>\u5404\u89b3\u6e2c\u70b9\\((x_i, y_i, z_i)\\)\u304c\u3042\u308a\u3001\u56de\u5e30\u5e73\u9762\u306f\u3053\u308c\u3089\u306e\u70b9\u304b\u3089\u306e\u5782\u76f4\u8ddd\u96e2\u306e\u4e8c\u4e57\u548c\u3092\u6700\u5c0f\u306b\u3059\u308b\u5e73\u9762\u3067\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#5","title":"5. \u5177\u4f53\u4f8b","text":""},{"location":"lectures/LA/18-system-of-linear-equation/#51","title":"5.1 \u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u63a8\u5b9a","text":"<p>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u63a8\u5b9a\u306b\u306f\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3092\u7528\u3044\u307e\u3059\u3002\u8aa4\u5dee\u306e\u4e8c\u4e57\u548c\uff1a</p> \\[S(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}))^2 = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2\\] <p>\u3092\u6700\u5c0f\u5316\u3059\u308b\\(\\boldsymbol{\\beta}\\)\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\u884c\u5217\u8868\u8a18\u3067\u8868\u3059\u3068\uff1a</p> \\[S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\] <p>\u5177\u4f53\u4f8b\uff1a \u4ee5\u4e0b\u306e5\u3064\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> i \\(x_{i1}\\) \\(x_{i2}\\) \\(y_i\\) 1 2 3 10 2 1 5 12 3 3 2 11 4 4 4 18 5 5 1 15 <p>\u3053\u306e\u3068\u304d\u3001</p> \\[\\mathbf{y} = \\begin{bmatrix} 10 \\\\ 12 \\\\ 11 \\\\ 18 \\\\ 15 \\end{bmatrix}, \\quad \\mathbf{X} = \\begin{bmatrix}  1 &amp; 2 &amp; 3 \\\\ 1 &amp; 1 &amp; 5 \\\\ 1 &amp; 3 &amp; 2 \\\\ 1 &amp; 4 &amp; 4 \\\\ 1 &amp; 5 &amp; 1  \\end{bmatrix}, \\quad \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\\] <p>\u6700\u5c0f\u5316\u3057\u305f\u3044\u8aa4\u5dee\u4e8c\u4e57\u548c\u306f\uff1a \\(\\(S(\\boldsymbol{\\beta}) = (10 - \\beta_0 - 2\\beta_1 - 3\\beta_2)^2 + (12 - \\beta_0 - \\beta_1 - 5\\beta_2)^2 + \\cdots + (15 - \\beta_0 - 5\\beta_1 - \\beta_2)^2\\)\\)</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#52","title":"5.2 \u6b63\u898f\u65b9\u7a0b\u5f0f\u306e\u5c0e\u51fa","text":"<p>\\(S(\\boldsymbol{\\beta})\\)\u3092\u6700\u5c0f\u5316\u3059\u308b\u305f\u3081\u306b\u3001\\(\\boldsymbol{\\beta}\\)\u306b\u95a2\u3059\u308b\u504f\u5fae\u5206\u30920\u3068\u304a\u304d\u307e\u3059\uff1a</p> \\[\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}\\] <p>\u3053\u308c\u3092\u6574\u7406\u3059\u308b\u3068\uff1a</p> \\[\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\\] <p>\u3053\u308c\u304c\u6b63\u898f\u65b9\u7a0b\u5f0f\u3067\u3059\u3002</p> <p>\u8a08\u7b97\u4f8b\uff1a \u5148\u307b\u3069\u306e\u4f8b\u3067\\(\\mathbf{X}^T\\mathbf{X}\\)\u3068\\(\\mathbf{X}^T\\mathbf{y}\\)\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}  1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 1 &amp; 3 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 2 &amp; 4 &amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix}  1 &amp; 2 &amp; 3 \\\\ 1 &amp; 1 &amp; 5 \\\\ 1 &amp; 3 &amp; 2 \\\\ 1 &amp; 4 &amp; 4 \\\\ 1 &amp; 5 &amp; 1  \\end{bmatrix} =  \\begin{bmatrix}  5 &amp; 15 &amp; 15 \\\\ 15 &amp; 55 &amp; 37 \\\\ 15 &amp; 37 &amp; 55 \\end{bmatrix}\\] \\[\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix}  1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 1 &amp; 3 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 2 &amp; 4 &amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 10 \\\\ 12 \\\\ 11 \\\\ 18 \\\\ 15 \\end{bmatrix} =  \\begin{bmatrix}  66 \\\\ 219 \\\\ 178 \\end{bmatrix}\\] <p>\u6b63\u898f\u65b9\u7a0b\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a \\(\\(\\begin{bmatrix}  5 &amp; 15 &amp; 15 \\\\ 15 &amp; 55 &amp; 37 \\\\ 15 &amp; 37 &amp; 55 \\end{bmatrix} \\cdot \\begin{bmatrix}  \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix} =  \\begin{bmatrix}  66 \\\\ 219 \\\\ 178 \\end{bmatrix}\\)\\)</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#53","title":"5.3 \u6b63\u898f\u65b9\u7a0b\u5f0f\u306e\u89e3","text":"<p>\u6b63\u898f\u65b9\u7a0b\u5f0f\u304b\u3089\\(\\boldsymbol{\\beta}\\)\u306e\u63a8\u5b9a\u5024\u3092\u6c42\u3081\u308b\u3068\uff1a</p> <p>\u6700\u5c0f\u4e8c\u4e57\u63a8\u5b9a\u91cf: \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)</p> <p>\u305f\u3060\u3057\u3001\\(\\mathbf{X}^T\\mathbf{X}\\)\u304c\u6b63\u5247\uff08\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\uff09\u3053\u3068\u304c\u5fc5\u8981\u3067\u3059\u3002\u3053\u308c\u306f\u8aac\u660e\u5909\u6570\u9593\u306b\u5b8c\u5168\u306a\u7dda\u5f62\u95a2\u4fc2\u304c\u306a\u3044\uff08\u591a\u91cd\u5171\u7dda\u6027\u304c\u306a\u3044\uff09\u5834\u5408\u306b\u6210\u7acb\u3057\u307e\u3059\u3002</p> <p>\u8a08\u7b97\u4f8b\u306e\u7d9a\u304d\uff1a \u5148\u307b\u3069\u306e\u6b63\u898f\u65b9\u7a0b\u5f0f\u306e\u89e3\u3092\u6c42\u3081\u308b\u306b\u306f\u3001\u307e\u305a\\((\\mathbf{X}^T\\mathbf{X})^{-1}\\)\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> \\[(\\mathbf{X}^T\\mathbf{X})^{-1} = \\begin{bmatrix}  5 &amp; 15 &amp; 15 \\\\ 15 &amp; 55 &amp; 37 \\\\ 15 &amp; 37 &amp; 55 \\end{bmatrix}^{-1}\\] <p>\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u8a08\u7b97\u3059\u308b\u3068\uff1a \\(\\((\\mathbf{X}^T\\mathbf{X})^{-1} \\approx \\begin{bmatrix}  2.12 &amp; -0.39 &amp; -0.29 \\\\ -0.39 &amp; 0.16 &amp; -0.03 \\\\ -0.29 &amp; -0.03 &amp; 0.13 \\end{bmatrix}\\)\\)</p> <p>\u3053\u308c\u3092\\(\\mathbf{X}^T\\mathbf{y}\\)\u306b\u4e57\u3058\u3066\\(\\hat{\\boldsymbol{\\beta}}\\)\u3092\u6c42\u3081\u307e\u3059\uff1a \\(\\(\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}  2.12 &amp; -0.39 &amp; -0.29 \\\\ -0.39 &amp; 0.16 &amp; -0.03 \\\\ -0.29 &amp; -0.03 &amp; 0.13 \\end{bmatrix} \\cdot \\begin{bmatrix}  66 \\\\ 219 \\\\ 178 \\end{bmatrix} \\approx \\begin{bmatrix}  3.24 \\\\ 2.15 \\\\ 1.63 \\end{bmatrix}\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\hat{\\beta}_0 \\approx 3.24\\)\u3001\\(\\hat{\\beta}_1 \\approx 2.15\\)\u3001\\(\\hat{\\beta}_2 \\approx 1.63\\)\u3068\u306a\u308a\u3001\u56de\u5e30\u5f0f\u306f\uff1a \\(\\(\\hat{y} = 3.24 + 2.15x_1 + 1.63x_2\\)\\)</p> <p>\u3053\u306e\u7d50\u679c\u304b\u3089\u3001\\(x_1\\)\u304c1\u5358\u4f4d\u5897\u52a0\u3059\u308b\u3068\\(y\\)\u306f\u7d042.15\u5358\u4f4d\u5897\u52a0\u3057\u3001\\(x_2\\)\u304c1\u5358\u4f4d\u5897\u52a0\u3059\u308b\u3068\\(y\\)\u306f\u7d041.63\u5358\u4f4d\u5897\u52a0\u3059\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#54","title":"5.4 \u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":"<p>\u884c\u5217\\(\\mathbf{P} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\u306f\u5c04\u5f71\u884c\u5217\u3068\u547c\u3070\u308c\u3001\\(\\mathbf{y}\\)\u3092\\(\\mathbf{X}\\)\u306e\u5217\u7a7a\u9593\u306b\u5c04\u5f71\u3059\u308b\u5f79\u5272\u3092\u6301\u3061\u307e\u3059\u3002</p> <p>\u4e88\u6e2c\u5024\\(\\hat{\\mathbf{y}}\\)\u306f\uff1a</p> \\[\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} = \\mathbf{P}\\mathbf{y}\\] <p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\\(\\mathbf{y}\\)\u3068\u4e88\u6e2c\u5024\\(\\hat{\\mathbf{y}}\\)\u306e\u5dee\uff08\u6b8b\u5dee\uff09\\(\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}\\)\u306f\\(\\mathbf{X}\\)\u306e\u5217\u7a7a\u9593\u306b\u76f4\u4ea4\u3057\u307e\u3059\u3002</p> <p>\u5177\u4f53\u4f8b\u306e\u7d9a\u304d\uff1a \u5148\u307b\u3069\u306e\u4f8b\u3067\u5c04\u5f71\u884c\u5217\\(\\mathbf{P}\\)\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> \\[\\mathbf{P} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\] <p>\u3053\u308c\u3092\u7528\u3044\u3066\u4e88\u6e2c\u5024\\(\\hat{\\mathbf{y}}\\)\u3092\u6c42\u3081\u308b\u3068\uff1a</p> \\[\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} =  \\begin{bmatrix}  1 &amp; 2 &amp; 3 \\\\ 1 &amp; 1 &amp; 5 \\\\ 1 &amp; 3 &amp; 2 \\\\ 1 &amp; 4 &amp; 4 \\\\ 1 &amp; 5 &amp; 1  \\end{bmatrix} \\cdot \\begin{bmatrix}  3.24 \\\\ 2.15 \\\\ 1.63 \\end{bmatrix} =  \\begin{bmatrix}  11.13 \\\\ 13.39 \\\\ 11.67 \\\\ 17.56 \\\\ 14.25 \\end{bmatrix}\\] <p>\u5b9f\u969b\u306e\\(\\mathbf{y}\\)\u3068\u306e\u5dee\uff08\u6b8b\u5dee\uff09\u306f\uff1a</p> \\[\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} =  \\begin{bmatrix}  10 \\\\ 12 \\\\ 11 \\\\ 18 \\\\ 15 \\end{bmatrix} -  \\begin{bmatrix}  11.13 \\\\ 13.39 \\\\ 11.67 \\\\ 17.56 \\\\ 14.25 \\end{bmatrix} =  \\begin{bmatrix}  -1.13 \\\\ -1.39 \\\\ -0.67 \\\\ 0.44 \\\\ 0.75 \\end{bmatrix}\\] <p>\u3053\u306e\u6b8b\u5dee\u30d9\u30af\u30c8\u30eb\\(\\mathbf{e}\\)\u306f\\(\\mathbf{X}\\)\u306e\u5217\u7a7a\u9593\u306b\u76f4\u4ea4\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u4ee5\u4e0b\u304c\u6210\u308a\u7acb\u3061\u307e\u3059\uff1a</p> \\[\\mathbf{X}^T\\mathbf{e} = \\mathbf{0}\\] <p>\u5b9f\u969b\u306b\u78ba\u8a8d\u3059\u308b\u3068\uff1a</p> \\[\\mathbf{X}^T\\mathbf{e} =  \\begin{bmatrix}  1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 1 &amp; 3 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 2 &amp; 4 &amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix}  -1.13 \\\\ -1.39 \\\\ -0.67 \\\\ 0.44 \\\\ 0.75 \\end{bmatrix} \\approx \\begin{bmatrix}  0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\] <p>\u3053\u308c\u306f\u6570\u5024\u8a08\u7b97\u306e\u8aa4\u5dee\u3092\u9664\u3051\u30700\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u3001\u6b8b\u5dee\u30d9\u30af\u30c8\u30eb\\(\\mathbf{e}\\)\u304c\\(\\mathbf{X}\\)\u306e\u5217\u7a7a\u9593\u306b\u76f4\u4ea4\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#55-2","title":"5.5 2\u3064\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306e\u5e7e\u4f55\u5b66\u7684\u30a4\u30e1\u30fc\u30b8","text":"<p>2\u3064\u306e\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306f3\u6b21\u5143\u7a7a\u9593\u306b\u304a\u3051\u308b\u5e73\u9762\u3092\u8868\u3057\u307e\u3059\uff1a</p> \\[z = \\beta_0 + \\beta_1 x + \\beta_2 y\\] <p>\u5404\u89b3\u6e2c\u70b9\\((x_i, y_i, z_i)\\)\u304c\u3042\u308a\u3001\u56de\u5e30\u5e73\u9762\u306f\u3053\u308c\u3089\u306e\u70b9\u304b\u3089\u306e\u5782\u76f4\u8ddd\u96e2\u306e\u4e8c\u4e57\u548c\u3092\u6700\u5c0f\u306b\u3059\u308b\u5e73\u9762\u3067\u3059\u3002</p> <p>\u5177\u4f53\u4f8b\uff1a \u5148\u307b\u3069\u306e5\u3064\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u30923\u6b21\u5143\u7a7a\u9593\u306b\u30d7\u30ed\u30c3\u30c8\u3059\u308b\u3068\uff1a - \u70b91: \\((2, 3, 10)\\) - \u70b92: \\((1, 5, 12)\\) - \u70b93: \\((3, 2, 11)\\) - \u70b94: \\((4, 4, 18)\\) - \u70b95: \\((5, 1, 15)\\)</p> <p>\u6c42\u3081\u305f\u56de\u5e30\u5e73\u9762\u306f\uff1a\\(z = 3.24 + 2.15x + 1.63y\\)</p> <p>\u3053\u306e\u5e73\u9762\u306f3\u6b21\u5143\u7a7a\u9593\u5185\u306e\u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u304b\u3089\u5782\u76f4\u8ddd\u96e2\u306e\u4e8c\u4e57\u548c\u304c\u6700\u5c0f\u306b\u306a\u308b\u3088\u3046\u306b\u914d\u7f6e\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u5404\u70b9\u304b\u3089\u5e73\u9762\u3078\u306e\u5782\u76f4\u8ddd\u96e2\u306f\u6b8b\u5dee\u306b\u5bfe\u5fdc\u3057\u3001\u305d\u306e\u4e8c\u4e57\u548c\uff1a</p> \\[\\sum_{i=1}^5 e_i^2 = (-1.13)^2 + (-1.39)^2 + (-0.67)^2 + (0.44)^2 + (0.75)^2 = 4.54\\] <p>\u304c\u6700\u5c0f\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p> <p>\u7570\u306a\u308b\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\)\u306e\u5024\u3067\u5b9a\u7fa9\u3055\u308c\u308b\u4ed6\u306e\u3069\u306e\u5e73\u9762\u3067\u3082\u3001\u3053\u306e\u5e73\u65b9\u548c\u306f4.54\u3088\u308a\u5927\u304d\u304f\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u304c\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306e\u672c\u8cea\u3067\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#6-python","title":"6. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/18-system-of-linear-equation/#61-2","title":"6.1 2\u5909\u6570\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.linear_model import LinearRegression\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u751f\u6210\nnp.random.seed(42)\nn = 100\nX = np.random.rand(n, 2) * 10\nbeta_true = np.array([5, 2, -1])  # \u771f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf: \u5207\u7247, beta_1, beta_2\ny = beta_true[0] + beta_true[1] * X[:, 0] + beta_true[2] * X[:, 1] + np.random.randn(n) * 2\n\n# \u30c7\u30b6\u30a4\u30f3\u884c\u5217\u306e\u4f5c\u6210\nX_design = np.column_stack([np.ones(n), X])\n\n# \u6b63\u898f\u65b9\u7a0b\u5f0f\u306b\u3088\u308b\u89e3\nbeta_hat = np.linalg.inv(X_design.T @ X_design) @ X_design.T @ y\nprint(\"\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u63a8\u5b9a\u5024:\")\nprint(f\"\u03b2\u2080 (\u5207\u7247): {beta_hat[0]:.4f}\")\nprint(f\"\u03b2\u2081: {beta_hat[1]:.4f}\")\nprint(f\"\u03b2\u2082: {beta_hat[2]:.4f}\")\n\n# scikit-learn\u306b\u3088\u308b\u89e3\nmodel = LinearRegression()\nmodel.fit(X, y)\nprint(\"\\nscikit-learn\u306b\u3088\u308b\u63a8\u5b9a\u5024:\")\nprint(f\"\u03b2\u2080 (\u5207\u7247): {model.intercept_:.4f}\")\nprint(f\"\u03b2\u2081: {model.coef_[0]:.4f}\")\nprint(f\"\u03b2\u2082: {model.coef_[1]:.4f}\")\n\n# \u4e88\u6e2c\u5024\u306e\u8a08\u7b97\ny_hat = X_design @ beta_hat\n\n# \u6c7a\u5b9a\u4fc2\u6570\uff08R\u00b2\uff09\u306e\u8a08\u7b97\nSS_total = np.sum((y - np.mean(y))**2)\nSS_residual = np.sum((y - y_hat)**2)\nr_squared = 1 - SS_residual / SS_total\nprint(f\"\\n\u6c7a\u5b9a\u4fc2\u6570 (R\u00b2): {r_squared:.4f}\")\n</code></pre> <p>\u51fa\u529b\u4f8b: <pre><code>\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u63a8\u5b9a\u5024:\n\u03b2\u2080 (\u5207\u7247): 5.3361\n\u03b2\u2081: 1.9629\n\u03b2\u2082: -1.0743\n\nscikit-learn\u306b\u3088\u308b\u63a8\u5b9a\u5024:\n\u03b2\u2080 (\u5207\u7247): 5.3361\n\u03b2\u2081: 1.9629\n\u03b2\u2082: -1.0743\n\n\u6c7a\u5b9a\u4fc2\u6570 (R\u00b2): 0.8956\n</code></pre></p>"},{"location":"lectures/LA/18-system-of-linear-equation/#62-3d","title":"6.2 3D\u53ef\u8996\u5316","text":"<pre><code># 3D\u30d7\u30ed\u30c3\u30c8\u306b\u3088\u308b\u53ef\u8996\u5316\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# \u5143\u30c7\u30fc\u30bf\u306e\u30d7\u30ed\u30c3\u30c8\nax.scatter(X[:, 0], X[:, 1], y, c='blue', marker='o', alpha=0.6, label='\u89b3\u6e2c\u30c7\u30fc\u30bf')\n\n# \u56de\u5e30\u5e73\u9762\u306e\u30d7\u30ed\u30c3\u30c8\nx_surf = np.linspace(0, 10, 20)\ny_surf = np.linspace(0, 10, 20)\nx_surf, y_surf = np.meshgrid(x_surf, y_surf)\nz_surf = beta_hat[0] + beta_hat[1] * x_surf + beta_hat[2] * y_surf\nax.plot_surface(x_surf, y_surf, z_surf, alpha=0.3, color='red')\n\n# \u5b9f\u969b\u306e\u70b9\u304b\u3089\u5e73\u9762\u3078\u306e\u5782\u7dda\u3092\u63cf\u753b\nfor i in range(0, n, 10):  # 10\u70b9\u304a\u304d\u306b\u8868\u793a\n    z_plane = beta_hat[0] + beta_hat[1] * X[i, 0] + beta_hat[2] * X[i, 1]\n    ax.plot([X[i, 0], X[i, 0]], [X[i, 1], X[i, 1]], [y[i], z_plane], 'k-', alpha=0.2)\n\nax.set_xlabel('X\u2081')\nax.set_ylabel('X\u2082')\nax.set_zlabel('Y')\nax.set_title('2\u5909\u6570\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb: 3D\u53ef\u8996\u5316')\nax.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/18-system-of-linear-equation/#63","title":"6.3 \u5b9f\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u5206\u6790\u4f8b","text":"<pre><code># Boston\u4f4f\u5b85\u4fa1\u683c\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u305f\u5b9f\u4f8b\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nboston = load_boston()\nX = boston.data[:, [5, 12]]  # RM\uff08\u90e8\u5c4b\u6570\uff09\u3068LSTAT\uff08\u4f4e\u6240\u5f97\u8005\u5272\u5408\uff09\ny = boston.target\n\n# \u30c7\u30fc\u30bf\u306e\u5206\u5272\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# \u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# \u7d50\u679c\u306e\u8868\u793a\nprint(\"\u56de\u5e30\u4fc2\u6570:\")\nprint(f\"\u5207\u7247: {model.intercept_:.4f}\")\nprint(f\"RM (\u90e8\u5c4b\u6570): {model.coef_[0]:.4f}\")\nprint(f\"LSTAT (\u4f4e\u6240\u5f97\u8005\u5272\u5408): {model.coef_[1]:.4f}\")\n\n# \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"R\u00b2: {r2:.4f}\")\n\n# 3D\u30d7\u30ed\u30c3\u30c8\u306b\u3088\u308b\u53ef\u8996\u5316\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# \u30c7\u30fc\u30bf\u306e\u30d7\u30ed\u30c3\u30c8\nax.scatter(X_test[:, 0], X_test[:, 1], y_test, c='blue', marker='o', alpha=0.6, label='\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf')\n\n# \u56de\u5e30\u5e73\u9762\u306e\u30d7\u30ed\u30c3\u30c8\nx_surf = np.linspace(min(X[:, 0]), max(X[:, 0]), 20)\ny_surf = np.linspace(min(X[:, 1]), max(X[:, 1]), 20)\nx_surf, y_surf = np.meshgrid(x_surf, y_surf)\nz_surf = model.intercept_ + model.coef_[0] * x_surf + model.coef_[1] * y_surf\nax.plot_surface(x_surf, y_surf, z_surf, alpha=0.3, color='red')\n\nax.set_xlabel('\u90e8\u5c4b\u6570 (RM)')\nax.set_ylabel('\u4f4e\u6240\u5f97\u8005\u5272\u5408 (LSTAT)')\nax.set_zlabel('\u4f4f\u5b85\u4fa1\u683c (MEDV)')\nax.set_title('Boston\u4f4f\u5b85\u4fa1\u683c: 2\u5909\u6570\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb')\nax.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/18-system-of-linear-equation/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/18-system-of-linear-equation/#71","title":"7.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li>\u8a08\u7b97\u554f\u984c: \u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30012\u3064\u306e\u8aac\u660e\u5909\u6570\u3092\u6301\u3064\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u4f7f\u3063\u3066\u6c42\u3081\u306a\u3055\u3044\u3002</li> </ol> ID x\u2081 x\u2082 y 1 1 2 5 2 2 1 6 3 3 3 8 4 4 2 10 5 5 4 12 <p>\u89e3\u6790\u7684\u306b\\(\\mathbf{X}^T\\mathbf{X}\\)\u3001\\(\\mathbf{X}^T\\mathbf{y}\\)\u3001\\((\\mathbf{X}^T\\mathbf{X})^{-1}\\)\u3092\u8a08\u7b97\u3057\u3001\u6700\u7d42\u7684\u306b\\(\\hat{\\boldsymbol{\\beta}}\\)\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> <ol> <li> <p>\u7406\u8ad6\u554f\u984c: 2\u3064\u306e\u8aac\u660e\u5909\u6570\\(x_1\\)\u3068\\(x_2\\)\u306e\u9593\u306b\u5b8c\u5168\u306a\u7dda\u5f62\u95a2\u4fc2\uff08\u4f8b\u3048\u3070\\(x_2 = 2x_1\\)\uff09\u304c\u3042\u308b\u5834\u5408\u3001\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u63a8\u5b9a\u306b\u3069\u306e\u3088\u3046\u306a\u554f\u984c\u304c\u751f\u3058\u308b\u304b\u3001\u884c\u5217\\(\\mathbf{X}^T\\mathbf{X}\\)\u306e\u89b3\u70b9\u304b\u3089\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u6982\u5ff5\u554f\u984c: \u5358\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u3001\u6700\u5c0f\u4e8c\u4e57\u63a8\u5b9a\u91cf\u306f\u5e73\u9762\u4e0a\u306e\u6700\u9069\u306a\u300c\u76f4\u7dda\u300d\u3092\u8868\u3057\u307e\u3059\u304c\u30012\u5909\u6570\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306f\u4f55\u3092\u8868\u3059\u306e\u304b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002\u3053\u308c\u3092\u5e7e\u4f55\u5b66\u7684\u306b\u89e3\u91c8\u3057\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/18-system-of-linear-equation/#72","title":"7.2 \u5fdc\u7528\u554f\u984c","text":"<ol> <li>\u5fdc\u7528\u8a08\u7b97\u554f\u984c: \u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u306f\u3001\u5e74\u9f62(\\(x_1\\))\u3068\u6559\u80b2\u5e74\u6570(\\(x_2\\))\u304b\u3089\u5e74\u53ce(\\(y\\), \u5358\u4f4d:\u4e07\u5186)\u3092\u4e88\u6e2c\u3059\u308b\u3082\u306e\u3067\u3059\u3002</li> </ol> ID \u5e74\u9f62(\\(x_1\\)) \u6559\u80b2\u5e74\u6570(\\(x_2\\)) \u5e74\u53ce(\\(y\\)) 1 25 12 300 2 30 16 450 3 35 12 400 4 40 14 500 5 45 18 650 6 50 12 550 <p>a) 2\u5909\u6570\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\)\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u7528\u3044\u3066\u6c42\u3081\u306a\u3055\u3044\u3002    b) \u5e74\u9f62\u304c33\u6b73\u3067\u6559\u80b2\u5e74\u6570\u304c14\u5e74\u306e\u4eba\u306e\u5e74\u53ce\u3092\u4e88\u6e2c\u3057\u306a\u3055\u3044\u3002    c) \u5e74\u9f62\u3068\u5e74\u53ce\u306e\u95a2\u4fc2\u6027\u3001\u6559\u80b2\u5e74\u6570\u3068\u5e74\u53ce\u306e\u95a2\u4fc2\u6027\u3092\u305d\u308c\u305e\u308c\u89e3\u91c8\u3057\u306a\u3055\u3044\u3002</p> <ol> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5fdc\u7528\u554f\u984c: \u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u306f\u60a3\u8005\u306e\u4f53\u91cd(\\(x_1\\), kg)\u3068\u5e74\u9f62(\\(x_2\\), \u6b73)\u304b\u3089\u8840\u5727\u5024(\\(y\\), mmHg)\u3092\u4e88\u6e2c\u3059\u308b\u3082\u306e\u3067\u3059\u3002</li> </ol> ID \u4f53\u91cd(\\(x_1\\)) \u5e74\u9f62(\\(x_2\\)) \u8840\u5727(\\(y\\)) 1 55 25 110 2 60 30 115 3 65 40 120 4 70 45 125 5 75 50 130 6 80 55 135 7 85 60 140 8 90 65 145 <p>a) 2\u5909\u6570\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    b) \u4f53\u91cd75kg\u3001\u5e74\u9f6235\u6b73\u306e\u60a3\u8005\u306e\u8840\u5727\u3092\u4e88\u6e2c\u3057\u306a\u3055\u3044\u3002    c) \u4f53\u91cd\u3068\u8840\u5727\u306e\u95a2\u4fc2\u6027\u3001\u5e74\u9f62\u3068\u8840\u5727\u306e\u95a2\u4fc2\u6027\u3092\u305d\u308c\u305e\u308c\u89e3\u91c8\u3057\u306a\u3055\u3044\u3002    d) \u5065\u5eb7\u7ba1\u7406\u306e\u89b3\u70b9\u304b\u3089\u3001\u3053\u306e\u30e2\u30c7\u30eb\u306e\u9650\u754c\u70b9\u30923\u3064\u6319\u3052\u306a\u3055\u3044\u3002</p> <ol> <li>\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u5206\u6790\u554f\u984c: \u4ee5\u4e0b\u306e4\u3064\u306e\u5909\u6570\u304b\u3089\u306a\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\uff1a</li> <li>\\(x_1\\): \u904b\u52d5\u6642\u9593\uff08\u5206/\u65e5\uff09</li> <li>\\(x_2\\): \u7761\u7720\u6642\u9593\uff08\u6642\u9593/\u65e5\uff09</li> <li>\\(y\\): \u75b2\u52b4\u5ea6\u30b9\u30b3\u30a2\uff080-100\uff09</li> </ol> <p>\u8907\u6570\u306e\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u8003\u3048\u307e\u3059\uff1a    - \u30e2\u30c7\u30eb1: \\(y = \\beta_0 + \\beta_1 x_1 + \\varepsilon\\)    - \u30e2\u30c7\u30eb2: \\(y = \\beta_0 + \\beta_2 x_2 + \\varepsilon\\)    - \u30e2\u30c7\u30eb3: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\)</p> <p>\u4ee5\u4e0b\u306e\u554f\u3044\u306b\u7b54\u3048\u306a\u3055\u3044\uff1a</p> <p>a) \u6c7a\u5b9a\u4fc2\u6570\\(R^2\\)\u306e\u89b3\u70b9\u304b\u3089\u3001\u30e2\u30c7\u30eb3\u306f\u30e2\u30c7\u30eb1\u304a\u3088\u3073\u30e2\u30c7\u30eb2\u3088\u308a\u3082\u5fc5\u305a\u9ad8\u3044\u5024\u3092\u793a\u3059\u304b\u3002\u7406\u7531\u3092\u8aac\u660e\u3057\u306a\u3055\u3044\u3002    b) \u591a\u5909\u91cf\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u591a\u91cd\u5171\u7dda\u6027\u306e\u554f\u984c\u3068\u305d\u306e\u5bfe\u51e6\u6cd5\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u306a\u3055\u3044\u3002    c) \u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306e\u89b3\u70b9\u304b\u3089\u3001\u75b2\u52b4\u5ea6\u3092\u4e88\u6e2c\u3059\u308b\u969b\u306b\u8003\u616e\u3059\u3079\u304d\u4ed6\u306e\u5909\u6570\u30923\u3064\u63d0\u6848\u3057\u3001\u305d\u308c\u305e\u308c\u304c\u3069\u306e\u3088\u3046\u306b\u75b2\u52b4\u5ea6\u306b\u5f71\u97ff\u3059\u308b\u3068\u4e88\u60f3\u3055\u308c\u308b\u304b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/18-system-of-linear-equation/#q1","title":"Q1: \u5358\u56de\u5e30\u3068\u591a\u5909\u91cf\u56de\u5e30\u306e\u5927\u304d\u306a\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A1: \u5358\u56de\u5e30\u3067\u306f1\u3064\u306e\u8aac\u660e\u5909\u6570\u306e\u307f\u3092\u4f7f\u7528\u3057\u30012\u6b21\u5143\u5e73\u9762\u4e0a\u306e\u76f4\u7dda\u3067\u30e2\u30c7\u30eb\u5316\u3057\u307e\u3059\u3002\u591a\u5909\u91cf\u56de\u5e30\u3067\u306f\u8907\u6570\u306e\u8aac\u660e\u5909\u6570\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u3001\u3088\u308a\u9ad8\u6b21\u5143\u306e\u7a7a\u9593\uff082\u5909\u6570\u306a\u30893\u6b21\u5143\u7a7a\u9593\u5185\u306e\u5e73\u9762\u30013\u5909\u6570\u306a\u30894\u6b21\u5143\u7a7a\u9593\u5185\u306e\u8d85\u5e73\u9762\uff09\u3067\u30e2\u30c7\u30eb\u5316\u3057\u307e\u3059\u3002\u6570\u5b66\u7684\u306b\u306f\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30d9\u30af\u30c8\u30eb\\(\\boldsymbol{\\beta}\\)\u306e\u6b21\u5143\u304c\u5897\u52a0\u3057\u3001\u30e2\u30c7\u30eb\u306e\u8aac\u660e\u529b\u304c\u5411\u4e0a\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u904e\u5b66\u7fd2\u306e\u30ea\u30b9\u30af\u3082\u9ad8\u307e\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#q2","title":"Q2: \u6b63\u898f\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u969b\u306b\u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u3069\u3046\u3059\u308c\u3070\u826f\u3044\u3067\u3059\u304b\uff1f","text":"<p>A2: \u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u3001\u3053\u308c\u306f\\(\\mathbf{X}^T\\mathbf{X}\\)\u304c\u7279\u7570\uff08singular\uff09\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u3001\u901a\u5e38\u306f\u8aac\u660e\u5909\u6570\u9593\u306b\u5b8c\u5168\u306a\u7dda\u5f62\u95a2\u4fc2\uff08\u591a\u91cd\u5171\u7dda\u6027\uff09\u304c\u3042\u308b\u3053\u3068\u304c\u539f\u56e0\u3067\u3059\u3002\u3053\u306e\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u65b9\u6cd5\u304c\u8003\u3048\u3089\u308c\u307e\u3059\uff1a 1. \u76f8\u95a2\u306e\u9ad8\u3044\u5909\u6570\u306e\u3046\u3061\u4e00\u65b9\u3092\u9664\u5916\u3059\u308b 2. \u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u3067\u6b21\u5143\u3092\u524a\u6e1b\u3059\u308b 3. \u6b63\u5247\u5316\uff08\u30ea\u30c3\u30b8\u56de\u5e30\u3084Lasso\u56de\u5e30\uff09\u3092\u9069\u7528\u3059\u308b 4. \u30e0\u30fc\u30a2\u30fb\u30da\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\u3092\u7528\u3044\u308b</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#q3","title":"Q3: \u8907\u6570\u306e\u8aac\u660e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u3001\u5404\u5909\u6570\u306e\u91cd\u8981\u5ea6\u306f\u3069\u306e\u3088\u3046\u306b\u5224\u65ad\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A3: \u5909\u6570\u306e\u91cd\u8981\u5ea6\u3092\u5224\u65ad\u3059\u308b\u306b\u306f\u4ee5\u4e0b\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u6a19\u6e96\u5316\u56de\u5e30\u4fc2\u6570\uff08\u5404\u5909\u6570\u3092\u6a19\u6e96\u5316\u3057\u3066\u304b\u3089\u56de\u5e30\u5206\u6790\u3092\u884c\u3044\u3001\u5f97\u3089\u308c\u305f\u4fc2\u6570\u3092\u6bd4\u8f03\uff09 2. t\u5024\u3084p\u5024\uff08\u5404\u4fc2\u6570\u306e\u7d71\u8a08\u7684\u6709\u610f\u6027\u3092\u8a55\u4fa1\uff09 3. \u5909\u6570\u9078\u629e\u6cd5\uff08\u30b9\u30c6\u30c3\u30d7\u30ef\u30a4\u30ba\u6cd5\u3001AIC\u3001BIC\u306a\u3069\uff09\u306e\u9069\u7528 4. \u90e8\u5206\u7684\u6c7a\u5b9a\u4fc2\u6570\uff08\u5404\u5909\u6570\u306e\u5bc4\u4e0e\u5ea6\u3092\u500b\u5225\u306b\u8a55\u4fa1\uff09</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#q4","title":"Q4: \u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u5358\u56de\u5e30\u3088\u308a\u591a\u5909\u91cf\u56de\u5e30\u304c\u9069\u5207\u3067\u3059\u304b\uff1f","text":"<p>A4: \u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u591a\u5909\u91cf\u56de\u5e30\u304c\u9069\u5207\u3067\u3059\uff1a 1. \u53cd\u5fdc\u5909\u6570\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u8981\u56e0\u304c\u8907\u6570\u3042\u308b\u5834\u5408 2. \u4ea4\u7d61\u56e0\u5b50\uff08confounding factor\uff09\u306e\u5f71\u97ff\u3092\u5236\u5fa1\u3057\u305f\u3044\u5834\u5408 3. \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u7cbe\u5ea6\u3092\u5411\u4e0a\u3055\u305b\u305f\u3044\u5834\u5408 4. \u8907\u6570\u306e\u8981\u56e0\u306e\u76f8\u5bfe\u7684\u306a\u5f71\u97ff\u3092\u6bd4\u8f03\u3057\u305f\u3044\u5834\u5408</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#q5","title":"Q5: \u591a\u5909\u91cf\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6\u8a55\u4fa1\u306b\u306f\u3069\u306e\u3088\u3046\u306a\u6307\u6a19\u304c\u4f7f\u308f\u308c\u307e\u3059\u304b\uff1f","text":"<p>A5: \u4e3b\u306a\u8a55\u4fa1\u6307\u6a19\u306b\u306f\u4ee5\u4e0b\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u6c7a\u5b9a\u4fc2\u6570\uff08\\(R^2\\)\uff09\uff1a\u30e2\u30c7\u30eb\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u5206\u6563\u306e\u5272\u5408 2. \u8abf\u6574\u6e08\u307f\u6c7a\u5b9a\u4fc2\u6570\uff08adjusted \\(R^2\\)\uff09\uff1a\u5909\u6570\u306e\u6570\u3092\u8003\u616e\u3057\u305f\\(R^2\\) 3. \u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\uff08MSE\uff09\u3084\u5e73\u5747\u7d76\u5bfe\u8aa4\u5dee\uff08MAE\uff09 4. \u60c5\u5831\u91cf\u898f\u6e96\uff08AIC\u3001BIC\uff09\uff1a\u30e2\u30c7\u30eb\u306e\u8907\u96d1\u3055\u3068\u30d5\u30a3\u30c3\u30c8\u306e\u30d0\u30e9\u30f3\u30b9\u3092\u8a55\u4fa1 5. \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u308b\u6c4e\u5316\u6027\u80fd\u306e\u8a55\u4fa1</p>"},{"location":"lectures/LA/18-system-of-linear-equation/#q6","title":"Q6: \u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u591a\u5909\u91cf\u56de\u5e30\u306e\u5178\u578b\u7684\u306a\u5fdc\u7528\u4f8b\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A6: \u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5fdc\u7528\u4f8b\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u8907\u6570\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u304b\u3089\u75be\u60a3\u30ea\u30b9\u30af\u3092\u4e88\u6e2c 2. \u751f\u6d3b\u7fd2\u6163\u8981\u56e0\uff08\u904b\u52d5\u3001\u98df\u4e8b\u3001\u7761\u7720\u306a\u3069\uff09\u304b\u3089\u5065\u5eb7\u6307\u6a19\u3092\u4e88\u6e2c 3. \u51e6\u65b9\u85ac\u306e\u8907\u5408\u52b9\u679c\u306e\u5206\u6790 4. \u5e74\u9f62\u30fb\u6027\u5225\u30fb\u907a\u4f1d\u7684\u8981\u56e0\u306a\u3069\u3092\u8003\u616e\u3057\u305f\u75be\u60a3\u9032\u884c\u4e88\u6e2c 5. \u74b0\u5883\u8981\u56e0\u3068\u5065\u5eb7\u30a2\u30a6\u30c8\u30ab\u30e0\u306e\u95a2\u9023\u5206\u6790</p>"},{"location":"lectures/LA/19-exercise/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II - \u7b2c19\u56de \u7dcf\u5408\u6f14\u7fd2","text":""},{"location":"lectures/LA/19-exercise/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c19\u56de \u95a2\u9023\u9805\u76ee: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3001\u30e9\u30f3\u30af\u3001\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3001\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9:  - \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe - \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u95a2\u4fc2 - \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u57fa\u672c\u6982\u5ff5</p>"},{"location":"lectures/LA/19-exercise/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u65e5\u306e\u7dcf\u5408\u6f14\u7fd2\u3067\u306f\u3001\u7b2c10\u56de\u301c\u7b2c18\u56de\u3067\u5b66\u3093\u3060\u5185\u5bb9\u306e\u7406\u89e3\u5ea6\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\u7279\u306b\u4ee5\u4e0b\u306e\u70b9\u306b\u91cd\u70b9\u3092\u7f6e\u304d\u307e\u3059\uff1a</p> <ol> <li>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u884c\u5217\u5f62\u5f0f\u3067\u8868\u73fe\u3057\u3001\u305d\u306e\u89e3\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3057\u3001\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3053\u3068\u304c\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u9006\u884c\u5217\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u308b</li> <li>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u6570\u5b66\u7684\u8868\u73fe\u3068\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u89e3\u6cd5\u3092\u7406\u89e3\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/19-exercise/#3","title":"3. \u57fa\u672c\u6982\u5ff5\u306e\u5fa9\u7fd2","text":""},{"location":"lectures/LA/19-exercise/#31","title":"3.1 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u884c\u5217\u8868\u73fe","text":"<p>\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306f\u884c\u5217\u3092\u7528\u3044\u3066\u7c21\u6f54\u306b\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9: \\(n\\)\u500b\u306e\u672a\u77e5\u6570 \\(x_1, x_2, \\ldots, x_n\\) \u306b\u95a2\u3059\u308b\\(m\\)\u500b\u306e\u4e00\u6b21\u65b9\u7a0b\u5f0f\u304b\u3089\u306a\u308b\u9023\u7acb\u65b9\u7a0b\u5f0f \\(\\(\\begin{cases} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1 \\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2 \\\\ \\vdots \\\\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n = b_m \\end{cases}\\)\\)</p> <p>\u3053\u308c\u306f\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u6b21\u306e\u3088\u3046\u306b\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a \\(\\(A\\mathbf{x} = \\mathbf{b}\\)\\) \u3053\u3053\u3067\u3001\\(A = (a_{ij})\\) \u306f \\(m \\times n\\) \u884c\u5217\u3001\\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)^T\\) \u306f \\(n\\) \u6b21\u5143\u5217\u30d9\u30af\u30c8\u30eb\u3001\\(\\mathbf{b} = (b_1, b_2, \\ldots, b_m)^T\\) \u306f \\(m\\) \u6b21\u5143\u5217\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#32","title":"3.2 \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u95a2\u4fc2","text":"<p>\u5b9a\u7406: \\(m \\times n\\) \u884c\u5217 \\(A\\) \u3068 \\(m\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb \\(\\mathbf{b}\\) \u306b\u5bfe\u3057\u3066\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f \\(A\\mathbf{x} = \\mathbf{b}\\) \u306e\u89e3\u306e\u5b58\u5728\u3068\u4e00\u610f\u6027\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u7279\u5fb4\u3065\u3051\u3089\u308c\u307e\u3059\uff1a</p> <ol> <li>\\(\\text{rank}(A) = \\text{rank}(A|\\mathbf{b})\\) \u306e\u3068\u304d\u3001\u89e3\u306f\u5b58\u5728\u3057\u307e\u3059</li> <li>\\(\\text{rank}(A) = \\text{rank}(A|\\mathbf{b}) = n\\) \u306e\u3068\u304d\u3001\u89e3\u306f\u552f\u4e00\u3064\u5b58\u5728\u3057\u307e\u3059</li> <li>\\(\\text{rank}(A) = \\text{rank}(A|\\mathbf{b}) &lt; n\\) \u306e\u3068\u304d\u3001\u89e3\u306f\u7121\u6570\u306b\u5b58\u5728\u3057\u307e\u3059</li> <li>\\(\\text{rank}(A) &lt; \\text{rank}(A|\\mathbf{b})\\) \u306e\u3068\u304d\u3001\u89e3\u306f\u5b58\u5728\u3057\u307e\u305b\u3093</li> </ol> <p>\u3053\u3053\u3067\u3001\\((A|\\mathbf{b})\\) \u306f\u4fc2\u6570\u884c\u5217 \\(A\\) \u306b\u53f3\u7aef\u304b\u3089 \\(\\mathbf{b}\\) \u3092\u8ffd\u52a0\u3057\u305f\u62e1\u5927\u4fc2\u6570\u884c\u5217\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#33","title":"3.3 \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5","text":"<p>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306f\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u6a19\u6e96\u7684\u306a\u65b9\u6cd5\u3067\u3001\u884c\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u3092\u7c21\u7d04\u968e\u6bb5\u5f62\u306b\u5909\u5f62\u3057\u307e\u3059\u3002</p> <p>\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff08\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\uff09: 1. \u62e1\u5927\u4fc2\u6570\u884c\u5217 \\((A|\\mathbf{b})\\) \u3092\u4f5c\u6210 2. \u884c\u57fa\u672c\u5909\u5f62\u3092\u9069\u7528\u3057\u3066\u884c\u5217\u3092\u968e\u6bb5\u5f62\uff08\u307e\u305f\u306f\u7c21\u7d04\u968e\u6bb5\u5f62\uff09\u306b\u5909\u5f62 3. \u5f8c\u9000\u4ee3\u5165\u6cd5\u306b\u3088\u308a\u672a\u77e5\u6570\u306e\u5024\u3092\u6c42\u3081\u308b</p>"},{"location":"lectures/LA/19-exercise/#34","title":"3.4 \u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u3068\u9006\u884c\u5217","text":"<p>\u9006\u884c\u5217\u306e\u8a08\u7b97\u306b\u306f\u884c\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u308b\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u5b9a\u7406: \\(n \\times n\\) \u884c\u5217 \\(A\\) \u304c\u6b63\u5247\uff08\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\uff09\u3067\u3042\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f \\(\\text{rank}(A) = n\\) \u3067\u3059\u3002</p> <p>\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff08\u9006\u884c\u5217\u306e\u8a08\u7b97\uff09: 1. \\((A|I)\\) \u3068\u3044\u3046\u62e1\u5927\u884c\u5217\u3092\u4f5c\u6210\uff08\\(I\\) \u306f \\(n \\times n\\) \u5358\u4f4d\u884c\u5217\uff09 2. \u884c\u57fa\u672c\u5909\u5f62\u3092\u9069\u7528\u3057\u3066\u5de6\u5074\u3092\u5358\u4f4d\u884c\u5217\u306b\u5909\u5f62 3. \u5f97\u3089\u308c\u305f\u884c\u5217\u306e\u53f3\u5074\u304c \\(A\\) \u306e\u9006\u884c\u5217 \\(A^{-1}\\) \u3068\u306a\u308b</p>"},{"location":"lectures/LA/19-exercise/#35","title":"3.5 \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3068\u6700\u5c0f\u4e8c\u4e57\u6cd5","text":"<p>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u3001\u8aac\u660e\u5909\u6570 \\(x\\) \u3068\u76ee\u7684\u5909\u6570 \\(y\\) \u306e\u9593\u306e\u95a2\u4fc2\u3092\u7dda\u5f62\u95a2\u6570\u3067\u8fd1\u4f3c\u3057\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9\uff08\u5358\u56de\u5e30\u30e2\u30c7\u30eb\uff09: \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\)</p> <p>\u3053\u3053\u3067\u3001\\(\\beta_0\\) \u306f\u5207\u7247\u3001\\(\\beta_1\\) \u306f\u50be\u304d\u3001\\(\\varepsilon\\) \u306f\u8aa4\u5dee\u9805\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9\uff08\u91cd\u56de\u5e30\u30e2\u30c7\u30eb\uff09: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\varepsilon\\)</p> <p>\u3053\u308c\u306f\u884c\u5217\u8868\u8a18\u3067 \\(\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\) \u3068\u66f8\u3051\u307e\u3059\u3002</p> <p>\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3067\u306f\u3001\u6b8b\u5dee\u5e73\u65b9\u548c \\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) \u3092\u6700\u5c0f\u5316\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\boldsymbol{\\beta}\\) \u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\u5b9a\u7406\uff08\u6b63\u898f\u65b9\u7a0b\u5f0f\uff09: \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6700\u5c0f\u4e8c\u4e57\u63a8\u5b9a\u91cf\u306f\u6b21\u306e\u5f0f\u3067\u4e0e\u3048\u3089\u308c\u307e\u3059\uff1a \\(\\(\\hat{\\boldsymbol{\\beta}} = (X^T X)^{-1} X^T \\mathbf{y}\\)\\) \u305f\u3060\u3057\u3001\\(X^T X\\) \u304c\u6b63\u5247\u3067\u3042\u308b\u3053\u3068\u304c\u5fc5\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#4","title":"4. \u7406\u8ad6\u3068\u8a08\u7b97\u65b9\u6cd5\u306e\u5fa9\u7fd2","text":""},{"location":"lectures/LA/19-exercise/#41","title":"4.1 \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5","text":"<p>\u4f8b\u984c: \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u3044\u3066\u304f\u3060\u3055\u3044\u3002 \\(\\(\\begin{cases} 2x + y - z = 8 \\\\ -3x + 4y + 2z = 1 \\\\ x + 2y + 2z = 3 \\end{cases}\\)\\)</p> <p>\u89e3\u7b54:</p> <p>Step 1: \u884c\u5217\u5f62\u5f0f\u3067\u8868\u73fe \\(\\(A = \\begin{pmatrix}  2 &amp; 1 &amp; -1 \\\\  -3 &amp; 4 &amp; 2 \\\\  1 &amp; 2 &amp; 2 \\end{pmatrix}, \\quad  \\mathbf{x} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}, \\quad  \\mathbf{b} = \\begin{pmatrix} 8 \\\\ 1 \\\\ 3 \\end{pmatrix}\\)\\)</p> <p>Step 2: \u62e1\u5927\u4fc2\u6570\u884c\u5217\u3092\u4f5c\u6210 \\(\\((A|\\mathbf{b}) = \\begin{pmatrix}  2 &amp; 1 &amp; -1 &amp; | &amp; 8 \\\\  -3 &amp; 4 &amp; 2 &amp; | &amp; 1 \\\\  1 &amp; 2 &amp; 2 &amp; | &amp; 3 \\end{pmatrix}\\)\\)</p> <p>Step 3: \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528</p> <p>(i) \u7b2c1\u884c\u3092\u7528\u3044\u3066\u7b2c2\u884c\u3068\u7b2c3\u884c\u3092\u5909\u5f62 \\(\\((A|\\mathbf{b}) = \\begin{pmatrix}  2 &amp; 1 &amp; -1 &amp; | &amp; 8 \\\\  0 &amp; \\frac{11}{2} &amp; \\frac{1}{2} &amp; | &amp; 13 \\\\  0 &amp; \\frac{3}{2} &amp; \\frac{5}{2} &amp; | &amp; -1 \\end{pmatrix}\\)\\)</p> <p>(ii) \u7b2c2\u884c\u3092\u7528\u3044\u3066\u7b2c3\u884c\u3092\u5909\u5f62 \\(\\((A|\\mathbf{b}) = \\begin{pmatrix}  2 &amp; 1 &amp; -1 &amp; | &amp; 8 \\\\  0 &amp; \\frac{11}{2} &amp; \\frac{1}{2} &amp; | &amp; 13 \\\\  0 &amp; 0 &amp; \\frac{10}{11} &amp; | &amp; -\\frac{52}{11} \\end{pmatrix}\\)\\)</p> <p>Step 4: \u5f8c\u9000\u4ee3\u5165</p> <p>(i) \u7b2c3\u884c\u3088\u308a\u3001\\(z = -\\frac{52}{11} \\div \\frac{10}{11} = -\\frac{26}{5}\\)</p> <p>(ii) \u7b2c2\u884c\u3088\u308a\u3001\\(\\frac{11}{2}y + \\frac{1}{2} \\cdot (-\\frac{26}{5}) = 13\\) \\(\\Rightarrow \\frac{11}{2}y - \\frac{13}{5} = 13\\) \\(\\Rightarrow \\frac{11}{2}y = 13 + \\frac{13}{5} = \\frac{65}{5} + \\frac{13}{5} = \\frac{78}{5}\\) \\(\\Rightarrow y = \\frac{78}{5} \\cdot \\frac{2}{11} = \\frac{156}{55}\\)</p> <p>(iii) \u7b2c1\u884c\u3088\u308a\u3001\\(2x + \\frac{156}{55} - (-\\frac{26}{5}) = 8\\) \\(\\Rightarrow 2x + \\frac{156}{55} + \\frac{26}{5} = 8\\) \\(\\Rightarrow 2x = 8 - \\frac{156}{55} - \\frac{26}{5} = 8 - \\frac{156}{55} - \\frac{286}{55} = 8 - \\frac{442}{55}\\) \\(\\Rightarrow 2x = \\frac{440}{55} - \\frac{442}{55} = -\\frac{2}{55}\\) \\(\\Rightarrow x = -\\frac{1}{55}\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u89e3\u306f \\(x = -\\frac{1}{55}\\), \\(y = \\frac{156}{55}\\), \\(z = -\\frac{26}{5}\\) \u3067\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#42","title":"4.2 \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u89e3\u306e\u5b58\u5728\u6761\u4ef6","text":"<p>\u4f8b\u984c: \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306b\u3064\u3044\u3066\u3001\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3092\u8abf\u3079\u3066\u304f\u3060\u3055\u3044\u3002 \\(\\(\\begin{cases} x + 2y + 3z = 4 \\\\ 2x + 4y + 6z = \\alpha \\\\ 3x + 6y + 9z = 12 \\end{cases}\\)\\)</p> <p>\u89e3\u7b54:</p> <p>Step 1: \u4fc2\u6570\u884c\u5217\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u3092\u6c42\u3081\u308b \\(\\(A = \\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  2 &amp; 4 &amp; 6 \\\\  3 &amp; 6 &amp; 9 \\end{pmatrix}, \\quad  (A|\\mathbf{b}) = \\begin{pmatrix}  1 &amp; 2 &amp; 3 &amp; | &amp; 4 \\\\  2 &amp; 4 &amp; 6 &amp; | &amp; \\alpha \\\\  3 &amp; 6 &amp; 9 &amp; | &amp; 12 \\end{pmatrix}\\)\\)</p> <p>Step 2: \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u9069\u7528\u3057\u3066\u968e\u6bb5\u5f62\u306b\u5909\u5f62</p> <p>(i) \u7b2c1\u884c\u3092\u7528\u3044\u3066\u7b2c2\u884c\u3068\u7b2c3\u884c\u3092\u5909\u5f62 \\(\\((A|\\mathbf{b}) = \\begin{pmatrix}  1 &amp; 2 &amp; 3 &amp; | &amp; 4 \\\\  0 &amp; 0 &amp; 0 &amp; | &amp; \\alpha - 8 \\\\  0 &amp; 0 &amp; 0 &amp; | &amp; 0 \\end{pmatrix}\\)\\)</p> <p>Step 3: \u30e9\u30f3\u30af\u3092\u6c42\u3081\u308b</p> <p>\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f \\(\\text{rank}(A) = 1\\) \u3067\u3059\u3002 \u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\uff1a - \\(\\alpha = 8\\) \u306e\u3068\u304d\u3001\\(\\text{rank}(A|\\mathbf{b}) = 1\\) - \\(\\alpha \\neq 8\\) \u306e\u3068\u304d\u3001\\(\\text{rank}(A|\\mathbf{b}) = 2\\)</p> <p>Step 4: \u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3092\u5224\u5b9a</p> <p>\u89e3\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u6761\u4ef6\u306f \\(\\text{rank}(A) = \\text{rank}(A|\\mathbf{b})\\) \u306a\u306e\u3067\u3001\\(\\alpha = 8\\) \u306e\u3068\u304d\u306e\u307f\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\u3002 \u307e\u305f\u3001\\(\\text{rank}(A) = 1 &lt; 3 = n\\) \u306a\u306e\u3067\u3001\\(\\alpha = 8\\) \u306e\u3068\u304d\u89e3\u306f\u7121\u6570\u306b\u5b58\u5728\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#43","title":"4.3 \u9006\u884c\u5217\u306e\u8a08\u7b97","text":"<p>\u4f8b\u984c: \u6b21\u306e\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u3066\u304f\u3060\u3055\u3044\u3002 \\(\\(A = \\begin{pmatrix}  1 &amp; 2 \\\\  3 &amp; 4 \\end{pmatrix}\\)\\)</p> <p>\u89e3\u7b54:</p> <p>Step 1: \u62e1\u5927\u884c\u5217\u3092\u4f5c\u6210 \\(\\((A|I) = \\begin{pmatrix}  1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\\\  3 &amp; 4 &amp; | &amp; 0 &amp; 1 \\end{pmatrix}\\)\\)</p> <p>Step 2: \u884c\u57fa\u672c\u5909\u5f62\u3092\u9069\u7528</p> <p>(i) \u7b2c1\u884c\u3092\u7528\u3044\u3066\u7b2c2\u884c\u3092\u5909\u5f62 \\(\\((A|I) = \\begin{pmatrix}  1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\\\  0 &amp; -2 &amp; | &amp; -3 &amp; 1 \\end{pmatrix}\\)\\)</p> <p>(ii) \u7b2c2\u884c\u3092\\(-\\frac{1}{2}\\)\u500d \\(\\((A|I) = \\begin{pmatrix}  1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\\\  0 &amp; 1 &amp; | &amp; \\frac{3}{2} &amp; -\\frac{1}{2} \\end{pmatrix}\\)\\)</p> <p>(iii) \u7b2c2\u884c\u3092\u7528\u3044\u3066\u7b2c1\u884c\u3092\u5909\u5f62 \\(\\((A|I) = \\begin{pmatrix}  1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\\\  0 &amp; 1 &amp; | &amp; \\frac{3}{2} &amp; -\\frac{1}{2} \\end{pmatrix}\\)\\)</p> <p>Step 3: \u9006\u884c\u5217\u3092\u6c42\u3081\u308b \\(\\(A^{-1} = \\begin{pmatrix}  -2 &amp; 1 \\\\  \\frac{3}{2} &amp; -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix}  -2 &amp; 1 \\\\  \\frac{3}{2} &amp; -\\frac{1}{2} \\end{pmatrix}\\)\\)</p> <p>\u3053\u308c\u3092\u691c\u7b97\u3059\u308b\u3068\uff1a \\(\\(A \\cdot A^{-1} = \\begin{pmatrix}  1 &amp; 2 \\\\  3 &amp; 4 \\end{pmatrix} \\cdot \\begin{pmatrix}  -2 &amp; 1 \\\\  \\frac{3}{2} &amp; -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix}  -2 + 3 &amp; 1 - 1 \\\\  -6 + 6 &amp; 3 - 2 \\end{pmatrix} = \\begin{pmatrix}  1 &amp; 0 \\\\  0 &amp; 1 \\end{pmatrix}\\)\\)</p>"},{"location":"lectures/LA/19-exercise/#44","title":"4.4 \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u6700\u5c0f\u4e8c\u4e57\u89e3","text":"<p>\u4f8b\u984c: \u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u5358\u56de\u5e30\u30e2\u30c7\u30eb \\(y = \\beta_0 + \\beta_1 x\\) \u3092\u5f53\u3066\u306f\u3081\u3066\u304f\u3060\u3055\u3044\u3002</p> \\(x\\) 1 2 3 4 5 \\(y\\) 3 5 4 8 10 <p>\u89e3\u7b54:</p> <p>Step 1: \u30c7\u30fc\u30bf\u884c\u5217\u3092\u6e96\u5099 \\(\\(X = \\begin{pmatrix}  1 &amp; 1 \\\\  1 &amp; 2 \\\\  1 &amp; 3 \\\\  1 &amp; 4 \\\\  1 &amp; 5 \\end{pmatrix}, \\quad  \\mathbf{y} = \\begin{pmatrix} 3 \\\\ 5 \\\\ 4 \\\\ 8 \\\\ 10 \\end{pmatrix}\\)\\)</p> <p>Step 2: \\(X^T X\\) \u3068 \\(X^T \\mathbf{y}\\) \u3092\u8a08\u7b97 \\(\\(X^T X = \\begin{pmatrix}  1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\  1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\end{pmatrix} \\cdot \\begin{pmatrix}  1 &amp; 1 \\\\  1 &amp; 2 \\\\  1 &amp; 3 \\\\  1 &amp; 4 \\\\  1 &amp; 5 \\end{pmatrix} = \\begin{pmatrix}  5 &amp; 15 \\\\  15 &amp; 55 \\end{pmatrix}\\)\\)</p> \\[X^T \\mathbf{y} = \\begin{pmatrix}  1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\  1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\end{pmatrix} \\cdot \\begin{pmatrix} 3 \\\\ 5 \\\\ 4 \\\\ 8 \\\\ 10 \\end{pmatrix} = \\begin{pmatrix}  30 \\\\  110 \\end{pmatrix}\\] <p>Step 3: \\((X^T X)^{-1}\\) \u3092\u8a08\u7b97 \\(\\((X^T X)^{-1} = \\begin{pmatrix}  5 &amp; 15 \\\\  15 &amp; 55 \\end{pmatrix}^{-1}\\)\\)</p> <p>\u884c\u5217\u5f0f\u3092\u8a08\u7b97\uff1a\\(\\det(X^T X) = 5 \\cdot 55 - 15 \\cdot 15 = 275 - 225 = 50\\)</p> <p>\u9006\u884c\u5217\u3092\u8a08\u7b97\uff1a \\(\\((X^T X)^{-1} = \\frac{1}{50} \\begin{pmatrix}  55 &amp; -15 \\\\  -15 &amp; 5 \\end{pmatrix} = \\begin{pmatrix}  1.1 &amp; -0.3 \\\\  -0.3 &amp; 0.1 \\end{pmatrix}\\)\\)</p> <p>Step 4: \u6700\u5c0f\u4e8c\u4e57\u89e3\u3092\u8a08\u7b97 \\(\\(\\hat{\\boldsymbol{\\beta}} = (X^T X)^{-1} X^T \\mathbf{y} = \\begin{pmatrix}  1.1 &amp; -0.3 \\\\  -0.3 &amp; 0.1 \\end{pmatrix} \\cdot \\begin{pmatrix}  30 \\\\  110 \\end{pmatrix} = \\begin{pmatrix}  1.1 \\cdot 30 - 0.3 \\cdot 110 \\\\  -0.3 \\cdot 30 + 0.1 \\cdot 110 \\end{pmatrix} = \\begin{pmatrix}  33 - 33 \\\\  -9 + 11 \\end{pmatrix} = \\begin{pmatrix}  0 \\\\  2 \\end{pmatrix}\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u56de\u5e30\u30e2\u30c7\u30eb\u306f \\(y = 0 + 2x = 2x\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/19-exercise/#51","title":"5.1 \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5","text":"<pre><code>import numpy as np\nfrom scipy import linalg\n\n# \u9023\u7acb\u65b9\u7a0b\u5f0f 2x + y - z = 8, -3x + 4y + 2z = 1, x + 2y + 2z = 3\nA = np.array([[2, 1, -1], \n              [-3, 4, 2], \n              [1, 2, 2]])\nb = np.array([8, 1, 3])\n\n# NumPy\u3092\u4f7f\u3063\u3066\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\nx = linalg.solve(A, b)\nprint(\"\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3:\")\nprint(f\"x = {x[0]}\")\nprint(f\"y = {x[1]}\")\nprint(f\"z = {x[2]}\")\n\n# \u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\uff08\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u884c\u57fa\u672c\u5909\u5f62\uff09\u3092\u624b\u52d5\u3067\u5b9f\u884c\naugmented = np.column_stack((A, b))\nprint(\"\\n\u62e1\u5927\u4fc2\u6570\u884c\u5217:\")\nprint(augmented)\n\n# \u30e9\u30f3\u30af\u3092\u8a08\u7b97\nrank_A = np.linalg.matrix_rank(A)\nrank_aug = np.linalg.matrix_rank(augmented)\nprint(f\"\\n\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af: {rank_A}\")\nprint(f\"\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af: {rank_aug}\")\n</code></pre>"},{"location":"lectures/LA/19-exercise/#52","title":"5.2 \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\nx = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\ny = np.array([3, 5, 4, 8, 10])\n\n# \u624b\u52d5\u3067\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3092\u5b9f\u88c5\nX = np.column_stack((np.ones(len(x)), x))  # \u5207\u7247\u9805\u3092\u8ffd\u52a0\nX_T_X = X.T @ X\nX_T_y = X.T @ y\nbeta = np.linalg.inv(X_T_X) @ X_T_y\nprint(\"\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u56de\u5e30\u4fc2\u6570:\")\nprint(f\"\u03b2\u2080 (\u5207\u7247) = {beta[0]}\")\nprint(f\"\u03b2\u2081 (\u50be\u304d) = {beta[1]}\")\n\n# scikit-learn\u3092\u4f7f\u3063\u305f\u7dda\u5f62\u56de\u5e30\nmodel = LinearRegression()\nmodel.fit(x, y)\nprint(\"\\nscikit-learn\u306b\u3088\u308b\u56de\u5e30\u4fc2\u6570:\")\nprint(f\"\u03b2\u2080 (\u5207\u7247) = {model.intercept_}\")\nprint(f\"\u03b2\u2081 (\u50be\u304d) = {model.coef_[0]}\")\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, color='blue', label='\u30c7\u30fc\u30bf\u70b9')\nplt.plot(x, model.predict(x), color='red', label='\u56de\u5e30\u76f4\u7dda')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# \u4e88\u6e2c\u5024\u3068\u6b8b\u5dee\ny_pred = model.predict(x)\nresiduals = y - y_pred\nprint(\"\\n\u4e88\u6e2c\u5024\u3068\u6b8b\u5dee:\")\nfor i in range(len(x)):\n    print(f\"x = {x[i][0]}: y = {y[i]}, \u4e88\u6e2c\u5024 = {y_pred[i]:.2f}, \u6b8b\u5dee = {residuals[i]:.2f}\")\n\n# \u6b8b\u5dee\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.scatter(x, residuals, color='green')\nplt.axhline(y=0, color='red', linestyle='-')\nplt.xlabel('x')\nplt.ylabel('\u6b8b\u5dee')\nplt.title('\u6b8b\u5dee\u30d7\u30ed\u30c3\u30c8')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/19-exercise/#53","title":"5.3 \u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u306e\u5b9f\u9a13","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u30d1\u30e9\u30e1\u30fc\u30bf\u03b1\u3092\u5909\u3048\u306a\u304c\u3089\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3092\u8abf\u3079\u308b\ndef check_solution_existence(alpha):\n    A = np.array([[1, 2, 3], [2, 4, 6], [3, 6, 9]])\n    b = np.array([4, alpha, 12])\n\n    # \u4fc2\u6570\u884c\u5217\u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\n    rank_A = np.linalg.matrix_rank(A)\n    rank_aug = np.linalg.matrix_rank(np.column_stack((A, b)))\n\n    # \u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3092\u30c1\u30a7\u30c3\u30af\n    if rank_A == rank_aug:\n        if rank_A == A.shape[1]:  # n = 3\n            return \"\u4e00\u610f\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\"\n        else:\n            return \"\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\"\n    else:\n        return \"\u89e3\u306f\u5b58\u5728\u3057\u307e\u305b\u3093\"\n\n# \u03b1 = 8 \u306e\u5834\u5408\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u7a7a\u9593\u3092\u53ef\u8996\u5316\nA = np.array([[1, 2, 3], [2, 4, 6], [3, 6, 9]])\nb = np.array([4, 8, 12])\n\n# \u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8868\u793a\nrank_A = np.linalg.matrix_rank(A)\nrank_aug = np.linalg.matrix_rank(np.column_stack((A, b)))\nprint(f\"\u03b1 = 8 \u306e\u5834\u5408:\")\nprint(f\"\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af: {rank_A}\")\nprint(f\"\u62e1\u5927\u4fc2\u6570\u884c\u5217\u306e\u30e9\u30f3\u30af: {rank_aug}\")\nprint(f\"\u89e3\u306e\u5b58\u5728: {check_solution_existence(8)}\")\n\n# \u4e00\u822c\u89e3\u3092\u6c42\u3081\u308b\uff08y\u3068z\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066\u3001x\u3092\u8868\u73fe\uff09\n# x + 2y + 3z = 4 \u3088\u308a x = 4 - 2y - 3z\n# \u89e3\u7a7a\u9593\u30923D\u3067\u53ef\u8996\u5316\ny = np.linspace(-5, 5, 10)\nz = np.linspace(-5, 5, 10)\nY, Z = np.meshgrid(y, z)\nX = 4 - 2*Y - 3*Z\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, alpha=0.5, cmap='viridis')\nax.set_xlabel('x\u8ef8')\nax.set_ylabel('y\u8ef8')\nax.set_zlabel('z\u8ef8')\nax.set_title('\u03b1 = 8 \u306e\u3068\u304d\u306e\u89e3\u7a7a\u9593\uff08\u5e73\u9762\uff09')\nplt.show()\n\n# \u3044\u304f\u3064\u304b\u306e\u03b1\u306b\u3064\u3044\u3066\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3092\u30c1\u30a7\u30c3\u30af\nalpha_values = [7, 8, 9]\nfor alpha in alpha_values:\n    print(f\"\\n\u03b1 = {alpha} \u306e\u5834\u5408:\")\n    print(f\"\u89e3\u306e\u5b58\u5728: {check_solution_existence(alpha)}\")\n</code></pre>"},{"location":"lectures/LA/19-exercise/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/19-exercise/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u89e3\u3044\u3066\u304f\u3060\u3055\u3044\u3002    \\(\\(\\begin{cases}    3x - 2y + z = 7 \\\\    x + y - z = 0 \\\\    2x - y + 2z = 10    \\end{cases}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u3066\u304f\u3060\u3055\u3044\u3002    \\(\\(A = \\begin{pmatrix}     2 &amp; 1 &amp; 0 \\\\     3 &amp; 2 &amp; 1 \\\\     1 &amp; 0 &amp; 1    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u6c42\u3081\u3066\u304f\u3060\u3055\u3044\u3002    \\(\\(B = \\begin{pmatrix}     1 &amp; 2 &amp; 3 \\\\     0 &amp; 1 &amp; 2 \\\\     1 &amp; 3 &amp; 5    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u5358\u56de\u5e30\u30e2\u30c7\u30eb \\(y = \\beta_0 + \\beta_1 x\\) \u3092\u5f53\u3066\u306f\u3081\u3001\\(\\beta_0\\) \u3068 \\(\\beta_1\\) \u3092\u6c42\u3081\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> \\(x\\) 1 3 5 7 9 \\(y\\) 2 5 7 8 12"},{"location":"lectures/LA/19-exercise/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u304c\u89e3\u3092\u6301\u3064\u305f\u3081\u306e \\(\\lambda\\) \u306e\u6761\u4ef6\u3092\u6c42\u3081\u3066\u304f\u3060\u3055\u3044\u3002    \\(\\(\\begin{cases}    x + y + z = 1 \\\\    x + 2y + 3z = 2 \\\\    x + 2y + \\lambda z = 3    \\end{cases}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u91cd\u56de\u5e30\u30e2\u30c7\u30eb \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) \u3092\u5f53\u3066\u306f\u3081\u3066\u304f\u3060\u3055\u3044\u3002\u307e\u305f\u3001\u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u3092\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> \\(x_1\\) 1 2 3 4 5 \\(x_2\\) 3 2 4 1 5 \\(y\\) 10 12 18 14 25 <ol> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u95a2\u9023\u3059\u308b\u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u304c\u5f97\u3089\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u5e74\u9f62\u3001\u904b\u52d5\u6642\u9593\uff08\u9031\u3042\u305f\u308a\u306e\u6642\u9593\uff09\u3001\u5065\u5eb7\u30b9\u30b3\u30a2\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002</li> </ol> \u5e74\u9f62 \u904b\u52d5\u6642\u9593 \u5065\u5eb7\u30b9\u30b3\u30a2 25 3 70 35 2 65 45 4 72 55 1 58 65 5 68 <p>(a) \u3053\u306e\u30c7\u30fc\u30bf\u306b\u91cd\u56de\u5e30\u30e2\u30c7\u30eb \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) \u3092\u5f53\u3066\u306f\u3081\u3066\u304f\u3060\u3055\u3044\u3002\u3053\u3053\u3067\u3001\\(x_1\\) \u306f\u5e74\u9f62\u3001\\(x_2\\) \u306f\u904b\u52d5\u6642\u9593\u3001\\(y\\) \u306f\u5065\u5eb7\u30b9\u30b3\u30a2\u3067\u3059\u3002</p> <p>(b) \u5f97\u3089\u308c\u305f\u30e2\u30c7\u30eb\u3092\u89e3\u91c8\u3057\u3001\u5e74\u9f62\u3068\u904b\u52d5\u6642\u9593\u304c\u5065\u5eb7\u30b9\u30b3\u30a2\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u304b\u3092\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>(c) 50\u6b73\u3067\u9031\u306b3\u6642\u9593\u904b\u52d5\u3059\u308b\u4eba\u306e\u5065\u5eb7\u30b9\u30b3\u30a2\u3092\u4e88\u6e2c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/LA/19-exercise/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/19-exercise/#q1","title":"Q1: \u9023\u7acb\u65b9\u7a0b\u5f0f\u306b\u89e3\u304c\u306a\u3044\u5834\u5408\u3068\u7121\u6570\u306b\u3042\u308b\u5834\u5408\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A1: \u9023\u7acb\u65b9\u7a0b\u5f0f \\(A\\mathbf{x} = \\mathbf{b}\\) \u306b\u304a\u3044\u3066\u3001\u89e3\u306e\u5b58\u5728\u3068\u4e00\u610f\u6027\u306f\u4fc2\u6570\u884c\u5217 \\(A\\) \u3068\u62e1\u5927\u4fc2\u6570\u884c\u5217 \\((A|\\mathbf{b})\\) \u306e\u30e9\u30f3\u30af\u306b\u3088\u3063\u3066\u6c7a\u307e\u308a\u307e\u3059\u3002</p> <ul> <li> <p>\u89e3\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408: \\(\\text{rank}(A) &lt; \\text{rank}(A|\\mathbf{b})\\) \u306e\u3068\u304d\u3002\u3053\u308c\u306f\u3001\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u6761\u4ef6\u304c\u4e92\u3044\u306b\u77db\u76fe\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u65b9\u7a0b\u5f0f\u304c\u8868\u3059\u76f4\u7dda\u3084\u5e73\u9762\u304c\u4ea4\u308f\u3089\u306a\u3044\u3053\u3068\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u89e3\u304c\u7121\u6570\u306b\u3042\u308b\u5834\u5408: \\(\\text{rank}(A) = \\text{rank}(A|\\mathbf{b}) &lt; n\\) \u306e\u3068\u304d\uff08\\(n\\) \u306f\u672a\u77e5\u6570\u306e\u6570\uff09\u3002\u3053\u308c\u306f\u3001\u65b9\u7a0b\u5f0f\u306e\u6570\u304c\u5b9f\u8cea\u7684\u306b\u672a\u77e5\u6570\u306e\u6570\u3088\u308a\u5c11\u306a\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u89e3\u304c\u76f4\u7dda\u3084\u5e73\u9762\u306a\u3069\u306e\u96c6\u5408\u306b\u306a\u308b\u3053\u3068\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"lectures/LA/19-exercise/#q2","title":"Q2: \u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u3069\u306e\u3088\u3046\u306b\u8a08\u7b97\u3057\u307e\u3059\u304b\uff1f","text":"<p>A2: \u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\u3059\u308b\u4e00\u822c\u7684\u306a\u65b9\u6cd5\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u884c\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u3092\u968e\u6bb5\u5f62\uff08\u307e\u305f\u306f\u7c21\u7d04\u968e\u6bb5\u5f62\uff09\u306b\u5909\u5f62\u3059\u308b</li> <li>\u968e\u6bb5\u5f62\u306b\u304a\u3051\u308b\u975e\u30bc\u30ed\u884c\u306e\u6570\u304c\u30e9\u30f3\u30af\u3068\u306a\u308b</li> </ol> <p>Python\u3067\u306f <code>np.linalg.matrix_rank()</code> \u95a2\u6570\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002</p> <p>\u30e9\u30f3\u30af\u306f\u300c\u7dda\u5f62\u72ec\u7acb\u306a\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u6570\u300d\u3068\u3082\u5b9a\u7fa9\u3055\u308c\u3001\u884c\u5217\u306e\u50cf\u7a7a\u9593\u306e\u6b21\u5143\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#q3","title":"Q3: \u6700\u5c0f\u4e8c\u4e57\u6cd5\u3068\u306f\u3069\u306e\u3088\u3046\u306a\u65b9\u6cd5\u3067\u3059\u304b\uff1f\u306a\u305c\u7dda\u5f62\u56de\u5e30\u3067\u4f7f\u308f\u308c\u308b\u306e\u3067\u3059\u304b\uff1f","text":"<p>A3: \u6700\u5c0f\u4e8c\u4e57\u6cd5\u306f\u3001\u6e2c\u5b9a\u5024\u3068\u4e88\u6e2c\u5024\u306e\u5dee\uff08\u6b8b\u5dee\uff09\u306e\u4e8c\u4e57\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c42\u3081\u308b\u65b9\u6cd5\u3067\u3059\u3002\u7dda\u5f62\u56de\u5e30\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u7406\u7531\u3067\u5e83\u304f\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\u6570\u5b66\u7684\u306b\u6271\u3044\u3084\u3059\u3044: \u4e8c\u6b21\u95a2\u6570\u306e\u6700\u5c0f\u5316\u554f\u984c\u306b\u306a\u308b\u305f\u3081\u3001\u5fae\u5206\u306b\u3088\u3063\u3066\u89e3\u6790\u7684\u306b\u89e3\u304c\u5f97\u3089\u308c\u308b</li> <li>\u5916\u308c\u5024\u306b\u5bfe\u3057\u3066\u9811\u5065: \u6b8b\u5dee\u306e\u4e8c\u4e57\u3092\u8003\u3048\u308b\u3053\u3068\u3067\u3001\u5927\u304d\u306a\u8aa4\u5dee\u3092\u6301\u3064\u30c7\u30fc\u30bf\u70b9\u306b\u91cd\u70b9\u3092\u7f6e\u304f</li> <li>\u7d71\u8a08\u7684\u306b\u89e3\u91c8\u3057\u3084\u3059\u3044: \u6b63\u898f\u5206\u5e03\u3092\u4eee\u5b9a\u3059\u308b\u3068\u6700\u5c24\u63a8\u5b9a\u3068\u4e00\u81f4\u3059\u308b</li> </ol> <p>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb \\(y = X\\beta + \\varepsilon\\) \u306b\u304a\u3044\u3066\u3001\u6700\u5c0f\u4e8c\u4e57\u63a8\u5b9a\u91cf\u306f \\(\\hat{\\beta} = (X^TX)^{-1}X^Ty\\) \u3067\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#q4","title":"Q4: \u884c\u5217\u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u3001\u7dda\u5f62\u56de\u5e30\u306f\u3069\u3046\u306a\u308a\u307e\u3059\u304b\uff1f","text":"<p>A4: \u8a2d\u8a08\u884c\u5217 \\(X\\) \u306b\u5bfe\u3057\u3066 \\(X^TX\\) \u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\uff08\u591a\u91cd\u5171\u7dda\u6027\u304c\u3042\u308b\u5834\u5408\u306a\u3069\uff09\u3001\u901a\u5e38\u306e\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306f\u4f7f\u3048\u307e\u305b\u3093\u3002\u3053\u306e\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u65b9\u6cd5\u304c\u8003\u3048\u3089\u308c\u307e\u3059\uff1a</p> <ol> <li>\u64ec\u4f3c\u9006\u884c\u5217\uff08\u30e0\u30fc\u30a2\u30fb\u30da\u30f3\u30ed\u30fc\u30ba\u306e\u4e00\u822c\u9006\u884c\u5217\uff09\u3092\u7528\u3044\u308b</li> <li>\u30ea\u30c3\u30b8\u56de\u5e30\u3084LASSO\u306a\u3069\u306e\u6b63\u5247\u5316\u624b\u6cd5\u3092\u7528\u3044\u308b</li> <li>\u4e3b\u6210\u5206\u56de\u5e30\uff08PCR\uff09\u3084\u90e8\u5206\u7684\u6700\u5c0f\u4e8c\u4e57\u6cd5\uff08PLS\uff09\u306a\u3069\u3001\u6b21\u5143\u524a\u6e1b\u3068\u7d44\u307f\u5408\u308f\u305b\u305f\u65b9\u6cd5\u3092\u7528\u3044\u308b</li> </ol> <p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u591a\u91cd\u5171\u7dda\u6027\u306e\u691c\u51fa\u3068\u305d\u306e\u5bfe\u51e6\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#q5","title":"Q5: \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u65b9\u6cd5\u306f\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A5: \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u4e3b\u306a\u6307\u6a19\u306b\u306f\u4ee5\u4e0b\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u6c7a\u5b9a\u4fc2\u6570\uff08\\(R^2\\)\uff09: \u30e2\u30c7\u30eb\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u5206\u6563\u306e\u5272\u5408</li> <li>\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\uff08MSE\uff09\u307e\u305f\u306f\u5e73\u65b9\u6839\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\uff08RMSE\uff09: \u4e88\u6e2c\u8aa4\u5dee\u306e\u5927\u304d\u3055</li> <li>\u5e73\u5747\u7d76\u5bfe\u8aa4\u5dee\uff08MAE\uff09: \u4e88\u6e2c\u5024\u3068\u5b9f\u6e2c\u5024\u306e\u7d76\u5bfe\u5dee\u306e\u5e73\u5747</li> <li>\u8abf\u6574\u6e08\u307f\u6c7a\u5b9a\u4fc2\u6570\uff08adjusted \\(R^2\\)\uff09: \u8aac\u660e\u5909\u6570\u306e\u6570\u3092\u8003\u616e\u3057\u305f\u6c7a\u5b9a\u4fc2\u6570</li> <li>AIC\uff08\u8d64\u6c60\u60c5\u5831\u91cf\u898f\u6e96\uff09\u3084BIC\uff08\u30d9\u30a4\u30ba\u60c5\u5831\u91cf\u898f\u6e96\uff09: \u30e2\u30c7\u30eb\u306e\u8907\u96d1\u3055\u3068\u30d5\u30a3\u30c3\u30c8\u306e\u826f\u3055\u306e\u30d0\u30e9\u30f3\u30b9\u3092\u8a55\u4fa1</li> </ol> <p>\u307e\u305f\u3001\u6b8b\u5dee\u5206\u6790\uff08\u6b8b\u5dee\u306e\u6b63\u898f\u6027\u3001\u7b49\u5206\u6563\u6027\u3001\u72ec\u7acb\u6027\u306e\u78ba\u8a8d\uff09\u3082\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#q6-1","title":"Q6: \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u72ec\u7acb\u6027\u3068\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f\u3069\u306e\u3088\u3046\u306a\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A6: \u884c\u5217 \\(A\\) \u306e\u30e9\u30f3\u30af\u306f\u3001\u305d\u306e\u5217\u30d9\u30af\u30c8\u30eb\uff08\u307e\u305f\u306f\u884c\u30d9\u30af\u30c8\u30eb\uff09\u306e\u6700\u5927\u7dda\u5f62\u72ec\u7acb\u96c6\u5408\u306e\u30b5\u30a4\u30ba\u3068\u7b49\u3057\u3044\u3067\u3059\u3002\u3064\u307e\u308a\uff1a</p> <ul> <li>\\(\\text{rank}(A)\\) = \\(A\\) \u306e\u7dda\u5f62\u72ec\u7acb\u306a\u5217\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u6570</li> <li>\\(\\text{rank}(A)\\) = \\(A\\) \u306e\u7dda\u5f62\u72ec\u7acb\u306a\u884c\u30d9\u30af\u30c8\u30eb\u306e\u6700\u5927\u6570</li> </ul> <p>\\(n\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u5e95\u306f \\(n\\) \u500b\u306e\u7dda\u5f62\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u304b\u3089\u306a\u308b\u305f\u3081\u3001\\(n \\times n\\) \u884c\u5217 \\(A\\) \u304c\u6b63\u5247\uff08\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\uff09\u3067\u3042\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f \\(\\text{rank}(A) = n\\) \u3067\u3059\u3002\u3053\u308c\u306f\u3001\\(A\\) \u306e\u3059\u3079\u3066\u306e\u5217\u30d9\u30af\u30c8\u30eb\uff08\u307e\u305f\u306f\u884c\u30d9\u30af\u30c8\u30eb\uff09\u304c\u7dda\u5f62\u72ec\u7acb\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#q7","title":"Q7: \u91cd\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A7: \u91cd\u56de\u5e30\u30e2\u30c7\u30eb \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\\) \u306b\u304a\u3044\u3066\uff1a</p> <ul> <li>\\(\\beta_0\\) \u306f\u5207\u7247\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3001\u3059\u3079\u3066\u306e\u8aac\u660e\u5909\u6570\u304c0\u306e\u3068\u304d\u306e\u76ee\u7684\u5909\u6570\u306e\u4e88\u6e2c\u5024</li> <li>\\(\\beta_j\\) (\\(j = 1, 2, \\ldots, p\\)) \u306f\u504f\u56de\u5e30\u4fc2\u6570\u3067\u3001\u4ed6\u306e\u8aac\u660e\u5909\u6570\u3092\u4e00\u5b9a\u306b\u4fdd\u3063\u305f\u3068\u304d\u3001\\(x_j\\) \u304c1\u5358\u4f4d\u5897\u52a0\u3057\u305f\u3068\u304d\u306e \\(y\\) \u306e\u5e73\u5747\u7684\u306a\u5909\u5316\u91cf</li> </ul> <p>\u305f\u3060\u3057\u3001\u8aac\u660e\u5909\u6570\u9593\u306b\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3001\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u89e3\u91c8\u306f\u96e3\u3057\u304f\u306a\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u6a19\u6e96\u5316\u504f\u56de\u5e30\u4fc2\u6570\uff08\u8aac\u660e\u5909\u6570\u3068\u76ee\u7684\u5909\u6570\u3092\u6a19\u6e96\u5316\u3057\u305f\u4e0a\u3067\u306e\u504f\u56de\u5e30\u4fc2\u6570\uff09\u3092\u7528\u3044\u308b\u3068\u3001\u76f8\u5bfe\u7684\u306a\u91cd\u8981\u5ea6\u3092\u6bd4\u8f03\u3057\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#q8","title":"Q8: \u884c\u5217\u5f0f\u3068\u9006\u884c\u5217\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A8: \\(n \\times n\\) \u884c\u5217 \\(A\\) \u306b\u3064\u3044\u3066\uff1a</p> <ul> <li>\\(A\\) \u304c\u9006\u884c\u5217\u3092\u6301\u3064\uff08\u6b63\u5247\u3067\u3042\u308b\uff09\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f \\(\\det(A) \\neq 0\\) \u3067\u3059\u3002</li> <li>\\(A\\) \u306e\u9006\u884c\u5217\u306f\u6b21\u306e\u5f0f\u3067\u4e0e\u3048\u3089\u308c\u307e\u3059\uff1a\\(A^{-1} = \\frac{1}{\\det(A)} \\text{adj}(A)\\)\u3001\u3053\u3053\u3067 \\(\\text{adj}(A)\\) \u306f \\(A\\) \u306e\u4f59\u56e0\u5b50\u884c\u5217\u306e\u8ee2\u7f6e\u3067\u3059\u3002</li> </ul> <p>\u884c\u5217\u5f0f\u304c0\u306b\u8fd1\u3044\u5834\u5408\u3001\u9006\u884c\u5217\u306f\u6570\u5024\u7684\u306b\u4e0d\u5b89\u5b9a\u306b\u306a\u308a\u3001\u8a08\u7b97\u8aa4\u5dee\u304c\u5927\u304d\u304f\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u5834\u5408\u306f\u3001\u64ec\u4f3c\u9006\u884c\u5217\u3084\u6b63\u5247\u5316\u624b\u6cd5\u3092\u691c\u8a0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#q9","title":"Q9: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\u3068\u3057\u3066\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u4ee5\u5916\u306b\u3069\u306e\u3088\u3046\u306a\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A9: \u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f \\(A\\mathbf{x} = \\mathbf{b}\\) \u3092\u89e3\u304f\u4e3b\u306a\u65b9\u6cd5\u306b\u306f\u4ee5\u4e0b\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5: \u884c\u57fa\u672c\u5909\u5f62\u3067\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u63db\u3057\u3001\u5f8c\u9000\u4ee3\u5165\u3067\u89e3\u3092\u6c42\u3081\u308b</li> <li>\u30ac\u30a6\u30b9\u30fb\u30b8\u30e7\u30eb\u30c0\u30f3\u6cd5: \u884c\u57fa\u672c\u5909\u5f62\u3067\u7c21\u7d04\u968e\u6bb5\u5f62\u306b\u5909\u63db\u3057\u3001\u76f4\u63a5\u89e3\u3092\u8aad\u307f\u53d6\u308b</li> <li>LU\u5206\u89e3: \u884c\u5217 \\(A\\) \u3092\u4e0b\u4e09\u89d2\u884c\u5217 \\(L\\) \u3068\u4e0a\u4e09\u89d2\u884c\u5217 \\(U\\) \u306b\u5206\u89e3\u3057\u3001\u524d\u9032\u4ee3\u5165\u3068\u5f8c\u9000\u4ee3\u5165\u3067\u89e3\u304f</li> <li>\u30b3\u30ec\u30b9\u30ad\u30fc\u5206\u89e3: \u5bfe\u79f0\u6b63\u5b9a\u5024\u884c\u5217\u306b\u5bfe\u3057\u3066 \\(A = LL^T\\) \u3068\u5206\u89e3\u3057\u3001\u89e3\u304f</li> <li>QR\u5206\u89e3: \u884c\u5217 \\(A\\) \u3092\u76f4\u4ea4\u884c\u5217 \\(Q\\) \u3068\u4e0a\u4e09\u89d2\u884c\u5217 \\(R\\) \u306b\u5206\u89e3\u3057\u3001\u89e3\u304f</li> <li>\u53cd\u5fa9\u6cd5\uff08\u30e4\u30b3\u30d3\u6cd5\u3001\u30ac\u30a6\u30b9\u30fb\u30b6\u30a4\u30c7\u30eb\u6cd5\u3001SOR\u6cd5\u306a\u3069\uff09: \u5927\u898f\u6a21\u306a\u758e\u884c\u5217\u306b\u5bfe\u3057\u3066\u52b9\u7387\u7684</li> </ol> <p>\u5404\u65b9\u6cd5\u306b\u306f\u9069\u7528\u6761\u4ef6\u3084\u8a08\u7b97\u52b9\u7387\u306e\u9055\u3044\u304c\u3042\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u5bfe\u79f0\u6b63\u5b9a\u5024\u884c\u5217\u306b\u5bfe\u3057\u3066\u306f\u30b3\u30ec\u30b9\u30ad\u30fc\u5206\u89e3\u304c\u52b9\u7387\u7684\u3067\u3042\u308a\u3001\u5927\u898f\u6a21\u306a\u758e\u884c\u5217\u306b\u5bfe\u3057\u3066\u306f\u53cd\u5fa9\u6cd5\u304c\u6709\u52b9\u3067\u3059\u3002</p>"},{"location":"lectures/LA/19-exercise/#q10","title":"Q10: \u7dda\u5f62\u4ee3\u6570\u3068\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u95a2\u9023\u6027\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A10: \u7dda\u5f62\u4ee3\u6570\u306f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u76e4\u3068\u306a\u308b\u6570\u5b66\u5206\u91ce\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\u30c7\u30fc\u30bf\u8868\u73fe: \u30c7\u30fc\u30bf\u306f\u4e00\u822c\u306b\u884c\u5217\u3084\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u73fe\u3055\u308c\u3001\u64cd\u4f5c\u3055\u308c\u308b</li> <li>\u7dda\u5f62\u56de\u5e30: \u6700\u3082\u57fa\u672c\u7684\u306a\u4e88\u6e2c\u30e2\u30c7\u30eb\u3067\u3042\u308a\u3001\u7dda\u5f62\u65b9\u7a0b\u5f0f\u7cfb\u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3055\u308c\u308b</li> <li>\u6b21\u5143\u524a\u6e1b: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3084\u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u306f\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u5206\u6790\u306b\u4e0d\u53ef\u6b20</li> <li>\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0: k-means\u306a\u3069\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u306e\u8ddd\u96e2\u306b\u57fa\u3065\u304f</li> <li>\u4fe1\u53f7\u51e6\u7406: \u753b\u50cf\u3084\u97f3\u58f0\u306a\u3069\u306e\u4fe1\u53f7\u51e6\u7406\u306b\u304a\u3044\u3066\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u306a\u3069\u306e\u7dda\u5f62\u6f14\u7b97\u304c\u7528\u3044\u3089\u308c\u308b</li> <li>\u6df1\u5c64\u5b66\u7fd2: \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u57fa\u672c\u7684\u306a\u6f14\u7b97\u306f\u884c\u5217\u306e\u4e57\u7b97</li> <li>\u81ea\u7136\u8a00\u8a9e\u51e6\u7406: \u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u3084\u6587\u66f8\u8868\u73fe\u306f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u30e2\u30c7\u30eb\u306b\u57fa\u3065\u304f</li> </ol> <p>\u7dda\u5f62\u4ee3\u6570\u306e\u7406\u89e3\u306f\u3001\u3053\u308c\u3089\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u52d5\u4f5c\u539f\u7406\u3092\u7406\u89e3\u3057\u3001\u9069\u5207\u306b\u5fdc\u7528\u3059\u308b\u305f\u3081\u306b\u4e0d\u53ef\u6b20\u3067\u3059\u3002</p>"},{"location":"lectures/LA/21-determinant/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I - \u7b2c21\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/21-determinant/#_1","title":"\u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c21\u56de \u30c6\u30fc\u30de: \u884c\u5217\u5f0f\u306e\u5b9a\u7fa9\u3068\u57fa\u672c\u6027\u8cea \u95a2\u9023\u9805\u76ee: \u884c\u5217\u5f0f\u3001\u591a\u91cd\u7dda\u5f62\u6027\u3001\u4ea4\u4ee3\u6027\u3001\u884c\u5217\u306e\u7a4d\u3068\u884c\u5217\u5f0f \u4e88\u7fd2\u5185\u5bb9: \u884c\u5217\u306e\u57fa\u672c\u6982\u5ff5\u3001\u884c\u57fa\u672c\u5909\u5f62\u306e\u5fa9\u7fd2</p>"},{"location":"lectures/LA/21-determinant/#1","title":"1. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u4ee5\u4e0b\u306e\u76ee\u6a19\u9054\u6210\u3092\u76ee\u6307\u3057\u307e\u3059\uff1a</p> <ol> <li>\u884c\u5217\u5f0f\u306e\u5b9a\u7fa9\u3092\u7406\u89e3\u3057\u3001\u305d\u306e\u610f\u5473\u3092\u628a\u63e1\u3059\u308b</li> <li>2\u6b21\u30fb3\u6b21\u306e\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u884c\u5217\u5f0f\u306e\u57fa\u672c\u6027\u8cea\uff08\u591a\u91cd\u7dda\u5f62\u6027\u3001\u4ea4\u4ee3\u6027\uff09\u3092\u7406\u89e3\u3059\u308b</li> <li>\u884c\u5217\u5f0f\u3068\u9006\u884c\u5217\u306e\u95a2\u4fc2\u6027\u3092\u7406\u89e3\u3059\u308b</li> <li>\u884c\u5217\u5f0f\u306e\u5fdc\u7528\u4f8b\u3092\u628a\u63e1\u3059\u308b</li> </ol>"},{"location":"lectures/LA/21-determinant/#2","title":"2. \u57fa\u672c\u6982\u5ff5\uff1a\u884c\u5217\u5f0f\u3068\u306f","text":""},{"location":"lectures/LA/21-determinant/#21","title":"2.1 \u884c\u5217\u5f0f\u306e\u5c0e\u5165","text":"<p>\u884c\u5217\u5f0f\uff08determinant\uff09\u306f\u6b63\u65b9\u884c\u5217\u306b\u5bfe\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u308b\u6570\u5024\u3067\u3042\u308a\u3001\u7dda\u5f62\u4ee3\u6570\u5b66\u306b\u304a\u3044\u3066\u6975\u3081\u3066\u91cd\u8981\u306a\u6982\u5ff5\u3067\u3059\u3002\u884c\u5217\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u6570\u5b66\u7684\u30fb\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u6301\u3061\u307e\u3059\uff1a</p> <ul> <li>\u884c\u5217\u304c\u8868\u3059\u7dda\u5f62\u5909\u63db\u306b\u3088\u308b\u5358\u4f4d\u4f53\u7a4d\u306e\u5909\u5316\u7387</li> <li>\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u304b\u3069\u3046\u304b\u306e\u5224\u5b9a\u57fa\u6e96</li> <li>\u9023\u7acb1\u6b21\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6</li> </ul> <p>\u5b9a\u7fa9: n\u6b21\u6b63\u65b9\u884c\u5217 \\(A\\) \u306e\u884c\u5217\u5f0f\u306f \\(\\det(A)\\) \u307e\u305f\u306f \\(|A|\\) \u3068\u8868\u8a18\u3055\u308c\u3001\u884c\u5217 \\(A\\) \u304b\u3089\u8a08\u7b97\u3055\u308c\u308b\u4e00\u3064\u306e\u30b9\u30ab\u30e9\u30fc\u5024\u3067\u3059\u3002</p>"},{"location":"lectures/LA/21-determinant/#22-2","title":"2.2 2\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f","text":"<p>2\u6b21\u6b63\u65b9\u884c\u5217 \\(A = \\begin{pmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix}\\) \u306e\u884c\u5217\u5f0f\u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> <p>2\u6b21\u884c\u5217\u5f0f: \\(\\det(A) = a_{11}a_{22} - a_{12}a_{21}\\)</p> <p>\u3053\u308c\u306f\u300c\u4e3b\u5bfe\u89d2\u7dda\u306e\u7a4d\u304b\u3089\u526f\u5bfe\u89d2\u7dda\u306e\u7a4d\u3092\u5f15\u304f\u300d\u3068\u899a\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u4f8b\u984c 2.1: \u884c\u5217 \\(A = \\begin{pmatrix} 3 &amp; 5 \\\\ 2 &amp; 7 \\end{pmatrix}\\) \u306e\u884c\u5217\u5f0f\u3092\u6c42\u3081\u3088\u3002</p> <p>\u89e3\u7b54: \\(\\det(A) = 3 \\times 7 - 5 \\times 2 = 21 - 10 = 11\\)</p>"},{"location":"lectures/LA/21-determinant/#23-3","title":"2.3 3\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f","text":"<p>3\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u6c42\u3081\u308b\u65b9\u6cd5\u3068\u3057\u3066\u306f\u3001\u30b5\u30e9\u30b9\u306e\u65b9\u6cd5\uff08Sarrus's rule\uff09\u304c\u3042\u308a\u307e\u3059\uff1a</p> <p>3\u6b21\u884c\u5217\u5f0f\uff08\u30b5\u30e9\u30b9\u306e\u65b9\u6cd5\uff09:  \\(\\det\\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{pmatrix} = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33}\\)</p> <p>\u3053\u308c\u306f\u6b21\u306e\u3088\u3046\u306a\u56f3\u5f0f\u3067\u899a\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>+ | a11 a12 a13 | a11 a12 |\n  | a21 a22 a23 | a21 a22 |\n  | a31 a32 a33 | a31 a32 |\n\n- | a13 a12 a11 | a13 a12 |\n  | a23 a22 a21 | a23 a22 |\n  | a33 a32 a31 | a33 a32 |\n</code></pre> <p>\u4f8b\u984c 2.2: \u884c\u5217 \\(B = \\begin{pmatrix} 2 &amp; 0 &amp; 1 \\\\ 3 &amp; 1 &amp; 2 \\\\ 1 &amp; 2 &amp; 0 \\end{pmatrix}\\) \u306e\u884c\u5217\u5f0f\u3092\u6c42\u3081\u3088\u3002</p> <p>\u89e3\u7b54: \\(\\det(B) = 2 \\times 1 \\times 0 + 0 \\times 2 \\times 1 + 1 \\times 3 \\times 2 - 1 \\times 1 \\times 1 - 2 \\times 2 \\times 1 - 0 \\times 3 \\times 0\\) \\(= 0 + 0 + 6 - 1 - 4 - 0 = 1\\)</p>"},{"location":"lectures/LA/21-determinant/#3","title":"3. \u7406\u8ad6\u3068\u624b\u6cd5\uff1a\u884c\u5217\u5f0f\u306e\u57fa\u672c\u6027\u8cea","text":""},{"location":"lectures/LA/21-determinant/#31","title":"3.1 \u5358\u4f4d\u884c\u5217\u306e\u884c\u5217\u5f0f","text":"<p>\u6027\u8cea 1: \u5358\u4f4d\u884c\u5217 \\(I_n\\) \u306e\u884c\u5217\u5f0f\u306f 1 \u3067\u3042\u308b\u3002 \\(\\det(I_n) = 1\\)</p> <p>\u8a3c\u660e\u306e\u6982\u7565: 2\u6b21\u5358\u4f4d\u884c\u5217 \\(I_2 = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\) \u306e\u5834\u5408\uff1a \\(\\det(I_2) = 1 \\times 1 - 0 \\times 0 = 1\\)</p> <p>3\u6b21\u5358\u4f4d\u884c\u5217 \\(I_3 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) \u306e\u5834\u5408\uff1a \\(\\det(I_3) = 1 \\times 1 \\times 1 + 0 \\times 0 \\times 0 + 0 \\times 0 \\times 0 - 0 \\times 1 \\times 0 - 1 \\times 0 \\times 0 - 0 \\times 0 \\times 1 = 1\\)</p>"},{"location":"lectures/LA/21-determinant/#32","title":"3.2 \u884c\u5217\u5f0f\u306e\u591a\u91cd\u7dda\u5f62\u6027","text":"<p>\u884c\u5217\u5f0f\u306f\u5404\u884c\uff08\u307e\u305f\u306f\u5404\u5217\uff09\u306b\u3064\u3044\u3066\u7dda\u5f62\u3067\u3042\u308b\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\u3002</p> <p>\u6027\u8cea 2\uff08\u884c\u306b\u95a2\u3059\u308b\u591a\u91cd\u7dda\u5f62\u6027\uff09: 1. \u3042\u308b\u884c\u306e\u3059\u3079\u3066\u306e\u8981\u7d20\u306b\u540c\u3058\u6570 \\(c\\) \u3092\u639b\u3051\u308b\u3068\u3001\u884c\u5217\u5f0f\u306f \\(c\\) \u500d\u306b\u306a\u308b\u3002 2. \u3042\u308b\u884c\u30922\u3064\u306e\u884c\u306e\u548c\u3068\u3057\u3066\u8868\u305b\u308b\u5834\u5408\u3001\u884c\u5217\u5f0f\u306f2\u3064\u306e\u884c\u5217\u5f0f\u306e\u548c\u3068\u3057\u3066\u8868\u305b\u308b\u3002</p> <p>\u4f8b\u3048\u3070\u30012\u6b21\u884c\u5217\u3067\u898b\u308b\u3068\uff1a</p> <p>\\(\\det\\begin{pmatrix} ca_{11} &amp; ca_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix} = c \\cdot \\det\\begin{pmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix}\\)</p> <p>\\(\\det\\begin{pmatrix} a_{11}+b_{11} &amp; a_{12}+b_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix} = \\det\\begin{pmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix} + \\det\\begin{pmatrix} b_{11} &amp; b_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix}\\)</p> <p>\u4f8b\u984c 3.1: \u884c\u5217 \\(C = \\begin{pmatrix} 6 &amp; 10 \\\\ 2 &amp; 7 \\end{pmatrix}\\) \u306e\u884c\u5217\u5f0f\u3092\u591a\u91cd\u7dda\u5f62\u6027\u3092\u7528\u3044\u3066\u8a08\u7b97\u305b\u3088\u3002</p> <p>\u89e3\u7b54: \\(C\\) \u306e\u7b2c1\u884c\u306f \\(A = \\begin{pmatrix} 3 &amp; 5 \\\\ 2 &amp; 7 \\end{pmatrix}\\) \u306e\u7b2c1\u884c\u306e2\u500d\u3067\u3042\u308b\u305f\u3081\u3001 \\(\\det(C) = 2 \\cdot \\det(A) = 2 \\times 11 = 22\\)</p>"},{"location":"lectures/LA/21-determinant/#33","title":"3.3 \u884c\u5217\u5f0f\u306e\u4ea4\u4ee3\u6027","text":"<p>\u6027\u8cea 3\uff08\u4ea4\u4ee3\u6027\uff09: \u884c\u5217\u306e2\u3064\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u3092\u5165\u308c\u66ff\u3048\u308b\u3068\u3001\u884c\u5217\u5f0f\u306e\u7b26\u53f7\u304c\u53cd\u8ee2\u3059\u308b\u3002</p> <p>\u3053\u308c\u306f\u30012\u3064\u306e\u884c\u304c\u540c\u3058\u3067\u3042\u308b\u5834\u5408\u3001\u884c\u5217\u5f0f\u306f0\u306b\u306a\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> <p>\u4f8b\u984c 3.2: \u884c\u5217 \\(D = \\begin{pmatrix} 2 &amp; 7 \\\\ 3 &amp; 5 \\end{pmatrix}\\) \u306e\u884c\u5217\u5f0f\u3092\u6c42\u3081\u3001\u884c\u3092\u5165\u308c\u66ff\u3048\u305f\u884c\u5217 \\(E = \\begin{pmatrix} 3 &amp; 5 \\\\ 2 &amp; 7 \\end{pmatrix}\\) \u306e\u884c\u5217\u5f0f\u3068\u306e\u95a2\u4fc2\u3092\u78ba\u8a8d\u305b\u3088\u3002</p> <p>\u89e3\u7b54: \\(\\det(D) = 2 \\times 5 - 7 \\times 3 = 10 - 21 = -11\\) \\(\\det(E) = 3 \\times 7 - 5 \\times 2 = 21 - 10 = 11\\)</p> <p>\u78ba\u304b\u306b \\(\\det(E) = -\\det(D)\\) \u304c\u6210\u308a\u7acb\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/21-determinant/#34","title":"3.4 \u884c\u5217\u306e\u7a4d\u3068\u884c\u5217\u5f0f","text":"<p>\u6027\u8cea 4: 2\u3064\u306e\u6b63\u65b9\u884c\u5217 \\(A\\) \u3068 \\(B\\) \u306e\u7a4d\u306e\u884c\u5217\u5f0f\u306f\u3001\u305d\u308c\u305e\u308c\u306e\u884c\u5217\u5f0f\u306e\u7a4d\u306b\u7b49\u3057\u3044\u3002 \\(\\det(AB) = \\det(A) \\cdot \\det(B)\\)</p> <p>\u4f8b\u984c 3.4: \u884c\u5217 \\(A = \\begin{pmatrix} 2 &amp; 1 \\\\ 3 &amp; 2 \\end{pmatrix}\\) \u3068 \\(B = \\begin{pmatrix} 3 &amp; 4 \\\\ 1 &amp; 2 \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\det(AB) = \\det(A) \\cdot \\det(B)\\) \u3092\u78ba\u8a8d\u305b\u3088\u3002</p> <p>\u89e3\u7b54: \\(\\det(A) = 2 \\times 2 - 1 \\times 3 = 4 - 3 = 1\\) \\(\\det(B) = 3 \\times 2 - 4 \\times 1 = 6 - 4 = 2\\) \\(\\det(A) \\cdot \\det(B) = 1 \\times 2 = 2\\)</p> <p>\u4e00\u65b9\u3001 \\(AB = \\begin{pmatrix} 2 &amp; 1 \\\\ 3 &amp; 2 \\end{pmatrix} \\begin{pmatrix} 3 &amp; 4 \\\\ 1 &amp; 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\times 3 + 1 \\times 1 &amp; 2 \\times 4 + 1 \\times 2 \\\\ 3 \\times 3 + 2 \\times 1 &amp; 3 \\times 4 + 2 \\times 2 \\end{pmatrix} = \\begin{pmatrix} 7 &amp; 10 \\\\ 11 &amp; 16 \\end{pmatrix}\\)</p> <p>\\(\\det(AB) = 7 \\times 16 - 10 \\times 11 = 112 - 110 = 2\\)</p> <p>\u78ba\u304b\u306b \\(\\det(AB) = \\det(A) \\cdot \\det(B)\\) \u304c\u6210\u308a\u7acb\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/21-determinant/#35","title":"3.5 \u9006\u884c\u5217\u306e\u5b58\u5728\u3068\u884c\u5217\u5f0f","text":"<p>\u6027\u8cea 5: \u6b63\u65b9\u884c\u5217 \\(A\\) \u304c\u9006\u884c\u5217\u3092\u6301\u3064\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f \\(\\det(A) \\neq 0\\) \u3067\u3042\u308b\u3002</p> <p>\u4f8b\u984c 3.5: \u884c\u5217 \\(F = \\begin{pmatrix} 2 &amp; 4 \\\\ 1 &amp; 2 \\end{pmatrix}\\) \u306b\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u304b\u5224\u5b9a\u305b\u3088\u3002</p> <p>\u89e3\u7b54: \\(\\det(F) = 2 \\times 2 - 4 \\times 1 = 4 - 4 = 0\\) \u884c\u5217\u5f0f\u304c0\u3067\u3042\u308b\u305f\u3081\u3001\u884c\u5217 \\(F\\) \u306f\u9006\u884c\u5217\u3092\u6301\u3061\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/21-determinant/#4-python","title":"4. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/21-determinant/#41-numpy","title":"4.1 NumPy\u3092\u4f7f\u3063\u305f\u884c\u5217\u5f0f\u306e\u8a08\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# 2\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\nA = np.array([[3, 5], [2, 7]])\ndet_A = np.linalg.det(A)\nprint(f\"\u884c\u5217A:\\n{A}\")\nprint(f\"\u884c\u5217A\u306e\u884c\u5217\u5f0f: {det_A}\")  # \u7d50\u679c: 11.0\n\n# 3\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\nB = np.array([[2, 0, 1], [3, 1, 2], [1, 2, 0]])\ndet_B = np.linalg.det(B)\nprint(f\"\\n\u884c\u5217B:\\n{B}\")\nprint(f\"\u884c\u5217B\u306e\u884c\u5217\u5f0f: {det_B}\")  # \u7d50\u679c: 1.0\n\n# \u884c\u5217\u5f0f\u306e\u591a\u91cd\u7dda\u5f62\u6027\u306e\u691c\u8a3c\nC = np.array([[6, 10], [2, 7]])  # A\u306e\u7b2c1\u884c\u30922\u500d\u3057\u305f\u884c\u5217\ndet_C = np.linalg.det(C)\nprint(f\"\\n\u884c\u5217C:\\n{C}\")\nprint(f\"\u884c\u5217C\u306e\u884c\u5217\u5f0f: {det_C}\")  # \u7d50\u679c: 22.0\nprint(f\"2\u00d7det(A): {2*det_A}\")     # \u7d50\u679c: 22.0\n\n# \u884c\u5217\u5f0f\u306e\u4ea4\u4ee3\u6027\u306e\u691c\u8a3c\nD = np.array([[2, 7], [3, 5]])\nE = np.array([[3, 5], [2, 7]])  # D\u306e\u884c\u3092\u5165\u308c\u66ff\u3048\u305f\u884c\u5217\ndet_D = np.linalg.det(D)\ndet_E = np.linalg.det(E)\nprint(f\"\\n\u884c\u5217D:\\n{D}\")\nprint(f\"\u884c\u5217D\u306e\u884c\u5217\u5f0f: {det_D}\")  # \u7d50\u679c: -11.0\nprint(f\"\\n\u884c\u5217E:\\n{E}\")\nprint(f\"\u884c\u5217E\u306e\u884c\u5217\u5f0f: {det_E}\")  # \u7d50\u679c: 11.0\nprint(f\"det(E)\u3068det(D)\u306e\u95a2\u4fc2: {det_E} = {-det_D}\")  # det(E) = -det(D) \u3092\u78ba\u8a8d\n\n# \u884c\u5217\u306e\u7a4d\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2\nA2 = np.array([[2, 1], [3, 2]])\nB2 = np.array([[3, 4], [1, 2]])\nAB = np.dot(A2, B2)\ndet_A2 = np.linalg.det(A2)\ndet_B2 = np.linalg.det(B2)\ndet_AB = np.linalg.det(AB)\nprint(f\"\\n\u884c\u5217A2:\\n{A2}\")\nprint(f\"\u884c\u5217A2\u306e\u884c\u5217\u5f0f: {det_A2}\")  # \u7d50\u679c: 1.0\nprint(f\"\\n\u884c\u5217B2:\\n{B2}\")\nprint(f\"\u884c\u5217B2\u306e\u884c\u5217\u5f0f: {det_B2}\")  # \u7d50\u679c: 2.0\nprint(f\"\\n\u884c\u5217A2B2:\\n{AB}\")\nprint(f\"\u884c\u5217A2B2\u306e\u884c\u5217\u5f0f: {det_AB}\")  # \u7d50\u679c: 2.0\nprint(f\"det(A2)\u00d7det(B2): {det_A2*det_B2}\")  # \u7d50\u679c: 2.0\n</code></pre>"},{"location":"lectures/LA/21-determinant/#42","title":"4.2 \u884c\u5217\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u306e\u53ef\u8996\u5316","text":"<p>\u884c\u5217\u5f0f\u306f\u3001\u5358\u4f4d\u6b63\u65b9\u5f62\uff08\u307e\u305f\u306f\u5358\u4f4d\u7acb\u65b9\u4f53\uff09\u304c\u7dda\u5f62\u5909\u63db\u5f8c\u306b\u3069\u308c\u3060\u3051\u9762\u7a4d\uff08\u307e\u305f\u306f\u4f53\u7a4d\uff09\u304c\u5909\u5316\u3059\u308b\u304b\u3092\u8868\u3057\u307e\u3059\u3002</p> <pre><code>def plot_transformation(A, title=\"\u7dda\u5f62\u5909\u63db\"):\n    # \u5358\u4f4d\u6b63\u65b9\u5f62\u306e\u9802\u70b9\n    square = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n\n    # \u5909\u63db\u5f8c\u306e\u6b63\u65b9\u5f62\n    transformed_square = np.dot(square, A.T)\n\n    # \u5143\u306e\u6b63\u65b9\u5f62\u3068\u5909\u63db\u5f8c\u306e\u6b63\u65b9\u5f62\u3092\u30d7\u30ed\u30c3\u30c8\n    plt.figure(figsize=(10, 5))\n\n    # \u5143\u306e\u6b63\u65b9\u5f62\n    plt.subplot(1, 2, 1)\n    plt.plot(square[:, 0], square[:, 1], 'b-')\n    plt.fill(square[:, 0], square[:, 1], 'lightblue', alpha=0.5)\n    plt.grid(True)\n    plt.axis('equal')\n    plt.title(\"\u5143\u306e\u5358\u4f4d\u6b63\u65b9\u5f62\")\n    plt.xlim(-0.5, 1.5)\n    plt.ylim(-0.5, 1.5)\n\n    # \u5909\u63db\u5f8c\u306e\u6b63\u65b9\u5f62\n    plt.subplot(1, 2, 2)\n    plt.plot(transformed_square[:, 0], transformed_square[:, 1], 'r-')\n    plt.fill(transformed_square[:, 0], transformed_square[:, 1], 'salmon', alpha=0.5)\n    plt.grid(True)\n    plt.axis('equal')\n    plt.title(f\"{title}\\ndet(A) = {np.linalg.det(A):.2f}\")\n\n    # \u8ef8\u306e\u7bc4\u56f2\u3092\u9069\u5f53\u306b\u8a2d\u5b9a\n    max_val = max(np.max(np.abs(transformed_square)), 1.5)\n    plt.xlim(-max_val, max_val)\n    plt.ylim(-max_val, max_val)\n\n    plt.tight_layout()\n    plt.show()\n\n# \u4f8b1: \u9762\u7a4d\u304c\u5897\u52a0\u3059\u308b\u5909\u63db\nA1 = np.array([[2, 0], [0, 2]])  # 2\u500d\u306b\u62e1\u5927\nplot_transformation(A1, \"2\u500d\u306b\u62e1\u5927\u3059\u308b\u5909\u63db\")\n\n# \u4f8b2: \u9762\u7a4d\u306f\u540c\u3058\u3060\u304c\u5f62\u304c\u5909\u308f\u308b\u5909\u63db\nA2 = np.array([[0, 1], [-1, 0]])  # 90\u5ea6\u56de\u8ee2\nplot_transformation(A2, \"90\u5ea6\u56de\u8ee2\u3059\u308b\u5909\u63db\")\n\n# \u4f8b3: \u9762\u7a4d\u304c\u6e1b\u5c11\u3059\u308b\u5909\u63db\nA3 = np.array([[0.5, 0], [0, 0.5]])  # \u534a\u5206\u306b\u7e2e\u5c0f\nplot_transformation(A3, \"\u534a\u5206\u306b\u7e2e\u5c0f\u3059\u308b\u5909\u63db\")\n\n# \u4f8b4: \u9762\u7a4d\u304c0\u306b\u306a\u308b\u5909\u63db\uff08\u884c\u5217\u5f0f = 0\uff09\nA4 = np.array([[1, 1], [2, 2]])  # \u30e9\u30f3\u30af\u843d\u3061\nplot_transformation(A4, \"1\u6b21\u5143\u306b\u62bc\u3057\u3064\u3076\u3059\u5909\u63db\")\n\n# \u4f8b5: \u9762\u7a4d\u304c\u8ca0\u306b\u306a\u308b\u5909\u63db\uff08\u884c\u5217\u5f0f &lt; 0\uff09\nA5 = np.array([[0, 1], [1, 0]])  # x\u8ef8\u3068y\u8ef8\u3092\u5165\u308c\u66ff\u3048\nplot_transformation(A5, \"x\u8ef8\u3068y\u8ef8\u3092\u5165\u308c\u66ff\u3048\u308b\u5909\u63db\")\n</code></pre>"},{"location":"lectures/LA/21-determinant/#5","title":"5. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/21-determinant/#51","title":"5.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u6c42\u3081\u3088\u3002    (a) \\(\\begin{pmatrix} 4 &amp; 3 \\\\ 2 &amp; 5 \\end{pmatrix}\\)    (b) \\(\\begin{pmatrix} 2 &amp; 0 &amp; 3 \\\\ 1 &amp; 4 &amp; 2 \\\\ 0 &amp; 1 &amp; 5 \\end{pmatrix}\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u3001\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u304b\u5224\u5b9a\u305b\u3088\u3002    (a) \\(\\begin{pmatrix} 2 &amp; 6 \\\\ 1 &amp; 3 \\end{pmatrix}\\)    (b) \\(\\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{pmatrix}\\)</p> </li> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\det(A^T) = \\det(A)\\) \u3092\u793a\u305b\u3002</p> </li> <li> <p>\\(\\det(A) = 3\\) \u304b\u3064 \\(\\det(B) = 4\\) \u306e\u3068\u304d\u3001\\(\\det(AB)\\) \u3068 \\(\\det(2A)\\) \u3092\u6c42\u3081\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/21-determinant/#52","title":"5.2 \u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\det(A^2) = (\\det(A))^2\\) \u3092\u793a\u305b\u3002</p> </li> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; k \\end{pmatrix}\\) \u306b\u304a\u3044\u3066\u3001\\(\\det(A) = 0\\) \u3068\u306a\u308b\u3088\u3046\u306a \\(k\\) \u306e\u5024\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306e\u5fdc\u7528\u4f8b\uff1a\u60a3\u8005\u306e\u8840\u5727\u3001\u5fc3\u62cd\u6570\u3001\u4f53\u6e29\u306e3\u3064\u306e\u6307\u6a19\u306b\u3064\u3044\u3066\u3001\u6b63\u5e38\u5024\u304b\u3089\u306e\u504f\u5dee\u3092\u8868\u3059\u884c\u5217 \\(\\begin{pmatrix} x_1 &amp; y_1 &amp; z_1 \\\\ x_2 &amp; y_2 &amp; z_2 \\\\ x_3 &amp; y_3 &amp; z_3 \\end{pmatrix}\\) \u306e\u884c\u5217\u5f0f\u304c0\u306b\u8fd1\u3044\u5024\u3092\u3068\u308b\u3068\u304d\u3001\u3053\u308c\u30893\u3064\u306e\u6307\u6a19\u9593\u306b\u5f37\u3044\u76f8\u95a2\u95a2\u4fc2\u304c\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u30023\u4eba\u306e\u60a3\u8005\u30c7\u30fc\u30bf \\(\\begin{pmatrix} 10 &amp; 5 &amp; 3 \\\\ 20 &amp; 10 &amp; 6 \\\\ 30 &amp; 15 &amp; 9 \\end{pmatrix}\\) \u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u3001\u3053\u308c\u3089\u306e\u6307\u6a19\u9593\u306e\u95a2\u4fc2\u6027\u306b\u3064\u3044\u3066\u8003\u5bdf\u305b\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/21-determinant/#6","title":"6. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/21-determinant/#q1","title":"Q1: \u884c\u5217\u5f0f\u306f\u306a\u305c\u91cd\u8981\u306a\u306e\u3067\u3059\u304b\uff1f","text":"<p>A1: \u884c\u5217\u5f0f\u306f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3092\u542b\u3080\u69d8\u3005\u306a\u5206\u91ce\u3067\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\uff1a - \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u3092\u8abf\u3079\u308b - \u884c\u5217\u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u304b\u3092\u5224\u5b9a\u3059\u308b - \u7dda\u5f62\u5909\u63db\u306b\u3088\u308b\u9762\u7a4d\u3084\u4f53\u7a4d\u306e\u5909\u5316\u7387\u3092\u8868\u3059 - \u56fa\u6709\u5024\u554f\u984c\u3084\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u3001\u591a\u304f\u306e\u6570\u5b66\u7684\u624b\u6cd5\u306e\u57fa\u790e\u3068\u306a\u308b</p>"},{"location":"lectures/LA/21-determinant/#q2-0","title":"Q2: \u884c\u5217\u5f0f\u304c0\u3067\u3042\u308b\u3053\u3068\u306f\u4f55\u3092\u610f\u5473\u3057\u307e\u3059\u304b\uff1f","text":"<p>A2: \u884c\u5217\u5f0f\u304c0\u3067\u3042\u308b\u3053\u3068\u306f\u3001\u305d\u306e\u884c\u5217\u304c\u300c\u7279\u7570\uff08singular\uff09\u300d\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u3064\u307e\u308a\uff1a - \u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u306a\u3044 - \u5bfe\u5fdc\u3059\u308b\u7dda\u5f62\u5909\u63db\u304c\u3001\u3088\u308a\u4f4e\u3044\u6b21\u5143\u3078\u306e\u5c04\u5f71\u3092\u4f34\u3046\uff08\u60c5\u5831\u306e\u640d\u5931\u304c\u3042\u308b\uff09 - \u5bfe\u5fdc\u3059\u308b\u9023\u7acb\u65b9\u7a0b\u5f0f\u304c\u4e00\u610f\u306e\u89e3\u3092\u6301\u305f\u306a\u3044\uff08\u89e3\u306a\u3057\u304b\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\uff09 - \u884c\u5217\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u304c\u7dda\u5f62\u5f93\u5c5e\u3067\u3042\u308b</p>"},{"location":"lectures/LA/21-determinant/#q3","title":"Q3: \u5927\u304d\u306a\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u52b9\u7387\u7684\u306b\u8a08\u7b97\u3059\u308b\u65b9\u6cd5\u306f\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A3: \u306f\u3044\u3001\u4ee5\u4e0b\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\uff1a - \u57fa\u672c\u5909\u5f62\u3092\u5229\u7528\u3057\u3066\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u5f62\u3057\u3001\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3092\u6c42\u3081\u308b - \u4f59\u56e0\u5b50\u5c55\u958b\uff08\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u5b66\u7fd2\u3057\u307e\u3059\uff09 - \u6570\u5024\u8a08\u7b97\u30e9\u30a4\u30d6\u30e9\u30ea\uff08NumPy\u306a\u3069\uff09\u3092\u4f7f\u7528\u3059\u308b</p>"},{"location":"lectures/LA/21-determinant/#q4","title":"Q4: \u884c\u5217\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u306a\u610f\u5473\u3092\u3082\u3046\u5c11\u3057\u8a73\u3057\u304f\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A4: \u884c\u5217\u5f0f\u306f\u7dda\u5f62\u5909\u63db\u306b\u3088\u308b\u9762\u7a4d\u3084\u4f53\u7a4d\u306e\u5909\u5316\u7387\u3092\u8868\u3057\u307e\u3059\uff1a - 2\u00d72\u884c\u5217\u306e\u5834\u5408\u3001\u5358\u4f4d\u6b63\u65b9\u5f62\u304c\u5909\u63db\u5f8c\u306b\u3069\u308c\u3060\u3051\u306e\u9762\u7a4d\u306b\u306a\u308b\u304b\u3092\u793a\u3059 - \u884c\u5217\u5f0f\u304c\u6b63\u306a\u3089\u3001\u5909\u63db\u306f\u5411\u304d\u3092\u4fdd\u5b58\u3059\u308b - \u884c\u5217\u5f0f\u304c\u8ca0\u306a\u3089\u3001\u5909\u63db\u306f\u5411\u304d\u3092\u53cd\u8ee2\u3055\u305b\u308b - \u884c\u5217\u5f0f\u306e\u7d76\u5bfe\u5024\u304c\u5927\u304d\u3044\u307b\u3069\u3001\u5909\u63db\u306b\u3088\u308b\u62e1\u5927\u7387\u304c\u5927\u304d\u3044 - \u884c\u5217\u5f0f\u304c0\u306a\u3089\u3001\u5909\u63db\u5f8c\u306e\u56f3\u5f62\u306f\u4f4e\u6b21\u5143\uff08\u7dda\u3084\u70b9\uff09\u306b\u6f70\u308c\u308b</p>"},{"location":"lectures/LA/21-determinant/#q5","title":"Q5: \u7d71\u8a08\u5b66\u3084\u6a5f\u68b0\u5b66\u7fd2\u306b\u304a\u3051\u308b\u884c\u5217\u5f0f\u306e\u5fdc\u7528\u4f8b\u306f\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A5: \u306f\u3044\u3001\u3044\u304f\u3064\u304b\u306e\u91cd\u8981\u306a\u5fdc\u7528\u4f8b\u304c\u3042\u308a\u307e\u3059\uff1a - \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306b\u884c\u5217\u5f0f\u304c\u767b\u5834\u3059\u308b - \u4e3b\u6210\u5206\u5206\u6790\u3067\u306e\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u554f\u984c\u306b\u95a2\u9023\u3059\u308b - \u7dda\u5f62\u56de\u5e30\u306e\u6b63\u898f\u65b9\u7a0b\u5f0f\u306e\u4e00\u610f\u89e3\u306e\u5b58\u5728\u6761\u4ef6\u306e\u691c\u8a3c - \u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u8a08\u7b97\u306b\u9006\u884c\u5217\uff08\u304a\u3088\u3073\u884c\u5217\u5f0f\uff09\u304c\u4f7f\u308f\u308c\u308b</p>"},{"location":"lectures/LA/22-determinant/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u8b1b\u7fa9\u30ce\u30fc\u30c8 \u7b2c22\u56de","text":""},{"location":"lectures/LA/22-determinant/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c22\u56de \u30c6\u30fc\u30de: \u57fa\u672c\u5909\u5f62\u3068\u884c\u5217\u5f0f\u306e\u8a08\u7b97 \u95a2\u9023\u9805\u76ee: \u884c\u5217\u5f0f\u3001\u57fa\u672c\u5909\u5f62\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af\u3001\u9006\u884c\u5217 \u4e88\u7fd2\u5185\u5bb9:  - \u524d\u56de\u5b66\u7fd2\u3057\u305f\u884c\u5217\u5f0f\u306e\u5b9a\u7fa9\u3068\u57fa\u672c\u6027\u8cea\u306e\u5fa9\u7fd2 - \u57fa\u672c\u5909\u5f62\u306e\u6982\u5ff5\uff08\u7b2c11\u56de\u306e\u5185\u5bb9\uff09\u306e\u5fa9\u7fd2</p>"},{"location":"lectures/LA/22-determinant/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u3068\u884c\u5217\u5f0f\u306e\u5024\u306e\u5909\u5316\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3059\u308b</li> <li>\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u305f\u884c\u5217\u5f0f\u306e\u52b9\u7387\u7684\u306a\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3059\u308b</li> <li>\u57fa\u672c\u5909\u5f62\u3068\u884c\u5217\u5f0f\u3092\u7528\u3044\u3066\u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6\u3092\u8aac\u660e\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>2\u6b21\u30fb3\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u8a08\u7b97\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> </ol>"},{"location":"lectures/LA/22-determinant/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/22-determinant/#31","title":"3.1 \u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u306e\u5fa9\u7fd2","text":"<p>\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u306f\u3001\u884c\u5217\u3092\u5225\u306e\u884c\u5217\u306b\u5909\u5f62\u3059\u308b\u305f\u3081\u306e\u57fa\u672c\u7684\u306a\u64cd\u4f5c\u3067\u3059\u3002\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3067\u4f7f\u7528\u3057\u305f\u64cd\u4f5c\u3068\u540c\u3058\u3082\u306e\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9: \u884c\u5217\u306b\u5bfe\u3059\u308b\u57fa\u672c\u5909\u5f62\u3068\u306f\u3001\u4ee5\u4e0b\u306e3\u7a2e\u985e\u306e\u64cd\u4f5c\u306e\u3053\u3068\u3092\u6307\u3057\u307e\u3059\u3002 1. \u884c\u306e\u4ea4\u63db: 2\u3064\u306e\u884c\u3092\u5165\u308c\u66ff\u3048\u308b 2. \u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d: \u3042\u308b\u884c\u306e\u5168\u3066\u306e\u8981\u7d20\u30920\u3067\u306a\u3044\u5b9a\u6570\u500d\u3059\u308b 3. \u884c\u306e\u52a0\u7b97: \u3042\u308b\u884c\u306e\u5b9a\u6570\u500d\u3092\u5225\u306e\u884c\u306b\u52a0\u3048\u308b</p> <p>\u4f8b\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u884c\u5217\u306b\u5bfe\u3057\u3066\u57fa\u672c\u5909\u5f62\u3092\u884c\u3046\u3068\uff1a</p> \\[A = \\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9  \\end{pmatrix}\\] <ol> <li> <p>1\u884c\u76ee\u30682\u884c\u76ee\u3092\u4ea4\u63db\u3059\u308b\uff1a \\(\\(\\begin{pmatrix}  4 &amp; 5 &amp; 6 \\\\ 1 &amp; 2 &amp; 3 \\\\ 7 &amp; 8 &amp; 9  \\end{pmatrix}\\)\\)</p> </li> <li> <p>1\u884c\u76ee\u30922\u500d\u3059\u308b\uff1a \\(\\(\\begin{pmatrix}  2 &amp; 4 &amp; 6 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9  \\end{pmatrix}\\)\\)</p> </li> <li> <p>1\u884c\u76ee\u306e3\u500d\u30923\u884c\u76ee\u306b\u52a0\u3048\u308b\uff1a \\(\\(\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 10 &amp; 14 &amp; 18  \\end{pmatrix}\\)\\)</p> </li> </ol>"},{"location":"lectures/LA/22-determinant/#32","title":"3.2 \u884c\u5217\u5f0f\u3068\u57fa\u672c\u5909\u5f62\u306e\u95a2\u4fc2","text":"<p>\u57fa\u672c\u5909\u5f62\u3092\u884c\u3046\u3068\u3001\u884c\u5217\u5f0f\u306e\u5024\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u304b\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u306f\u3001\u884c\u5217\u5f0f\u3092\u52b9\u7387\u7684\u306b\u8a08\u7b97\u3059\u308b\u305f\u3081\u306b\u91cd\u8981\u3067\u3059\u3002</p> <p>\u5b9a\u7406: \u884c\u5217 \\(A\\) \u306e\u884c\u5217\u5f0f \\(\\det(A)\\) \u3068\u57fa\u672c\u5909\u5f62\u5f8c\u306e\u884c\u5217 \\(A'\\) \u306e\u884c\u5217\u5f0f \\(\\det(A')\\) \u306e\u95a2\u4fc2\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 1. \u884c\u306e\u4ea4\u63db: \\(\\det(A') = -\\det(A)\\)\uff08\u7b26\u53f7\u304c\u53cd\u8ee2\uff09 2. \u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d: \\(\\det(A') = c \\cdot \\det(A)\\)\uff08\\(c\\) \u306f\u5b9a\u6570\uff09 3. \u884c\u306e\u52a0\u7b97: \\(\\det(A') = \\det(A)\\)\uff08\u5909\u5316\u306a\u3057\uff09</p> <p>\u3053\u308c\u3089\u306e\u95a2\u4fc2\u306f\u3001\u884c\u5217\u5f0f\u306e\u6027\u8cea\u304b\u3089\u5c0e\u304b\u308c\u307e\u3059\u3002\u7279\u306b\u7b2c3\u306e\u6027\u8cea\u306f\u3001\u884c\u5217\u5f0f\u306e\u591a\u91cd\u7dda\u5f62\u6027\u304b\u3089\u5c0e\u304b\u308c\u308b\u91cd\u8981\u306a\u6027\u8cea\u3067\u3059\u3002</p> <p>\u5177\u4f53\u4f8b:</p> <p>\u6b21\u306e \\(3 \\times 3\\) \u884c\u5217\u3092\u8003\u3048\u307e\u3057\u3087\u3046\uff1a \\(\\(A = \\begin{pmatrix}  2 &amp; 1 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9  \\end{pmatrix}\\)\\)</p> <p>\u3053\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u5c55\u958b\u3059\u308b\u3068 \\(\\det(A) = -12\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <ol> <li>\u884c\u306e\u4ea4\u63db:     \u7b2c1\u884c\u3068\u7b2c2\u884c\u3092\u4ea4\u63db\u3059\u308b\u3068\uff1a    \\(\\(A' = \\begin{pmatrix}     4 &amp; 5 &amp; 6 \\\\    2 &amp; 1 &amp; 3 \\\\    7 &amp; 8 &amp; 9     \\end{pmatrix}\\)\\)</li> </ol> <p>\u884c\u5217\u5f0f\u306f \\(\\det(A') = 12 = -\\det(A)\\) \u3068\u306a\u308a\u3001\u7b26\u53f7\u304c\u53cd\u8ee2\u3057\u3066\u3044\u307e\u3059\u3002</p> <ol> <li>\u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d:    \u7b2c1\u884c\u30923\u500d\u3059\u308b\u3068\uff1a    \\(\\(A'' = \\begin{pmatrix}     6 &amp; 3 &amp; 9 \\\\    4 &amp; 5 &amp; 6 \\\\    7 &amp; 8 &amp; 9     \\end{pmatrix}\\)\\)</li> </ol> <p>\u884c\u5217\u5f0f\u306f \\(\\det(A'') = 3 \\cdot (-12) = -36 = 3 \\cdot \\det(A)\\) \u3068\u306a\u308a\u3001\u30b9\u30ab\u30e9\u30fc\u500d\uff08\u3053\u306e\u5834\u5408\u306f3\u500d\uff09\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <ol> <li>\u884c\u306e\u52a0\u7b97:    \u7b2c1\u884c\u306e2\u500d\u3092\u7b2c3\u884c\u306b\u52a0\u3048\u308b\u3068\uff1a    \\(\\(A''' = \\begin{pmatrix}     2 &amp; 1 &amp; 3 \\\\    4 &amp; 5 &amp; 6 \\\\    11 &amp; 10 &amp; 15     \\end{pmatrix}\\)\\)</li> </ol> <p>\u884c\u5217\u5f0f\u306f\u8a08\u7b97\u3059\u308b\u3068 \\(\\det(A''') = -12 = \\det(A)\\) \u3068\u306a\u308a\u3001\u5024\u306f\u5909\u5316\u3057\u3066\u3044\u307e\u305b\u3093\u3002</p> <p>\u3053\u308c\u3089\u306e\u4f8b\u304b\u3089\u3001\u57fa\u672c\u5909\u5f62\u304c\u884c\u5217\u5f0f\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\u7279\u306b\u884c\u306e\u52a0\u7b97\u306f\u884c\u5217\u5f0f\u306e\u5024\u3092\u5909\u3048\u306a\u3044\u3068\u3044\u3046\u6027\u8cea\u306f\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u305f\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u306b\u304a\u3044\u3066\u975e\u5e38\u306b\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/22-determinant/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/22-determinant/#41","title":"4.1 \u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u305f\u884c\u5217\u5f0f\u306e\u8a08\u7b97","text":"<p>\u57fa\u672c\u5909\u5f62\u3092\u5229\u7528\u3059\u308b\u3068\u3001\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u3092\u7c21\u7565\u5316\u3067\u304d\u307e\u3059\u3002\u7279\u306b\u3001\u4e09\u89d2\u884c\u5217\uff08\u4e0a\u4e09\u89d2\u307e\u305f\u306f\u4e0b\u4e09\u89d2\u884c\u5217\uff09\u306e\u884c\u5217\u5f0f\u306f\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3068\u306a\u308b\u305f\u3081\u3001\u57fa\u672c\u5909\u5f62\u306b\u3088\u3063\u3066\u884c\u5217\u3092\u4e09\u89d2\u884c\u5217\u306b\u5909\u5f62\u3059\u308b\u3053\u3068\u3067\u8a08\u7b97\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u5b9a\u7406: \u4e09\u89d2\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u3001\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u306b\u7b49\u3057\u3044\u3002 \\(\\(\\det\\begin{pmatrix}  a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ 0 &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_{nn}  \\end{pmatrix} = a_{11} \\cdot a_{22} \\cdot \\ldots \\cdot a_{nn}\\)\\)</p> <p>\u3053\u306e\u6027\u8cea\u3092\u5229\u7528\u3057\u3066\u3001\u4e00\u822c\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u624b\u9806\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> <ol> <li>\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u3001\u884c\u5217\u3092\u4e0a\u4e09\u89d2\uff08\u307e\u305f\u306f\u4e0b\u4e09\u89d2\uff09\u884c\u5217\u306b\u5909\u5f62\u3059\u308b</li> <li>\u57fa\u672c\u5909\u5f62\u306b\u3088\u3063\u3066\u884c\u5217\u5f0f\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3057\u305f\u304b\u3092\u8ffd\u8de1\u3059\u308b</li> <li>\u4e09\u89d2\u884c\u5217\u306e\u884c\u5217\u5f0f\uff08\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\uff09\u3092\u8a08\u7b97\u3059\u308b</li> <li>\u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u5909\u5316\u3092\u8003\u616e\u3057\u3066\u3001\u5143\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u6c42\u3081\u308b</li> </ol> <p>\u5177\u4f53\u4f8b: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[A = \\begin{pmatrix}  2 &amp; 1 &amp; 3 \\\\ 4 &amp; -1 &amp; 2 \\\\ -2 &amp; 5 &amp; 7  \\end{pmatrix}\\] <p>\u30b9\u30c6\u30c3\u30d71: \u57fa\u672c\u5909\u5f62\u3092\u4f7f\u3063\u3066\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u5f62\u3057\u307e\u3059\u3002</p> <p>\\(1\\)\u884c\u76ee\u306e\\(-2\\)\u500d\u3092\\(2\\)\u884c\u76ee\u306b\u52a0\u3048\u308b\uff1a \\(\\(\\begin{pmatrix}  2 &amp; 1 &amp; 3 \\\\ 0 &amp; -3 &amp; -4 \\\\ -2 &amp; 5 &amp; 7  \\end{pmatrix}\\)\\)</p> <p>\\(1\\)\u884c\u76ee\u3092\\(3\\)\u884c\u76ee\u306b\u52a0\u3048\u308b\uff1a \\(\\(\\begin{pmatrix}  2 &amp; 1 &amp; 3 \\\\ 0 &amp; -3 &amp; -4 \\\\ 0 &amp; 6 &amp; 10  \\end{pmatrix}\\)\\)</p> <p>\\(2\\)\u884c\u76ee\u306e\\(-2\\)\u500d\u3092\\(3\\)\u884c\u76ee\u306b\u52a0\u3048\u308b\uff1a \\(\\(\\begin{pmatrix}  2 &amp; 1 &amp; 3 \\\\ 0 &amp; -3 &amp; -4 \\\\ 0 &amp; 0 &amp; 2  \\end{pmatrix}\\)\\)</p> <p>\u30b9\u30c6\u30c3\u30d72: \u4e0a\u4e09\u89d2\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 \\(\\(\\det(A) = 2 \\times (-3) \\times 2 = -12\\)\\)</p> <p>\u884c\u306e\u52a0\u7b97\u306f\u884c\u5217\u5f0f\u306e\u5024\u3092\u5909\u5316\u3055\u305b\u306a\u3044\u305f\u3081\u3001\u5143\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3082 \\(\\det(A) = -12\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/22-determinant/#42","title":"4.2 \u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2","text":"<p>\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u884c\u5217\u5f0f\u306b\u306f\u5bc6\u63a5\u306a\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\u3002\u7279\u306b\u3001\u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\uff08\u30e9\u30f3\u30af\u304c\u884c\u6570\u3088\u308a\u3082\u5c0f\u3055\u3044\u884c\u5217\uff09\u306e\u884c\u5217\u5f0f\u306f0\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u5b9a\u7406: \\(n \\times n\\) \u884c\u5217 \\(A\\) \u306b\u3064\u3044\u3066\u3001\\(\\text{rank}(A) &lt; n\\) \u3067\u3042\u308c\u3070 \\(\\det(A) = 0\\) \u3067\u3042\u308b\u3002</p> <p>\u3053\u306e\u5b9a\u7406\u306f\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af\u304c\u884c\u6570\u3088\u308a\u3082\u5c0f\u3055\u3044\u3068\u3044\u3046\u3053\u3068\u306f\u3001\u5c11\u306a\u304f\u3068\u30821\u3064\u306e\u884c\u304c\u4ed6\u306e\u884c\u306e\u7dda\u5f62\u7d50\u5408\u3067\u8868\u3055\u308c\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u3001\u884c\u5217\u5f0f\u306e\u6027\u8cea\u304b\u3089\u3001\u305d\u306e\u3088\u3046\u306a\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f0\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u8a3c\u660e\u306e\u6982\u7565: \u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\u3067\u306f\u3001\u3042\u308b\u884c\u304c\u4ed6\u306e\u884c\u306e\u7dda\u5f62\u7d50\u5408\u3068\u3057\u3066\u8868\u3055\u308c\u307e\u3059\u3002\u884c\u306e\u52a0\u7b97\u306b\u3088\u308b\u57fa\u672c\u5909\u5f62\u306f\u884c\u5217\u5f0f\u306e\u5024\u3092\u5909\u5316\u3055\u305b\u306a\u3044\u305f\u3081\u3001\u57fa\u672c\u5909\u5f62\u306b\u3088\u3063\u3066\u3042\u308b\u884c\u3092\u5168\u30660\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u30020\u884c\u3092\u542b\u3080\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f0\u3067\u3059\u3002</p> <p>\u5177\u4f53\u4f8b: \u6b21\u306e\u884c\u5217\u306f\u30e9\u30f3\u30af2\u3067\u3042\u308a\u3001\u7b2c3\u884c\u306f\u7b2c1\u884c\u3068\u7b2c2\u884c\u306e\u7dda\u5f62\u7d50\u5408\uff08\u7b2c1\u884c + \u7b2c2\u884c\uff09\u3067\u8868\u3055\u308c\u307e\u3059\u3002</p> \\[B = \\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 5 &amp; 7 &amp; 9  \\end{pmatrix}\\] <p>\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u8a08\u7b97\u3059\u308b\u3068\uff1a</p> <p>\\(1\\)\u884c\u76ee\u306e\\(-4\\)\u500d\u3092\\(2\\)\u884c\u76ee\u306b\u52a0\u3048\u308b\uff1a \\(\\(\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\ 0 &amp; -3 &amp; -6 \\\\ 5 &amp; 7 &amp; 9  \\end{pmatrix}\\)\\)</p> <p>\\(1\\)\u884c\u76ee\u306e\\(-5\\)\u500d\u3092\\(3\\)\u884c\u76ee\u306b\u52a0\u3048\u308b\uff1a \\(\\(\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\ 0 &amp; -3 &amp; -6 \\\\ 0 &amp; -3 &amp; -6  \\end{pmatrix}\\)\\)</p> <p>\\(2\\)\u884c\u76ee\u306e\\(-1\\)\u500d\u3092\\(3\\)\u884c\u76ee\u306b\u52a0\u3048\u308b\uff1a \\(\\(\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\ 0 &amp; -3 &amp; -6 \\\\ 0 &amp; 0 &amp; 0  \\end{pmatrix}\\)\\)</p> <p>0\u884c\u3092\u542b\u3080\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f0\u306a\u306e\u3067\u3001\\(\\det(B) = 0\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/22-determinant/#43","title":"4.3 \u884c\u5217\u5f0f\u3068\u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6","text":"<p>\u884c\u5217\u5f0f\u306f\u3001\u9006\u884c\u5217\u306e\u5b58\u5728\u3092\u5224\u5b9a\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u57fa\u6e96\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u5b9a\u7406: \\(n \\times n\\) \u884c\u5217 \\(A\\) \u304c\u53ef\u9006\uff08\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\uff09\u3067\u3042\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f \\(\\det(A) \\neq 0\\) \u3067\u3042\u308b\u3002</p> <p>\u3053\u306e\u5b9a\u7406\u306f\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u9006\u884c\u5217\u306e\u5b58\u5728\u6761\u4ef6\u3092\u7d50\u3073\u3064\u3051\u308b\u3082\u306e\u3067\u3059\u3002\u524d\u8ff0\u3057\u305f\u3088\u3046\u306b\u3001\\(\\det(A) = 0\\) \u306e\u3068\u304d\u3001\\(\\text{rank}(A) &lt; n\\) \u3068\u306a\u308a\u3001\u884c\u5217\u306f\u6b63\u5247\uff08\u975e\u7279\u7570\uff09\u3067\u306f\u306a\u304f\u306a\u308a\u307e\u3059\u3002</p> <p>\u5177\u4f53\u4f8b: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u3001\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u304b\u3092\u5224\u5b9a\u3057\u307e\u3059\u3002</p> \\[C = \\begin{pmatrix}  2 &amp; 1 \\\\ 4 &amp; 2  \\end{pmatrix}\\] <p>\u884c\u5217\u5f0f\u306f \\(\\det(C) = 2 \\times 2 - 1 \\times 4 = 4 - 4 = 0\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\det(C) = 0\\) \u306a\u306e\u3067\u3001\u3053\u306e\u884c\u5217\u306f\u9006\u884c\u5217\u3092\u6301\u3061\u307e\u305b\u3093\u3002\u5b9f\u969b\u3001\u3053\u306e\u884c\u5217\u306e\u7b2c2\u884c\u306f\u7b2c1\u884c\u306e2\u500d\u3067\u3042\u308a\u3001\u30e9\u30f3\u30af\u306f1\uff08\u884c\u65702\u3088\u308a\u3082\u5c0f\u3055\u3044\uff09\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/22-determinant/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u884c\u5217\u306e\u57fa\u672c\u5909\u5f62\u3068\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u3092Python\u3067\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002NumPy\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[2, 1, 3],\n              [4, -1, 2],\n              [-2, 5, 7]])\n\n# \u884c\u5217\u5f0f\u306e\u8a08\u7b97\ndet_A = np.linalg.det(A)\nprint(\"\u884c\u5217\u5f0f det(A) =\", det_A)\n\n# \u884c\u306e\u57fa\u672c\u5909\u5f62\u3092\u5b9f\u884c\u3059\u308b\u95a2\u6570\ndef row_exchange(matrix, i, j):\n    \"\"\"\u884ci\u3068\u884cj\u3092\u4ea4\u63db\"\"\"\n    result = matrix.copy()\n    result[[i, j]] = result[[j, i]]\n    return result\n\ndef row_multiply(matrix, i, c):\n    \"\"\"\u884ci\u3092c\u500d\"\"\"\n    result = matrix.copy()\n    result[i] = c * result[i]\n    return result\n\ndef row_add(matrix, i, j, c):\n    \"\"\"\u884cj\u306bc\u500d\u3057\u305f\u884ci\u3092\u52a0\u3048\u308b\"\"\"\n    result = matrix.copy()\n    result[j] = result[j] + c * result[i]\n    return result\n\n# \u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3067\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u63db\ndef to_upper_triangular(matrix):\n    n = matrix.shape[0]\n    result = matrix.copy().astype(float)  # \u6d6e\u52d5\u5c0f\u6570\u70b9\u306b\u5909\u63db\n    multipliers = []  # \u884c\u5217\u5f0f\u306e\u5909\u5316\u3092\u8ffd\u8de1\u3059\u308b\u305f\u3081\u306e\u4e57\u6570\n\n    for i in range(n):\n        # \u30d4\u30dc\u30c3\u30c8\u304c0\u306a\u3089\u884c\u3092\u4ea4\u63db\n        if result[i, i] == 0:\n            for j in range(i+1, n):\n                if result[j, i] != 0:\n                    result = row_exchange(result, i, j)\n                    multipliers.append(-1)  # \u884c\u4ea4\u63db\u3067\u7b26\u53f7\u53cd\u8ee2\n                    break\n            else:\n                # \u30d4\u30dc\u30c3\u30c8\u5217\u304c\u3059\u3079\u30660\u306a\u3089\u30e9\u30f3\u30af\u843d\u3061\n                return result, 0\n\n        # \u5bfe\u89d2\u6210\u5206\u4ee5\u4e0b\u3092\u6d88\u53bb\n        for j in range(i+1, n):\n            if result[j, i] != 0:\n                factor = result[j, i] / result[i, i]\n                result = row_add(result, i, j, -factor)\n\n    # \u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3092\u8a08\u7b97\n    det = 1\n    for i in range(n):\n        det *= result[i, i]\n\n    # \u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u4e57\u6570\u3092\u8003\u616e\n    for m in multipliers:\n        det *= m\n\n    return result, det\n\n# \u4e0a\u4e09\u89d2\u884c\u5217\u3078\u306e\u5909\u63db\u3068\u884c\u5217\u5f0f\u306e\u8a08\u7b97\nupper_triangular_A, det_A_manual = to_upper_triangular(A)\nprint(\"\u4e0a\u4e09\u89d2\u884c\u5217:\")\nprint(upper_triangular_A)\nprint(\"\u624b\u52d5\u8a08\u7b97\u3057\u305f\u884c\u5217\u5f0f det(A) =\", det_A_manual)\n\n# \u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\u306e\u4f8b\nB = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [5, 7, 9]])\ndet_B = np.linalg.det(B)\nprint(\"\\n\u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\u306e\u884c\u5217\u5f0f det(B) =\", det_B)\n\n# \u884c\u5217\u5f0f\u306e\u5024\u3068\u30e9\u30f3\u30af\u306e\u53ef\u8996\u5316\nmatrices = []\ndeterminants = []\nranks = []\n\n# \u69d8\u3005\u306a\u884c\u5217\u3092\u751f\u6210\nfor i in range(20):\n    # \u30e9\u30f3\u30c0\u30e0\u306a\u884c\u5217\u3092\u751f\u6210\n    matrix = np.random.randint(-5, 6, (3, 3))\n    matrices.append(matrix)\n\n    # \u884c\u5217\u5f0f\u3092\u8a08\u7b97\n    det = np.linalg.det(matrix)\n    determinants.append(abs(det))\n\n    # \u30e9\u30f3\u30af\u3092\u8a08\u7b97\n    rank = np.linalg.matrix_rank(matrix)\n    ranks.append(rank)\n\n# \u884c\u5217\u5f0f\u306e\u7d76\u5bfe\u5024\u3068\u30e9\u30f3\u30af\u306e\u95a2\u4fc2\u306e\u6563\u5e03\u56f3\nplt.figure(figsize=(10, 6))\nfor rank in range(1, 4):\n    indices = [i for i, r in enumerate(ranks) if r == rank]\n    plt.scatter([determinants[i] for i in indices], \n                [rank] * len(indices),\n                label=f'Rank {rank}')\n\nplt.xlabel('\u884c\u5217\u5f0f\u306e\u7d76\u5bfe\u5024')\nplt.ylabel('\u884c\u5217\u306e\u30e9\u30f3\u30af')\nplt.title('3\u00d73\u884c\u5217\u306b\u304a\u3051\u308b\u884c\u5217\u5f0f\u306e\u7d76\u5bfe\u5024\u3068\u30e9\u30f3\u30af\u306e\u95a2\u4fc2')\nplt.legend()\nplt.grid(True)\nplt.yscale('linear')\nplt.xscale('log')  # \u884c\u5217\u5f0f\u306e\u5024\u304c\u5e83\u7bc4\u56f2\u306b\u5206\u5e03\u3059\u308b\u305f\u3081\u5bfe\u6570\u30b9\u30b1\u30fc\u30eb\nplt.show()\n\n# \u9006\u884c\u5217\u306e\u5b58\u5728\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2\ninvertible = [abs(np.linalg.det(matrices[i])) &gt; 1e-10 for i in range(len(matrices))]\nsns.histplot(determinants, bins=20)\nplt.xlabel('\u884c\u5217\u5f0f\u306e\u7d76\u5bfe\u5024')\nplt.ylabel('\u983b\u5ea6')\nplt.title('\u884c\u5217\u5f0f\u306e\u7d76\u5bfe\u5024\u306e\u5206\u5e03')\nplt.axvline(x=1e-10, color='r', linestyle='--', label='\u9006\u884c\u5217\u5b58\u5728\u306e\u95be\u5024')\nplt.legend()\nplt.show()\n\nprint(f\"\u751f\u6210\u3057\u305f\u884c\u5217\u306e\u3046\u3061 {sum(invertible)} \u500b\u306f\u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u307e\u3059\u3002\")\n</code></pre> <p>\u3053\u306e\u5b9f\u88c5\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u3092\u884c\u3044\u307e\u3059\uff1a</p> <ol> <li>NumPy\u3092\u4f7f\u7528\u3057\u3066\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97</li> <li>\u57fa\u672c\u5909\u5f62\u306e\u64cd\u4f5c\u3092\u5b9f\u88c5\u3057\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3067\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u63db</li> <li>\u4e0a\u4e09\u89d2\u884c\u5217\u304b\u3089\u884c\u5217\u5f0f\u3092\u8a08\u7b97</li> <li>\u30e9\u30f3\u30af\u843d\u3061\u884c\u5217\u306e\u884c\u5217\u5f0f\u304c0\u306b\u306a\u308b\u3053\u3068\u3092\u78ba\u8a8d</li> <li>\u30e9\u30f3\u30c0\u30e0\u306a\u884c\u5217\u3092\u751f\u6210\u3057\u3001\u884c\u5217\u5f0f\u3068\u30e9\u30f3\u30af\u306e\u95a2\u4fc2\u3092\u53ef\u8996\u5316</li> <li>\u884c\u5217\u5f0f\u306e\u7d76\u5bfe\u5024\u3068\u9006\u884c\u5217\u306e\u5b58\u5728\u95a2\u4fc2\u3092\u53ef\u8996\u5316</li> </ol>"},{"location":"lectures/LA/22-determinant/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/22-determinant/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u3001\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u8a08\u7b97\u305b\u3088\u3002    \\(\\(A = \\begin{pmatrix}     3 &amp; 1 &amp; -2 \\\\    -2 &amp; 4 &amp; 5 \\\\    6 &amp; -3 &amp; 1     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u3001\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u8a08\u7b97\u305b\u3088\u3002    \\(\\(B = \\begin{pmatrix}     2 &amp; -1 &amp; 3 &amp; 4 \\\\    1 &amp; 2 &amp; 0 &amp; -2 \\\\    -1 &amp; 3 &amp; 2 &amp; 1 \\\\    3 &amp; 0 &amp; -1 &amp; 2     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u306b\u3064\u3044\u3066\u3001\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u3001\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002    \\(\\(C = \\begin{pmatrix}     1 &amp; 3 &amp; 2 \\\\    -1 &amp; 2 &amp; 4 \\\\    2 &amp; 1 &amp; 6     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u884c\u5217 \\(D\\) \u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u57fa\u672c\u5909\u5f62\u306b\u3088\u3063\u3066\u884c\u5217 \\(E\\) \u306b\u5909\u63db\u3055\u308c\u305f\u3068\u304d\u3001\\(\\det(D)\\) \u3068 \\(\\det(E)\\) \u306e\u95a2\u4fc2\u3092\u6c42\u3081\u3088\u3002</p> </li> <li>1\u884c\u76ee\u30683\u884c\u76ee\u3092\u4ea4\u63db</li> <li>2\u884c\u76ee\u30925\u500d</li> <li>1\u884c\u76ee\u306e2\u500d\u30923\u884c\u76ee\u306b\u52a0\u3048\u308b</li> </ol>"},{"location":"lectures/LA/22-determinant/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u4ee5\u4e0b\u306e4\u00d74\u884c\u5217 \\(F\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u3001\\(\\det(F)\\) \u3092\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u52b9\u7387\u7684\u306b\u8a08\u7b97\u305b\u3088\u3002    \\(\\(F = \\begin{pmatrix}     1 &amp; 0 &amp; 2 &amp; -1 \\\\    3 &amp; 4 &amp; -2 &amp; 0 \\\\    -2 &amp; 1 &amp; 3 &amp; 5 \\\\    4 &amp; -3 &amp; 0 &amp; 2     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u884c\u5217\u306e\u30e9\u30f3\u30af\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u8003\u3048\u308b\u3002\\(n \\times n\\) \u884c\u5217 \\(A\\) \u306b\u3064\u3044\u3066\u3001\\(\\text{rank}(A) = n-1\\) \u306e\u3068\u304d\u3001\\(A\\) \u306e\u4f59\u56e0\u5b50\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u3069\u3046\u306a\u308b\u304b\u8003\u5bdf\u305b\u3088\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u8907\u6570\u306e\u751f\u7406\u6307\u6a19\u306e\u95a2\u4fc2\u6027\u3092\u8abf\u3079\u308b\u305f\u3081\u306b\u4e3b\u6210\u5206\u5206\u6790\u3092\u884c\u3046\u524d\u51e6\u7406\u3068\u3057\u3066\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217 \\(\\Sigma\\) \u304c\u4f7f\u7528\u3055\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u3002\u3053\u306e\u5171\u5206\u6563\u884c\u5217\u304c\u7279\u7570\uff08\\(\\det(\\Sigma) = 0\\)\uff09\u3067\u3042\u308b\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306a\u554f\u984c\u304c\u751f\u3058\u308b\u304b\u3001\u307e\u305f\u3001\u305d\u306e\u5bfe\u51e6\u6cd5\u306b\u3064\u3044\u3066\u8aac\u660e\u305b\u3088\u3002\u6b21\u306e\u5171\u5206\u6563\u884c\u5217\u306b\u3064\u3044\u3066\u8003\u5bdf\u305b\u3088\u3002    \\(\\(\\Sigma = \\begin{pmatrix}     10 &amp; 5 &amp; 15 \\\\    5 &amp; 4 &amp; 9 \\\\    15 &amp; 9 &amp; 24     \\end{pmatrix}\\)\\)</p> </li> </ol>"},{"location":"lectures/LA/22-determinant/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/22-determinant/#q1","title":"Q1: \u57fa\u672c\u5909\u5f62\u306f\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3068\u4f55\u304c\u9055\u3046\u306e\u3067\u3059\u304b\uff1f","text":"<p>A1: \u57fa\u672c\u5909\u5f62\u306f\u884c\u5217\u3092\u5909\u5f62\u3059\u308b\u305f\u3081\u306e\u57fa\u672c\u7684\u306a\u64cd\u4f5c\u306e\u3053\u3068\u3067\u3042\u308a\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306f\u3053\u308c\u3089\u306e\u57fa\u672c\u5909\u5f62\u3092\u7cfb\u7d71\u7684\u306b\u9069\u7528\u3057\u3066\u9023\u7acb\u4e00\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u3044\u305f\u308a\u3001\u884c\u5217\u3092\u4e09\u89d2\u884c\u5217\u3084\u7c21\u7d04\u968e\u6bb5\u5f62\u306b\u5909\u63db\u3057\u305f\u308a\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u306f\u57fa\u672c\u5909\u5f62\u3092\u4f7f\u7528\u3059\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/22-determinant/#q2-0","title":"Q2: \u884c\u5217\u5f0f\u304c0\u306e\u884c\u5217\u306f\u5fc5\u305a\u30e9\u30f3\u30af\u843d\u3061\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304b\uff1f","text":"<p>A2: \u306f\u3044\u3001\\(n \\times n\\) \u884c\u5217 \\(A\\) \u306b\u3064\u3044\u3066 \\(\\det(A) = 0\\) \u3067\u3042\u308c\u3070\u3001\u5fc5\u305a \\(\\text{rank}(A) &lt; n\\) \u3068\u306a\u308a\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u884c\u5217\u5f0f\u304c0\u3067\u3042\u308b\u3053\u3068\u306f\u3001\u305d\u306e\u884c\u5217\u304c\u30e9\u30f3\u30af\u843d\u3061\u3057\u3066\u3044\u308b\u3053\u3068\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u3067\u3059\u3002</p>"},{"location":"lectures/LA/22-determinant/#q3","title":"Q3: \u57fa\u672c\u5909\u5f62\u3067\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u969b\u306b\u3001\u6ce8\u610f\u3059\u3079\u304d\u70b9\u306f\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A3: \u4ee5\u4e0b\u306e\u3088\u3046\u306a\u70b9\u306b\u6ce8\u610f\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff1a - \u884c\u306e\u4ea4\u63db\u306f\u884c\u5217\u5f0f\u306e\u7b26\u53f7\u3092\u53cd\u8ee2\u3055\u305b\u307e\u3059 - \u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d\u306f\u884c\u5217\u5f0f\u3092\u305d\u306e\u5b9a\u6570\u500d\u3057\u307e\u3059 - \u8a08\u7b97\u904e\u7a0b\u3067\u30d4\u30dc\u30c3\u30c8\u304c0\u306b\u306a\u3063\u305f\u5834\u5408\u306f\u3001\u884c\u306e\u4ea4\u63db\u304c\u5fc5\u8981\u306b\u306a\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059 - \u6570\u5024\u8a08\u7b97\u3067\u306f\u4e38\u3081\u8aa4\u5dee\u306b\u6ce8\u610f\u3057\u3001\u53ef\u80fd\u3067\u3042\u308c\u3070\u5206\u6570\u3084\u6b63\u78ba\u306a\u8a08\u7b97\u3092\u884c\u3046\u3053\u3068\u304c\u671b\u307e\u3057\u3044\u3067\u3059</p>"},{"location":"lectures/LA/22-determinant/#q4","title":"Q4: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u3001\u884c\u5217\u5f0f\u306f\u3069\u306e\u3088\u3046\u306a\u5834\u9762\u3067\u4f7f\u308f\u308c\u307e\u3059\u304b\uff1f","text":"<p>A4: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u884c\u5217\u5f0f\u306e\u6d3b\u7528\u4f8b\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\uff1a - \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306e\u8a08\u7b97 - \u4e3b\u6210\u5206\u5206\u6790\u306b\u304a\u3051\u308b\u5171\u5206\u6563\u884c\u5217\u306e\u975e\u7279\u7570\u6027\u306e\u78ba\u8a8d - \u7dda\u5f62\u5909\u63db\u306b\u304a\u3051\u308b\u4f53\u7a4d\u306e\u5909\u5316\u7387\u306e\u6e2c\u5b9a - \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u591a\u91cd\u5171\u7dda\u6027\u306e\u691c\u51fa - \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u624b\u6cd5\u3067\u306e\u7fa4\u306e\u5206\u6563\u69cb\u9020\u306e\u8a55\u4fa1</p>"},{"location":"lectures/LA/22-determinant/#q5-laplace","title":"Q5: \u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u65b9\u6cd5\u3068Laplace\u5c55\u958b\u306b\u3088\u308b\u65b9\u6cd5\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A5: \u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u65b9\u6cd5\u306f\u3001\u884c\u5217\u3092\u4e0a\u4e09\u89d2\u5f62\u307e\u305f\u306f\u4e0b\u4e09\u89d2\u5f62\u306b\u5909\u63db\u3057\u3001\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3068\u3057\u3066\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u4e00\u65b9\u3001Laplace\u5c55\u958b\uff08\u4f59\u56e0\u5b50\u5c55\u958b\uff09\u306f\u3001\u884c\u5217\u306e\u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306b\u6cbf\u3063\u3066\u3001\u305d\u306e\u8981\u7d20\u3068\u4f59\u56e0\u5b50\u306e\u7a4d\u306e\u548c\u3068\u3057\u3066\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u8a08\u7b97\u91cf\u306e\u89b3\u70b9\u3067\u306f\u3001\u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u65b9\u6cd5\u306f\u4e00\u822c\u7684\u306b \\(O(n^3)\\) \u306e\u8a08\u7b97\u91cf\u3067\u3059\u304c\u3001Laplace\u5c55\u958b\u306f\u518d\u5e30\u7684\u306b\u9069\u7528\u3059\u308b\u3068 \\(O(n!)\\) \u3068\u306a\u308a\u3001\u5927\u304d\u306a\u884c\u5217\u3067\u306f\u975e\u52b9\u7387\u3067\u3059\u3002</p>"},{"location":"lectures/LA/22-determinant/#8","title":"8. \u4ed8\u9332\uff1a\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u306e\u52b9\u7387\u5316\u30c6\u30af\u30cb\u30c3\u30af","text":"<p>\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u3092\u52b9\u7387\u5316\u3059\u308b\u305f\u3081\u306e\u3044\u304f\u3064\u304b\u306e\u30c6\u30af\u30cb\u30c3\u30af\u3092\u7d39\u4ecb\u3057\u307e\u3059\uff1a</p> <ol> <li> <p>\u758e\u884c\u5217\u306e\u6d3b\u7528: \u884c\u5217\u306b\u591a\u304f\u306e0\u8981\u7d20\u304c\u3042\u308b\u5834\u5408\u30010\u3092\u542b\u3080\u884c\u3084\u5217\u306b\u6cbf\u3063\u3066\u5c55\u958b\u3059\u308b\u3068\u8a08\u7b97\u304c\u7c21\u7565\u5316\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u7279\u5225\u306a\u30d1\u30bf\u30fc\u30f3\u306e\u8a8d\u8b58: </p> </li> <li>\u4e09\u89d2\u884c\u5217\uff08\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\uff09</li> <li>\u5bfe\u89d2\u30d6\u30ed\u30c3\u30af\u884c\u5217\uff08\u30d6\u30ed\u30c3\u30af\u306e\u884c\u5217\u5f0f\u306e\u7a4d\uff09</li> <li> <p>\u884c\u5217\u306e\u548c\u3068\u7a4d\u306e\u7279\u6b8a\u306a\u5f62</p> </li> <li> <p>\u65e2\u77e5\u306e\u884c\u5217\u5f0f\u516c\u5f0f\u306e\u6d3b\u7528:    \\(\\(\\det\\begin{pmatrix}     a &amp; b \\\\    c &amp; d     \\end{pmatrix} = ad - bc\\)\\)</p> </li> </ol> <p>\\(\\(\\det\\begin{pmatrix}     a &amp; b &amp; c \\\\    d &amp; e &amp; f \\\\    g &amp; h &amp; i     \\end{pmatrix} = a(ei - fh) - b(di - fg) + c(dh - eg)\\)\\)</p> <ol> <li>\u7279\u6b8a\u306a\u884c\u5217\u306e\u6027\u8cea\u306e\u5229\u7528:</li> <li>\u76f4\u4ea4\u884c\u5217: \\(\\det(Q) = \\pm 1\\)</li> <li>\u76f8\u4f3c\u884c\u5217: \\(\\det(P^{-1}AP) = \\det(A)\\)</li> <li>\u7a4d\u306e\u884c\u5217\u5f0f: \\(\\det(AB) = \\det(A) \\cdot \\det(B)\\)</li> </ol> <p>\u3053\u308c\u3089\u306e\u30c6\u30af\u30cb\u30c3\u30af\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u3092\u5927\u5e45\u306b\u52b9\u7387\u5316\u3067\u304d\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II\uff1a\u7b2c23\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/23-determinant/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c23\u56de \u30c6\u30fc\u30de: \u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u306b\u3088\u308b\u884c\u5217\u5f0f\u306e\u8a08\u7b97 \u95a2\u9023\u9805\u76ee: \u884c\u5217\u5f0f\u3001\u9084\u5143\u5b9a\u7406\u3001\u4f59\u56e0\u5b50\u5c55\u958b \u4e88\u7fd2\u5185\u5bb9: \u524d\u56de\u306e\u57fa\u672c\u5909\u5f62\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u5fa9\u7fd2\u3057\u3066\u304a\u304f\u3053\u3068</p>"},{"location":"lectures/LA/23-determinant/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u9805\u76ee\u306b\u3064\u3044\u3066\u7406\u89e3\u3057\u5b9f\u884c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\uff1a</p> <ol> <li>\u884c\u5217\u5f0f\u8a08\u7b97\u306b\u304a\u3051\u308b\u9084\u5143\u5b9a\u7406\u306e\u610f\u5473\u3092\u7406\u89e3\u3057\u3001\u9069\u7528\u3067\u304d\u308b</li> <li>\u4f59\u56e0\u5b50\u5c55\u958b\u306e\u5b9a\u7fa9\u3092\u7406\u89e3\u3057\u3001\u884c\u5217\u5f0f\u8a08\u7b97\u306b\u5fdc\u7528\u3067\u304d\u308b</li> <li>\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u52b9\u7387\u7684\u306b\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u57fa\u672c\u5909\u5f62\u304b\u3089\u9084\u5143\u5b9a\u7406\u304c\u9069\u7528\u3067\u304d\u308b\u5f62\u306b\u884c\u5217\u3092\u5909\u5f62\u3067\u304d\u308b</li> <li>Google Colaboratory\u3092\u7528\u3044\u3066\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u3092\u5b9f\u884c\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/23-determinant/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/23-determinant/#31","title":"3.1 \u9084\u5143\u5b9a\u7406\u3068\u306f","text":"<p>\u9084\u5143\u5b9a\u7406\u306f\u3001\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u3092\u52b9\u7387\u5316\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u5b9a\u7406\u3067\u3059\u3002\u7279\u306b\u884c\u5217\u306e\u4e00\u90e8\u306b0\u306e\u8981\u7d20\u304c\u591a\u304f\u542b\u307e\u308c\u308b\u5834\u5408\u306b\u6709\u52b9\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u9084\u5143\u5b9a\u7406 \\(n\\) \u6b21\u6b63\u65b9\u884c\u5217 \\(A\\) \u306b\u304a\u3044\u3066\u3001\u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u4e2d\u306b \\(n-1\\) \u500b\u306e0\u304c\u3042\u308b\u5834\u5408\u3001\u6b8b\u308a\u306e1\u3064\u306e\u8981\u7d20 \\(a_{ij}\\) \u3068\u3001\u305d\u306e\u4f59\u56e0\u5b50 \\(A_{ij}\\) \u306e\u7a4d\u304c\u884c\u5217\u5f0f\u306e\u5024\u3068\u306a\u308b\u3002</p> <p>\\(\\det(A) = a_{ij} \\cdot A_{ij}\\)</p>"},{"location":"lectures/LA/23-determinant/#_1","title":"\u9084\u5143\u5b9a\u7406\u306e\u76f4\u611f\u7684\u7406\u89e3\u3068\u5177\u4f53\u4f8b","text":"<p>\u9084\u5143\u5b9a\u7406\u306e\u672c\u8cea\u306f\u3001\u884c\u5217\u5f0f\u3092\u5c55\u958b\u3059\u308b\u969b\u306b\u30010\u3068\u306a\u308b\u9805\u3092\u7121\u8996\u3067\u304d\u308b\u3053\u3068\u306b\u3042\u308a\u307e\u3059\u3002\u3042\u308b\u884c\u307e\u305f\u306f\u5217\u306b\u591a\u304f\u306e0\u304c\u3042\u308b\u5834\u5408\u3001\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u304c\u5927\u5e45\u306b\u7c21\u7565\u5316\u3055\u308c\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u6b21\u306e3\u00d73\u884c\u5217\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> \\[A = \\begin{pmatrix}  4 &amp; 0 &amp; 0 \\\\ 2 &amp; 3 &amp; 5 \\\\ 1 &amp; 2 &amp; 6 \\end{pmatrix}\\] <p>\u3053\u306e\u884c\u5217\u306e\u7b2c1\u884c\u306b\u306f\u30010\u304c2\u3064\u3042\u308a\u307e\u3059\uff08\\(n-1=3-1=2\\)\u500b\u306e0\uff09\u3002\u9084\u5143\u5b9a\u7406\u306b\u3088\u308c\u3070\u3001\u884c\u5217\u5f0f\u306f\u6b21\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a</p> \\[\\det(A) = a_{11} \\cdot A_{11} = 4 \\cdot A_{11}\\] <p>\u3053\u3053\u3067 \\(A_{11}\\) \u306f\u3001\\(a_{11}\\) \u306e\u4f59\u56e0\u5b50\u3067\u3059\u3002\u3064\u307e\u308a\u3001\\(A\\) \u304b\u3089\u7b2c1\u884c\u3068\u7b2c1\u5217\u3092\u53d6\u308a\u9664\u3044\u305f\u5c0f\u884c\u5217\u306e\u884c\u5217\u5f0f\u306b \\((-1)^{1+1}=1\\) \u3092\u639b\u3051\u305f\u3082\u306e\u3067\u3059\uff1a</p> \\[A_{11} = \\det\\begin{pmatrix}  3 &amp; 5 \\\\ 2 &amp; 6 \\end{pmatrix} = 3 \\cdot 6 - 5 \\cdot 2 = 18 - 10 = 8\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\det(A) = 4 \\cdot 8 = 32\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u901a\u5e38\u306e\u884c\u5217\u5f0f\u306e\u5c55\u958b\u65b9\u6cd5\u3092\u4f7f\u3046\u3068\u3001\u591a\u304f\u306e\u9805\u306e\u8a08\u7b97\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u9084\u5143\u5b9a\u7406\u3092\u4f7f\u3046\u3053\u3068\u3067\u4e00\u5ea6\u306b\u7b54\u3048\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/23-determinant/#32","title":"3.2 \u4f59\u56e0\u5b50\u306e\u5b9a\u7fa9\u3068\u8a08\u7b97","text":"<p>\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u306b\u304a\u3044\u3066\u3001\u4f59\u56e0\u5b50\uff08cofactor\uff09\u306f\u975e\u5e38\u306b\u91cd\u8981\u306a\u6982\u5ff5\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u4f59\u56e0\u5b50 \\(n\\) \u6b21\u6b63\u65b9\u884c\u5217 \\(A = (a_{ij})\\) \u306e\u8981\u7d20 \\(a_{ij}\\) \u306e\u4f59\u56e0\u5b50 \\(A_{ij}\\) \u306f\u3001\u884c\u5217 \\(A\\) \u304b\u3089 \\(i\\) \u884c\u3068 \\(j\\) \u5217\u3092\u53d6\u308a\u9664\u3044\u3066\u3067\u304d\u308b \\((n-1)\\) \u6b21\u306e\u884c\u5217\uff08\u5c0f\u884c\u5217\uff09\u306e\u884c\u5217\u5f0f\u306b \\((-1)^{i+j}\\) \u3092\u639b\u3051\u305f\u3082\u306e\u3067\u3042\u308b\u3002</p> <p>\\(A_{ij} = (-1)^{i+j} \\det(M_{ij})\\)</p> <p>\u3053\u3053\u3067\u3001\\(M_{ij}\\) \u306f \\(A\\) \u304b\u3089 \\(i\\) \u884c\u3068 \\(j\\) \u5217\u3092\u53d6\u308a\u9664\u3044\u3066\u3067\u304d\u308b\u5c0f\u884c\u5217\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/23-determinant/#_2","title":"\u5c0f\u884c\u5217\u3068\u4f59\u56e0\u5b50\u306e\u5177\u4f53\u4f8b","text":"<p>\u4f8b\u3048\u3070\u3001\u6b21\u306e3\u00d73\u884c\u5217\u306e\u5404\u8981\u7d20\u306e\u4f59\u56e0\u5b50\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> \\[D = \\begin{pmatrix}  2 &amp; 3 &amp; 1 \\\\ 4 &amp; 1 &amp; 5 \\\\ 0 &amp; 2 &amp; 6 \\end{pmatrix}\\] <p>\u307e\u305a\u3001\\(d_{11} = 2\\) \u306e\u5c0f\u884c\u5217 \\(M_{11}\\) \u306f\u3001\\(D\\) \u304b\u3089\u7b2c1\u884c\u3068\u7b2c1\u5217\u3092\u53d6\u308a\u9664\u3044\u305f\u3082\u306e\u3067\u3059\uff1a</p> \\[M_{11} = \\begin{pmatrix}  1 &amp; 5 \\\\ 2 &amp; 6 \\end{pmatrix}\\] <p>\u3053\u306e\u5c0f\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\uff1a \\(\\(\\det(M_{11}) = 1 \\cdot 6 - 5 \\cdot 2 = 6 - 10 = -4\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(d_{11}\\) \u306e\u4f59\u56e0\u5b50\u306f\uff1a \\(\\(D_{11} = (-1)^{1+1} \\cdot \\det(M_{11}) = 1 \\cdot (-4) = -4\\)\\)</p> <p>\u540c\u69d8\u306b\u3001\\(d_{12} = 3\\) \u306e\u5c0f\u884c\u5217\u3068\u4f59\u56e0\u5b50\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> \\[M_{12} = \\begin{pmatrix}  4 &amp; 5 \\\\ 0 &amp; 6 \\end{pmatrix}\\] \\[\\det(M_{12}) = 4 \\cdot 6 - 5 \\cdot 0 = 24\\] \\[D_{12} = (-1)^{1+2} \\cdot \\det(M_{12}) = (-1) \\cdot 24 = -24\\] <p>\u3053\u306e\u3088\u3046\u306b\u3001\u884c\u5217\u306e\u5404\u8981\u7d20\u306b\u3064\u3044\u3066\u3001\u5bfe\u5fdc\u3059\u308b\u5c0f\u884c\u5217\u306e\u884c\u5217\u5f0f\u306b\u9069\u5207\u306a\u7b26\u53f7\u3092\u4ed8\u3051\u308b\u3053\u3068\u3067\u4f59\u56e0\u5b50\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#_3","title":"\u4f59\u56e0\u5b50\u884c\u5217","text":"<p>\u884c\u5217\u306e\u3059\u3079\u3066\u306e\u8981\u7d20\u306e\u4f59\u56e0\u5b50\u3092\u96c6\u3081\u305f\u3082\u306e\u3092\u4f59\u56e0\u5b50\u884c\u5217\u3068\u547c\u3073\u307e\u3059\u30023\u00d73\u884c\u5217 \\(D\\) \u306e\u4f59\u56e0\u5b50\u884c\u5217\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[D^* = \\begin{pmatrix}  D_{11} &amp; D_{12} &amp; D_{13} \\\\ D_{21} &amp; D_{22} &amp; D_{23} \\\\ D_{31} &amp; D_{32} &amp; D_{33} \\end{pmatrix}\\] <p>\u4f59\u56e0\u5b50\u884c\u5217\u306f\u3001\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u6c42\u3081\u308b\u969b\u306b\u3082\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002\u5b9f\u969b\u3001\u6b63\u5247\u884c\u5217 \\(A\\) \u306e\u9006\u884c\u5217\u306f\u3001\\(A\\) \u306e\u4f59\u56e0\u5b50\u884c\u5217\u306e\u8ee2\u7f6e\u3092 \\(\\det(A)\\) \u3067\u5272\u3063\u305f\u3082\u306e\u3068\u3057\u3066\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a</p> \\[A^{-1} = \\frac{1}{\\det(A)} \\cdot (A^*)^T\\] <p>\u3053\u3053\u3067\u3001\\(A^*\\) \u306f \\(A\\) \u306e\u4f59\u56e0\u5b50\u884c\u5217\u3001\\((A^*)^T\\) \u306f\u305d\u306e\u8ee2\u7f6e\u3067\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#33","title":"3.3 \u4f59\u56e0\u5b50\u5c55\u958b\u3068\u306f","text":"<p>\u4f59\u56e0\u5b50\u5c55\u958b\u306f\u3001\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u4e00\u822c\u7684\u306a\u624b\u6cd5\u3067\u3042\u308a\u3001\u4efb\u610f\u306e\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3093\u3067\u5c55\u958b\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u4f59\u56e0\u5b50\u5c55\u958b \\(n\\) \u6b21\u6b63\u65b9\u884c\u5217 \\(A = (a_{ij})\\) \u306e\u884c\u5217\u5f0f\u306f\u3001\u4efb\u610f\u306e\u884c \\(i\\) \u307e\u305f\u306f\u5217 \\(j\\) \u306b\u3064\u3044\u3066\u3001\u305d\u306e\u884c\u307e\u305f\u306f\u5217\u306e\u5404\u8981\u7d20\u3068\u5bfe\u5fdc\u3059\u308b\u4f59\u56e0\u5b50\u306e\u7a4d\u306e\u548c\u3068\u3057\u3066\u8868\u3055\u308c\u308b\u3002</p> <p>\u884c\u306b\u3088\u308b\u5c55\u958b\uff1a\\(\\det(A) = \\sum_{j=1}^{n} a_{ij} \\cdot A_{ij}\\) \uff08\u4efb\u610f\u306e\u884c \\(i\\) \u306b\u3064\u3044\u3066\uff09</p> <p>\u5217\u306b\u3088\u308b\u5c55\u958b\uff1a\\(\\det(A) = \\sum_{i=1}^{n} a_{ij} \\cdot A_{ij}\\) \uff08\u4efb\u610f\u306e\u5217 \\(j\\) \u306b\u3064\u3044\u3066\uff09</p>"},{"location":"lectures/LA/23-determinant/#_4","title":"\u4f59\u56e0\u5b50\u5c55\u958b\u306e\u5177\u4f53\u4f8b","text":"<p>\u5148\u307b\u3069\u306e\u884c\u5217 \\(D\\) \u306e\u884c\u5217\u5f0f\u3092\u3001\u7b2c1\u884c\u306b\u95a2\u3059\u308b\u4f59\u56e0\u5b50\u5c55\u958b\u3067\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> \\[\\det(D) = d_{11} \\cdot D_{11} + d_{12} \\cdot D_{12} + d_{13} \\cdot D_{13}\\] <p>\u5404\u8981\u7d20\u3068\u305d\u306e\u4f59\u56e0\u5b50\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a</p> <ol> <li> <p>\\(d_{11} = 2\\), \\(D_{11} = -4\\) \uff08\u5148\u307b\u3069\u8a08\u7b97\u3057\u305f\u5024\uff09</p> </li> <li> <p>\\(d_{12} = 3\\), \\(D_{12} = -24\\) \uff08\u5148\u307b\u3069\u8a08\u7b97\u3057\u305f\u5024\uff09</p> </li> <li> <p>\\(d_{13} = 1\\), \\(D_{13} = (-1)^{1+3} \\det\\begin{pmatrix} 4 &amp; 1 \\\\ 0 &amp; 2 \\end{pmatrix} = 1 \\cdot (4 \\cdot 2 - 1 \\cdot 0) = 8\\)</p> </li> </ol> <p>\u3057\u305f\u304c\u3063\u3066\u3001 \\(\\(\\det(D) = 2 \\cdot (-4) + 3 \\cdot (-24) + 1 \\cdot 8 = -8 - 72 + 8 = -72\\)\\)</p> <p>\u4f59\u56e0\u5b50\u5c55\u958b\u306f\u3001\u3069\u306e\u884c\u307e\u305f\u306f\u5217\u3067\u5c55\u958b\u3057\u3066\u3082\u540c\u3058\u7d50\u679c\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u8a08\u7b97\u3092\u7c21\u5358\u306b\u3059\u308b\u306b\u306f0\u306e\u591a\u3044\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3076\u3068\u52b9\u7387\u7684\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u3053\u306e\u884c\u5217\u306e\u7b2c3\u884c\u306b\u306f0\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u7b2c3\u884c\u3067\u5c55\u958b\u3059\u308b\u3068\u8a08\u7b97\u304c\u5c11\u3057\u7c21\u5358\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[\\det(D) = d_{31} \\cdot D_{31} + d_{32} \\cdot D_{32} + d_{33} \\cdot D_{33}\\] \\[= 0 \\cdot D_{31} + 2 \\cdot D_{32} + 6 \\cdot D_{33}\\] <p>\\(d_{31} = 0\\) \u306a\u306e\u3067\u3001\u7b2c1\u9805\u306f0\u306b\u306a\u308a\u3001\u6b8b\u308a\u306e2\u9805\u3060\u3051\u8a08\u7b97\u3059\u308c\u3070\u3088\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#34","title":"3.4 \u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u306e\u95a2\u4fc2","text":"<p>\u9084\u5143\u5b9a\u7406\u306f\u3001\u4f59\u56e0\u5b50\u5c55\u958b\u306e\u7279\u6b8a\u306a\u30b1\u30fc\u30b9\u3068\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306b \\(n-1\\) \u500b\u306e0\u304c\u3042\u308b\u5834\u5408\u3001\u305d\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306b\u95a2\u3059\u308b\u4f59\u56e0\u5b50\u5c55\u958b\u3067\u306f\u30010\u3067\u306a\u3044\u8981\u7d20\u304c1\u3064\u3060\u3051\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> \\[C = \\begin{pmatrix}  2 &amp; 0 &amp; 0 \\\\ 3 &amp; 4 &amp; 1 \\\\ 5 &amp; 2 &amp; 6 \\end{pmatrix}\\] <p>\u7b2c1\u884c\u306b\u95a2\u3059\u308b\u4f59\u56e0\u5b50\u5c55\u958b\u306f\uff1a \\(\\(\\det(C) = c_{11} \\cdot C_{11} + c_{12} \\cdot C_{12} + c_{13} \\cdot C_{13}\\)\\)</p> <p>\u305f\u3060\u3057\u3001\\(c_{12} = 0\\) \u3068 \\(c_{13} = 0\\) \u306a\u306e\u3067\uff1a \\(\\(\\det(C) = c_{11} \\cdot C_{11} = 2 \\cdot C_{11}\\)\\)</p> <p>\u3053\u308c\u306f\u307e\u3055\u306b\u9084\u5143\u5b9a\u7406\u306e\u5f62\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u9084\u5143\u5b9a\u7406\u306f\u300c\u3042\u308b\u884c\u307e\u305f\u306f\u5217\u306b\u307b\u3068\u3093\u30690\u304c\u3042\u308b\u5834\u5408\u306e\u4f59\u56e0\u5b50\u5c55\u958b\u306e\u7c21\u7565\u5316\u300d\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u306f\u5bc6\u63a5\u306b\u95a2\u9023\u3057\u3066\u304a\u308a\u3001\u3069\u3061\u3089\u3082\u884c\u5217\u5f0f\u8a08\u7b97\u306e\u91cd\u8981\u306a\u30c4\u30fc\u30eb\u3067\u3059\u3002\u884c\u5217\u306e\u69cb\u9020\u306b\u5fdc\u3058\u3066\u3001\u6700\u3082\u52b9\u7387\u7684\u306a\u8a08\u7b97\u65b9\u6cd5\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/23-determinant/#41","title":"4.1 \u9084\u5143\u5b9a\u7406\u306e\u5fdc\u7528","text":"<p>\u9084\u5143\u5b9a\u7406\u3092\u52b9\u679c\u7684\u306b\u9069\u7528\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u3092\u9084\u5143\u5f62\u306b\u5909\u5f62\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u4ee5\u4e0b\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u884c\u3044\u307e\u3059\uff1a</p> <ol> <li>\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u3001\u3042\u308b\u884c\u307e\u305f\u306f\u5217\u306b\u3067\u304d\u308b\u3060\u3051\u591a\u304f\u306e0\u3092\u4f5c\u308b</li> <li>\u7406\u60f3\u7684\u306b\u306f\u30011\u3064\u306e\u8981\u7d20\u3092\u9664\u3044\u3066\u3059\u3079\u3066\u304c0\u306b\u306a\u308b\u3088\u3046\u306b\u3059\u308b</li> <li>\u9084\u5143\u5b9a\u7406\u3092\u9069\u7528\u3057\u3066\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b</li> </ol> <p>\u305f\u3060\u3057\u3001\u57fa\u672c\u5909\u5f62\u3092\u884c\u3063\u305f\u969b\u306b\u306f\u3001\u884c\u5217\u5f0f\u306e\u5024\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u304b\u306b\u6ce8\u610f\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#42","title":"4.2 \u4f59\u56e0\u5b50\u5c55\u958b\u306e\u8a08\u7b97\u624b\u9806","text":"<p>\u9084\u5143\u5f62\u306b\u306a\u3089\u306a\u3044\u5834\u5408\u3067\u3082\u3001\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u7528\u3044\u308c\u3070\u4efb\u610f\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002\u8a08\u7b97\u624b\u9806\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308b\u3088\u3046\u306b\u30010\u306e\u591a\u3044\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3076</li> <li>\u9078\u3093\u3060\u884c\u307e\u305f\u306f\u5217\u306e\u5404\u8981\u7d20\u306b\u3064\u3044\u3066\u3001\u5bfe\u5fdc\u3059\u308b\u4f59\u56e0\u5b50\u3092\u8a08\u7b97\u3059\u308b</li> <li>\u8981\u7d20\u3068\u4f59\u56e0\u5b50\u306e\u7a4d\u306e\u548c\u3092\u6c42\u3081\u308b</li> </ol>"},{"location":"lectures/LA/23-determinant/#43","title":"4.3 \u57fa\u672c\u5909\u5f62\u3068\u9084\u5143\u5b9a\u7406\u30fb\u4f59\u56e0\u5b50\u5c55\u958b\u306e\u7d44\u307f\u5408\u308f\u305b","text":"<p>\u5b9f\u969b\u306e\u884c\u5217\u5f0f\u8a08\u7b97\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u65b9\u6cd5\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u4f7f\u7528\u3057\u307e\u3059\uff1a</p> <ol> <li>\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u3092\u7c21\u5358\u306a\u5f62\u306b\u5909\u5f62\u3059\u308b</li> <li>\u53ef\u80fd\u3067\u3042\u308c\u3070\u9084\u5143\u5b9a\u7406\u3092\u9069\u7528\u3059\u308b</li> <li>\u305d\u308c\u3067\u3082\u96e3\u3057\u3044\u5834\u5408\u306f\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u884c\u3046</li> <li>\u5c0f\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u969b\u3082\u540c\u69d8\u306e\u624b\u9806\u3092\u7e70\u308a\u8fd4\u3059</li> </ol>"},{"location":"lectures/LA/23-determinant/#44","title":"4.4 \u8a08\u7b97\u52b9\u7387\u5316\u306e\u305f\u3081\u306e\u30dd\u30a4\u30f3\u30c8","text":"<p>\u884c\u5217\u5f0f\u8a08\u7b97\u3092\u52b9\u7387\u5316\u3059\u308b\u305f\u3081\u306e\u30dd\u30a4\u30f3\u30c8\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u3067\u304d\u308b\u3060\u3051\u591a\u304f\u306e0\u3092\u542b\u3080\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3076</li> <li>1\u307e\u305f\u306f-1\u306e\u8981\u7d20\u3092\u591a\u304f\u542b\u3080\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3076\u3068\u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308b</li> <li>\u5927\u304d\u306a\u884c\u5217\u306e\u884c\u5217\u5f0f\u8a08\u7b97\u3067\u306f\u3001\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u7d44\u307f\u5408\u308f\u305b\u308b</li> <li>\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u5c55\u958b\u524d\u306b\u884c\u57fa\u672c\u5909\u5f62\u3092\u884c\u3044\u3001\u8a08\u7b97\u3092\u7c21\u5358\u306b\u3059\u308b</li> </ol>"},{"location":"lectures/LA/23-determinant/#5","title":"5. \u5177\u4f53\u7684\u306a\u8a08\u7b97\u4f8b","text":""},{"location":"lectures/LA/23-determinant/#51","title":"5.1 \u9084\u5143\u5b9a\u7406\u306e\u9069\u7528\u4f8b","text":"<p>\u6b21\u306e3\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u9084\u5143\u5b9a\u7406\u3092\u7528\u3044\u3066\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[A = \\begin{pmatrix}  2 &amp; 0 &amp; 0 \\\\ 1 &amp; 3 &amp; 4 \\\\ 2 &amp; 1 &amp; 5 \\end{pmatrix}\\] <p>\u3053\u306e\u884c\u5217\u306e\u7b2c1\u884c\u306b\u306f0\u304c2\u3064\u3042\u308b\u306e\u3067\u3001\u9084\u5143\u5b9a\u7406\u304c\u9069\u7528\u3067\u304d\u307e\u3059\u3002</p> \\[\\det(A) = a_{11} \\cdot A_{11} = 2 \\cdot A_{11}\\] <p>\u3053\u3053\u3067\u3001\\(A_{11}\\) \u306f \\(a_{11}\\) \u306e\u4f59\u56e0\u5b50\u3067\u3042\u308a\u3001\u6b21\u306e\u3088\u3046\u306b\u8a08\u7b97\u3057\u307e\u3059\uff1a</p> \\[A_{11} = (-1)^{1+1} \\det\\begin{pmatrix}  3 &amp; 4 \\\\ 1 &amp; 5 \\end{pmatrix} = \\det\\begin{pmatrix}  3 &amp; 4 \\\\ 1 &amp; 5 \\end{pmatrix} = 3 \\cdot 5 - 4 \\cdot 1 = 15 - 4 = 11\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\det(A) = 2 \\cdot 11 = 22\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#52","title":"5.2 \u4f59\u56e0\u5b50\u5c55\u958b\u306e\u9069\u7528\u4f8b","text":"<p>\u6b21\u306e3\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u4f59\u56e0\u5b50\u5c55\u958b\u3067\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[B = \\begin{pmatrix}  3 &amp; 1 &amp; 2 \\\\ 0 &amp; 2 &amp; -1 \\\\ 4 &amp; 0 &amp; 3 \\end{pmatrix}\\] <p>\u3053\u306e\u884c\u5217\u306f\u7b2c2\u5217\u306b0\u304c1\u3064\u3042\u308b\u306e\u3067\u3001\u7b2c2\u5217\u3067\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u884c\u3046\u3068\u52b9\u7387\u7684\u3067\u3059\u3002</p> \\[\\det(B) = b_{12} \\cdot B_{12} + b_{22} \\cdot B_{22} + b_{32} \\cdot B_{32}\\] <p>\u3053\u3053\u3067\u3001\u5404\u4f59\u56e0\u5b50\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a</p> \\[B_{12} = (-1)^{1+2} \\det\\begin{pmatrix}  0 &amp; -1 \\\\ 4 &amp; 3 \\end{pmatrix} = -1 \\cdot (0 \\cdot 3 - (-1) \\cdot 4) = -1 \\cdot 4 = -4\\] \\[B_{22} = (-1)^{2+2} \\det\\begin{pmatrix}  3 &amp; 2 \\\\ 4 &amp; 3 \\end{pmatrix} = 1 \\cdot (3 \\cdot 3 - 2 \\cdot 4) = 9 - 8 = 1\\] \\[B_{32} = (-1)^{3+2} \\det\\begin{pmatrix}  3 &amp; 2 \\\\ 0 &amp; -1 \\end{pmatrix} = -1 \\cdot (3 \\cdot (-1) - 2 \\cdot 0) = -1 \\cdot (-3) = 3\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001 \\(\\(\\det(B) = 1 \\cdot (-4) + 2 \\cdot 1 + 0 \\cdot 3 = -4 + 2 = -2\\)\\)</p>"},{"location":"lectures/LA/23-determinant/#53","title":"5.3 \u57fa\u672c\u5909\u5f62\u3068\u9084\u5143\u5b9a\u7406\u306e\u7d44\u307f\u5408\u308f\u305b\u4f8b","text":"<p>\u6b21\u306e3\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[C = \\begin{pmatrix}  2 &amp; 3 &amp; 1 \\\\ 4 &amp; 5 &amp; 2 \\\\ 1 &amp; 3 &amp; 2 \\end{pmatrix}\\] <p>\u307e\u305a\u3001\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u306b0\u3092\u3088\u308a\u591a\u304f\u4f5c\u308a\u307e\u3059\u3002 \\(R_2 - 2R_1\\) \u3068 \\(R_3 - \\frac{1}{2}R_1\\) \u3092\u884c\u3046\u3068\u3001\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[C' = \\begin{pmatrix}  2 &amp; 3 &amp; 1 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 1.5 &amp; 1.5 \\end{pmatrix}\\] <p>\u3053\u308c\u3067\u7b2c2\u884c\u306b0\u304c2\u3064\u3067\u304d\u305f\u306e\u3067\u3001\u9084\u5143\u5b9a\u7406\u304c\u9069\u7528\u3067\u304d\u307e\u3059\uff1a</p> \\[\\det(C') = c'_{22} \\cdot C'_{22} = -1 \\cdot C'_{22}\\] <p>\u3053\u3053\u3067\u3001</p> \\[C'_{22} = (-1)^{2+2} \\det\\begin{pmatrix}  2 &amp; 1 \\\\ 0 &amp; 1.5 \\end{pmatrix} = 1 \\cdot (2 \\cdot 1.5 - 1 \\cdot 0) = 3\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\det(C') = -1 \\cdot 3 = -3\\)</p> <p>\u57fa\u672c\u5909\u5f62\u306b\u3088\u308a\u884c\u5217\u5f0f\u306e\u5024\u306f\u5909\u5316\u3059\u308b\u305f\u3081\u3001\u5143\u306e\u884c\u5217\u5f0f\u3068\u306e\u95a2\u4fc2\u3092\u8003\u616e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 \\(R_2 - 2R_1\\) \u306f\u884c\u5217\u5f0f\u306e\u5024\u3092\u5909\u3048\u307e\u305b\u3093\u304c\u3001\\(R_3 - \\frac{1}{2}R_1\\) \u3082\u884c\u5217\u5f0f\u306e\u5024\u3092\u5909\u3048\u307e\u305b\u3093\u3002</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\det(C) = \\det(C') = -3\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#6-python","title":"6. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/23-determinant/#61","title":"6.1 \u884c\u5217\u5f0f\u306e\u8a08\u7b97","text":"<p>NumPy\u3092\u7528\u3044\u3066\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002</p> <pre><code>import numpy as np\n\n# \u4f8b1: \u9084\u5143\u5b9a\u7406\u306e\u4f8b\nA = np.array([[2, 0, 0],\n              [1, 3, 4],\n              [2, 1, 5]])\n\n# NumPy\u306elinalg.det\u3092\u4f7f\u7528\u3057\u3066\u884c\u5217\u5f0f\u3092\u8a08\u7b97\ndet_A = np.linalg.det(A)\nprint(f\"det(A) = {det_A}\")\n\n# \u4f8b2: \u4f59\u56e0\u5b50\u5c55\u958b\u306e\u4f8b\nB = np.array([[3, 1, 2],\n              [0, 2, -1],\n              [4, 0, 3]])\n\ndet_B = np.linalg.det(B)\nprint(f\"det(B) = {det_B}\")\n\n# \u4f8b3: \u57fa\u672c\u5909\u5f62\u3068\u9084\u5143\u5b9a\u7406\u306e\u7d44\u307f\u5408\u308f\u305b\u4f8b\nC = np.array([[2, 3, 1],\n              [4, 5, 2],\n              [1, 3, 2]])\n\ndet_C = np.linalg.det(C)\nprint(f\"det(C) = {det_C}\")\n</code></pre>"},{"location":"lectures/LA/23-determinant/#62","title":"6.2 \u4f59\u56e0\u5b50\u306e\u8a08\u7b97\u3068\u884c\u5217\u5f0f\u306e\u5c55\u958b","text":"<p>\u4f59\u56e0\u5b50\u3092\u8a08\u7b97\u3057\u3001\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u5b9f\u88c5\u3059\u308b\u4f8b\u3067\u3059\u3002</p> <pre><code>import numpy as np\n\ndef minor(A, i, j):\n    \"\"\"\u884c\u5217A\u304b\u3089\u7b2ci\u884c\u3068\u7b2cj\u5217\u3092\u9664\u3044\u305f\u5c0f\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\"\"\"\n    # numpy\u3067\u306f0\u304b\u3089\u59cb\u307e\u308b\u306e\u3067\u3001i,j\u30820\u304b\u3089\u59cb\u307e\u308b\u3068\u3059\u308b\n    n = A.shape[0]\n    # \u884c\u3068\u5217\u3092\u9664\u53bb\n    minor_matrix = np.delete(np.delete(A, i, axis=0), j, axis=1)\n    return np.linalg.det(minor_matrix)\n\ndef cofactor(A, i, j):\n    \"\"\"\u884c\u5217A\u306e(i,j)\u6210\u5206\u306e\u4f59\u56e0\u5b50\u3092\u8a08\u7b97\"\"\"\n    return (-1)**(i+j) * minor(A, i, j)\n\ndef determinant_by_cofactor_expansion(A, row=0):\n    \"\"\"\u4f59\u56e0\u5b50\u5c55\u958b\u306b\u3088\u308a\u884c\u5217\u5f0f\u3092\u8a08\u7b97\uff08\u6307\u5b9a\u3057\u305f\u884c\u306b\u3064\u3044\u3066\u5c55\u958b\uff09\"\"\"\n    n = A.shape[0]\n    if n == 1:\n        return A[0, 0]\n\n    det = 0\n    for j in range(n):\n        det += A[row, j] * cofactor(A, row, j)\n\n    return det\n\n# \u4f8b\uff1a3\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u8a08\u7b97\nB = np.array([[3, 1, 2],\n              [0, 2, -1],\n              [4, 0, 3]])\n\n# NumPy\u306e\u95a2\u6570\u3067\u8a08\u7b97\ndet_B_numpy = np.linalg.det(B)\nprint(f\"det(B) by NumPy = {det_B_numpy}\")\n\n# \u4f59\u56e0\u5b50\u5c55\u958b\u3067\u8a08\u7b97\uff08\u884c0\u306b\u3064\u3044\u3066\u5c55\u958b\uff09\ndet_B_cofactor = determinant_by_cofactor_expansion(B, row=0)\nprint(f\"det(B) by cofactor expansion (row 0) = {det_B_cofactor}\")\n\n# \u4f59\u56e0\u5b50\u5c55\u958b\u3067\u8a08\u7b97\uff08\u884c1\u306b\u3064\u3044\u3066\u5c55\u958b\uff09\ndet_B_cofactor_row1 = determinant_by_cofactor_expansion(B, row=1)\nprint(f\"det(B) by cofactor expansion (row 1) = {det_B_cofactor_row1}\")\n</code></pre>"},{"location":"lectures/LA/23-determinant/#63","title":"6.3 \u9084\u5143\u5f62\u306e\u78ba\u8a8d\u3068\u9084\u5143\u5b9a\u7406\u306e\u9069\u7528","text":"<p>\u9084\u5143\u5f62\u306e\u6761\u4ef6\u3092\u78ba\u8a8d\u3057\u3001\u9084\u5143\u5b9a\u7406\u3092\u9069\u7528\u3059\u308b\u30b3\u30fc\u30c9\u3067\u3059\u3002</p> <pre><code>import numpy as np\n\ndef is_reducible(A):\n    \"\"\"\u884c\u5217\u304c\u3042\u308b\u884c\u307e\u305f\u306f\u5217\u306bn-1\u500b\u306e0\u3092\u6301\u3064\u304b\u30c1\u30a7\u30c3\u30af\"\"\"\n    n = A.shape[0]\n\n    # \u5404\u884c\u306b\u3064\u3044\u30660\u306e\u6570\u3092\u30c1\u30a7\u30c3\u30af\n    for i in range(n):\n        if np.count_nonzero(A[i, :] == 0) == n-1:\n            # 0\u3067\u306a\u3044\u8981\u7d20\u306e\u4f4d\u7f6e\n            j = np.where(A[i, :] != 0)[0][0]\n            return True, ('row', i, j)\n\n    # \u5404\u5217\u306b\u3064\u3044\u30660\u306e\u6570\u3092\u30c1\u30a7\u30c3\u30af\n    for j in range(n):\n        if np.count_nonzero(A[:, j] == 0) == n-1:\n            # 0\u3067\u306a\u3044\u8981\u7d20\u306e\u4f4d\u7f6e\n            i = np.where(A[:, j] != 0)[0][0]\n            return True, ('col', i, j)\n\n    return False, None\n\ndef determinant_with_reduction(A):\n    \"\"\"\u9084\u5143\u5b9a\u7406\u3092\u9069\u7528\u3067\u304d\u308b\u5834\u5408\u306f\u305d\u308c\u3092\u4f7f\u3063\u3066\u884c\u5217\u5f0f\u3092\u8a08\u7b97\"\"\"\n    reducible, info = is_reducible(A)\n\n    if reducible:\n        type_pos, i, j = info\n        if type_pos == 'row':\n            # \u884c\u306b\u6ce8\u76ee\u3059\u308b\u5834\u5408\n            return A[i, j] * cofactor(A, i, j)\n        else:\n            # \u5217\u306b\u6ce8\u76ee\u3059\u308b\u5834\u5408\n            return A[i, j] * cofactor(A, i, j)\n    else:\n        # \u9084\u5143\u5f62\u3067\u306a\u3044\u5834\u5408\u306fNumPy\u306edet\u95a2\u6570\u3092\u4f7f\u7528\n        return np.linalg.det(A)\n\n# \u4f8b\uff1a\u9084\u5143\u5b9a\u7406\u304c\u9069\u7528\u3067\u304d\u308b\u884c\u5217\nA = np.array([[2, 0, 0],\n              [1, 3, 4],\n              [2, 1, 5]])\n\n# \u9084\u5143\u5f62\u304b\u30c1\u30a7\u30c3\u30af\nreducible, info = is_reducible(A)\nif reducible:\n    type_pos, i, j = info\n    print(f\"\u884c\u5217A\u306f\u9084\u5143\u5f62\u3067\u3059\u3002{type_pos} {i+1}\u306b\u6ce8\u76ee\u3057\u307e\u3059\u3002\")\nelse:\n    print(\"\u884c\u5217A\u306f\u9084\u5143\u5f62\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\")\n\n# \u9084\u5143\u5b9a\u7406\u3092\u9069\u7528\u3057\u3066\u884c\u5217\u5f0f\u3092\u8a08\u7b97\ndet_A_reduction = determinant_with_reduction(A)\nprint(f\"det(A) by reduction theorem = {det_A_reduction}\")\n\n# NumPy\u306e\u95a2\u6570\u3067\u691c\u8a3c\ndet_A_numpy = np.linalg.det(A)\nprint(f\"det(A) by NumPy = {det_A_numpy}\")\n</code></pre>"},{"location":"lectures/LA/23-determinant/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/23-determinant/#_5","title":"\u57fa\u672c\u554f\u984c\uff08\u6982\u5ff5\u7406\u89e3\u306e\u78ba\u8a8d\uff09","text":"<ol> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(A = \\begin{pmatrix}     3 &amp; 0 &amp; 0 \\\\    2 &amp; 4 &amp; 0 \\\\    1 &amp; 2 &amp; 5    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u9084\u5143\u5b9a\u7406\u3092\u9069\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u6b21\u306e\u884c\u5217\u306b1\u56de\u306e\u884c\u57fa\u672c\u5909\u5f62\u3092\u884c\u3044\u3001\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(B = \\begin{pmatrix}     2 &amp; 1 &amp; 0 \\\\    3 &amp; 0 &amp; 2 \\\\    1 &amp; 2 &amp; 3    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u4f59\u56e0\u5b50\u5c55\u958b\u3067\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(C = \\begin{pmatrix}     1 &amp; 2 &amp; 3 \\\\    0 &amp; 1 &amp; 4 \\\\    1 &amp; 0 &amp; 2    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u884c\u5217 \\(D\\) \u306e\u4f59\u56e0\u5b50\u884c\u5217\u3092 \\(D^{*}\\) \u3068\u3059\u308b\u3068\u304d\u3001\\(D \\cdot D^{*} = \\det(D) \\cdot I\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002    \\(\\(D = \\begin{pmatrix}     2 &amp; 1 \\\\    3 &amp; 4    \\end{pmatrix}\\)\\)</p> </li> </ol>"},{"location":"lectures/LA/23-determinant/#_6","title":"\u5fdc\u7528\u554f\u984c\uff08\u5fdc\u7528\u80fd\u529b\u306e\u78ba\u8a8d\uff09","text":"<ol> <li> <p>4\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002\uff08\u30d2\u30f3\u30c8\uff1a\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\uff09    \\(\\(E = \\begin{pmatrix}     1 &amp; 0 &amp; 2 &amp; 0 \\\\    0 &amp; 3 &amp; 1 &amp; 0 \\\\    0 &amp; 0 &amp; 2 &amp; 0 \\\\    4 &amp; 1 &amp; 3 &amp; 5    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u3001\u69d8\u3005\u306a\u65b9\u6cd5\u3067\u5c55\u958b\u3057\u3001\u7d50\u679c\u304c\u4e00\u81f4\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002    \\(\\(F = \\begin{pmatrix}     3 &amp; 2 &amp; 1 \\\\    0 &amp; 4 &amp; 2 \\\\    1 &amp; 0 &amp; 5    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u5fdc\u7528\uff1a\u76f8\u95a2\u884c\u5217\u306f\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308a\u3001\u305d\u306e\u884c\u5217\u5f0f\u306f\u30c7\u30fc\u30bf\u306e\u5909\u6570\u9593\u306e\u76f8\u95a2\u306e\u5f37\u3055\u3092\u8868\u3059\u6307\u6a19\u3068\u306a\u308a\u307e\u3059\u3002\u6b21\u306e3\u5909\u6570\u306e\u5065\u5eb7\u30c7\u30fc\u30bf\u306b\u95a2\u3059\u308b\u76f8\u95a2\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u3001\u5909\u6570\u9593\u306e\u76f8\u95a2\u306e\u5f37\u3055\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002    \\(\\(R = \\begin{pmatrix}     1 &amp; 0.7 &amp; 0.3 \\\\    0.7 &amp; 1 &amp; 0.5 \\\\    0.3 &amp; 0.5 &amp; 1    \\end{pmatrix}\\)\\)</p> </li> </ol> <p>\u307e\u305f\u3001\u76f8\u95a2\u884c\u5217\u306e\u884c\u5217\u5f0f\u304c0\u306b\u8fd1\u3044\u3068\u304d\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u3069\u306e\u3088\u3046\u306a\u554f\u984c\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u304b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/23-determinant/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/23-determinant/#q1","title":"Q1: \u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u969b\u306b\u3001\u3069\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u3092\u9078\u3093\u3067\u5c55\u958b\u3059\u308b\u306e\u304c\u6700\u3082\u52b9\u7387\u7684\u3067\u3059\u304b\uff1f","text":"<p>A1: \u4e00\u822c\u7684\u306b\u306f\u30010\u306e\u8981\u7d20\u3092\u591a\u304f\u542b\u3080\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3076\u3068\u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308a\u307e\u3059\u3002\u53ef\u80fd\u3067\u3042\u308c\u3070\u3001n-1\u500b\u306e0\u3092\u6301\u3064\u884c\u307e\u305f\u306f\u5217\u304c\u3042\u308c\u3070\u3001\u9084\u5143\u5b9a\u7406\u3092\u9069\u7528\u3067\u304d\u308b\u305f\u3081\u3001\u305d\u306e\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3076\u306e\u304c\u6700\u3082\u52b9\u7387\u7684\u3067\u3059\u3002\u307e\u305f\u30011\u3084-1\u306e\u8981\u7d20\u3092\u542b\u3080\u884c\u307e\u305f\u306f\u5217\u3082\u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308b\u3053\u3068\u304c\u591a\u3044\u3067\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#q2","title":"Q2: \u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A2: \u9084\u5143\u5b9a\u7406\u306f\u3001\u3042\u308b\u884c\u307e\u305f\u306f\u5217\u306bn-1\u500b\u306e0\u304c\u5b58\u5728\u3059\u308b\u7279\u6b8a\u306a\u5834\u5408\u306b\u306e\u307f\u9069\u7528\u3067\u304d\u308b\u7c21\u7565\u5316\u3055\u308c\u305f\u8a08\u7b97\u65b9\u6cd5\u3067\u3059\u3002\u4e00\u65b9\u3001\u4f59\u56e0\u5b50\u5c55\u958b\u306f\u4efb\u610f\u306e\u884c\u307e\u305f\u306f\u5217\u306b\u3064\u3044\u3066\u9069\u7528\u3067\u304d\u308b\u4e00\u822c\u7684\u306a\u5c55\u958b\u65b9\u6cd5\u3067\u3059\u3002\u9084\u5143\u5b9a\u7406\u306f\u4f59\u56e0\u5b50\u5c55\u958b\u306e\u7279\u6b8a\u306a\u30b1\u30fc\u30b9\u3068\u8003\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#q3-0","title":"Q3: \u884c\u5217\u5f0f\u304c0\u306b\u306a\u308b\u306e\u306f\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u3067\u3059\u304b\uff1f","text":"<p>A3: \u884c\u5217\u5f0f\u304c0\u306b\u306a\u308b\u306e\u306f\u3001\u884c\u5217\u306e\u884c\u307e\u305f\u306f\u5217\u306e\u30d9\u30af\u30c8\u30eb\u304c\u7dda\u5f62\u5f93\u5c5e\u3067\u3042\u308b\u5834\u5408\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\uff1a - \u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u304c\u3059\u3079\u30660\u306e\u5834\u5408 - \u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u304c\u5225\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u5b9a\u6570\u500d\u3067\u3042\u308b\u5834\u5408 - \u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u304c\u4ed6\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u7dda\u5f62\u7d50\u5408\u3067\u8868\u305b\u308b\u5834\u5408</p>"},{"location":"lectures/LA/23-determinant/#q4","title":"Q4: \u884c\u5217\u5f0f\u306e\u8a08\u7b97\u306b\u304a\u3044\u3066\u57fa\u672c\u5909\u5f62\u3092\u4f7f\u3046\u969b\u306e\u6ce8\u610f\u70b9\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A4: \u57fa\u672c\u5909\u5f62\u306b\u3088\u3063\u3066\u884c\u5217\u5f0f\u306e\u5024\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5909\u5316\u3057\u307e\u3059\uff1a 1. \u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u4ea4\u63db\uff1a\u884c\u5217\u5f0f\u306e\u7b26\u53f7\u304c\u53cd\u8ee2\u3059\u308b 2. \u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u5b9a\u6570\u500d\uff1a\u884c\u5217\u5f0f\u3082\u305d\u306e\u5b9a\u6570\u500d\u3068\u306a\u308b 3. \u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u5b9a\u6570\u500d\u3092\u5225\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306b\u52a0\u3048\u308b\uff1a\u884c\u5217\u5f0f\u306e\u5024\u306f\u5909\u5316\u3057\u306a\u3044</p> <p>\u3053\u308c\u3089\u306e\u6027\u8cea\u3092\u7406\u89e3\u3057\u3066\u304a\u304f\u3053\u3068\u3067\u3001\u57fa\u672c\u5909\u5f62\u5f8c\u306e\u884c\u5217\u5f0f\u304b\u3089\u5143\u306e\u884c\u5217\u5f0f\u3092\u6b63\u78ba\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/23-determinant/#q5","title":"Q5: \u4f59\u56e0\u5b50\u5c55\u958b\u3092\u7406\u89e3\u3059\u308b\u4e0a\u3067\u3088\u304f\u3042\u308b\u8aa4\u89e3\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A5: \u3088\u304f\u3042\u308b\u8aa4\u89e3\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\uff1a - \u4f59\u56e0\u5b50\u306e\u7b26\u53f7\uff08\\((-1)^{i+j}\\)\uff09\u3092\u5fd8\u308c\u308b - \u5c0f\u884c\u5217\u3092\u53d6\u308b\u969b\u306b\u884c\u3068\u5217\u306e\u756a\u53f7\u3092\u6df7\u540c\u3059\u308b - \u4f59\u56e0\u5b50\u884c\u5217\u3068\u9006\u884c\u5217\u306e\u95a2\u4fc2\u3092\u8aa4\u89e3\u3059\u308b\uff08\u4f59\u56e0\u5b50\u884c\u5217\u306e\u8ee2\u7f6e\u3092\u884c\u5217\u5f0f\u3067\u5272\u308b\u3068\u9006\u884c\u5217\u306b\u306a\u308b\uff09 - \u3059\u3079\u3066\u306e\u884c\u307e\u305f\u306f\u5217\u3067\u5c55\u958b\u3059\u308b\u3068\u7d50\u679c\u304c\u7570\u306a\u308b\u3068\u8003\u3048\u308b\uff08\u5b9f\u969b\u306b\u306f\u3069\u306e\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3093\u3067\u3082\u540c\u3058\u5024\u306b\u306a\u308b\uff09</p>"},{"location":"lectures/LA/23-determinant/#9","title":"9. \u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u884c\u5217\u5f0f\u8a08\u7b97\u306e\u52b9\u7387\u7684\u306a\u65b9\u6cd5\u3068\u3057\u3066\u3001\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\u3002</p> <ol> <li>\u9084\u5143\u5b9a\u7406\u306f\u3001\u884c\u5217\u306e\u3042\u308b\u884c\u307e\u305f\u306f\u5217\u306bn-1\u500b\u306e0\u304c\u3042\u308b\u7279\u6b8a\u306a\u5834\u5408\u306b\u9069\u7528\u3067\u304d\u3001\u8a08\u7b97\u3092\u5927\u5e45\u306b\u7c21\u7565\u5316\u3067\u304d\u307e\u3059\u3002</li> <li>\u4f59\u56e0\u5b50\u5c55\u958b\u306f\u3001\u4efb\u610f\u306e\u884c\u307e\u305f\u306f\u5217\u306b\u3064\u3044\u3066\u9069\u7528\u3067\u304d\u308b\u4e00\u822c\u7684\u306a\u5c55\u958b\u65b9\u6cd5\u3067\u3042\u308a\u3001\u518d\u5e30\u7684\u306b\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002</li> <li>\u5b9f\u969b\u306e\u8a08\u7b97\u3067\u306f\u3001\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u3092\u7c21\u5358\u306a\u5f62\u306b\u5909\u5f62\u3057\u3066\u304b\u3089\u3001\u9084\u5143\u5b9a\u7406\u3084\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u9069\u7528\u3059\u308b\u3068\u3088\u3044\u3067\u3057\u3087\u3046\u3002</li> <li>\u8a08\u7b97\u306e\u52b9\u7387\u5316\u306e\u305f\u3081\u306b\u306f\u30010\u306e\u591a\u3044\u884c\u307e\u305f\u306f\u5217\u3001\u307e\u305f\u306f1\u3084-1\u3092\u542b\u3080\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3076\u3068\u3088\u3044\u3067\u3059\u3002</li> <li>\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u306f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u3082\u91cd\u8981\u3067\u3042\u308a\u3001\u7279\u306b\u76f8\u95a2\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u30c7\u30fc\u30bf\u306e\u591a\u91cd\u5171\u7dda\u6027\u306e\u8a3a\u65ad\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002</li> </ol> <p>\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u77e5\u8b58\u3092\u5fdc\u7528\u3057\u3066\u3001\u3088\u308a\u5927\u304d\u306a\u6b21\u6570\uff084\u6b21\u4ee5\u4e0a\uff09\u306e\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u65b9\u6cd5\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"lectures/LA/24-determinant/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c24\u56de\uff1a4\u6b21\u4ee5\u4e0a\u306e\u884c\u5217\u5f0f\u306e\u8a08\u7b97","text":""},{"location":"lectures/LA/24-determinant/#_1","title":"\u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c24\u56de \u5206\u91ce: \u7b2c3\u90e8 \u884c\u5217\u5f0f \u95a2\u9023\u9805\u76ee: \u884c\u5217\u5f0f\u306e\u8a08\u7b97\u6cd5\u3001\u57fa\u672c\u5909\u5f62\u3001\u9084\u5143\u5b9a\u7406\u3001\u4f59\u56e0\u5b50\u5c55\u958b \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u7b2c21-23\u56de\u306e\u5185\u5bb9\uff08\u884c\u5217\u5f0f\u306e\u5b9a\u7fa9\u3068\u57fa\u672c\u6027\u8cea\u3001\u57fa\u672c\u5909\u5f62\u3068\u884c\u5217\u5f0f\u3001\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\uff09</p>"},{"location":"lectures/LA/24-determinant/#_2","title":"\u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>4\u6b21\u4ee5\u4e0a\u306e\u884c\u5217\u5f0f\u3092\u52b9\u7387\u7684\u306b\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u3092\u7406\u89e3\u3059\u308b</li> <li>\u57fa\u672c\u5909\u5f62\u3001\u9084\u5143\u5b9a\u7406\u3001\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u884c\u5217\u5f0f\u8a08\u7b97\u306e\u6226\u7565\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u7279\u6b8a\u306a\u5f62\u306e\u884c\u5217\uff08\u4e09\u89d2\u884c\u5217\u3001\u5bfe\u79f0\u884c\u5217\u306a\u3069\uff09\u306e\u884c\u5217\u5f0f\u8a08\u7b97\u3092\u7406\u89e3\u3059\u308b</li> <li>\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u904e\u7a0b\u3067\u767a\u751f\u3057\u3046\u308b\u6570\u5024\u7684\u306a\u554f\u984c\u3068\u305d\u306e\u5bfe\u7b56\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/24-determinant/#1","title":"1. \u57fa\u672c\u6982\u5ff5\u306e\u5fa9\u7fd2","text":""},{"location":"lectures/LA/24-determinant/#11","title":"1.1 \u884c\u5217\u5f0f\u3068\u306f","text":"<p>\u5b9a\u7fa9: \\(n \\times n\\)\u884c\u5217 \\(A = [a_{ij}]\\) \u306e\u884c\u5217\u5f0f\u306f\u4ee5\u4e0b\u3067\u5b9a\u7fa9\u3055\u308c\u308b</p> \\[\\det(A) = |A| = \\sum_{\\sigma \\in S_n} \\text{sgn}(\\sigma) \\prod_{i=1}^n a_{i,\\sigma(i)}\\] <p>\u3053\u3053\u3067 \\(S_n\\) \u306f \\(n\\) \u6b21\u306e\u7f6e\u63db\u5168\u4f53\u306e\u96c6\u5408\u3001\\(\\text{sgn}(\\sigma)\\) \u306f\u7f6e\u63db \\(\\sigma\\) \u306e\u7b26\u53f7</p> <p>\u9ad8\u6b21\u306e\u884c\u5217\u5f0f\u3092\u76f4\u63a5\u3053\u306e\u5b9a\u7fa9\u304b\u3089\u8a08\u7b97\u3059\u308b\u306e\u306f\u975e\u5e38\u306b\u52b4\u529b\u304c\u304b\u304b\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u30014\u6b21\u306e\u884c\u5217\u5f0f\u306b\u306f24\u9805\u30015\u6b21\u3067\u306f120\u9805\u306e\u548c\u304c\u542b\u307e\u308c\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u8a08\u7b97\u3092\u7c21\u7565\u5316\u3059\u308b\u69d8\u3005\u306a\u65b9\u6cd5\u304c\u958b\u767a\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/24-determinant/#12","title":"1.2 \u884c\u5217\u5f0f\u306e\u57fa\u672c\u6027\u8cea\uff08\u5fa9\u7fd2\uff09","text":"<ol> <li>\u884c\u5217\u306e\u884c\u307e\u305f\u306f\u5217\u3092\u4ea4\u63db\u3059\u308b\u3068\u3001\u884c\u5217\u5f0f\u306e\u7b26\u53f7\u304c\u53cd\u8ee2\u3059\u308b</li> <li>\u884c\u5217\u306e\u3042\u308b\u884c\uff08\u5217\uff09\u306b\u4ed6\u306e\u884c\uff08\u5217\uff09\u306e\u5b9a\u6570\u500d\u3092\u52a0\u3048\u3066\u3082\u3001\u884c\u5217\u5f0f\u306e\u5024\u306f\u5909\u308f\u3089\u306a\u3044</li> <li>\u884c\u5217\u306e\u3042\u308b\u884c\uff08\u5217\uff09\u3092\u5b9a\u6570\u500d\u3059\u308b\u3068\u3001\u884c\u5217\u5f0f\u306f\u305d\u306e\u5b9a\u6570\u500d\u306b\u306a\u308b</li> <li>\u884c\u307e\u305f\u306f\u5217\u306b\u30bc\u30ed\u304c\u542b\u307e\u308c\u308b\u3068\u3001\u884c\u5217\u5f0f\u306e\u5024\u306f0\u306b\u306a\u308b</li> <li>\u4e09\u89d2\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3067\u3042\u308b</li> <li>\u884c\u5217 \\(A\\) \u3068 \\(B\\) \u306e\u7a4d\u306e\u884c\u5217\u5f0f\u306f\u3001\u305d\u308c\u305e\u308c\u306e\u884c\u5217\u5f0f\u306e\u7a4d\u306b\u7b49\u3057\u3044\uff1a\\(\\det(AB) = \\det(A) \\cdot \\det(B)\\)</li> </ol>"},{"location":"lectures/LA/24-determinant/#2-4","title":"2. 4\u6b21\u4ee5\u4e0a\u306e\u884c\u5217\u5f0f\u8a08\u7b97\u306e\u624b\u6cd5","text":""},{"location":"lectures/LA/24-determinant/#21","title":"2.1 \u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u65b9\u6cd5","text":"<p>\u9ad8\u6b21\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u6700\u3082\u57fa\u672c\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u3001\u57fa\u672c\u884c\u5909\u5f62\u3092\u7528\u3044\u3066\u884c\u5217\u3092\u4e09\u89d2\u884c\u5217\u307e\u305f\u306f\u5bfe\u89d2\u884c\u5217\u306b\u5909\u63db\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p> <p>\u624b\u9806: 1. \u57fa\u672c\u884c\u5909\u5f62\u3092\u7528\u3044\u3066\u3001\u3067\u304d\u308b\u3060\u3051\u591a\u304f\u306e0\u3092\u4f5c\u308b 2. \u7406\u60f3\u7684\u306b\u306f\u4e0a\u4e09\u89d2\u307e\u305f\u306f\u4e0b\u4e09\u89d2\u884c\u5217\u306b\u5909\u63db\u3059\u308b 3. \u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3092\u8a08\u7b97\u3059\u308b</p> <p>\u4f8b\u984c1: \u6b21\u306e4\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002</p> \\[A = \\begin{pmatrix}  1 &amp; 2 &amp; 3 &amp; 4 \\\\ 2 &amp; 3 &amp; 4 &amp; 5 \\\\ 3 &amp; 4 &amp; 5 &amp; 6 \\\\ 4 &amp; 5 &amp; 6 &amp; 7 \\end{pmatrix}\\] <p>\u89e3\u6cd5: 1. \u7b2c1\u884c\u3092\u57fa\u6e96\u3068\u3057\u3066\u3001\u4ed6\u306e\u884c\u304b\u3089\u9069\u5207\u306a\u500d\u6570\u3092\u5f15\u304f\u64cd\u4f5c\u3092\u884c\u3046    - \\(R_2 \\leftarrow R_2 - 2R_1\\)\uff08\u7b2c2\u884c\u304b\u3089\u7b2c1\u884c\u306e2\u500d\u3092\u5f15\u304f\uff09    - \\(R_3 \\leftarrow R_3 - 3R_1\\)\uff08\u7b2c3\u884c\u304b\u3089\u7b2c1\u884c\u306e3\u500d\u3092\u5f15\u304f\uff09    - \\(R_4 \\leftarrow R_4 - 4R_1\\)\uff08\u7b2c4\u884c\u304b\u3089\u7b2c1\u884c\u306e4\u500d\u3092\u5f15\u304f\uff09</p> \\[\\begin{pmatrix}  1 &amp; 2 &amp; 3 &amp; 4 \\\\ 0 &amp; -1 &amp; -2 &amp; -3 \\\\ 0 &amp; -2 &amp; -4 &amp; -6 \\\\ 0 &amp; -3 &amp; -6 &amp; -9 \\end{pmatrix}\\] <ol> <li>\u7b2c2\u884c\u3092\u57fa\u6e96\u3068\u3057\u3066\u3001\u7b2c3\u884c\u3068\u7b2c4\u884c\u304b\u3089\u9069\u5207\u306a\u500d\u6570\u3092\u5f15\u304f</li> <li>\\(R_3 \\leftarrow R_3 - 2R_2\\)\uff08\u7b2c3\u884c\u304b\u3089\u7b2c2\u884c\u306e2\u500d\u3092\u5f15\u304f\uff09</li> <li>\\(R_4 \\leftarrow R_4 - 3R_2\\)\uff08\u7b2c4\u884c\u304b\u3089\u7b2c2\u884c\u306e3\u500d\u3092\u5f15\u304f\uff09</li> </ol> \\[\\begin{pmatrix}  1 &amp; 2 &amp; 3 &amp; 4 \\\\ 0 &amp; -1 &amp; -2 &amp; -3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix}\\] <ol> <li>\u884c\u5217\u306e\u7b2c3\u884c\u3068\u7b2c4\u884c\u304c\u30bc\u30ed\u884c\u306b\u306a\u3063\u305f\u305f\u3081\u3001\u884c\u5217\u5f0f\u306f0\u3068\u306a\u308b</li> </ol> \\[\\det(A) = 0\\]"},{"location":"lectures/LA/24-determinant/#22","title":"2.2 \u9084\u5143\u5b9a\u7406\u3092\u6d3b\u7528\u3057\u305f\u65b9\u6cd5","text":"<p>\u9ad8\u6b21\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u969b\u306b\u3001\u9084\u5143\u5b9a\u7406\u306f\u975e\u5e38\u306b\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u3067\u3059\u30022\u6b21\u307e\u305f\u306f3\u6b21\u306e\u5c0f\u884c\u5217\u5f0f\u306b\u9084\u5143\u3059\u308b\u3053\u3068\u3067\u3001\u8a08\u7b97\u304c\u5927\u5e45\u306b\u7c21\u7565\u5316\u3055\u308c\u307e\u3059\u3002</p> <p>\u9084\u5143\u5b9a\u7406: \u884c\u5217\u306e\u7b2ci\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u5404\u6210\u5206\u306b\u3001\u305d\u306e\u4f59\u56e0\u5b50\u3092\u639b\u3051\u3066\u548c\u3092\u53d6\u308b\u3068\u3001\u884c\u5217\u5f0f\u306e\u5024\u304c\u5f97\u3089\u308c\u308b</p> \\[\\det(A) = \\sum_{j=1}^{n} a_{ij} \\cdot A_{ij}\\] <p>\u3053\u3053\u3067 \\(A_{ij}\\) \u306f\u6210\u5206 \\(a_{ij}\\) \u306e\u4f59\u56e0\u5b50</p> <p>\u4f8b\u984c2: \u6b21\u306e4\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u9084\u5143\u5b9a\u7406\u3092\u7528\u3044\u3066\u8a08\u7b97\u305b\u3088\u3002</p> \\[B = \\begin{pmatrix}  3 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 3 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 3 \\end{pmatrix}\\] <p>\u89e3\u6cd5: 1. \u7b2c1\u5217\u3092\u7528\u3044\u3066\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u884c\u3046</p> \\[\\det(B) = 3 \\cdot M_{11} - 1 \\cdot M_{21}\\] <p>\u3053\u3053\u3067\u3001\\(M_{11}\\)\u306f\u4f59\u56e0\u5b50\u884c\u5217\u3001\u3059\u306a\u308f\u3061\\(B\\)\u306e\u7b2c1\u884c\u7b2c1\u5217\u3092\u9664\u3044\u305f\u5c0f\u884c\u5217\u5f0f\u3001\\(M_{21}\\)\u306f\\(B\\)\u306e\u7b2c2\u884c\u7b2c1\u5217\u3092\u9664\u3044\u305f\u5c0f\u884c\u5217\u5f0f\u3067\u3042\u308b\u3002</p> \\[M_{11} = \\begin{vmatrix}  3 &amp; 1 &amp; 0 \\\\ 1 &amp; 3 &amp; 1 \\\\ 0 &amp; 1 &amp; 3 \\end{vmatrix}\\] \\[M_{21} = \\begin{vmatrix}  1 &amp; 0 &amp; 0 \\\\ 1 &amp; 3 &amp; 1 \\\\ 0 &amp; 1 &amp; 3 \\end{vmatrix}\\] <ol> <li>\u3053\u308c\u3089\u306e3\u6b21\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b</li> </ol> <p>\\(M_{11}\\)\u306e\u8a08\u7b97: - \u7b2c1\u5217\u3067\u4f59\u56e0\u5b50\u5c55\u958b: \\(3 \\cdot \\begin{vmatrix} 3 &amp; 1 \\\\ 1 &amp; 3 \\end{vmatrix} - 1 \\cdot \\begin{vmatrix} 1 &amp; 0 \\\\ 1 &amp; 3 \\end{vmatrix}\\) - \\(= 3 \\cdot (3 \\cdot 3 - 1 \\cdot 1) - 1 \\cdot (1 \\cdot 3 - 0 \\cdot 1)\\) - \\(= 3 \\cdot 8 - 1 \\cdot 3 = 24 - 3 = 21\\)</p> <p>\\(M_{21}\\)\u306e\u8a08\u7b97: - \u7b2c1\u5217\u3067\u4f59\u56e0\u5b50\u5c55\u958b: \\(1 \\cdot \\begin{vmatrix} 3 &amp; 1 \\\\ 1 &amp; 3 \\end{vmatrix} - 1 \\cdot \\begin{vmatrix} 0 &amp; 0 \\\\ 1 &amp; 3 \\end{vmatrix}\\) - \\(= 1 \\cdot 8 - 1 \\cdot 0 = 8\\)</p> <ol> <li>\u5143\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97</li> </ol> \\[\\det(B) = 3 \\cdot 21 - 1 \\cdot 8 = 63 - 8 = 55\\]"},{"location":"lectures/LA/24-determinant/#23","title":"2.3 \u52b9\u7387\u7684\u306a\u5c55\u958b\u3092\u9078\u629e\u3059\u308b\u6226\u7565","text":"<p>\u9ad8\u6b21\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u969b\u306b\u306f\u3001\u8a08\u7b97\u3092\u6700\u3082\u7c21\u7565\u5316\u3067\u304d\u308b\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3093\u3067\u5c55\u958b\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p> <p>\u5c55\u958b\u306e\u9078\u629e\u57fa\u6e96: 1. \u30bc\u30ed\u6210\u5206\u304c\u6700\u3082\u591a\u3044\u884c\u307e\u305f\u306f\u5217\u3092\u9078\u3076 2. \u30d1\u30bf\u30fc\u30f3\u304c\u8a8d\u8b58\u3067\u304d\u308b\u5834\u5408\uff08\u4e09\u89d2\u884c\u5217\u3001\u5bfe\u89d2\u884c\u5217\u306a\u3069\uff09\u306f\u3001\u305d\u308c\u3092\u6d3b\u7528\u3059\u308b 3. \u5c0f\u884c\u5217\u5f0f\u306b\u65e2\u77e5\u306e\u30d1\u30bf\u30fc\u30f3\u304c\u73fe\u308c\u308b\u3088\u3046\u5c55\u958b\u65b9\u6cd5\u3092\u9078\u3076</p> <p>\u4f8b\u984c3: \u6b21\u306e5\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002</p> \\[C = \\begin{pmatrix}  1 &amp; 0 &amp; 2 &amp; 0 &amp; 3 \\\\ 0 &amp; 4 &amp; 0 &amp; 5 &amp; 0 \\\\ 0 &amp; 0 &amp; 6 &amp; 0 &amp; 0 \\\\ 0 &amp; 7 &amp; 0 &amp; 8 &amp; 0 \\\\ 9 &amp; 0 &amp; 0 &amp; 0 &amp; 10 \\end{pmatrix}\\] <p>\u89e3\u6cd5: \u30bc\u30ed\u304c\u6700\u3082\u591a\u3044\u7b2c3\u884c\u3067\u5c55\u958b\u3059\u308b</p> \\[\\det(C) = 6 \\cdot \\begin{vmatrix}  1 &amp; 0 &amp; 0 &amp; 3 \\\\ 0 &amp; 4 &amp; 5 &amp; 0 \\\\ 0 &amp; 7 &amp; 8 &amp; 0 \\\\ 9 &amp; 0 &amp; 0 &amp; 10 \\end{vmatrix}\\] <p>\u3053\u306e4\u6b21\u884c\u5217\u3082\u591a\u304f\u306e\u30bc\u30ed\u3092\u542b\u3093\u3067\u3044\u308b\u305f\u3081\u3001\u7b2c4\u5217\u3067\u5c55\u958b\u3059\u308b\u306e\u304c\u52b9\u7387\u7684</p> \\[6 \\cdot \\left( 3 \\cdot \\begin{vmatrix}  0 &amp; 4 &amp; 5 \\\\ 0 &amp; 7 &amp; 8 \\\\ 9 &amp; 0 &amp; 0 \\end{vmatrix} - 0 \\cdot (\\cdots) + 0 \\cdot (\\cdots) - 10 \\cdot \\begin{vmatrix}  1 &amp; 0 &amp; 0 \\\\ 0 &amp; 4 &amp; 5 \\\\ 0 &amp; 7 &amp; 8 \\end{vmatrix} \\right)\\] \\[= 6 \\cdot \\left( 3 \\cdot 9 \\cdot \\begin{vmatrix}  4 &amp; 5 \\\\ 7 &amp; 8 \\end{vmatrix} - 10 \\cdot \\begin{vmatrix}  4 &amp; 5 \\\\ 7 &amp; 8 \\end{vmatrix} \\right)\\] \\[= 6 \\cdot (3 \\cdot 9 - 10) \\cdot (4 \\cdot 8 - 5 \\cdot 7)$$ $$= 6 \\cdot (27 - 10) \\cdot (32 - 35)$$ $$= 6 \\cdot 17 \\cdot (-3) = -306\\]"},{"location":"lectures/LA/24-determinant/#3","title":"3. \u7279\u6b8a\u306a\u5f62\u72b6\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f","text":"<p>\u7279\u6b8a\u306a\u5f62\u72b6\u306e\u884c\u5217\u306b\u5bfe\u3057\u3066\u306f\u3001\u8a08\u7b97\u304c\u5927\u5e45\u306b\u7c21\u7565\u5316\u3055\u308c\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/24-determinant/#31","title":"3.1 \u4e09\u89d2\u884c\u5217","text":"<p>\u5b9a\u7406: \u4e09\u89d2\u884c\u5217\uff08\u4e0a\u4e09\u89d2\u307e\u305f\u306f\u4e0b\u4e09\u89d2\uff09\u306e\u884c\u5217\u5f0f\u306f\u3001\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u306b\u7b49\u3057\u3044</p> \\[\\det(T) = t_{11} \\cdot t_{22} \\cdot \\ldots \\cdot t_{nn}\\] <p>\u4f8b\u984c4: \u6b21\u306e\u4e0a\u4e09\u89d2\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002</p> \\[D = \\begin{pmatrix}  2 &amp; 3 &amp; 4 &amp; 5 \\\\ 0 &amp; 3 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 4 &amp; 6 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] <p>\u89e3\u6cd5: \u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3092\u8a08\u7b97\u3059\u308b \\(\\(\\det(D) = 2 \\cdot 3 \\cdot 4 \\cdot 1 = 24\\)\\)</p>"},{"location":"lectures/LA/24-determinant/#32","title":"3.2 \u30d6\u30ed\u30c3\u30af\u5bfe\u89d2\u884c\u5217","text":"<p>\u5b9a\u7406: \u30d6\u30ed\u30c3\u30af\u5bfe\u89d2\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u3001\u5404\u30d6\u30ed\u30c3\u30af\u306e\u884c\u5217\u5f0f\u306e\u7a4d\u306b\u7b49\u3057\u3044</p> \\[\\det\\begin{pmatrix} A &amp; 0 \\\\ 0 &amp; B \\end{pmatrix} = \\det(A) \\cdot \\det(B)\\] <p>\u4f8b\u984c5: \u6b21\u306e\u30d6\u30ed\u30c3\u30af\u5bfe\u89d2\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002</p> \\[E = \\begin{pmatrix}  2 &amp; 1 &amp; 0 &amp; 0 \\\\ 3 &amp; 4 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 5 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; 3 \\end{pmatrix}\\] <p>\u89e3\u6cd5: 2\u3064\u306e\u30d6\u30ed\u30c3\u30af\u306e\u884c\u5217\u5f0f\u3092\u500b\u5225\u306b\u8a08\u7b97\u3057\u3001\u305d\u308c\u3089\u306e\u7a4d\u3092\u6c42\u3081\u308b</p> \\[\\det(E) = \\begin{vmatrix} 2 &amp; 1 \\\\ 3 &amp; 4 \\end{vmatrix} \\cdot \\begin{vmatrix} 5 &amp; 2 \\\\ 1 &amp; 3 \\end{vmatrix}$$ $$= (2 \\cdot 4 - 1 \\cdot 3) \\cdot (5 \\cdot 3 - 2 \\cdot 1)$$ $$= 5 \\cdot 13 = 65\\]"},{"location":"lectures/LA/24-determinant/#33","title":"3.3 \u7279\u6b8a\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u6301\u3064\u884c\u5217","text":"<p>\u4f8b\u984c6: \u4ee5\u4e0b\u306e\u7279\u6b8a\u306a\u5f62\u72b6\u306e5\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002</p> \\[F = \\begin{pmatrix}  a &amp; b &amp; b &amp; b &amp; b \\\\ b &amp; a &amp; b &amp; b &amp; b \\\\ b &amp; b &amp; a &amp; b &amp; b \\\\ b &amp; b &amp; b &amp; a &amp; b \\\\ b &amp; b &amp; b &amp; b &amp; a \\end{pmatrix}\\] <p>\u89e3\u6cd5: \u3053\u306e\u884c\u5217\u306f\u3059\u3079\u3066\u306e\u5bfe\u89d2\u6210\u5206\u304c \\(a\\) \u3067\u3001\u4ed6\u306e\u6210\u5206\u304c\u3059\u3079\u3066 \\(b\\) \u3067\u3042\u308b\u7279\u6b8a\u306a\u5f62\u3092\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u4ee5\u4e0b\u306e\u516c\u5f0f\u3067\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a</p> \\[\\det(F) = (a-b)^{n-1} \\cdot (a+(n-1)b)\\] <p>\u3053\u3053\u3067 \\(n = 5\\) \u306a\u306e\u3067\uff1a</p> \\[\\det(F) = (a-b)^4 \\cdot (a+4b)\\] <p>\u3053\u308c\u306f\u30d1\u30bf\u30fc\u30f3\u3092\u8a8d\u8b58\u3057\u3066\u7279\u6b8a\u306a\u516c\u5f0f\u3092\u9069\u7528\u3057\u305f\u4f8b\u3067\u3059\u3002</p>"},{"location":"lectures/LA/24-determinant/#4","title":"4. \u6570\u5024\u8a08\u7b97\u4e0a\u306e\u8ab2\u984c\u3068\u305d\u306e\u5bfe\u7b56","text":""},{"location":"lectures/LA/24-determinant/#41","title":"4.1 \u6570\u5024\u7684\u5b89\u5b9a\u6027\u306e\u554f\u984c","text":"<p>\u9ad8\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u969b\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u6570\u5024\u8a08\u7b97\u4e0a\u306e\u554f\u984c\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u6841\u843d\u3061\uff1a\u4f3c\u305f\u5927\u304d\u3055\u306e\u6570\u306e\u5f15\u304d\u7b97\u3067\u7cbe\u5ea6\u304c\u5931\u308f\u308c\u308b</li> <li>\u4e38\u3081\u8aa4\u5dee\u306e\u84c4\u7a4d\uff1a\u591a\u304f\u306e\u6f14\u7b97\u3092\u884c\u3046\u3053\u3068\u3067\u8aa4\u5dee\u304c\u7d2f\u7a4d\u3059\u308b</li> <li>\u30aa\u30fc\u30d0\u30fc\u30d5\u30ed\u30fc/\u30a2\u30f3\u30c0\u30fc\u30d5\u30ed\u30fc\uff1a\u975e\u5e38\u306b\u5927\u304d\u306a/\u5c0f\u3055\u306a\u6570\u5024\u304c\u767a\u751f\u3059\u308b</li> </ol>"},{"location":"lectures/LA/24-determinant/#42","title":"4.2 \u5bfe\u7b56\u65b9\u6cd5","text":"<ol> <li>\u30d4\u30dc\u30c3\u30c8\u9078\u629e\uff1a\u8a08\u7b97\u9014\u4e2d\u3067\u6700\u3082\u9069\u5207\u306a\u884c\u30fb\u5217\u3092\u9078\u3093\u3067\u5c55\u958b\u3059\u308b</li> <li>\u6570\u5024\u7684\u306b\u5b89\u5b9a\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff1a\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3084LU\u5206\u89e3\u306a\u3069\u3092\u4f7f\u7528\u3059\u308b</li> <li>\u884c\u5217\u5f0f\u3092\u76f4\u63a5\u8a08\u7b97\u3057\u306a\u3044\uff1a\u884c\u5217\u5f0f\u304c\u5fc5\u8981\u306a\u5834\u5408\u3067\u3082\u3001\u4ed6\u306e\u65b9\u6cd5\uff08\u4f8b\uff1aLU\u5206\u89e3\uff09\u3092\u7528\u3044\u308b</li> </ol>"},{"location":"lectures/LA/24-determinant/#43-lu","title":"4.3 \u884c\u5217\u5f0f\u306e\u8a08\u7b97\u306b\u304a\u3051\u308bLU\u5206\u89e3\u306e\u5229\u7528","text":"<p>\u5b9a\u7406: \\(A = LU\\) \u3068\u5206\u89e3\u3067\u304d\u308b\u5834\u5408\uff08\\(L\\)\u306f\u5bfe\u89d2\u6210\u5206\u304c1\u306e\u4e0b\u4e09\u89d2\u884c\u5217\u3001\\(U\\)\u306f\u4e0a\u4e09\u89d2\u884c\u5217\uff09\u3001</p> \\[\\det(A) = \\det(L) \\cdot \\det(U) = \\prod_{i=1}^n u_{ii}\\] <p>\u4f8b\u984c7: \u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092LU\u5206\u89e3\u3092\u7528\u3044\u3066\u8a08\u7b97\u305b\u3088\u3002</p> \\[G = \\begin{pmatrix}  4 &amp; 3 &amp; 2 &amp; 1 \\\\ 3 &amp; 4 &amp; 3 &amp; 2 \\\\ 2 &amp; 3 &amp; 4 &amp; 3 \\\\ 1 &amp; 2 &amp; 3 &amp; 4 \\end{pmatrix}\\] <p>\u89e3\u6cd5: LU\u5206\u89e3\u3092\u7528\u3044\u3066\u884c\u5217\u3092\u5206\u89e3\u3057\u3001\\(U\\)\u306e\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\uff08\u8a08\u7b97\u904e\u7a0b\u306f\u7701\u7565\u3057\u3001\u7d50\u679c\u306e\u307f\u793a\u3057\u307e\u3059\uff09</p> \\[L = \\begin{pmatrix}  1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0.75 &amp; 1 &amp; 0 &amp; 0 \\\\ 0.5 &amp; 0.8 &amp; 1 &amp; 0 \\\\ 0.25 &amp; 0.6 &amp; 0.833 &amp; 1 \\end{pmatrix},  U = \\begin{pmatrix}  4 &amp; 3 &amp; 2 &amp; 1 \\\\ 0 &amp; 1.75 &amp; 1.5 &amp; 1.25 \\\\ 0 &amp; 0 &amp; 0.7 &amp; 1.0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0.2 \\end{pmatrix}\\] \\[\\det(G) = 4 \\cdot 1.75 \\cdot 0.7 \\cdot 0.2 = 0.98\\]"},{"location":"lectures/LA/24-determinant/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u306f\u3001NumPy \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u7528\u3044\u3066\u7c21\u5358\u306b\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import det, qr\nimport time\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[1, 2, 3, 4],\n              [2, 3, 4, 5],\n              [3, 4, 5, 6],\n              [4, 5, 6, 7]])\n\nB = np.array([[3, 1, 0, 0],\n              [1, 3, 1, 0],\n              [0, 1, 3, 1],\n              [0, 0, 1, 3]])\n\n# \u884c\u5217\u5f0f\u306e\u8a08\u7b97\ndet_A = det(A)\ndet_B = det(B)\n\nprint(f\"det(A) = {det_A}\")\nprint(f\"det(B) = {det_B}\")\n\n# \u6027\u80fd\u6bd4\u8f03\uff1a\u884c\u5217\u306e\u30b5\u30a4\u30ba\u306b\u3088\u308b\u8a08\u7b97\u6642\u9593\u306e\u9055\u3044\nsizes = range(2, 11)\ntimes_det = []\n\nfor n in sizes:\n    # \u30e9\u30f3\u30c0\u30e0\u306a\u884c\u5217\u3092\u4f5c\u6210\n    M = np.random.rand(n, n)\n\n    # \u8a08\u7b97\u6642\u9593\u3092\u6e2c\u5b9a\n    start = time.time()\n    d = det(M)\n    end = time.time()\n\n    times_det.append(end - start)\n\n# \u8a08\u7b97\u6642\u9593\u306e\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.plot(sizes, times_det, 'o-', label='NumPy det()')\nplt.xlabel('\u884c\u5217\u30b5\u30a4\u30ba')\nplt.ylabel('\u8a08\u7b97\u6642\u9593 (\u79d2)')\nplt.title('\u884c\u5217\u5f0f\u8a08\u7b97\u306e\u8a08\u7b97\u6642\u9593 vs. \u884c\u5217\u30b5\u30a4\u30ba')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# LU\u5206\u89e3\u3092\u7528\u3044\u305f\u884c\u5217\u5f0f\u306e\u8a08\u7b97\ndef det_using_lu(M):\n    # QR\u5206\u89e3\u3092\u4f7f\u7528\uff08\u5b89\u5b9a\u6027\u306e\u305f\u3081\uff09\n    Q, R = qr(M)\n    # \u4e0a\u4e09\u89d2\u884c\u5217R\u306e\u5bfe\u89d2\u8981\u7d20\u306e\u7a4d\n    return np.prod(np.diag(R)) * np.linalg.det(Q)\n\n# \u4f8b\nG = np.array([[4, 3, 2, 1],\n              [3, 4, 3, 2],\n              [2, 3, 4, 3],\n              [1, 2, 3, 4]])\n\ndet_G_direct = det(G)\ndet_G_lu = det_using_lu(G)\n\nprint(f\"det(G) (\u76f4\u63a5\u8a08\u7b97) = {det_G_direct}\")\nprint(f\"det(G) (LU\u5206\u89e3) = {det_G_lu}\")\n</code></pre> <p>\u3053\u306e\u30b3\u30fc\u30c9\u306f\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u3092\u5b9f\u6f14\u3057\u3001\u884c\u5217\u30b5\u30a4\u30ba\u306b\u5fdc\u3058\u305f\u8a08\u7b97\u6642\u9593\u306e\u5909\u5316\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u3001LU\u5206\u89e3\u3092\u7528\u3044\u305f\u8a08\u7b97\u65b9\u6cd5\u3082\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/24-determinant/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/24-determinant/#_3","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e4\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(\\begin{pmatrix}     1 &amp; 2 &amp; 0 &amp; 0 \\\\    3 &amp; 4 &amp; 0 &amp; 0 \\\\    0 &amp; 0 &amp; 5 &amp; 6 \\\\    0 &amp; 0 &amp; 7 &amp; 8    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e4\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(\\begin{pmatrix}     1 &amp; 1 &amp; 1 &amp; 1 \\\\    0 &amp; 2 &amp; 2 &amp; 2 \\\\    0 &amp; 0 &amp; 3 &amp; 3 \\\\    0 &amp; 0 &amp; 0 &amp; 4    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e5\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(\\begin{pmatrix}     2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\    1 &amp; 3 &amp; 0 &amp; 0 &amp; 0 \\\\    1 &amp; 1 &amp; 4 &amp; 0 &amp; 0 \\\\    1 &amp; 1 &amp; 1 &amp; 5 &amp; 0 \\\\    1 &amp; 1 &amp; 1 &amp; 1 &amp; 6    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(\\begin{pmatrix}     a &amp; 0 &amp; 0 &amp; 0 \\\\    b &amp; c &amp; 0 &amp; 0 \\\\    0 &amp; d &amp; e &amp; 0 \\\\    0 &amp; 0 &amp; f &amp; g    \\end{pmatrix}\\)\\)</p> </li> </ol>"},{"location":"lectures/LA/24-determinant/#_4","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e4\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(\\begin{pmatrix}     1 &amp; 2 &amp; 3 &amp; 4 \\\\    5 &amp; 6 &amp; 7 &amp; 8 \\\\    9 &amp; 10 &amp; 11 &amp; 12 \\\\    13 &amp; 14 &amp; 15 &amp; 16    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u533b\u7642\u30c7\u30fc\u30bf\u306e\u5206\u6790\u306b\u95a2\u3059\u308b\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u3001\u3053\u306e\u884c\u5217\u306e\u53ef\u9006\u6027\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002\u884c\u5217\\(X\\)\u306f\u60a3\u8005\u306e\u7279\u5fb4\u91cf\uff08\u5e74\u9f62\u3001\u4f53\u91cd\u3001\u8eab\u9577\u3001\u8840\u5727\uff09\u3092\u8868\u3057\u3066\u304a\u308a\u3001\u3053\u308c\u3092\u7528\u3044\u3066\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3002    \\(\\(X^TX = \\begin{pmatrix}     100 &amp; 350 &amp; 520 &amp; 420 \\\\    350 &amp; 1300 &amp; 1850 &amp; 1500 \\\\    520 &amp; 1850 &amp; 2700 &amp; 2150 \\\\    420 &amp; 1500 &amp; 2150 &amp; 1800    \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u5f62\u306e\\(n\\)\u6b21\u6b63\u65b9\u884c\u5217\u306e\u884c\u5217\u5f0f\u306e\u4e00\u822c\u516c\u5f0f\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    \\(\\(\\begin{pmatrix}     x &amp; a &amp; a &amp; \\cdots &amp; a \\\\    a &amp; x &amp; a &amp; \\cdots &amp; a \\\\    a &amp; a &amp; x &amp; \\cdots &amp; a \\\\    \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\    a &amp; a &amp; a &amp; \\cdots &amp; x    \\end{pmatrix}\\)\\)</p> </li> </ol>"},{"location":"lectures/LA/24-determinant/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":"<p>Q1: \u9ad8\u6b21\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u969b\u3001\u6700\u3082\u52b9\u7387\u7684\u306a\u65b9\u6cd5\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A1: \u4e00\u822c\u7684\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u65b9\u6cd5\u304c\u52b9\u7387\u7684\u3067\u3059\uff1a 1. \u884c\u5217\u306b\u30bc\u30ed\u6210\u5206\u304c\u591a\u3044\u5834\u5408\u306f\u3001\u305d\u308c\u3089\u3092\u542b\u3080\u884c\u307e\u305f\u306f\u5217\u3067\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u884c\u3046 2. \u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u4e09\u89d2\u884c\u5217\u306b\u5909\u63db\u3059\u308b 3. \u7279\u6b8a\u306a\u30d1\u30bf\u30fc\u30f3\u304c\u3042\u308b\u5834\u5408\u306f\u3001\u305d\u308c\u306b\u5bfe\u5fdc\u3057\u305f\u516c\u5f0f\u3092\u4f7f\u7528\u3059\u308b 4. \u6570\u5024\u8a08\u7b97\u3067\u306f\u3001\u5b89\u5b9a\u6027\u3092\u8003\u616e\u3057\u3066LU\u5206\u89e3\u306a\u3069\u306e\u624b\u6cd5\u3092\u7528\u3044\u308b</p> <p>Q2: \u884c\u5217\u304c\u7279\u7570\uff08singular\uff09\u3067\u3042\u308b\u3068\u306f\u3069\u3046\u3044\u3046\u610f\u5473\u3067\u3059\u304b\uff1f</p> <p>A2: \u884c\u5217\u304c\u7279\u7570\u3067\u3042\u308b\u3068\u306f\u3001\u305d\u306e\u884c\u5217\u5f0f\u304c0\u306b\u306a\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u7279\u7570\u884c\u5217\u306f\u9006\u884c\u5217\u3092\u6301\u305f\u305a\u3001\u5bfe\u5fdc\u3059\u308b\u9023\u7acb\u65b9\u7a0b\u5f0f\u30b7\u30b9\u30c6\u30e0\u306f\u4e00\u610f\u306e\u89e3\u3092\u6301\u3061\u307e\u305b\u3093\u3002\u3053\u306e\u3088\u3046\u306a\u884c\u5217\u306f\u3001\u7dda\u5f62\u5f93\u5c5e\u306a\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002</p> <p>Q3: \u884c\u5217\u5f0f\u306e\u8a08\u7b97\u306b\u304a\u3044\u3066\u3001\u57fa\u672c\u5909\u5f62\u3092\u884c\u3046\u3068\u304d\u6ce8\u610f\u3059\u3079\u304d\u70b9\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A3: \u57fa\u672c\u5909\u5f62\u3067\u306f\u4ee5\u4e0b\u306e\u70b9\u306b\u6ce8\u610f\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u884c\u307e\u305f\u306f\u5217\u306e\u4ea4\u63db\u3092\u884c\u3046\u3068\u3001\u884c\u5217\u5f0f\u306e\u7b26\u53f7\u304c\u53cd\u8ee2\u3059\u308b 2. \u884c\u307e\u305f\u306f\u5217\u306b\u5b9a\u6570\u3092\u304b\u3051\u308b\u3068\u3001\u884c\u5217\u5f0f\u306f\u305d\u306e\u5b9a\u6570\u500d\u306b\u306a\u308b 3. \u3042\u308b\u884c\uff08\u5217\uff09\u306b\u4ed6\u306e\u884c\uff08\u5217\uff09\u306e\u5b9a\u6570\u500d\u3092\u52a0\u3048\u308b\u64cd\u4f5c\u306f\u3001\u884c\u5217\u5f0f\u306e\u5024\u3092\u5909\u3048\u306a\u3044</p> <p>Q4: \u884c\u5217\u5f0f\u306e\u8a08\u7b97\u304c\u30c7\u30fc\u30bf\u5206\u6790\u3067\u3069\u306e\u3088\u3046\u306b\u5f79\u7acb\u3061\u307e\u3059\u304b\uff1f</p> <p>A4: \u884c\u5217\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5834\u9762\u3067\u5f79\u7acb\u3061\u307e\u3059\uff1a 1. \u7dda\u5f62\u5909\u63db\u306e\u4f53\u7a4d\u5909\u5316\u7387\u3092\u8868\u3059\uff08\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\uff09 2. \u884c\u5217\u306e\u53ef\u9006\u6027\u306e\u5224\u5b9a\uff08\u884c\u5217\u5f0f\u304c0\u3067\u306a\u3051\u308c\u3070\u53ef\u9006\uff09 3. \u7dda\u5f62\u65b9\u7a0b\u5f0f\u7cfb\u306e\u89e3\u306e\u5b58\u5728\u3068\u4e00\u610f\u6027\u306e\u5224\u5b9a 4. \u591a\u5909\u91cf\u7d71\u8a08\u89e3\u6790\u306b\u304a\u3051\u308b\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u7279\u6027\u8a55\u4fa1 5. \u4e3b\u6210\u5206\u5206\u6790\u3084\u56e0\u5b50\u5206\u6790\u306e\u524d\u51e6\u7406\u3068\u3057\u3066\u884c\u5217\u306e\u7279\u6027\u3092\u8abf\u3079\u308b\u969b\u306e\u6307\u6a19</p> <p>Q5: \u5927\u304d\u306a\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3059\u308b\u969b\u3001\u6570\u5024\u7684\u306a\u554f\u984c\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u304b\uff1f</p> <p>A5: \u306f\u3044\u3001\u7279\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u554f\u984c\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u4e38\u3081\u8aa4\u5dee\u306e\u7d2f\u7a4d 2. \u6841\u843d\u3061\uff08\u4f3c\u305f\u5024\u306e\u5f15\u304d\u7b97\u306b\u3088\u308b\u7cbe\u5ea6\u306e\u640d\u5931\uff09 3. \u30aa\u30fc\u30d0\u30fc\u30d5\u30ed\u30fc/\u30a2\u30f3\u30c0\u30fc\u30d5\u30ed\u30fc 4. \u6761\u4ef6\u6570\u306e\u60aa\u3044\u884c\u5217\u3067\u306e\u4e0d\u5b89\u5b9a\u6027</p> <p>\u3053\u308c\u3089\u306e\u554f\u984c\u3092\u56de\u907f\u3059\u308b\u306b\u306f\u3001\u76f4\u63a5\u7684\u306a\u8a08\u7b97\u3088\u308a\u3082\u3001QR\u5206\u89e3\u3084LU\u5206\u89e3\u306a\u3069\u306e\u6570\u5024\u7684\u306b\u5b89\u5b9a\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/25-exercise/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II - \u7b2c25\u56de \u7dcf\u5408\u6f14\u7fd2","text":""},{"location":"lectures/LA/25-exercise/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c25\u56de \u30c6\u30fc\u30de: \u884c\u5217\u5f0f\u306b\u95a2\u3059\u308b\u7dcf\u5408\u6f14\u7fd2 \u95a2\u9023\u9805\u76ee: \u884c\u5217\u5f0f\u306e\u5b9a\u7fa9\u3001\u57fa\u672c\u5909\u5f62\u3068\u884c\u5217\u5f0f\u3001\u9084\u5143\u5b9a\u7406\u3001\u4f59\u56e0\u5b50\u5c55\u958b \u4e88\u7fd2\u5185\u5bb9: \u7b2c21\u56de\u301c\u7b2c24\u56de\u306e\u5185\u5bb9\uff08\u884c\u5217\u5f0f\u306e\u57fa\u672c\u6027\u8cea\u3001\u8a08\u7b97\u65b9\u6cd5\u3001\u4f59\u56e0\u5b50\u5c55\u958b\u306a\u3069\uff09\u3092\u5fa9\u7fd2\u3057\u3066\u304a\u304f\u3053\u3068</p>"},{"location":"lectures/LA/25-exercise/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u884c\u5217\u5f0f\u306e\u57fa\u672c\u7684\u306a\u6027\u8cea\u3092\u7406\u89e3\u3057\u3001\u9069\u5207\u306b\u5fdc\u7528\u3067\u304d\u308b</li> <li>\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u305f\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u9084\u5143\u5b9a\u7406\u3068\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u4f7f\u3063\u3066\u69d8\u3005\u306a\u884c\u5217\u5f0f\u3092\u52b9\u7387\u7684\u306b\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u884c\u5217\u5f0f\u3068\u9006\u884c\u5217\u306e\u95a2\u4fc2\u6027\u3092\u7406\u89e3\u3057\u3001\u6d3b\u7528\u3067\u304d\u308b</li> <li>\u884c\u5217\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u7406\u89e3\u3057\u3001\u30c7\u30fc\u30bf\u5206\u6790\u3078\u306e\u5fdc\u7528\u306e\u57fa\u790e\u3092\u8eab\u306b\u3064\u3051\u308b</li> </ol>"},{"location":"lectures/LA/25-exercise/#3","title":"3. \u57fa\u672c\u6982\u5ff5\u306e\u5fa9\u7fd2","text":""},{"location":"lectures/LA/25-exercise/#31","title":"3.1 \u884c\u5217\u5f0f\u306e\u5b9a\u7fa9\u3068\u57fa\u672c\u6027\u8cea","text":"<p>\u5b9a\u7fa9: \\(n\\) \u6b21\u6b63\u65b9\u884c\u5217 \\(A = [a_{ij}]\\) \u306e\u884c\u5217\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a</p> \\[\\det(A) = |A| = \\sum_{\\sigma \\in S_n} \\text{sgn}(\\sigma) \\prod_{i=1}^{n} a_{i\\sigma(i)}\\] <p>\u3053\u3053\u3067 \\(S_n\\) \u306f \\(n\\) \u500b\u306e\u8981\u7d20\u306e\u7f6e\u63db\u5168\u4f53\u306e\u96c6\u5408\u3001\\(\\text{sgn}(\\sigma)\\) \u306f\u7f6e\u63db \\(\\sigma\\) \u306e\u7b26\u53f7\u3092\u8868\u3059\u3002</p> <p>\u4e3b\u306a\u6027\u8cea:</p> <ol> <li>\u5358\u4f4d\u884c\u5217\u306e\u884c\u5217\u5f0f: \\(\\det(I) = 1\\)</li> <li>\u591a\u91cd\u7dda\u5f62\u6027: \u884c\u5217\u306e1\u3064\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306b\u30b9\u30ab\u30e9\u30fc \\(c\\) \u3092\u304b\u3051\u308b\u3068\u3001\u884c\u5217\u5f0f\u306f \\(c\\) \u500d\u306b\u306a\u308b</li> <li>\u4ea4\u4ee3\u6027: 2\u3064\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u3092\u5165\u308c\u66ff\u3048\u308b\u3068\u3001\u884c\u5217\u5f0f\u306e\u7b26\u53f7\u304c\u53cd\u8ee2\u3059\u308b</li> <li>\u52a0\u6cd5\u6027: \u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u304c2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u306e\u548c\u3067\u3042\u308b\u5834\u5408\u3001\u884c\u5217\u5f0f\u306f2\u3064\u306e\u884c\u5217\u5f0f\u306e\u548c\u306b\u5206\u89e3\u3067\u304d\u308b</li> <li>\u884c\u5217\u306e\u7a4d\u306e\u884c\u5217\u5f0f: \\(\\det(AB) = \\det(A) \\cdot \\det(B)\\)</li> <li>\u8ee2\u7f6e\u884c\u5217\u306e\u884c\u5217\u5f0f: \\(\\det(A^T) = \\det(A)\\)</li> <li>\u9006\u884c\u5217\u3068\u884c\u5217\u5f0f: \\(A\\) \u304c\u6b63\u5247\u306a\u3089\u3070 \\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\)</li> </ol>"},{"location":"lectures/LA/25-exercise/#32","title":"3.2 \u884c\u5217\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473","text":"<p>\\(n\\) \u6b21\u5143\u7a7a\u9593\u306b\u304a\u3051\u308b \\(n\\) \u500b\u306e\u30d9\u30af\u30c8\u30eb\u304c\u5f35\u308b\u5e73\u884c\u4f53\u306e\u300c\u7b26\u53f7\u4ed8\u304d\u4f53\u7a4d\u300d\u3092\u8868\u3059\u3002\u7279\u306b\uff1a</p> <ul> <li>2\u6b21\u5143: 2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u304c\u5f35\u308b\u5e73\u884c\u56db\u8fba\u5f62\u306e\u9762\u7a4d</li> <li>3\u6b21\u5143: 3\u3064\u306e\u30d9\u30af\u30c8\u30eb\u304c\u5f35\u308b\u5e73\u884c\u516d\u9762\u4f53\u306e\u4f53\u7a4d</li> </ul> <p>\u3053\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u306f\u3001\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u3084\u5909\u63db\u306e\u7279\u6027\u3092\u7406\u89e3\u3059\u308b\u4e0a\u3067\u91cd\u8981\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/25-exercise/#4","title":"4. \u884c\u5217\u5f0f\u306e\u8a08\u7b97\u65b9\u6cd5","text":""},{"location":"lectures/LA/25-exercise/#41","title":"4.1 \u76f4\u63a5\u8a08\u7b97\u6cd5\uff08\u5c0f\u3055\u306a\u884c\u5217\u306e\u5834\u5408\uff09","text":"<p>2\u00d72\u884c\u5217\u306e\u5834\u5408: \\(\\(\\det\\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix} = ad - bc\\)\\)</p> <p>3\u00d73\u884c\u5217\u306e\u5834\u5408: \\(\\(\\det\\begin{pmatrix}  a_{11} &amp; a_{12} &amp; a_{13} \\\\  a_{21} &amp; a_{22} &amp; a_{23} \\\\  a_{31} &amp; a_{32} &amp; a_{33}  \\end{pmatrix} =  a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33}\\)\\)</p> <p>\u307e\u305f\u306f\u3001\u30b5\u30e9\u30b9\u306e\u516c\u5f0f\u3092\u7528\u3044\u308b\uff1a \\(\\(\\det(A) =  \\begin{vmatrix}  a_{11} &amp; a_{12} &amp; a_{13} \\\\  a_{21} &amp; a_{22} &amp; a_{23} \\\\  a_{31} &amp; a_{32} &amp; a_{33}  \\end{vmatrix} =  \\sum_{j=1}^{3} a_{1j} \\cdot \\text{Cof}(1,j)\\)\\)</p>"},{"location":"lectures/LA/25-exercise/#42","title":"4.2 \u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u305f\u884c\u5217\u5f0f\u306e\u8a08\u7b97","text":"<p>\u57fa\u672c\u5909\u5f62\u3092\u884c\u5217\u5f0f\u306b\u9069\u7528\u3059\u308b\u969b\u306e\u5909\u5316\uff1a</p> <ol> <li>\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u4ea4\u63db: \u884c\u5217\u5f0f\u306e\u7b26\u53f7\u304c\u53cd\u8ee2\u3059\u308b</li> <li>\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306b\u30b9\u30ab\u30e9\u30fc \\(c\\) \u3092\u304b\u3051\u308b: \u884c\u5217\u5f0f\u304c \\(c\\) \u500d\u306b\u306a\u308b</li> <li>\u3042\u308b\u884c\u306b\u5225\u306e\u884c\u306e \\(c\\) \u500d\u3092\u52a0\u3048\u308b: \u884c\u5217\u5f0f\u306e\u5024\u306f\u5909\u308f\u3089\u306a\u3044</li> </ol> <p>\u4f8b: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u57fa\u672c\u5909\u5f62\u3067\u8a08\u7b97 \\(\\(A = \\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  4 &amp; 5 &amp; 6 \\\\  7 &amp; 8 &amp; 9  \\end{pmatrix}\\)\\)</p> <p>\u89e3\u6cd5: \u7b2c3\u884c\u304b\u3089\u7b2c2\u884c\u306e2\u500d\u3092\u5f15\u304f\uff1a \\(\\(\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  4 &amp; 5 &amp; 6 \\\\  -1 &amp; -2 &amp; -3 \\end{pmatrix}\\)\\)</p> <p>\u7b2c3\u884c\u306f\u7b2c1\u884c\u306e\\(-1\\)\u500d\u3068\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001\u884c\u5217\u306e\u30e9\u30f3\u30af\u306f2\u4ee5\u4e0b\u3068\u306a\u308a\u3001\\(\\det(A) = 0\\)</p>"},{"location":"lectures/LA/25-exercise/#43","title":"4.3 \u9084\u5143\u5b9a\u7406\u3092\u7528\u3044\u305f\u884c\u5217\u5f0f\u306e\u8a08\u7b97","text":"<p>\u9084\u5143\u5b9a\u7406: \u884c\u5217\u306e\u4e2d\u306b\u96f6\u884c\u307e\u305f\u306f\u96f6\u5217\u304c\u3042\u308b\u5834\u5408\u3001\u305d\u306e\u884c\u5217\u5f0f\u306f0\u3067\u3042\u308b\u3002\u307e\u305f\u3001\u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u5168\u3066\u306e\u8981\u7d20\u306b\u5171\u901a\u306e\u56e0\u5b50 \\(c\\) \u304c\u3042\u308b\u5834\u5408\u3001\u305d\u306e\u56e0\u5b50\u3092\u884c\u5217\u5f0f\u306e\u5916\u306b\u51fa\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002</p> <p>\u4f8b: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u9084\u5143\u5b9a\u7406\u3067\u8a08\u7b97 \\(\\(B = \\begin{pmatrix}  3 &amp; 6 &amp; 9 \\\\  2 &amp; 5 &amp; 8 \\\\  1 &amp; 4 &amp; 7  \\end{pmatrix}\\)\\)</p> <p>\u89e3\u6cd5: \u7b2c1\u884c\u304b\u3089\u5171\u901a\u56e0\u5b503\u3092\u53d6\u308a\u51fa\u3059\uff1a \\(\\(\\det(B) = 3 \\cdot \\det\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  2 &amp; 5 &amp; 8 \\\\  1 &amp; 4 &amp; 7  \\end{pmatrix}\\)\\)</p> <p>\u7b2c1\u5217\u304b\u3089\u7b2c3\u5217\u306e3\u500d\u3092\u5f15\u304f\uff1a \\(\\(\\det(B) = 3 \\cdot \\det\\begin{pmatrix}  1 &amp; 2 &amp; 0 \\\\  2 &amp; 5 &amp; 2 \\\\  1 &amp; 4 &amp; 4  \\end{pmatrix}\\)\\)</p> <p>\u7b2c3\u5217\u306b\u3064\u3044\u3066\u4f59\u56e0\u5b50\u5c55\u958b\uff1a \\(\\(\\det(B) = 3 \\cdot \\left( 0 \\cdot M_{13} + 2 \\cdot M_{23} + 4 \\cdot M_{33} \\right)\\)\\)</p> <p>\u3053\u3053\u3067 \\(M_{ij}\\) \u306f\u4f59\u56e0\u5b50\u3092\u8868\u3059\u3002</p>"},{"location":"lectures/LA/25-exercise/#44","title":"4.4 \u4f59\u56e0\u5b50\u5c55\u958b\u306b\u3088\u308b\u884c\u5217\u5f0f\u306e\u8a08\u7b97","text":"<p>\u4f59\u56e0\u5b50\u5c55\u958b: \\(n\\) \u6b21\u884c\u5217 \\(A = [a_{ij}]\\) \u306e\u884c\u5217\u5f0f\u306f\u3001\u4efb\u610f\u306e\u884c\u307e\u305f\u306f\u5217\u306b\u95a2\u3057\u3066\u3001\u5404\u8981\u7d20\u3068\u305d\u306e\u4f59\u56e0\u5b50\u306e\u7a4d\u306e\u548c\u3068\u3057\u3066\u8a08\u7b97\u3067\u304d\u308b\uff1a</p> \\[\\det(A) = \\sum_{j=1}^{n} a_{ij} \\cdot \\text{Cof}(i,j) = \\sum_{i=1}^{n} a_{ij} \\cdot \\text{Cof}(i,j)\\] <p>\u3053\u3053\u3067 \\(\\text{Cof}(i,j) = (-1)^{i+j} \\cdot M_{ij}\\) \u3067\u3042\u308a\u3001\\(M_{ij}\\) \u306f \\((i,j)\\) \u8981\u7d20\u3092\u9664\u3044\u305f\u5c0f\u884c\u5217\u306e\u884c\u5217\u5f0f\u3067\u3042\u308b\u3002</p> <p>\u4f8b: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u4f59\u56e0\u5b50\u5c55\u958b\u3067\u8a08\u7b97 \\(\\(C = \\begin{pmatrix}  2 &amp; 0 &amp; 1 \\\\  3 &amp; 1 &amp; 2 \\\\  1 &amp; 0 &amp; 3  \\end{pmatrix}\\)\\)</p> <p>\u89e3\u6cd5: \u7b2c2\u5217\uff080\u304c\u591a\u3044\u5217\uff09\u306b\u95a2\u3057\u3066\u4f59\u56e0\u5b50\u5c55\u958b\uff1a \\(\\(\\det(C) = 0 \\cdot \\text{Cof}(1,2) + 1 \\cdot \\text{Cof}(2,2) + 0 \\cdot \\text{Cof}(3,2)\\)\\)</p> \\[\\det(C) = 1 \\cdot (-1)^{2+2} \\cdot \\det\\begin{pmatrix}  2 &amp; 1 \\\\  1 &amp; 3  \\end{pmatrix} = 1 \\cdot (2 \\cdot 3 - 1 \\cdot 1) = 1 \\cdot 5 = 5\\]"},{"location":"lectures/LA/25-exercise/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/25-exercise/#51-numpy","title":"5.1 NumPy\u3092\u7528\u3044\u305f\u884c\u5217\u5f0f\u306e\u8a08\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[1, 2, 3], \n              [4, 5, 6], \n              [7, 8, 9]])\n\nB = np.array([[3, 6, 9], \n              [2, 5, 8], \n              [1, 4, 7]])\n\nC = np.array([[2, 0, 1], \n              [3, 1, 2], \n              [1, 0, 3]])\n\n# \u884c\u5217\u5f0f\u306e\u8a08\u7b97\ndet_A = np.linalg.det(A)\ndet_B = np.linalg.det(B)\ndet_C = np.linalg.det(C)\n\nprint(f\"det(A) = {det_A}\")\nprint(f\"det(B) = {det_B}\")\nprint(f\"det(C) = {det_C}\")\n\n# \u6570\u5024\u8aa4\u5dee\u3092\u8003\u616e\u3059\u308b\u30680\u306b\u8fd1\u3044\u5024\u306f0\u3068\u3057\u3066\u6271\u3046\nprint(f\"det(A) \u2248 {0 if abs(det_A) &lt; 1e-10 else det_A}\")\nprint(f\"det(B) \u2248 {0 if abs(det_B) &lt; 1e-10 else det_B}\")\nprint(f\"det(C) \u2248 {0 if abs(det_C) &lt; 1e-10 else det_C}\")\n</code></pre>"},{"location":"lectures/LA/25-exercise/#52-2","title":"5.2 \u884c\u5217\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u306e\u53ef\u8996\u5316\uff082\u6b21\u5143\uff09","text":"<pre><code># 2\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e\u30da\u30a2\u3092\u5b9a\u7fa9\nv1 = np.array([3, 1])\nv2 = np.array([1, 2])\n\n# \u884c\u5217\u5f0f\u306e\u8a08\u7b97\ndet_2d = np.linalg.det(np.column_stack([v1, v2]))\n\n# \u5e73\u884c\u56db\u8fba\u5f62\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(8, 6))\nplt.grid(True)\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n# \u539f\u70b9\u304b\u3089\u59cb\u307e\u308b\u30d9\u30af\u30c8\u30eb\nplt.arrow(0, 0, v1[0], v1[1], head_width=0.2, head_length=0.3, fc='blue', ec='blue', label='v1')\nplt.arrow(0, 0, v2[0], v2[1], head_width=0.2, head_length=0.3, fc='red', ec='red', label='v2')\n\n# \u5e73\u884c\u56db\u8fba\u5f62\u3092\u5b8c\u6210\u3055\u305b\u308b\nplt.arrow(v2[0], v2[1], v1[0], v1[1], head_width=0.2, head_length=0.3, fc='blue', ec='blue', alpha=0.5)\nplt.arrow(v1[0], v1[1], v2[0], v2[1], head_width=0.2, head_length=0.3, fc='red', ec='red', alpha=0.5)\n\n# \u5e73\u884c\u56db\u8fba\u5f62\u3092\u5857\u308a\u3064\u3076\u3059\nplt.fill([0, v1[0], v1[0]+v2[0], v2[0]], [0, v1[1], v1[1]+v2[1], v2[1]], 'gray', alpha=0.2)\n\nplt.title(f'2\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e\u884c\u5217\u5f0f = {det_2d:.2f}\\n(\u5e73\u884c\u56db\u8fba\u5f62\u306e\u9762\u7a4d)', fontsize=12)\nplt.xlabel('x', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.legend()\nplt.axis('equal')\nplt.xlim(-1, 5)\nplt.ylim(-1, 4)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/25-exercise/#53-3","title":"5.3 \u884c\u5217\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u306e\u53ef\u8996\u5316\uff083\u6b21\u5143\uff09","text":"<pre><code># 3\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e\u30c8\u30ea\u30d7\u30eb\u3092\u5b9a\u7fa9\nv1 = np.array([1, 0, 0])\nv2 = np.array([0, 2, 0])\nv3 = np.array([0, 0, 3])\n\n# \u884c\u5217\u5f0f\u306e\u8a08\u7b97\ndet_3d = np.linalg.det(np.column_stack([v1, v2, v3]))\n\n# 3\u6b21\u5143\u5e73\u884c\u516d\u9762\u4f53\u306e\u53ef\u8996\u5316\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# \u539f\u70b9\norigin = np.array([0, 0, 0])\n\n# \u5e73\u884c\u516d\u9762\u4f53\u306e8\u3064\u306e\u9802\u70b9\nvertices = [\n    origin,\n    origin + v1,\n    origin + v2,\n    origin + v3,\n    origin + v1 + v2,\n    origin + v1 + v3,\n    origin + v2 + v3,\n    origin + v1 + v2 + v3\n]\n\n# \u5404\u9802\u70b9\u306e\u5ea7\u6a19\u3092\u62bd\u51fa\nx = [v[0] for v in vertices]\ny = [v[1] for v in vertices]\nz = [v[2] for v in vertices]\n\n# \u9802\u70b9\u3092\u30d7\u30ed\u30c3\u30c8\nax.scatter(x, y, z, c='r', s=50)\n\n# \u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\nax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='b', arrow_length_ratio=0.1, label='v1')\nax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='g', arrow_length_ratio=0.1, label='v2')\nax.quiver(0, 0, 0, v3[0], v3[1], v3[2], color='r', arrow_length_ratio=0.1, label='v3')\n\n# \u5e73\u884c\u516d\u9762\u4f53\u306e\u8fba\u3092\u63cf\u753b\nfor i, j in [(0, 1), (0, 2), (0, 3), (1, 4), (1, 5), (2, 4), (2, 6), (3, 5), (3, 6), (4, 7), (5, 7), (6, 7)]:\n    ax.plot([vertices[i][0], vertices[j][0]], \n            [vertices[i][1], vertices[j][1]], \n            [vertices[i][2], vertices[j][2]], 'k-', alpha=0.6)\n\nax.set_title(f'3\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e\u884c\u5217\u5f0f = {det_3d:.2f}\\n(\u5e73\u884c\u516d\u9762\u4f53\u306e\u4f53\u7a4d)', fontsize=12)\nax.set_xlabel('x', fontsize=12)\nax.set_ylabel('y', fontsize=12)\nax.set_zlabel('z', fontsize=12)\nax.legend()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/25-exercise/#54","title":"5.4 \u57fa\u672c\u5909\u5f62\u306b\u3088\u308b\u884c\u5217\u5f0f\u306e\u5909\u5316\u306e\u30c7\u30e2\u30f3\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3","text":"<pre><code># \u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u884c\u5217\nD = np.array([[2, 1, 3], \n              [1, 2, 1], \n              [3, 2, 1]])\n\ndet_D = np.linalg.det(D)\nprint(f\"Original det(D) = {det_D}\")\n\n# \u57fa\u672c\u5909\u5f621: \u884c\u306e\u4ea4\u63db\nD1 = D.copy()\nD1[[0, 1]] = D1[[1, 0]]  # \u7b2c1\u884c\u3068\u7b2c2\u884c\u3092\u4ea4\u63db\ndet_D1 = np.linalg.det(D1)\nprint(f\"Row swap det(D1) = {det_D1}\")\nprint(f\"Relation: det(D1) \u2248 -det(D) ? {np.isclose(det_D1, -det_D)}\")\n\n# \u57fa\u672c\u5909\u5f622: \u884c\u306e\u30b9\u30ab\u30e9\u30fc\u500d\nD2 = D.copy()\nD2[0] = D2[0] * 2  # \u7b2c1\u884c\u30922\u500d\ndet_D2 = np.linalg.det(D2)\nprint(f\"Row scaling det(D2) = {det_D2}\")\nprint(f\"Relation: det(D2) \u2248 2*det(D) ? {np.isclose(det_D2, 2*det_D)}\")\n\n# \u57fa\u672c\u5909\u5f623: \u884c\u306e\u52a0\u7b97\nD3 = D.copy()\nD3[2] = D3[2] + 3 * D3[0]  # \u7b2c3\u884c\u306b\u7b2c1\u884c\u306e3\u500d\u3092\u52a0\u3048\u308b\ndet_D3 = np.linalg.det(D3)\nprint(f\"Row addition det(D3) = {det_D3}\")\nprint(f\"Relation: det(D3) \u2248 det(D) ? {np.isclose(det_D3, det_D)}\")\n</code></pre>"},{"location":"lectures/LA/25-exercise/#55","title":"5.5 \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u884c\u5217\u5f0f\u306e\u5fdc\u7528\u4f8b","text":"<pre><code># \u30d8\u30eb\u30b9\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\uff08\u4f8b: \u5fc3\u62cd\u6570\u3001\u8840\u5727\u3001\u8840\u7cd6\u5024\u306e\u6e2c\u5b9a\u5024\uff09\nhealth_data = np.array([\n    [72, 120, 95],  # \u5fc3\u62cd\u6570, \u53ce\u7e2e\u671f\u8840\u5727, \u8840\u7cd6\u5024\n    [68, 118, 92],\n    [70, 125, 98],\n    [75, 130, 105],\n    [65, 115, 90]\n])\n\n# \u5404\u5909\u6570\u306e\u5e73\u5747\nmeans = np.mean(health_data, axis=0)\n\n# \u4e2d\u5fc3\u5316\u3057\u305f\u30c7\u30fc\u30bf\ncentered_data = health_data - means\n\n# \u5171\u5206\u6563\u884c\u5217\u306e\u8a08\u7b97\ncov_matrix = np.cov(centered_data, rowvar=False)\nprint(\"\u5171\u5206\u6563\u884c\u5217:\")\nprint(cov_matrix)\n\n# \u5171\u5206\u6563\u884c\u5217\u306e\u884c\u5217\u5f0f\ncov_det = np.linalg.det(cov_matrix)\nprint(f\"\\n\u5171\u5206\u6563\u884c\u5217\u306e\u884c\u5217\u5f0f: {cov_det:.2f}\")\n\n# \u884c\u5217\u5f0f\u306e\u89e3\u91c8\nprint(\"\\n\u89e3\u91c8:\")\nif cov_det &gt; 0:\n    print(\"- \u5171\u5206\u6563\u884c\u5217\u306f\u6b63\u5b9a\u5024\u3067\u3059\uff08\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u304c\u6b63\uff09\")\n    print(\"- \u30c7\u30fc\u30bf\u5206\u5e03\u306f\u6b63\u5247\u3067\u30013\u6b21\u5143\u7a7a\u9593\u5185\u3067\u9069\u5207\u306b\u5e83\u304c\u3063\u3066\u3044\u307e\u3059\")\n    print(\"- \u5909\u6570\u9593\u306b\u7dda\u5f62\u4f9d\u5b58\u95a2\u4fc2\u306f\u3042\u308a\u307e\u305b\u3093\")\nelif np.isclose(cov_det, 0):\n    print(\"- \u5171\u5206\u6563\u884c\u5217\u306f\u30e9\u30f3\u30af\u843d\u3061\u3057\u3066\u3044\u307e\u3059\uff08\u5c11\u306a\u304f\u3068\u30821\u3064\u306e\u56fa\u6709\u5024\u304c0\uff09\")\n    print(\"- \u5909\u6570\u9593\u306b\u7dda\u5f62\u4f9d\u5b58\u95a2\u4fc2\u304c\u5b58\u5728\u3057\u307e\u3059\")\n    print(\"- \u6b21\u5143\u524a\u6e1b\uff08\u4f8b\uff1a\u4e3b\u6210\u5206\u5206\u6790\uff09\u304c\u9069\u5207\u304b\u3082\u3057\u308c\u307e\u305b\u3093\")\nelse:\n    print(\"- \u5171\u5206\u6563\u884c\u5217\u306f\u4e0d\u6b63\u307e\u305f\u306f\u6570\u5024\u8a08\u7b97\u4e0a\u306e\u554f\u984c\u304c\u3042\u308a\u307e\u3059\")\n\n# \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306e\u4fc2\u6570\npdf_coef = 1.0 / (np.sqrt((2 * np.pi) ** 3 * cov_det))\nprint(f\"\\n\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306ePDF\u4fc2\u6570: {pdf_coef:.6f}\")\n</code></pre>"},{"location":"lectures/LA/25-exercise/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/25-exercise/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002    \\(\\(A = \\begin{pmatrix}     4 &amp; 3 \\\\     2 &amp; 5     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002    \\(\\(B = \\begin{pmatrix}     1 &amp; 2 &amp; 3 \\\\     0 &amp; 4 &amp; 5 \\\\     0 &amp; 0 &amp; 6     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u8a08\u7b97\u305b\u3088\u3002    \\(\\(C = \\begin{pmatrix}     2 &amp; 4 &amp; 6 \\\\     1 &amp; 3 &amp; 5 \\\\     7 &amp; 8 &amp; 9     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u4f59\u56e0\u5b50\u5c55\u958b\u3092\u7528\u3044\u3066\u8a08\u7b97\u305b\u3088\u3002    \\(\\(D = \\begin{pmatrix}     3 &amp; 1 &amp; 0 \\\\     2 &amp; 4 &amp; 1 \\\\     1 &amp; 2 &amp; 5     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u3057\u3001\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002    \\(\\(E = \\begin{pmatrix}     1 &amp; 2 &amp; 3 \\\\     4 &amp; 5 &amp; 6 \\\\     7 &amp; 8 &amp; 10     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\det(A^2) = (\\det(A))^2\\) \u3067\u3042\u308b\u3053\u3068\u3092\u8a3c\u660e\u305b\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/25-exercise/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002    \\(\\(F = \\begin{pmatrix}     2 &amp; 3 &amp; 1 &amp; 4 \\\\     0 &amp; 1 &amp; -1 &amp; 2 \\\\     0 &amp; 0 &amp; 3 &amp; 5 \\\\     0 &amp; 0 &amp; 0 &amp; 2     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002    \\(\\(G = \\begin{pmatrix}     1 &amp; 1 &amp; 1 &amp; 1 \\\\     1 &amp; 2 &amp; 3 &amp; 4 \\\\     1 &amp; 3 &amp; 6 &amp; 10 \\\\     1 &amp; 4 &amp; 10 &amp; 20     \\end{pmatrix}\\)\\)</p> </li> <li> <p>\\(n\\) \u6b21\u306e\u6b63\u65b9\u884c\u5217 \\(A\\) \u3068 \\(B\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\det(AB) = \\det(A) \\cdot \\det(B)\\) \u3067\u3042\u308b\u3053\u3068\u3092\u8a3c\u660e\u305b\u3088\u3002</p> </li> <li> <p>\u30d8\u30eb\u30b9\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5fdc\u7528\u554f\u984c:    \u3042\u308b\u75be\u60a3\u306e\u30ea\u30b9\u30af\u4e88\u6e2c\u30e2\u30c7\u30eb\u3067\u30013\u3064\u306e\u751f\u4f53\u6307\u6a19\uff08\u8840\u5727\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u3001\u8840\u7cd6\u5024\uff09\u3092\u7528\u3044\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u6e2c\u5b9a\u5024\u306e\u5171\u5206\u6563\u884c\u5217 \\(\\Sigma\\) \u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff1a</p> </li> </ol> <p>\\(\\(\\Sigma = \\begin{pmatrix}     100 &amp; 40 &amp; 30 \\\\     40 &amp; 64 &amp; 24 \\\\     30 &amp; 24 &amp; 49     \\end{pmatrix}\\)\\)</p> <p>(a) \u3053\u306e\u5171\u5206\u6563\u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u8a08\u7b97\u305b\u3088\u3002    (b) \u884c\u5217\u5f0f\u306e\u5024\u304b\u3089\u30013\u3064\u306e\u751f\u4f53\u6307\u6a19\u9593\u306e\u95a2\u4fc2\u6027\u306b\u3064\u3044\u3066\u4f55\u304c\u8a00\u3048\u308b\u304b\u8aac\u660e\u305b\u3088\u3002    (c) \u3082\u3057\u884c\u5217\u5f0f\u304c0\u306b\u8fd1\u3044\u5024\u3060\u3063\u305f\u5834\u5408\u3001\u591a\u5909\u91cf\u89e3\u6790\u306b\u304a\u3044\u3066\u3069\u306e\u3088\u3046\u306a\u554f\u984c\u304c\u751f\u3058\u308b\u304b\u8aac\u660e\u305b\u3088\u3002    (d) \u4e3b\u6210\u5206\u5206\u6790\u3092\u9069\u7528\u3059\u308b\u5834\u5408\u3001\u56fa\u6709\u5024\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2\u3092\u8aac\u660e\u305b\u3088\u3002</p>"},{"location":"lectures/LA/25-exercise/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/25-exercise/#q1-0","title":"Q1: \u884c\u5217\u5f0f\u304c0\u306b\u306a\u308b\u306e\u306f\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u3067\u3059\u304b\uff1f","text":"<p>A: \u884c\u5217\u5f0f\u304c0\u306b\u306a\u308b\u306e\u306f\u3001\u884c\u5217\u304c\u30e9\u30f3\u30af\u843d\u3061\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u3064\u307e\u308a\u884c\u307e\u305f\u306f\u5217\u30d9\u30af\u30c8\u30eb\u304c\u7dda\u5f62\u5f93\u5c5e\u3067\u3042\u308b\u5834\u5408\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\uff1a - \u884c\u5217\u306e\u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u304c\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b - 2\u3064\u4ee5\u4e0a\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u304c\u5e73\u884c\uff08\u4e00\u65b9\u304c\u4ed6\u65b9\u306e\u5b9a\u6570\u500d\uff09\u3067\u3042\u308b - \u3042\u308b\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u304c\u4ed6\u306e\u884c\uff08\u307e\u305f\u306f\u5217\uff09\u306e\u7dda\u5f62\u7d50\u5408\u3067\u8868\u305b\u308b</p> <p>\u884c\u5217\u5f0f\u304c0\u306e\u884c\u5217\u306f\u3001\u53ef\u9006\u3067\u306f\u306a\u3044\u305f\u3081\u9006\u884c\u5217\u3092\u6301\u3061\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/25-exercise/#q2","title":"Q2: \u884c\u5217\u5f0f\u306e\u8a08\u7b97\u3067\u6700\u3082\u52b9\u7387\u7684\u306a\u65b9\u6cd5\u306f\u3069\u308c\u3067\u3059\u304b\uff1f","text":"<p>A: \u884c\u5217\u306e\u30b5\u30a4\u30ba\u3068\u69cb\u9020\u306b\u3088\u3063\u3066\u6700\u9069\u306a\u65b9\u6cd5\u306f\u7570\u306a\u308a\u307e\u3059\uff1a - \u5c0f\u3055\u306a\u884c\u5217\uff082\u00d72, 3\u00d73\uff09: \u76f4\u63a5\u8a08\u7b97\u516c\u5f0f - \u4e09\u89d2\u884c\u5217\u307e\u305f\u306f\u5bfe\u89d2\u884c\u5217: \u5bfe\u89d2\u6210\u5206\u306e\u7a4d - \u591a\u304f\u306e\u30bc\u30ed\u8981\u7d20\u3092\u6301\u3064\u884c\u5217: \u4f59\u56e0\u5b50\u5c55\u958b\uff08\u30bc\u30ed\u306e\u591a\u3044\u884c\u307e\u305f\u306f\u5217\u3067\u5c55\u958b\uff09 - \u4e00\u822c\u7684\u306a\u5927\u304d\u306a\u884c\u5217: \u57fa\u672c\u5909\u5f62\u3067\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u63db\u3057\u3001\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u3092\u8a08\u7b97</p> <p>\u8a08\u7b97\u52b9\u7387\u4e0a\u306f\u3001\u57fa\u672c\u5909\u5f62\u3092\u7528\u3044\u3066\u30ac\u30a6\u30b9\u6d88\u53bb\u6cd5\u3067\u4e0a\u4e09\u89d2\u884c\u5217\u306b\u5909\u63db\u3059\u308b\u65b9\u6cd5\u304c\u4e00\u822c\u306b\u52b9\u7387\u7684\u3067\u3059\u3002</p>"},{"location":"lectures/LA/25-exercise/#q3","title":"Q3: \u884c\u5217\u5f0f\u304c\u8ca0\u306e\u5024\u306b\u306a\u308b\u3053\u3068\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A: \u884c\u5217\u5f0f\u304c\u8ca0\u306e\u5024\u306b\u306a\u308b\u306e\u306f\u3001\u884c\u5217\u304c\u8868\u3059\u7dda\u5f62\u5909\u63db\u304c\u300c\u5411\u304d\u3092\u53cd\u8ee2\u3055\u305b\u308b\u300d\u5834\u5408\u3067\u3059\u3002\u4f8b\u3048\u3070\uff1a - 2\u6b21\u5143\u3067\u306f\u30012\u3064\u306e\u30d9\u30af\u30c8\u30eb\u306e\u5411\u304d\u304c\u53cd\u6642\u8a08\u56de\u308a\u304b\u3089\u6642\u8a08\u56de\u308a\u306b\u5909\u308f\u308b - 3\u6b21\u5143\u3067\u306f\u30013\u3064\u306e\u30d9\u30af\u30c8\u30eb\u304c\u4f5c\u308b\u5ea7\u6a19\u7cfb\u306e\u5411\u304d\u304c\u5909\u308f\u308b\uff08\u53f3\u624b\u7cfb\u304b\u3089\u5de6\u624b\u7cfb\u3001\u307e\u305f\u306f\u305d\u306e\u9006\uff09</p> <p>\u884c\u5217\u5f0f\u306e\u7d76\u5bfe\u5024\u306f\u5e73\u884c\u4f53\u306e\u4f53\u7a4d\u3092\u8868\u3057\u3001\u7b26\u53f7\u306f\u305d\u306e\u5411\u304d\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/25-exercise/#q4","title":"Q4: \u884c\u5217\u5f0f\u306f\u7d71\u8a08\u5b66\u3084\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u3069\u306e\u3088\u3046\u306b\u4f7f\u308f\u308c\u307e\u3059\u304b\uff1f","text":"<p>A: \u7d71\u8a08\u5b66\u3084\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306e\u884c\u5217\u5f0f\u306e\u4e3b\u306a\u7528\u9014\u306f\uff1a - \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306e\u8a08\u7b97 - \u5171\u5206\u6563\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u5909\u6570\u306e\u300c\u4e00\u822c\u5316\u5206\u6563\u300d\u3092\u8868\u3059 - \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3067\u306e\u8aac\u660e\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u306e\u8a55\u4fa1 - \u591a\u5909\u91cf\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u306e\u591a\u91cd\u5171\u7dda\u6027\u306e\u691c\u51fa - \u30ab\u30eb\u30de\u30f3\u30d5\u30a3\u30eb\u30bf\u306a\u3069\u306e\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u51e6\u7406\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0</p> <p>\u7279\u306b\u3001\u5171\u5206\u6563\u884c\u5217\u306e\u884c\u5217\u5f0f\u304c0\u306b\u8fd1\u3044\u3068\u3001\u30c7\u30fc\u30bf\u306b\u5f37\u3044\u76f8\u95a2\u95a2\u4fc2\u304c\u3042\u308a\u3001\u6b21\u5143\u524a\u6e1b\uff08PCA\u306a\u3069\uff09\u304c\u6709\u52b9\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/25-exercise/#q5","title":"Q5: \u9ad8\u6b21\u5143\u306e\u884c\u5217\u306b\u5bfe\u3059\u308b\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u306b\u304a\u3051\u308b\u6ce8\u610f\u70b9\u306f\uff1f","text":"<p>A: \u9ad8\u6b21\u5143\u884c\u5217\u306e\u884c\u5217\u5f0f\u8a08\u7b97\u306b\u304a\u3051\u308b\u6ce8\u610f\u70b9\uff1a - \u6570\u5024\u8a08\u7b97\u4e0a\u306e\u4e38\u3081\u8aa4\u5dee\u304c\u84c4\u7a4d\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b - \u8a08\u7b97\u91cf\u306f\u4e00\u822c\u7684\u306bO(n\u00b3)\u4ee5\u4e0a\u306b\u306a\u308b\u305f\u3081\u3001\u975e\u5e38\u306b\u5927\u304d\u306a\u884c\u5217\u3067\u306f\u8a08\u7b97\u52b9\u7387\u306b\u6ce8\u610f - LU\u5206\u89e3\u306a\u3069\u306e\u6570\u5024\u7684\u306b\u5b89\u5b9a\u3057\u305f\u65b9\u6cd5\u3092\u4f7f\u3046\u3079\u304d - Python/NumPy\u3067\u306f <code>np.linalg.det()</code> \u304c\u6700\u9069\u5316\u3055\u308c\u3066\u3044\u308b - \u7279\u306b\u5927\u304d\u306a\u884c\u5217\u3067\u306f\u3001\u884c\u5217\u5f0f\u306e\u4ee3\u308f\u308a\u306b\u884c\u5217\u306e\u30e9\u30f3\u30af\u3084\u6761\u4ef6\u6570\u3092\u8003\u616e\u3059\u308b\u65b9\u304c\u6709\u7528\u306a\u5834\u5408\u3082\u591a\u3044</p>"},{"location":"lectures/LA/25-exercise/#8","title":"8. \u89e3\u7b54\u4f8b\uff08\u57fa\u672c\u554f\u984c\uff09","text":""},{"location":"lectures/LA/25-exercise/#1_1","title":"\u554f\u984c1\u306e\u89e3\u7b54","text":"\\[A = \\begin{pmatrix}  4 &amp; 3 \\\\  2 &amp; 5  \\end{pmatrix}\\] \\[\\det(A) = 4 \\times 5 - 3 \\times 2 = 20 - 6 = 14\\]"},{"location":"lectures/LA/25-exercise/#2_1","title":"\u554f\u984c2\u306e\u89e3\u7b54","text":"\\[B = \\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  0 &amp; 4 &amp; 5 \\\\  0 &amp; 0 &amp; 6  \\end{pmatrix}\\] <p>\\(B\\) \u306f\u4e0a\u4e09\u89d2\u884c\u5217\u306a\u306e\u3067\u3001\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u304c\u884c\u5217\u5f0f\u3068\u306a\u308b\uff1a \\(\\(\\det(B) = 1 \\times 4 \\times 6 = 24\\)\\)</p>"},{"location":"lectures/LA/25-exercise/#3_1","title":"\u554f\u984c3\u306e\u89e3\u7b54","text":"\\[C = \\begin{pmatrix}  2 &amp; 4 &amp; 6 \\\\  1 &amp; 3 &amp; 5 \\\\  7 &amp; 8 &amp; 9  \\end{pmatrix}\\] <p>\u57fa\u672c\u5909\u5f62\u3067\u8a08\u7b97\u3059\u308b\uff1a 1. \u7b2c1\u884c\u30922\u3067\u5272\u308b\uff08\u884c\u5217\u5f0f\u306f1/2\u500d\u306b\u306a\u308b\uff09\uff1a \\(\\(\\frac{1}{2} \\cdot \\det\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  1 &amp; 3 &amp; 5 \\\\  7 &amp; 8 &amp; 9  \\end{pmatrix}\\)\\)</p> <ol> <li> <p>\u7b2c2\u884c\u304b\u3089\u7b2c1\u884c\u3092\u5f15\u304f\u3001\u7b2c3\u884c\u304b\u3089\u7b2c1\u884c\u306e7\u500d\u3092\u5f15\u304f\uff1a \\(\\(\\frac{1}{2} \\cdot \\det\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  0 &amp; 1 &amp; 2 \\\\  0 &amp; -6 &amp; -12  \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u7b2c3\u884c\u306b\u7b2c2\u884c\u306e6\u500d\u3092\u8db3\u3059\uff1a \\(\\(\\frac{1}{2} \\cdot \\det\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  0 &amp; 1 &amp; 2 \\\\  0 &amp; 0 &amp; 0  \\end{pmatrix}\\)\\)</p> </li> </ol> <p>\u884c\u5217\u306b\u96f6\u884c\u304c\u542b\u307e\u308c\u308b\u305f\u3081\u3001\\(\\det(C) = 0\\)</p>"},{"location":"lectures/LA/25-exercise/#4_1","title":"\u554f\u984c4\u306e\u89e3\u7b54","text":"\\[D = \\begin{pmatrix}  3 &amp; 1 &amp; 0 \\\\  2 &amp; 4 &amp; 1 \\\\  1 &amp; 2 &amp; 5  \\end{pmatrix}\\] <p>\u7b2c3\u5217\uff080\u3092\u542b\u3080\u5217\uff09\u306b\u95a2\u3057\u3066\u4f59\u56e0\u5b50\u5c55\u958b\uff1a \\(\\(\\det(D) = 0 \\cdot \\text{Cof}(1,3) + 1 \\cdot \\text{Cof}(2,3) + 5 \\cdot \\text{Cof}(3,3)\\)\\)</p> <p>\\(\\text{Cof}(2,3) = (-1)^{2+3} \\cdot \\det\\begin{pmatrix} 3 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix} = -1 \\cdot (3 \\cdot 2 - 1 \\cdot 1) = -1 \\cdot 5 = -5\\)</p> <p>\\(\\text{Cof}(3,3) = (-1)^{3+3} \\cdot \\det\\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 4 \\end{pmatrix} = 1 \\cdot (3 \\cdot 4 - 1 \\cdot 2) = 1 \\cdot 10 = 10\\)</p> \\[\\det(D) = 1 \\cdot (-5) + 5 \\cdot 10 = -5 + 50 = 45\\]"},{"location":"lectures/LA/25-exercise/#5","title":"\u554f\u984c5\u306e\u89e3\u7b54","text":"\\[E = \\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  4 &amp; 5 &amp; 6 \\\\  7 &amp; 8 &amp; 10  \\end{pmatrix}\\] <p>\u57fa\u672c\u5909\u5f62\u3067\u8a08\u7b97\uff1a 1. \u7b2c2\u884c\u304b\u3089\u7b2c1\u884c\u306e4\u500d\u3092\u5f15\u304f\u3001\u7b2c3\u884c\u304b\u3089\u7b2c1\u884c\u306e7\u500d\u3092\u5f15\u304f\uff1a \\(\\(\\det(E) = \\det\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  0 &amp; -3 &amp; -6 \\\\  0 &amp; -6 &amp; -11  \\end{pmatrix}\\)\\)</p> <ol> <li>\u7b2c3\u884c\u304b\u3089\u7b2c2\u884c\u306e2\u500d\u3092\u5f15\u304f\uff1a \\(\\(\\det(E) = \\det\\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\  0 &amp; -3 &amp; -6 \\\\  0 &amp; 0 &amp; 1  \\end{pmatrix}\\)\\)</li> </ol> <p>\u4e0a\u4e09\u89d2\u884c\u5217\u306a\u306e\u3067\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\uff1a \\(\\(\\det(E) = 1 \\cdot (-3) \\cdot 1 = -3\\)\\)</p> <p>\u884c\u5217\u5f0f\u304c0\u3067\u306a\u3044\u305f\u3081\u3001\u884c\u5217\\(E\\)\u306f\u9006\u884c\u5217\u3092\u6301\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/LA/25-exercise/#6_1","title":"\u554f\u984c6\u306e\u89e3\u7b54","text":"<p>\\(A^2 = A \\cdot A\\) \u306a\u306e\u3067\u3001\u884c\u5217\u306e\u7a4d\u306e\u884c\u5217\u5f0f\u306e\u6027\u8cea\u304b\u3089\uff1a \\(\\(\\det(A^2) = \\det(A \\cdot A) = \\det(A) \\cdot \\det(A) = (\\det(A))^2\\)\\)</p> <p>\u8a3c\u660e\u7d42\u4e86\u3002</p>"},{"location":"lectures/LA/25-exercise/#9","title":"9. \u307e\u3068\u3081","text":"<p>\u3053\u306e\u7dcf\u5408\u6f14\u7fd2\u3067\u306f\u3001\u884c\u5217\u5f0f\u306e\u57fa\u672c\u7684\u306a\u6027\u8cea\u304b\u3089\u8a08\u7b97\u624b\u6cd5\u3001\u5fdc\u7528\u4f8b\u307e\u3067\u3092\u5305\u62ec\u7684\u306b\u5fa9\u7fd2\u3057\u307e\u3057\u305f\u3002\u4e3b\u8981\u306a\u30dd\u30a4\u30f3\u30c8\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li> <p>\u884c\u5217\u5f0f\u306e\u57fa\u672c\u6027\u8cea\uff1a\u591a\u91cd\u7dda\u5f62\u6027\u3001\u4ea4\u4ee3\u6027\u3001\u5358\u4f4d\u884c\u5217\u306e\u884c\u5217\u5f0f\u304c1\u306b\u306a\u308b\u306a\u3069\u306e\u6027\u8cea\u3092\u7406\u89e3\u3057\u6d3b\u7528\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p> </li> <li> <p>\u52b9\u7387\u7684\u306a\u8a08\u7b97\u65b9\u6cd5\uff1a\u884c\u5217\u306e\u30b5\u30a4\u30ba\u3084\u5f62\u72b6\u306b\u5fdc\u3058\u3066\u3001\u76f4\u63a5\u8a08\u7b97\u3001\u57fa\u672c\u5909\u5f62\u3001\u9084\u5143\u5b9a\u7406\u3001\u4f59\u56e0\u5b50\u5c55\u958b\u306a\u3069\u9069\u5207\u306a\u65b9\u6cd5\u3092\u9078\u629e\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p> </li> <li> <p>\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\uff1a\u884c\u5217\u5f0f\u306f\u7dda\u5f62\u5909\u63db\u306b\u3088\u308b\u5e73\u884c\u4f53\u306e\u4f53\u7a4d\u5909\u5316\u7387\u3092\u8868\u3057\u3001\u7b26\u53f7\u306f\u5411\u304d\u306e\u5909\u5316\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u89e3\u91c8\u306f\u76f4\u611f\u7684\u306a\u7406\u89e3\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306e\u5fdc\u7528\uff1a\u884c\u5217\u5f0f\u306f\u591a\u5909\u91cf\u7d71\u8a08\u3084\u6a5f\u68b0\u5b66\u7fd2\u306b\u304a\u3044\u3066\u3001\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u306e\u8a55\u4fa1\u3084\u5909\u63db\u306e\u7279\u6027\u3092\u89e3\u6790\u3059\u308b\u305f\u3081\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002</p> </li> </ol> <p>\u4eca\u56de\u306e\u6f14\u7fd2\u3067\u30de\u30b9\u30bf\u30fc\u3057\u305f\u884c\u5217\u5f0f\u306e\u77e5\u8b58\u306f\u3001\u4eca\u5f8c\u5b66\u3076\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3001\u5bfe\u89d2\u5316\u3001\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u306e\u7406\u89e3\u306b\u3082\u76f4\u63a5\u3064\u306a\u304c\u308a\u307e\u3059\u3002\u7279\u306b\u7dda\u5f62\u56de\u5e30\u3084\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u624b\u6cd5\u306e\u7406\u8ad6\u7684\u7406\u89e3\u306b\u5fc5\u9808\u306e\u6982\u5ff5\u3067\u3059\u3002</p> <p>\u6700\u5f8c\u306b\u3001\u8ab2\u984c\u3067\u51fa\u305f\u8a08\u7b97\u3092\u5b9f\u969b\u306bPython\u3067\u5b9f\u884c\u3057\u3001\u7d50\u679c\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3067\u7406\u89e3\u3092\u6df1\u3081\u307e\u3057\u3087\u3046\u3002\u8a08\u7b97\u30d7\u30ed\u30bb\u30b9\u305d\u306e\u3082\u306e\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u306f\u91cd\u8981\u3067\u3059\u304c\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3092\u52b9\u679c\u7684\u306b\u6d3b\u7528\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II - \u7b2c26\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/26-vector-space-and-inner-product/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c26\u56de \u30c6\u30fc\u30de: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u790e \u95a2\u9023\u9805\u76ee: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3001\u90e8\u5206\u7a7a\u9593\u30011\u6b21\u72ec\u7acb\u6027\u3001\u57fa\u5e95\u3001\u6b21\u5143 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u30d9\u30af\u30c8\u30eb\u306e1\u6b21\u7d50\u5408\u30011\u6b21\u72ec\u7acb\u30fb1\u6b21\u5f93\u5c5e\uff08\u7b2c17\u56de\u8b1b\u7fa9\u5185\u5bb9\uff09</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> <ol> <li>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u5b9a\u7fa9\u3092\u7406\u89e3\u3057\u3001\u5177\u4f53\u4f8b\u3092\u6319\u3052\u308b\u3053\u3068\u304c\u3067\u304d\u308b</li> <li>\u90e8\u5206\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u6761\u4ef6\u3092\u78ba\u8a8d\u3057\u3001\u4e0e\u3048\u3089\u308c\u305f\u96c6\u5408\u304c\u90e8\u5206\u7a7a\u9593\u3067\u3042\u308b\u304b\u3092\u5224\u5b9a\u3067\u304d\u308b</li> <li>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u5e95\u3068\u6b21\u5143\u3092\u7406\u89e3\u3057\u3001\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b</li> <li>\u57fa\u5e95\u306e\u5909\u63db\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b</li> <li>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u6982\u5ff5\u3092\u30c7\u30fc\u30bf\u89e3\u6790\u306e\u6587\u8108\u3067\u7406\u89e3\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/26-vector-space-and-inner-product/#31","title":"3.1 \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u306f","text":"<p>\u3053\u308c\u307e\u3067\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u4e3b\u306b\\(\\mathbb{R}^n\\)\uff08n\u6b21\u5143\u5b9f\u6570\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\uff09\u3092\u6271\u3063\u3066\u304d\u307e\u3057\u305f\u3002\u4eca\u56de\u306f\u3001\u3088\u308a\u4e00\u822c\u7684\u306a\u300c\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u300d\u306e\u6982\u5ff5\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593</p> <p>\u4f53\\(F\\)\uff08\u4f8b\u3048\u3070\u5b9f\u6570\u4f53\\(\\mathbb{R}\\)\uff09\u4e0a\u306e\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\\(V\\)\u3068\u306f\u3001\u52a0\u6cd5\u6f14\u7b97\\(+: V \\times V \\rightarrow V\\)\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u6f14\u7b97\\(\\cdot: F \\times V \\rightarrow V\\)\u304c\u5b9a\u7fa9\u3055\u308c\u305f\u96c6\u5408\u3067\u3042\u308a\u3001\u4efb\u610f\u306e\\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V\\)\u3068\u4efb\u610f\u306e\u30b9\u30ab\u30e9\u30fc\\(a, b \\in F\\)\u306b\u5bfe\u3057\u3066\u4ee5\u4e0b\u306e\u6027\u8cea\u3092\u6e80\u305f\u3059\u3082\u306e\u3067\u3059\uff1a</p> <ol> <li>\u52a0\u6cd5\u306e\u7d50\u5408\u6cd5\u5247: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>\u52a0\u6cd5\u306e\u4ea4\u63db\u6cd5\u5247: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</li> <li>\u52a0\u6cd5\u306e\u5358\u4f4d\u5143: \u30bc\u30ed\u30d9\u30af\u30c8\u30eb\\(\\mathbf{0} \\in V\\)\u304c\u5b58\u5728\u3057\u3001\\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)</li> <li>\u52a0\u6cd5\u306e\u9006\u5143: \u5404\\(\\mathbf{v} \\in V\\)\u306b\u5bfe\u3057\u3066\u3001\\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)\u3068\u306a\u308b\\(-\\mathbf{v} \\in V\\)\u304c\u5b58\u5728\u3059\u308b</li> <li>\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u7d50\u5408\u6cd5\u5247: \\(a(b\\mathbf{v}) = (ab)\\mathbf{v}\\)</li> <li>\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u5206\u914d\u6cd5\u5247(1): \\(a(\\mathbf{u} + \\mathbf{v}) = a\\mathbf{u} + a\\mathbf{v}\\)</li> <li>\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u5206\u914d\u6cd5\u5247(2): \\((a + b)\\mathbf{v} = a\\mathbf{v} + b\\mathbf{v}\\)</li> <li>\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u5358\u4f4d\u5143: \\(1\\mathbf{v} = \\mathbf{v}\\)</li> </ol> <p>\u7c21\u5358\u306b\u8a00\u3048\u3070\u3001\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306f\u300c\u30d9\u30af\u30c8\u30eb\u306e\u8db3\u3057\u7b97\u300d\u3068\u300c\u30b9\u30ab\u30e9\u30fc\u500d\u300d\u304c\u81ea\u7136\u306a\u5f62\u3067\u5b9a\u7fa9\u3067\u304d\u308b\u7a7a\u9593\u3067\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#32","title":"3.2 \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u4f8b","text":"<p>\u4ee5\u4e0b\u306b\u4ee3\u8868\u7684\u306a\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u4f8b\u3092\u793a\u3057\u307e\u3059\uff1a</p> <ol> <li> <p>\\(\\mathbb{R}^n\\): \\(n\\)\u6b21\u5143\u5b9f\u6570\u30d9\u30af\u30c8\u30eb\u7a7a\u9593    \u4f8b: \\(\\mathbb{R}^2 = \\{(x,y) \\mid x,y \\in \\mathbb{R}\\}\\)</p> </li> <li> <p>\\(M_{m,n}(\\mathbb{R})\\): \\(m \\times n\\)\u5b9f\u6570\u884c\u5217\u306e\u96c6\u5408    \u4f8b: \\(2 \\times 2\\)\u884c\u5217\u306e\u96c6\u5408 \\(M_{2,2}(\\mathbb{R})\\)</p> </li> <li> <p>\\(P_n(\\mathbb{R})\\): \u6b21\u6570\u304c\\(n\\)\u4ee5\u4e0b\u306e\u591a\u9805\u5f0f\u306e\u96c6\u5408    \u4f8b: \\(P_2(\\mathbb{R}) = \\{a_0 + a_1x + a_2x^2 \\mid a_0, a_1, a_2 \\in \\mathbb{R}\\}\\)</p> </li> <li> <p>\\(C[a,b]\\): \u533a\u9593\\([a,b]\\)\u4e0a\u306e\u9023\u7d9a\u95a2\u6570\u306e\u96c6\u5408    \u4f8b: \\(C[0,1] = \\{f:[0,1] \\rightarrow \\mathbb{R} \\mid f \\text{\u306f\u9023\u7d9a}\\}\\)</p> </li> <li> <p>\\(\\mathbb{R}^\\infty\\): \u7121\u9650\u6b21\u5143\u5b9f\u6570\u7a7a\u9593\uff08\u5404\u6210\u5206\u304c\u5b9f\u6570\u3067\u3042\u308b\u7121\u9650\u5217\u306e\u96c6\u5408\uff09</p> </li> </ol>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#33","title":"3.3 \u90e8\u5206\u30d9\u30af\u30c8\u30eb\u7a7a\u9593","text":"<p>\u5b9a\u7fa9: \u90e8\u5206\u30d9\u30af\u30c8\u30eb\u7a7a\u9593</p> <p>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\\(V\\)\u306e\u90e8\u5206\u96c6\u5408\\(W\\)\u304c\\(V\\)\u306e\u90e8\u5206\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u3042\u308b\u3068\u306f\u3001\\(W\\)\u81ea\u4f53\u304c\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u306a\u308b\u3053\u3068\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u4ee5\u4e0b\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\\(W\\)\u306f\u7a7a\u96c6\u5408\u3067\u306f\u306a\u3044\uff08\u5c11\u306a\u304f\u3068\u3082\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\u3092\u542b\u3080\uff09</li> <li>\\(\\mathbf{u}, \\mathbf{v} \\in W\\)\u306a\u3089\u3070\\(\\mathbf{u} + \\mathbf{v} \\in W\\)\uff08\u52a0\u6cd5\u306b\u95a2\u3057\u3066\u9589\u3058\u3066\u3044\u308b\uff09</li> <li>\\(a \\in F, \\mathbf{v} \\in W\\)\u306a\u3089\u3070\\(a\\mathbf{v} \\in W\\)\uff08\u30b9\u30ab\u30e9\u30fc\u500d\u306b\u95a2\u3057\u3066\u9589\u3058\u3066\u3044\u308b\uff09</li> </ol> <p>\u5b9f\u306f\u3053\u308c\u3089\u306f\u4ee5\u4e0b\u306e\uff11\u6761\u4ef6\u306b\u307e\u3068\u3081\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff1a</p> <p>\u90e8\u5206\u7a7a\u9593\u306e\u5224\u5b9a\u6761\u4ef6</p> <p>\u7a7a\u3067\u306a\u3044\u90e8\u5206\u96c6\u5408\\(W\\)\u304c\u90e8\u5206\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u3042\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\uff1a</p> <p>\u4efb\u610f\u306e\\(\\mathbf{u}, \\mathbf{v} \\in W\\)\u3068\u4efb\u610f\u306e\u30b9\u30ab\u30e9\u30fc\\(a, b \\in F\\)\u306b\u5bfe\u3057\u3066\u3001 \\(a\\mathbf{u} + b\\mathbf{v} \\in W\\)\u304c\u6210\u308a\u7acb\u3064\u3053\u3068\uff08\u7dda\u5f62\u7d50\u5408\u306b\u95a2\u3057\u3066\u9589\u3058\u3066\u3044\u308b\uff09</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#_1","title":"\u90e8\u5206\u7a7a\u9593\u306e\u4f8b","text":"<ol> <li>\\(\\mathbb{R}^3\\)\u306e\u90e8\u5206\u7a7a\u9593\u306e\u4f8b\uff1a</li> <li>\u539f\u70b9\u3092\u901a\u308b\u5e73\u9762: \\(\\{(x,y,z) \\in \\mathbb{R}^3 \\mid ax + by + cz = 0\\}\\)\uff08\\(a,b,c\\)\u306f\u5b9a\u6570\uff09</li> <li>\u539f\u70b9\u3092\u901a\u308b\u76f4\u7dda: \\(\\{t(a,b,c) \\mid t \\in \\mathbb{R}\\}\\)\uff08\\(a,b,c\\)\u306f\u5b9a\u6570\uff09</li> <li> <p>\\(\\mathbb{R}^3\\)\u81ea\u4f53\u3068\\(\\{\\mathbf{0}\\}\\)\uff08\u96f6\u30d9\u30af\u30c8\u30eb\u306e\u307f\u306e\u96c6\u5408\uff09</p> </li> <li> <p>\\(P_n(\\mathbb{R})\\)\u306e\u90e8\u5206\u7a7a\u9593\u306e\u4f8b\uff1a</p> </li> <li>\u5076\u95a2\u6570\u306e\u591a\u9805\u5f0f: \\(\\{a_0 + a_2x^2 + a_4x^4 + \\cdots \\mid a_i \\in \\mathbb{R}\\}\\)</li> <li>\u5947\u95a2\u6570\u306e\u591a\u9805\u5f0f: \\(\\{a_1x + a_3x^3 + a_5x^5 + \\cdots \\mid a_i \\in \\mathbb{R}\\}\\)</li> </ol>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#34","title":"3.4 \u90e8\u5206\u7a7a\u9593\u3067\u3042\u308b\u304b\u306e\u5224\u5b9a\u65b9\u6cd5","text":"<p>\u96c6\u5408\\(W\\)\u304c\u90e8\u5206\u7a7a\u9593\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u3059\u308b\u306b\u306f\u4ee5\u4e0b\u306e\u624b\u9806\u3092\u8e0f\u307f\u307e\u3059\uff1a</p> <ol> <li>\\(\\mathbf{0} \\in W\\)\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\uff08\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\u3092\u542b\u307e\u306a\u3044\u5834\u5408\u306f\u90e8\u5206\u7a7a\u9593\u3067\u306f\u306a\u3044\uff09</li> <li>\u52a0\u6cd5\u306b\u95a2\u3057\u3066\u9589\u3058\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\uff08\\(\\mathbf{u}, \\mathbf{v} \\in W \\Rightarrow \\mathbf{u} + \\mathbf{v} \\in W\\)\uff09</li> <li>\u30b9\u30ab\u30e9\u30fc\u500d\u306b\u95a2\u3057\u3066\u9589\u3058\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\uff08\\(a \\in F, \\mathbf{v} \\in W \\Rightarrow a\\mathbf{v} \\in W\\)\uff09</li> </ol> <p>\u4f8b: \\(W = \\{(x,y) \\in \\mathbb{R}^2 \\mid x + y = 1\\}\\)\u306f\\(\\mathbb{R}^2\\)\u306e\u90e8\u5206\u7a7a\u9593\u304b\uff1f</p> <p>\u89e3\u7b54: \\(W\\)\u306f\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\\((0,0)\\)\u3092\u542b\u307e\u306a\u3044\uff08\\((0,0)\\)\u306f\\(x + y = 1\\)\u3092\u6e80\u305f\u3055\u306a\u3044\uff09\u306e\u3067\u3001\\(W\\)\u306f\u90e8\u5206\u7a7a\u9593\u3067\u306f\u306a\u3044\u3002</p> <p>\u4f8b: \\(W = \\{(x,y,z) \\in \\mathbb{R}^3 \\mid 2x - 3y + z = 0\\}\\)\u306f\\(\\mathbb{R}^3\\)\u306e\u90e8\u5206\u7a7a\u9593\u304b\uff1f</p> <p>\u89e3\u7b54:  1. \\(\\mathbf{0} = (0,0,0) \\in W\\)\uff08\\(2 \\cdot 0 - 3 \\cdot 0 + 0 = 0\\)\uff09 2. \\(\\mathbf{u} = (u_1,u_2,u_3), \\mathbf{v} = (v_1,v_2,v_3) \\in W\\)\u3068\u3059\u308b\u3068\u3001    \\(2u_1 - 3u_2 + u_3 = 0\\)\u304a\u3088\u3073\\(2v_1 - 3v_2 + v_3 = 0\\)    \u3088\u3063\u3066\\(\\mathbf{u} + \\mathbf{v} = (u_1+v_1, u_2+v_2, u_3+v_3)\\)\u306b\u5bfe\u3057\u3066\u3001    \\(2(u_1+v_1) - 3(u_2+v_2) + (u_3+v_3)\\) \\(= (2u_1 - 3u_2 + u_3) + (2v_1 - 3v_2 + v_3) = 0 + 0 = 0\\)    \u3057\u305f\u304c\u3063\u3066\\(\\mathbf{u} + \\mathbf{v} \\in W\\) 3. \\(a \\in \\mathbb{R}, \\mathbf{u} = (u_1,u_2,u_3) \\in W\\)\u3068\u3059\u308b\u3068\u3001    \\(2u_1 - 3u_2 + u_3 = 0\\)    \u3088\u3063\u3066\\(a\\mathbf{u} = (au_1, au_2, au_3)\\)\u306b\u5bfe\u3057\u3066\u3001    \\(2(au_1) - 3(au_2) + (au_3) = a(2u_1 - 3u_2 + u_3) = a \\cdot 0 = 0\\)    \u3057\u305f\u304c\u3063\u3066\\(a\\mathbf{u} \\in W\\)</p> <p>\u4ee5\u4e0a\u3088\u308a\u3001\\(W\\)\u306f\u90e8\u5206\u7a7a\u9593\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/26-vector-space-and-inner-product/#41","title":"4.1 \u30d9\u30af\u30c8\u30eb\u306e\u7dda\u5f62\u7d50\u5408\u3068\u7dda\u5f62\u5305","text":"<p>\u5b9a\u7fa9: \u7dda\u5f62\u7d50\u5408</p> <p>\u30d9\u30af\u30c8\u30eb\\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k \\in V\\)\u306e\u7dda\u5f62\u7d50\u5408\u3068\u306f\u3001 \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k\\) \u3068\u3044\u3046\u5f62\u306e\u5f0f\u3067\u3042\u308a\u3001\u3053\u3053\u3067\\(c_1, c_2, \\ldots, c_k\\)\u306f\u30b9\u30ab\u30e9\u30fc\uff08\u5b9f\u6570\uff09\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9: \u7dda\u5f62\u5305\uff08\u30b9\u30d1\u30f3\uff09</p> <p>\u30d9\u30af\u30c8\u30eb\\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k \\in V\\)\u306e\u7dda\u5f62\u5305\uff08\u30b9\u30d1\u30f3\uff09\u3068\u306f\u3001 \u3053\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u306e\u3059\u3079\u3066\u306e\u7dda\u5f62\u7d50\u5408\u306e\u96c6\u5408\u3067\u3042\u308a\u3001\u6b21\u306e\u3088\u3046\u306b\u8868\u8a18\u3057\u307e\u3059\uff1a</p> <p>\\(\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\} = \\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k \\mid c_1, c_2, \\ldots, c_k \\in \\mathbb{R}\\}\\)</p> <p>\u91cd\u8981\u306a\u6027\u8cea: \\(\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}\\)\u306f\u5e38\u306b\\(V\\)\u306e\u90e8\u5206\u7a7a\u9593\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u4f8b: \\(\\mathbf{v}_1 = (1,0,0), \\mathbf{v}_2 = (0,1,0) \\in \\mathbb{R}^3\\)\u306e\u30b9\u30d1\u30f3\u3092\u8003\u3048\u308b\u3002</p> <p>\\(\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2\\} = \\{c_1(1,0,0) + c_2(0,1,0) \\mid c_1, c_2 \\in \\mathbb{R}\\} = \\{(c_1, c_2, 0) \\mid c_1, c_2 \\in \\mathbb{R}\\}\\)</p> <p>\u3053\u308c\u306f\\(xy\\)\u5e73\u9762\u3092\u8868\u3057\u3066\u304a\u308a\u3001\\(\\mathbb{R}^3\\)\u306e\u90e8\u5206\u7a7a\u9593\u3067\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#42","title":"4.2 \u7dda\u5f62\u72ec\u7acb\u6027\u3068\u7dda\u5f62\u5f93\u5c5e\u6027","text":"<p>\u5b9a\u7fa9: \u7dda\u5f62\u72ec\u7acb\u3068\u7dda\u5f62\u5f93\u5c5e</p> <p>\u30d9\u30af\u30c8\u30eb\\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k \\in V\\)\u304c\u7dda\u5f62\u72ec\u7acb\u3067\u3042\u308b\u3068\u306f\u3001 \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\) \u3068\u3044\u3046\u5f0f\u304c\\(c_1 = c_2 = \\cdots = c_k = 0\\)\u306e\u3068\u304d\u306e\u307f\u6210\u308a\u7acb\u3064\u3053\u3068\u3067\u3059\u3002</p> <p>\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u3001\u3059\u306a\u308f\u3061\u5c11\u306a\u304f\u3068\u3082\u4e00\u3064\u306e\\(c_i \\neq 0\\)\u304c\u5b58\u5728\u3057\u3066\u4e0a\u306e\u5f0f\u304c\u6210\u308a\u7acb\u3064\u5834\u5408\u3001\u3053\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u306f\u7dda\u5f62\u5f93\u5c5e\u3067\u3042\u308b\u3068\u3044\u3044\u307e\u3059\u3002</p> <p>\u7dda\u5f62\u5f93\u5c5e\u306e\u5225\u306e\u898b\u65b9: \u30d9\u30af\u30c8\u30eb\u7fa4\u304c\u7dda\u5f62\u5f93\u5c5e\u3067\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u306f\u3001\u305d\u306e\u3046\u3061\u306e\u5c11\u306a\u304f\u3068\u30821\u3064\u306e\u30d9\u30af\u30c8\u30eb\u304c\u4ed6\u306e\u30d9\u30af\u30c8\u30eb\u306e\u7dda\u5f62\u7d50\u5408\u3068\u3057\u3066\u8868\u305b\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#_2","title":"\u7dda\u5f62\u72ec\u7acb\u6027\u306e\u5224\u5b9a\u65b9\u6cd5","text":"<p>\u30d9\u30af\u30c8\u30eb\u7fa4\u304c\u7dda\u5f62\u72ec\u7acb\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u3059\u308b\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u65b9\u6cd5\u304c\u4f7f\u3048\u307e\u3059\uff1a</p> <ol> <li> <p>\u5b9a\u7fa9\u306b\u57fa\u3065\u304f\u65b9\u6cd5: \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\)\u3068\u3044\u3046\u5f0f\u3092\u89e3\u304d\u3001\u552f\u4e00\u306e\u89e3\u304c\\(c_1 = c_2 = \\cdots = c_k = 0\\)\u3067\u3042\u308c\u3070\u7dda\u5f62\u72ec\u7acb\u3002</p> </li> <li> <p>\u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u7528\u3044\u308b\u65b9\u6cd5: \u30d9\u30af\u30c8\u30eb\\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\)\u3092\u4e26\u3079\u305f\u884c\u5217\\(A = [\\mathbf{v}_1 \\; \\mathbf{v}_2 \\; \\cdots \\; \\mathbf{v}_k]\\)\u3092\u8003\u3048\u3001\\(\\text{rank}(A) = k\\)\u3067\u3042\u308c\u3070\u7dda\u5f62\u72ec\u7acb\u3002</p> </li> </ol> <p>\u4f8b: \u30d9\u30af\u30c8\u30eb\\(\\mathbf{v}_1 = (1,2,1), \\mathbf{v}_2 = (2,3,1), \\mathbf{v}_3 = (1,-1,-1) \\in \\mathbb{R}^3\\)\u304c\u7dda\u5f62\u72ec\u7acb\u304b\u3069\u3046\u304b\u3092\u8abf\u3079\u3088\u3002</p> <p>\u89e3\u7b54: \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + c_3\\mathbf{v}_3 = \\mathbf{0}\\)\u3068\u3044\u3046\u5f0f\u3092\u8003\u3048\u308b\u3002</p> <p>\\(c_1(1,2,1) + c_2(2,3,1) + c_3(1,-1,-1) = (0,0,0)\\)</p> <p>\u3053\u308c\u306f\u9023\u7acb\u65b9\u7a0b\u5f0f \\(c_1 + 2c_2 + c_3 = 0\\) \\(2c_1 + 3c_2 - c_3 = 0\\) \\(c_1 + c_2 - c_3 = 0\\) \u3092\u89e3\u304f\u3053\u3068\u306b\u7b49\u3057\u3044\u3002</p> <p>\u884c\u5217\u306e\u5f62\u3067\u8868\u3059\u3068\uff1a \\(\\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 3 &amp; -1 \\\\ 1 &amp; 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\)</p> <p>\u3053\u306e\u884c\u5217\u3092\u30ac\u30a6\u30b9\u306e\u6d88\u53bb\u6cd5\u3067\u7c21\u7d04\u3059\u308b\u3068\uff1a \\(\\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 0 &amp; -1 &amp; -3 \\\\ 0 &amp; -1 &amp; -2 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 0 &amp; -1 &amp; -3 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>\u3053\u308c\u3088\u308a\u3001\\(c_3 = 0, c_2 = 0, c_1 = 0\\)\u3068\u306a\u308b\u306e\u3067\u3001\u4e0e\u3048\u3089\u308c\u305f\u30d9\u30af\u30c8\u30eb\u7fa4\u306f\u7dda\u5f62\u72ec\u7acb\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#43","title":"4.3 \u57fa\u5e95\u3068\u6b21\u5143","text":"<p>\u5b9a\u7fa9: \u57fa\u5e95</p> <p>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\\(V\\)\u306e\u57fa\u5e95\u3068\u306f\u3001\u4ee5\u4e0b\u306e2\u3064\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u30d9\u30af\u30c8\u30eb\u306e\u96c6\u5408\\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\)\u306e\u3053\u3068\u3067\u3059\uff1a</p> <ol> <li>\\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\)\u306f\u7dda\u5f62\u72ec\u7acb\u3067\u3042\u308b</li> <li>\\(\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\} = V\\)\uff08\u3064\u307e\u308a\u3001\u3053\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u306fV\u3092\u751f\u6210\u3059\u308b\uff09</li> </ol> <p>\u57fa\u5e95\u306e\u91cd\u8981\u306a\u6027\u8cea\u306f\u3001\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb\u304c\u552f\u4e00\u306e\u65b9\u6cd5\u3067\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u306e\u7dda\u5f62\u7d50\u5408\u3068\u3057\u3066\u8868\u305b\u308b\u3053\u3068\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9: \u6b21\u5143</p> <p>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\\(V\\)\u306e\u6b21\u5143\uff08\u8a18\u53f7: \\(\\dim V\\)\uff09\u3068\u306f\u3001\\(V\\)\u306e\u57fa\u5e95\u306b\u542b\u307e\u308c\u308b\u30d9\u30af\u30c8\u30eb\u306e\u500b\u6570\u3067\u3059\u3002</p> <p>\u91cd\u8981\u306a\u5b9a\u7406: 1. \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u4efb\u610f\u306e\u57fa\u5e95\u306f\u3059\u3079\u3066\u540c\u3058\u500b\u6570\u306e\u30d9\u30af\u30c8\u30eb\u3092\u6301\u3064 2. \u6709\u9650\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\\(V\\)\u306e\u4efb\u610f\u306e\u7dda\u5f62\u72ec\u7acb\u306a\u96c6\u5408\u306f\u3001\\(V\\)\u306e\u57fa\u5e95\u306b\u62e1\u5f35\u3067\u304d\u308b 3. \u6709\u9650\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\\(V\\)\u306e\u4efb\u610f\u306e\u751f\u6210\u7cfb\u306f\u3001\\(V\\)\u306e\u57fa\u5e95\u3092\u90e8\u5206\u96c6\u5408\u3068\u3057\u3066\u542b\u3080</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#_3","title":"\u6a19\u6e96\u57fa\u5e95","text":"<p>\u591a\u304f\u306e\u4e00\u822c\u7684\u306a\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306b\u306f\u3001\u6a19\u6e96\uff08\u6b63\u6e96\uff09\u57fa\u5e95\u3068\u547c\u3070\u308c\u308b\u81ea\u7136\u306a\u57fa\u5e95\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li> <p>\\(\\mathbb{R}^n\\)\u306e\u6a19\u6e96\u57fa\u5e95: \\(\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}\\)    \u3053\u3053\u3067\\(\\mathbf{e}_i\\)\u306f\\(i\\)\u756a\u76ee\u306e\u6210\u5206\u3060\u3051\u304c1\u3067\u4ed6\u306f\u3059\u3079\u30660\u306e\u30d9\u30af\u30c8\u30eb</p> </li> <li> <p>\\(P_n(\\mathbb{R})\\)\u306e\u6a19\u6e96\u57fa\u5e95: \\(\\{1, x, x^2, \\ldots, x^n\\}\\)</p> </li> <li> <p>\\(M_{m,n}(\\mathbb{R})\\)\u306e\u6a19\u6e96\u57fa\u5e95: \\(\\{E_{ij} \\mid 1 \\leq i \\leq m, 1 \\leq j \\leq n\\}\\)    \u3053\u3053\u3067\\(E_{ij}\\)\u306f\\((i,j)\\)\u6210\u5206\u304c1\u3067\u4ed6\u306f\u3059\u3079\u30660\u306e\u884c\u5217</p> </li> </ol> <p>\u4f8b: \\(\\mathbb{R}^2\\)\u306e\u6a19\u6e96\u57fa\u5e95\u306f\\(\\{\\mathbf{e}_1, \\mathbf{e}_2\\} = \\{(1,0), (0,1)\\}\\)\u3067\u3059\u3002\\(\\mathbb{R}^2\\)\u5185\u306e\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb\\((a,b)\\)\u306f\u3001 \\((a,b) = a(1,0) + b(0,1) = a\\mathbf{e}_1 + b\\mathbf{e}_2\\) \u3068\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#44","title":"4.4 \u5ea7\u6a19\u8868\u73fe","text":"<p>\u5b9a\u7fa9: \u5ea7\u6a19\u8868\u73fe</p> <p>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\\(V\\)\u306e\u57fa\u5e95\\(B = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\)\u306b\u95a2\u3059\u308b \u30d9\u30af\u30c8\u30eb\\(\\mathbf{v} \\in V\\)\u306e\u5ea7\u6a19\u8868\u73fe\u3068\u306f\u3001 \\(\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n\\) \u3092\u6e80\u305f\u3059\u30b9\u30ab\u30e9\u30fc\\(c_1, c_2, \\ldots, c_n\\)\u3092\u7528\u3044\u3066\u3001 \\([\\mathbf{v}]_B = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\\) \u3068\u8868\u8a18\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p> <p>\u7570\u306a\u308b\u57fa\u5e95\u3092\u9078\u3076\u3068\u3001\u540c\u3058\u30d9\u30af\u30c8\u30eb\u3067\u3082\u5ea7\u6a19\u8868\u73fe\u304c\u5909\u308f\u308a\u307e\u3059\u3002\u3053\u308c\u304c\u300c\u57fa\u5e95\u306e\u5909\u63db\u300d\u306e\u8003\u3048\u65b9\u306b\u3064\u306a\u304c\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#_4","title":"\u57fa\u5e95\u306e\u5909\u63db","text":"<p>\\(V\\)\u306e2\u3064\u306e\u57fa\u5e95\\(B = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\)\u3068\\(B' = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_n\\}\\)\u304c\u3042\u308b\u3068\u304d\u3001 \u57fa\u5e95\\(B\\)\u306b\u95a2\u3059\u308b\u30d9\u30af\u30c8\u30eb\\(\\mathbf{v}\\)\u306e\u5ea7\u6a19\u304b\u3089\u3001\u57fa\u5e95\\(B'\\)\u306b\u95a2\u3059\u308b\u5ea7\u6a19\u3078\u306e\u5909\u63db\u306f\u7dda\u5f62\u5909\u63db\u3068\u3057\u3066\u8868\u3055\u308c\u307e\u3059\u3002</p> <p>\u3053\u306e\u5909\u63db\u306f\u300c\u57fa\u5e95\u5909\u63db\u884c\u5217\u300d\\(P_{B' \\leftarrow B}\\)\u306b\u3088\u3063\u3066\u884c\u308f\u308c\u3001 \\([\\mathbf{v}]_{B'} = P_{B' \\leftarrow B} [\\mathbf{v}]_B\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u57fa\u5e95\u5909\u63db\u884c\u5217\\(P_{B' \\leftarrow B}\\)\u306f\u3001\u57fa\u5e95\\(B\\)\u306e\u30d9\u30af\u30c8\u30eb\u3092\u57fa\u5e95\\(B'\\)\u3067\u8868\u3057\u305f\u3068\u304d\u306e\u5ea7\u6a19\u3092\u5217\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u4e26\u3079\u305f\u884c\u5217\u3067\u3059\uff1a \\(P_{B' \\leftarrow B} = \\begin{bmatrix} [\\mathbf{v}_1]_{B'} &amp; [\\mathbf{v}_2]_{B'} &amp; \\cdots &amp; [\\mathbf{v}_n]_{B'} \\end{bmatrix}\\)</p> <p>\u4f8b: \\(\\mathbb{R}^2\\)\u3067\u3001\u6a19\u6e96\u57fa\u5e95\\(B = \\{(1,0), (0,1)\\}\\)\u3068\u5225\u306e\u57fa\u5e95\\(B' = \\{(1,1), (1,-1)\\}\\)\u3092\u8003\u3048\u308b\u3002 \u30d9\u30af\u30c8\u30eb\\(\\mathbf{v} = (3,1)\\)\u306e\u57fa\u5e95\\(B'\\)\u306b\u95a2\u3059\u308b\u5ea7\u6a19\u3092\u6c42\u3081\u308b\u3002</p> <p>\u307e\u305a\u3001\u57fa\u5e95\u5909\u63db\u884c\u5217\\(P_{B' \\leftarrow B}\\)\u3092\u6c42\u3081\u308b\u3002 \\((1,0) = \\frac{1}{2}(1,1) + \\frac{1}{2}(1,-1)\\)\u306a\u306e\u3067\\([(1,0)]_{B'} = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{bmatrix}\\) \\((0,1) = \\frac{1}{2}(1,1) - \\frac{1}{2}(1,-1)\\)\u306a\u306e\u3067\\([(0,1)]_{B'} = \\begin{bmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{bmatrix}\\)</p> <p>\u3088\u3063\u3066\u3001 \\(P_{B' \\leftarrow B} = \\begin{bmatrix} \\frac{1}{2} &amp; \\frac{1}{2} \\\\ \\frac{1}{2} &amp; -\\frac{1}{2} \\end{bmatrix}\\)</p> <p>\u30d9\u30af\u30c8\u30eb\\(\\mathbf{v} = (3,1)\\)\u306e\u57fa\u5e95\\(B\\)\u306b\u95a2\u3059\u308b\u5ea7\u6a19\u306f\\([\\mathbf{v}]_B = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\\) \u3057\u305f\u304c\u3063\u3066\u3001\u57fa\u5e95\\(B'\\)\u306b\u95a2\u3059\u308b\u5ea7\u6a19\u306f \\([\\mathbf{v}]_{B'} = P_{B' \\leftarrow B} [\\mathbf{v}]_B = \\begin{bmatrix} \\frac{1}{2} &amp; \\frac{1}{2} \\\\ \\frac{1}{2} &amp; -\\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\)</p> <p>\u3053\u308c\u306f\\((3,1) = 2(1,1) + 1(1,-1)\\)\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/26-vector-space-and-inner-product/#51-numpy","title":"5.1 NumPy\u3092\u7528\u3044\u305f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u672c\u64cd\u4f5c","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\nv1 = np.array([1, 0, 0])\nv2 = np.array([0, 1, 0])\nv3 = np.array([0, 0, 1])\n\n# \u7dda\u5f62\u7d50\u5408\na, b, c = 2, 3, -1\nlinear_combination = a * v1 + b * v2 + c * v3\nprint(\"\u7dda\u5f62\u7d50\u5408:\", linear_combination)\n\n# \u7dda\u5f62\u72ec\u7acb\u6027\u306e\u5224\u5b9a\nvectors = np.array([v1, v2, v3])  # \u5404\u884c\u304c\u30d9\u30af\u30c8\u30eb\nrank = np.linalg.matrix_rank(vectors)\nprint(\"\u30e9\u30f3\u30af:\", rank)\nprint(\"\u7dda\u5f62\u72ec\u7acb\" if rank == len(vectors) else \"\u7dda\u5f62\u5f93\u5c5e\")\n\n# \u57fa\u5e95\u306e\u5909\u63db\n# \u6a19\u6e96\u57fa\u5e95\nstandard_basis = np.eye(3)  # 3x3\u306e\u5358\u4f4d\u884c\u5217\nprint(\"\u6a19\u6e96\u57fa\u5e95:\\n\", standard_basis)\n\n# \u65b0\u3057\u3044\u57fa\u5e95\nnew_basis = np.array([\n    [1, 1, 0],\n    [1, 0, 1],\n    [0, 1, 1]\n])\n\n# \u30d9\u30af\u30c8\u30ebv = (2, 3, 4)\u3092\u65b0\u3057\u3044\u57fa\u5e95\u3067\u8868\u73fe\u3059\u308b\nv = np.array([2, 3, 4])\n\n# \u57fa\u5e95\u5909\u63db\u884c\u5217\uff08\u65b0\u3057\u3044\u57fa\u5e95\u3092\u6a19\u6e96\u57fa\u5e95\u3067\u8868\u3057\u305f\u3082\u306e\uff09\u306e\u9006\u884c\u5217\ntransformation_matrix = np.linalg.inv(new_basis)\n\n# \u5909\u63db\u5b9f\u884c\nv_new_coords = transformation_matrix @ v\nprint(\"\u65b0\u3057\u3044\u57fa\u5e95\u3067\u306e\u5ea7\u6a19:\", v_new_coords)\n\n# \u9006\u5909\u63db\u3067\u78ba\u8a8d\nv_reconstructed = new_basis @ v_new_coords\nprint(\"\u5143\u306e\u5ea7\u6a19\u306b\u623b\u3057\u305f\u7d50\u679c:\", v_reconstructed)\n</code></pre>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#52","title":"5.2 \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u8996\u899a\u5316","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# 3D\u30d7\u30ed\u30c3\u30c8\u306e\u8a2d\u5b9a\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# \u539f\u70b9\norigin = np.zeros(3)\n\n# \u6a19\u6e96\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\ne1 = np.array([1, 0, 0])\ne2 = np.array([0, 1, 0])\ne3 = np.array([0, 0, 1])\n\n# \u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3092\u77e2\u5370\u3067\u8868\u793a\uff08\u6a19\u6e96\u57fa\u5e95\uff09\nax.quiver(*origin, *e1, color='r', label='e1')\nax.quiver(*origin, *e2, color='g', label='e2')\nax.quiver(*origin, *e3, color='b', label='e3')\n\n# \u65b0\u3057\u3044\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\nv1 = np.array([1, 1, 0])\nv2 = np.array([1, 0, 1])\nv3 = np.array([0, 1, 1])\n\n# \u65b0\u3057\u3044\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3092\u77e2\u5370\u3067\u8868\u793a\nax.quiver(*origin, *v1, color='r', linestyle='dashed', label='v1')\nax.quiver(*origin, *v2, color='g', linestyle='dashed', label='v2')\nax.quiver(*origin, *v3, color='b', linestyle='dashed', label='v3')\n\n# \u30c6\u30b9\u30c8\u7528\u30d9\u30af\u30c8\u30eb\ntest_vector = np.array([2, 3, 4])\nax.quiver(*origin, *test_vector, color='k', label='test vector')\n\n# \u30d7\u30ed\u30c3\u30c8\u306e\u8a2d\u5b9a\nax.set_xlim([-1, 3])\nax.set_ylim([-1, 3])\nax.set_zlim([-1, 3])\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u57fa\u5e95')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#53","title":"5.3 \u90e8\u5206\u7a7a\u9593\u306e\u8996\u899a\u5316","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# 3D\u30d7\u30ed\u30c3\u30c8\u306e\u8a2d\u5b9a\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# \u539f\u70b9\norigin = np.zeros(3)\n\n# \u30b0\u30ea\u30c3\u30c9\u30dd\u30a4\u30f3\u30c8\u306e\u751f\u6210\nx = np.linspace(-2, 2, 10)\ny = np.linspace(-2, 2, 10)\nX, Y = np.meshgrid(x, y)\n\n# \u5e73\u9762 ax + by + cz = 0 \u306e\u30c7\u30fc\u30bf\u70b9\na, b, c = 1, 1, 1  # \u4f8b: x + y + z = 0\nZ = -(a * X + b * Y) / c\n\n# \u5e73\u9762\u306e\u30d7\u30ed\u30c3\u30c8\nsurf = ax.plot_surface(X, Y, Z, alpha=0.5, color='cyan')\n\n# \u6a19\u6e96\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\ne1 = np.array([1, 0, 0])\ne2 = np.array([0, 1, 0])\ne3 = np.array([0, 0, 1])\n\nax.quiver(*origin, *e1, color='r', label='e1')\nax.quiver(*origin, *e2, color='g', label='e2')\nax.quiver(*origin, *e3, color='b', label='e3')\n\n# \u5e73\u9762\u4e0a\u306e\u30d9\u30af\u30c8\u30eb\uff08\u57fa\u5e95\uff09\u3092\u8868\u793a\nv1 = np.array([1, 0, -1])  # x + z = 0 \u3092\u6e80\u305f\u3059\nv2 = np.array([0, 1, -1])  # y + z = 0 \u3092\u6e80\u305f\u3059\n\nax.quiver(*origin, *v1, color='m', linestyle='dashed', label='v1')\nax.quiver(*origin, *v2, color='y', linestyle='dashed', label='v2')\n\n# \u30d7\u30ed\u30c3\u30c8\u306e\u8a2d\u5b9a\nax.set_xlim([-2, 2])\nax.set_ylim([-2, 2])\nax.set_zlim([-2, 2])\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('\u90e8\u5206\u7a7a\u9593\uff08\u5e73\u9762 x + y + z = 0\uff09\u3068\u305d\u306e\u57fa\u5e95')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#54","title":"5.4 \u57fa\u5e95\u5909\u63db\u306e\u8996\u899a\u5316","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# 2\u6b21\u5143\u7a7a\u9593\u3067\u306e\u57fa\u5e95\u5909\u63db\u3092\u8996\u899a\u5316\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# \u539f\u70b9\norigin = np.zeros(2)\n\n# \u6a19\u6e96\u57fa\u5e95\ne1 = np.array([1, 0])\ne2 = np.array([0, 1])\n\n# \u65b0\u3057\u3044\u57fa\u5e95\nv1 = np.array([1, 1])\nv2 = np.array([1, -1])\n\n# \u30c6\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\ntest_vector = np.array([3, 1])\n\n# \u6a19\u6e96\u57fa\u5e95\u3067\u306e\u8868\u793a\uff08\u5de6\u5074\uff09\nax1.quiver(*origin, *e1, color='r', angles='xy', scale_units='xy', scale=1, label='e1')\nax1.quiver(*origin, *e2, color='g', angles='xy', scale_units='xy', scale=1, label='e2')\nax1.quiver(*origin, *test_vector, color='k', angles='xy', scale_units='xy', scale=1, label='v=(3,1)')\n\n# \u6a19\u6e96\u57fa\u5e95\u3067\u306e\u30c6\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306e\u6210\u5206\nax1.quiver(*origin, test_vector[0], 0, color='r', alpha=0.3, angles='xy', scale_units='xy', scale=1)\nax1.quiver(*np.array([test_vector[0], 0]), 0, test_vector[1], color='g', alpha=0.3, angles='xy', scale_units='xy', scale=1)\n\n# \u65b0\u3057\u3044\u57fa\u5e95\u3067\u306e\u5ea7\u6a19\u3092\u8a08\u7b97\nP = np.column_stack([v1, v2])  # \u57fa\u5e95\u5909\u63db\u884c\u5217\nP_inv = np.linalg.inv(P)\nnew_coords = P_inv @ test_vector\nprint(f\"\u65b0\u3057\u3044\u57fa\u5e95\u3067\u306e\u5ea7\u6a19: {new_coords}\")\n\n# \u65b0\u3057\u3044\u57fa\u5e95\u3067\u306e\u8868\u793a\uff08\u53f3\u5074\uff09\nax2.quiver(*origin, *v1, color='r', angles='xy', scale_units='xy', scale=1, label='v1')\nax2.quiver(*origin, *v2, color='g', angles='xy', scale_units='xy', scale=1, label='v2')\nax2.quiver(*origin, *test_vector, color='k', angles='xy', scale_units='xy', scale=1, label='v=(3,1)')\n\n# \u65b0\u3057\u3044\u57fa\u5e95\u3067\u306e\u30c6\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306e\u6210\u5206\nax2.quiver(*origin, *new_coords[0] * v1, color='r', alpha=0.3, angles='xy', scale_units='xy', scale=1)\nax2.quiver(*new_coords[0] * v1, *new_coords[1] * v2, color='g', alpha=0.3, angles='xy', scale_units='xy', scale=1)\n\n# \u30d7\u30ed\u30c3\u30c8\u306e\u8a2d\u5b9a\nfor ax in [ax1, ax2]:\n    ax.set_xlim([-2, 4])\n    ax.set_ylim([-2, 4])\n    ax.grid(True)\n    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n    ax.set_aspect('equal')\n    ax.legend()\n\nax1.set_title('\u6a19\u6e96\u57fa\u5e95\u3067\u306e\u8868\u73fe')\nax2.set_title('\u65b0\u3057\u3044\u57fa\u5e95\u3067\u306e\u8868\u73fe')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#55","title":"5.5 \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u4f8b","text":"<p>\u4ee5\u4e0b\u306e\u4f8b\u3067\u306f\u3001\u30c7\u30fc\u30bf\u3092\u884c\u5217\u3068\u3057\u3066\u8868\u73fe\u3057\u3001\u305d\u306e\u5217\u7a7a\u9593\u3084\u884c\u7a7a\u9593\u3092\u8003\u3048\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u6301\u3064\u69cb\u9020\u3092\u7406\u89e3\u3059\u308b\u65b9\u6cd5\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_boston\n\n# \u30dc\u30b9\u30c8\u30f3\u4f4f\u5b85\u4fa1\u683c\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080\nboston = load_boston()\nX = boston.data\nfeature_names = boston.feature_names\n\n# \u6700\u521d\u306e5\u3064\u306e\u7279\u5fb4\u91cf\u3060\u3051\u3092\u4f7f\u7528\nX_subset = X[:, :5]\n\n# \u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\nrank = np.linalg.matrix_rank(X_subset)\nprint(f\"\u884c\u5217\u306e\u30e9\u30f3\u30af: {rank}\")\n\n# \u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\nU, S, Vt = np.linalg.svd(X_subset, full_matrices=False)\n\n# \u7279\u7570\u5024\u3092\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(S)), S)\nplt.title('\u30c7\u30fc\u30bf\u884c\u5217\u306e\u7279\u7570\u5024')\nplt.xlabel('\u6210\u5206')\nplt.ylabel('\u7279\u7570\u5024')\nplt.grid(True)\nplt.show()\n\n# \u7279\u7570\u5024\u304b\u3089\u5bc4\u4e0e\u7387\u3092\u8a08\u7b97\nexplained_variance_ratio = S**2 / np.sum(S**2)\nprint(f\"\u5404\u6210\u5206\u306e\u5bc4\u4e0e\u7387: {explained_variance_ratio}\")\nprint(f\"\u7d2f\u7a4d\u5bc4\u4e0e\u7387: {np.cumsum(explained_variance_ratio)}\")\n\n# \u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff08V\uff09\u3092\u57fa\u5e95\u3068\u307f\u306a\u3057\u3066\u7279\u5fb4\u91cf\u306e\u95a2\u4fc2\u3092\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nfor i, feature in enumerate(feature_names[:5]):\n    plt.arrow(0, 0, Vt[0, i], Vt[1, i], head_width=0.05, head_length=0.05, fc='k', ec='k')\n    plt.text(Vt[0, i]*1.1, Vt[1, i]*1.1, feature)\n\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.grid(True)\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nplt.gca().set_aspect('equal')\nplt.title('\u7279\u5fb4\u91cf\u306e\u7b2c1\u30fb\u7b2c2\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u8868\u73fe')\nplt.show()\n</code></pre>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/26-vector-space-and-inner-product/#61","title":"6.1 \u57fa\u672c\u554f\u984c\uff08\u6982\u5ff5\u7406\u89e3\u306e\u78ba\u8a8d\uff09","text":"<ol> <li> <p>\u4ee5\u4e0b\u306e\u96c6\u5408\u304c\\(\\mathbb{R}^3\\)\u306e\u90e8\u5206\u7a7a\u9593\u3067\u3042\u308b\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002    (a) \\(W_1 = \\{(x, y, z) \\in \\mathbb{R}^3 \\mid x - 2y + 3z = 0\\}\\)    (b) \\(W_2 = \\{(x, y, z) \\in \\mathbb{R}^3 \\mid x - 2y + 3z = 1\\}\\)    (c) \\(W_3 = \\{(x, y, z) \\in \\mathbb{R}^3 \\mid xy = 0\\}\\)</p> </li> <li> <p>\\(\\mathbb{R}^3\\)\u306e\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u96c6\u5408\u304c\u7dda\u5f62\u72ec\u7acb\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002    (a) \\(\\{(1,1,1), (1,2,3), (2,3,4)\\}\\)    (b) \\(\\{(1,0,1), (0,1,1), (1,1,2)\\}\\)</p> </li> <li> <p>\\(\\mathbb{R}^3\\)\u306b\u304a\u3044\u3066\u3001\u5e73\u9762\\(x + 2y - z = 0\\)\u306e\u57fa\u5e95\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\\(\\mathbb{R}^2\\)\u306b\u304a\u3051\u308b\u57fa\u5e95\\(B = \\{(1,1), (1,-1)\\}\\)\u306b\u3064\u3044\u3066\u3001\u30d9\u30af\u30c8\u30eb\\(v = (3,2)\\)\u306e\\(B\\)\u306b\u95a2\u3059\u308b\u5ea7\u6a19\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\u591a\u9805\u5f0f\u7a7a\u9593\\(P_2(\\mathbb{R})\\)\u306e\u57fa\u5e95\\(\\{1, 1+x, 1+x+x^2\\}\\)\u306b\u3064\u3044\u3066\u3001\u591a\u9805\u5f0f\\(p(x) = 2 + 3x + x^2\\)\u306e\u5ea7\u6a19\u8868\u793a\u3092\u6c42\u3081\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#62","title":"6.2 \u5fdc\u7528\u554f\u984c\uff08\u5fdc\u7528\u80fd\u529b\u306e\u78ba\u8a8d\uff09","text":"<ol> <li> <p>\\(\\mathbb{R}^4\\)\u306e\u90e8\u5206\u7a7a\u9593\\(W = \\text{span}\\{(1,2,0,1), (0,1,1,1), (2,5,1,3)\\}\\)\u306e\u6b21\u5143\u3068\u57fa\u5e95\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\\(\\mathbb{R}^3\\)\u306b\u304a\u3051\u308b2\u3064\u306e\u76f4\u7dda    \\(L_1 = \\{t(1,2,3) \\mid t \\in \\mathbb{R}\\}\\)\u3068    \\(L_2 = \\{s(2,1,0) + (1,1,1) \\mid s \\in \\mathbb{R}\\}\\)\u306b\u3064\u3044\u3066\u3001    (a) \\(L_1\\)\u3068\\(L_2\\)\u304c\u4ea4\u308f\u308b\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002    (b) \u4ea4\u308f\u308b\u5834\u5408\u306f\u305d\u306e\u4ea4\u70b9\u3092\u3001\u4ea4\u308f\u3089\u306a\u3044\u5834\u5408\u306f\\(L_1\\)\u3068\\(L_2\\)\u306e\u6700\u77ed\u8ddd\u96e2\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\\(\\mathbb{R}^3\\)\u306b\u304a\u3051\u308b3\u3064\u306e\u30d9\u30af\u30c8\u30eb\\(v_1 = (1,2,3)\\), \\(v_2 = (2,3,4)\\), \\(v_3 = (3,5,7)\\)\u306b\u3064\u3044\u3066\u3001    (a) \\(\\text{span}\\{v_1, v_2, v_3\\}\\)\u306e\u6b21\u5143\u3092\u6c42\u3081\u3088\u3002    (b) \\(v_3\\)\u3092\\(v_1\\)\u3068\\(v_2\\)\u306e\u7dda\u5f62\u7d50\u5408\u3067\u8868\u305b\u3002</p> </li> <li> <p>\\(\\mathbb{R}^3\\)\u306b\u304a\u3051\u308b\u6a19\u6e96\u57fa\u5e95\\(E = \\{e_1, e_2, e_3\\}\\)\u304b\u3089\u5225\u306e\u57fa\u5e95\\(B = \\{(1,1,1), (1,1,0), (1,0,0)\\}\\)\u3078\u306e\u57fa\u5e95\u5909\u63db\u884c\u5217\u3092\u6c42\u3081\u3001\u30d9\u30af\u30c8\u30eb\\(v = (2,3,4)\\)\u3092\u57fa\u5e95\\(B\\)\u306b\u95a2\u3059\u308b\u5ea7\u6a19\u3067\u8868\u305b\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306e\u5fdc\u7528\u554f\u984c\uff1a    \u3042\u308b\u30d5\u30a3\u30c3\u30c8\u30cd\u30b9\u30c8\u30e9\u30c3\u30ab\u30fc\u304b\u3089\u53ce\u96c6\u3055\u308c\u305f3\u3064\u306e\u5909\u6570\uff08\u6b69\u6570\u3001\u5fc3\u62cd\u6570\u3001\u7761\u7720\u6642\u9593\uff09\u306e\u30c7\u30fc\u30bf\u304c\u3042\u308a\u307e\u3059\u3002\u65e5\u3005\u306e\u6e2c\u5b9a\u5024\u306f3\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u70b9\u3068\u3057\u3066\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> </li> </ol> <p>\u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u884c\u5217\\(X\\)\u306e\u5404\u884c\u306f1\u65e5\u5206\u306e\u30c7\u30fc\u30bf\u3092\u8868\u3057\u3066\u3044\u307e\u3059\uff08\u6a19\u6e96\u5316\u6e08\u307f\uff09\uff1a    <pre><code>X = [\n    [ 1.2,  0.8, -0.5],\n    [ 0.9,  0.6, -0.3],\n    [ 0.6,  0.4, -0.2],\n    [-1.5, -1.0,  0.6],\n    [-1.2, -0.8,  0.4]\n]\n</code></pre></p> <p>(a) \u3053\u306e\u884c\u5217\u306e\u5217\u30d9\u30af\u30c8\u30eb\u304c\u7dda\u5f62\u72ec\u7acb\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002    (b) \u30c7\u30fc\u30bf\u70b9\u304c\u4e3b\u306b\u5b58\u5728\u3059\u308b\u90e8\u5206\u7a7a\u9593\u306e\u6b21\u5143\u3068\u57fa\u5e95\u3092\u6c42\u3081\u3088\u3002    (c) \u3053\u306e\u30c7\u30fc\u30bf\u304b\u3089\u5065\u5eb7\u72b6\u614b\u306b\u95a2\u3057\u3066\u3069\u306e\u3088\u3046\u306a\u6d1e\u5bdf\u304c\u5f97\u3089\u308c\u308b\u304b\u8003\u5bdf\u305b\u3088\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/26-vector-space-and-inner-product/#q1","title":"Q1: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u90e8\u5206\u7a7a\u9593\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306f\u3001\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u304c\u5b9a\u7fa9\u3055\u308c\u305f\u96c6\u5408\u3067\u3042\u308a\u30018\u3064\u306e\u516c\u7406\u3092\u6e80\u305f\u3059\u3082\u306e\u3067\u3059\u3002\u90e8\u5206\u7a7a\u9593\u306f\u3001\u5143\u306e\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306b\u542b\u307e\u308c\u308b\u96c6\u5408\u3067\u3042\u308a\u3001\u305d\u308c\u81ea\u4f53\u3082\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306b\u306a\u3063\u3066\u3044\u308b\u3082\u306e\u3092\u6307\u3057\u307e\u3059\u3002\u90e8\u5206\u7a7a\u9593\u306f\u5fc5\u305a\u539f\u70b9\uff08\u96f6\u30d9\u30af\u30c8\u30eb\uff09\u3092\u542b\u307f\u3001\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u306b\u95a2\u3057\u3066\u9589\u3058\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#q2","title":"Q2: \u7dda\u5f62\u72ec\u7acb\u3068\u57fa\u5e95\u306e\u95a2\u4fc2\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A: \u57fa\u5e95\u306f\u7dda\u5f62\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u306e\u96c6\u5408\u3067\u3042\u308a\u3001\u304b\u3064\u305d\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u304c\u7a7a\u9593\u5168\u4f53\u3092\u751f\u6210\uff08\u30b9\u30d1\u30f3\uff09\u3059\u308b\u3082\u306e\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u57fa\u5e95\u306f\u300c\u5fc5\u8981\u6700\u5c0f\u9650\u300d\u306e\u30d9\u30af\u30c8\u30eb\u96c6\u5408\u3067\u3042\u308a\u30011\u3064\u3082\u7121\u99c4\u306a\u304f\uff08\u7dda\u5f62\u72ec\u7acb\uff09\u3001\u304b\u3064\u5341\u5206\u306b\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3092\u30ab\u30d0\u30fc\uff08\u751f\u6210\uff09\u3059\u308b\u3068\u3044\u3046\u7279\u5fb4\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#q3","title":"Q3: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u6b21\u5143\u3068\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u6b21\u5143\u3068\u306f\u3001\u305d\u306e\u7a7a\u9593\u306e\u57fa\u5e95\u306b\u542b\u307e\u308c\u308b\u30d9\u30af\u30c8\u30eb\u306e\u6570\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\\(\\mathbb{R}^3\\)\u306e\u6b21\u5143\u306f3\u3001\\(P_2(\\mathbb{R})\\)\uff082\u6b21\u4ee5\u4e0b\u306e\u591a\u9805\u5f0f\u7a7a\u9593\uff09\u306e\u6b21\u5143\u306f3\u3067\u3059\u3002\u6b21\u5143\u306f\u3001\u305d\u306e\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3092\u8868\u73fe\u3059\u308b\u306e\u306b\u6700\u4f4e\u9650\u5fc5\u8981\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6570\u3068\u8003\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#q4","title":"Q4: \u7dda\u5f62\u5f93\u5c5e\u306a\u30d9\u30af\u30c8\u30eb\u96c6\u5408\u304b\u3089\u57fa\u5e95\u3092\u53d6\u308a\u51fa\u3059\u306b\u306f\u3069\u3046\u3059\u308c\u3070\u3044\u3044\u3067\u3059\u304b\uff1f","text":"<p>A: \u7dda\u5f62\u5f93\u5c5e\u306a\u30d9\u30af\u30c8\u30eb\u96c6\u5408\u304b\u3089\u57fa\u5e95\u3092\u53d6\u308a\u51fa\u3059\u306b\u306f\u3001\u30ac\u30a6\u30b9\u30fb\u30b8\u30e7\u30eb\u30c0\u30f3\u6d88\u53bb\u6cd5\u3092\u7528\u3044\u3066\u884c\u7c21\u7d04\u5f62\uff08\u307e\u305f\u306f\u5217\u7c21\u7d04\u5f62\uff09\u306b\u5909\u63db\u3057\u3001\u30d4\u30dc\u30c3\u30c8\u5217\uff08\u307e\u305f\u306f\u884c\uff09\u306b\u5bfe\u5fdc\u3059\u308b\u30d9\u30af\u30c8\u30eb\u3092\u9078\u3079\u3070\u3088\u3044\u3067\u3059\u3002\u3042\u308b\u3044\u306f\u3001\u30d9\u30af\u30c8\u30eb\u30921\u3064\u305a\u3064\u8003\u616e\u3057\u3066\u3044\u304d\u3001\u65e2\u306b\u9078\u3093\u3060\u30d9\u30af\u30c8\u30eb\u3068\u7dda\u5f62\u72ec\u7acb\u306a\u3082\u306e\u3060\u3051\u3092\u8ffd\u52a0\u3057\u3066\u3044\u304f\u65b9\u6cd5\u3082\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/26-vector-space-and-inner-product/#q5","title":"Q5: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u5fdc\u7528\u4f8b\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306e\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u5fdc\u7528\u306f\u591a\u5c90\u306b\u308f\u305f\u308a\u307e\u3059\uff1a - \u6b21\u5143\u524a\u6e1b: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306f\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u4f4e\u6b21\u5143\u306e\u90e8\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u3057\u307e\u3059 - \u7279\u5fb4\u62bd\u51fa: \u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u3092\u8868\u73fe\u3059\u308b\u65b0\u3057\u3044\u57fa\u5e95\uff08\u4e3b\u6210\u5206\u306a\u3069\uff09\u3092\u898b\u3064\u3051\u307e\u3059 - \u6587\u66f8\u30d9\u30af\u30c8\u30eb\u5316: \u6587\u66f8\u3092\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u30e2\u30c7\u30eb\uff08VSM\uff09\u3067\u8868\u73fe\u3057\u3001\u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\u3057\u307e\u3059 - \u753b\u50cf\u51e6\u7406: \u753b\u50cf\u3092\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u6271\u3044\u3001\u7279\u5fb4\u7a7a\u9593\u3067\u5206\u6790\u3057\u307e\u3059 - \u63a8\u85a6\u30b7\u30b9\u30c6\u30e0: \u30e6\u30fc\u30b6\u30fc\u306e\u597d\u307f\u3092\u8868\u3059\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3092\u69cb\u7bc9\u3057\u3001\u985e\u4f3c\u6027\u3092\u6e2c\u5b9a\u3057\u307e\u3059</p> <p>\u3053\u308c\u3089\u306e\u5fdc\u7528\u3067\u306f\u3001\u30c7\u30fc\u30bf\u3092\u9069\u5207\u306a\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u8868\u73fe\u3059\u308b\u3053\u3068\u3067\u3001\u30d1\u30bf\u30fc\u30f3\u306e\u767a\u898b\u3084\u4e88\u6e2c\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u8b1b\u7fa9\u30ce\u30fc\u30c8\uff1a\u7b2c27\u56de","text":""},{"location":"lectures/LA/27-vector-space-and-inner-product/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c27\u56de \u30c6\u30fc\u30de: \u5185\u7a4d\u3068\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u30fb\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5 \u95a2\u9023\u9805\u76ee: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3001\u5185\u7a4d\u7a7a\u9593\u3001\u76f4\u4ea4\u57fa\u5e95\u3001\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5 \u4e88\u7fd2\u5185\u5bb9: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u672c\u7684\u306a\u6027\u8cea\uff08\u7b2c26\u56de\u306e\u5185\u5bb9\uff09\u3092\u5fa9\u7fd2\u3057\u3066\u304a\u304f\u3053\u3068\u3002\u7279\u306b\u57fa\u5e95\u306e\u6982\u5ff5\u3068\u7dda\u5f62\u72ec\u7acb\u6027\u306b\u3064\u3044\u3066\u7406\u89e3\u3057\u3066\u304a\u304f\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u3068\u5185\u7a4d\u7a7a\u9593\u306e\u5b9a\u7fa9\u3092\u7406\u89e3\u3057\u3001\u305d\u306e\u6027\u8cea\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u5185\u7a4d\u3092\u7528\u3044\u3066\u30d9\u30af\u30c8\u30eb\u9593\u306e\u89d2\u5ea6\u3092\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u76f4\u4ea4\u57fa\u5e95\u3068\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306e\u9055\u3044\u3092\u7406\u89e3\u3057\u3001\u305d\u306e\u6027\u8cea\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u3092\u7528\u3044\u3066\u3001\u4e0e\u3048\u3089\u308c\u305f\u57fa\u5e95\u304b\u3089\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u69cb\u6210\u3067\u304d\u308b</li> <li>\u5185\u7a4d\u7a7a\u9593\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u3092\u7406\u89e3\u3057\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u5fdc\u7528\u4f8b\u3092\u8aac\u660e\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/27-vector-space-and-inner-product/#31","title":"3.1 \u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d","text":""},{"location":"lectures/LA/27-vector-space-and-inner-product/#311","title":"3.1.1 \u5185\u7a4d\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9\uff08\u5185\u7a4d\uff09: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(V\\) \u4e0a\u306e\u5185\u7a4d\u3068\u306f\u3001\u4efb\u610f\u306e \\(\\boldsymbol{u}, \\boldsymbol{v}, \\boldsymbol{w} \\in V\\) \u3068\u4efb\u610f\u306e\u30b9\u30ab\u30e9\u30fc \\(c\\) \u306b\u5bfe\u3057\u3066\u4ee5\u4e0b\u306e\u6027\u8cea\u3092\u6e80\u305f\u3059\u4e8c\u9805\u6f14\u7b97 \\(\\langle \\cdot, \\cdot \\rangle: V \\times V \\rightarrow \\mathbb{R}\\) \u3067\u3042\u308b\u3002</p> <ol> <li>\u5bfe\u79f0\u6027: \\(\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle = \\langle \\boldsymbol{v}, \\boldsymbol{u} \\rangle\\)</li> <li>\u7dda\u5f62\u6027: \\(\\langle c\\boldsymbol{u} + \\boldsymbol{v}, \\boldsymbol{w} \\rangle = c\\langle \\boldsymbol{u}, \\boldsymbol{w} \\rangle + \\langle \\boldsymbol{v}, \\boldsymbol{w} \\rangle\\)</li> <li>\u6b63\u5b9a\u5024\u6027: \\(\\langle \\boldsymbol{v}, \\boldsymbol{v} \\rangle \\geq 0\\) \u304b\u3064 \\(\\langle \\boldsymbol{v}, \\boldsymbol{v} \\rangle = 0 \\Leftrightarrow \\boldsymbol{v} = \\boldsymbol{0}\\)</li> </ol> <p>\u5b9f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(\\mathbb{R}^n\\) \u306b\u304a\u3051\u308b\u6a19\u6e96\u5185\u7a4d\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle = \\boldsymbol{u}^T \\boldsymbol{v} = \\sum_{i=1}^{n} u_i v_i\\] <p>\u3053\u3053\u3067\u3001\\(\\boldsymbol{u} = (u_1, u_2, \\ldots, u_n)^T\\) \u304a\u3088\u3073 \\(\\boldsymbol{v} = (v_1, v_2, \\ldots, v_n)^T\\) \u3067\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#312","title":"3.1.2 \u5185\u7a4d\u3068\u30ce\u30eb\u30e0","text":"<p>\u5185\u7a4d\u3092\u7528\u3044\u3066\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\uff08\u9577\u3055\uff09\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <p>\u5b9a\u7fa9\uff08\u30ce\u30eb\u30e0\uff09: \u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{v}\\) \u306e\u30ce\u30eb\u30e0\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u3002</p> \\[\\|\\boldsymbol{v}\\| = \\sqrt{\\langle \\boldsymbol{v}, \\boldsymbol{v} \\rangle}\\] <p>\\(\\mathbb{R}^n\\) \u306b\u304a\u3051\u308b\u6a19\u6e96\u5185\u7a4d\u306b\u3088\u308b\u30ce\u30eb\u30e0\u306f\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u30ce\u30eb\u30e0\u3068\u547c\u3070\u308c\u307e\u3059\uff1a</p> \\[\\|\\boldsymbol{v}\\| = \\sqrt{\\langle \\boldsymbol{v}, \\boldsymbol{v} \\rangle} = \\sqrt{\\sum_{i=1}^{n} v_i^2}\\]"},{"location":"lectures/LA/27-vector-space-and-inner-product/#313","title":"3.1.3 \u5185\u7a4d\u3068\u89d2\u5ea6","text":"<p>\u5185\u7a4d\u3092\u7528\u3044\u30662\u3064\u306e\u30d9\u30af\u30c8\u30eb\u9593\u306e\u89d2\u5ea6\u3092\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a</p> <p>\u5b9a\u7406: \u975e\u96f6\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{u}, \\boldsymbol{v}\\) \u306e\u9593\u306e\u89d2\u5ea6 \\(\\theta\\) \u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u6c42\u3081\u3089\u308c\u308b\u3002</p> \\[\\cos \\theta = \\frac{\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle}{\\|\\boldsymbol{u}\\| \\|\\boldsymbol{v}\\|}\\] <p>\u3053\u306e\u95a2\u4fc2\u306f\u30b3\u30fc\u30b7\u30fc\u30fb\u30b7\u30e5\u30ef\u30eb\u30c4\u306e\u4e0d\u7b49\u5f0f\u304b\u3089\u5c0e\u304b\u308c\u307e\u3059\uff1a</p> <p>\u5b9a\u7406\uff08\u30b3\u30fc\u30b7\u30fc\u30fb\u30b7\u30e5\u30ef\u30eb\u30c4\u306e\u4e0d\u7b49\u5f0f\uff09: \u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{u}, \\boldsymbol{v}\\) \u306b\u5bfe\u3057\u3066\u4ee5\u4e0b\u304c\u6210\u308a\u7acb\u3064\u3002</p> \\[|\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle| \\leq \\|\\boldsymbol{u}\\| \\|\\boldsymbol{v}\\|\\] <p>\u7b49\u53f7\u306f \\(\\boldsymbol{u}\\) \u3068 \\(\\boldsymbol{v}\\) \u304c\u7dda\u5f62\u5f93\u5c5e\u3067\u3042\u308b\u3068\u304d\u3001\u304b\u3064\u305d\u306e\u3068\u304d\u306b\u9650\u308b\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#32","title":"3.2 \u5185\u7a4d\u7a7a\u9593","text":"<p>\u5b9a\u7fa9\uff08\u5185\u7a4d\u7a7a\u9593\uff09: \u5185\u7a4d\u304c\u5b9a\u7fa9\u3055\u308c\u305f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3092\u5185\u7a4d\u7a7a\u9593\u3068\u3044\u3046\u3002</p> <p>\u5185\u7a4d\u7a7a\u9593\u3067\u306f\u3001\u30d9\u30af\u30c8\u30eb\u9593\u306e\u89d2\u5ea6\u3084\u8ddd\u96e2\u304c\u5b9a\u7fa9\u3067\u304d\u308b\u305f\u3081\u3001\u5e7e\u4f55\u5b66\u7684\u306a\u8b70\u8ad6\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#33","title":"3.3 \u76f4\u4ea4\u6027","text":"<p>\u5b9a\u7fa9\uff08\u76f4\u4ea4\uff09: \u4e8c\u3064\u306e\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{u}, \\boldsymbol{v}\\) \u304c\u76f4\u4ea4\u3059\u308b\u3068\u306f\u3001\\(\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle = 0\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3067\u3042\u308b\u3002</p> <p>\u76f4\u4ea4\u3059\u308b\u30d9\u30af\u30c8\u30eb\u306f\u3001\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u4e92\u3044\u306b\u5782\u76f4\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9\uff08\u76f4\u4ea4\u88dc\u7a7a\u9593\uff09: \u90e8\u5206\u7a7a\u9593 \\(W\\) \u306b\u5bfe\u3059\u308b\u76f4\u4ea4\u88dc\u7a7a\u9593 \\(W^{\\perp}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u3002</p> \\[W^{\\perp} = \\{\\boldsymbol{v} \\in V \\mid \\langle \\boldsymbol{v}, \\boldsymbol{w} \\rangle = 0 \\text{ for all } \\boldsymbol{w} \\in W\\}\\]"},{"location":"lectures/LA/27-vector-space-and-inner-product/#34","title":"3.4 \u76f4\u4ea4\u57fa\u5e95\u3068\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95","text":"<p>\u5b9a\u7fa9\uff08\u76f4\u4ea4\u57fa\u5e95\uff09: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(V\\) \u306e\u57fa\u5e95 \\(\\{\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_n\\}\\) \u304c\u76f4\u4ea4\u57fa\u5e95\u3067\u3042\u308b\u3068\u306f\u3001\u4efb\u610f\u306e \\(i \\neq j\\) \u306b\u5bfe\u3057\u3066 \\(\\langle \\boldsymbol{v}_i, \\boldsymbol{v}_j \\rangle = 0\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3067\u3042\u308b\u3002</p> <p>\u5b9a\u7fa9\uff08\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\uff09: \u76f4\u4ea4\u57fa\u5e95 \\(\\{\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_n\\}\\) \u304c\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3067\u3042\u308b\u3068\u306f\u3001\u3059\u3079\u3066\u306e \\(i\\) \u306b\u5bfe\u3057\u3066 \\(\\|\\boldsymbol{v}_i\\| = 1\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3067\u3042\u308b\u3002\u3064\u307e\u308a\u3001\\(\\langle \\boldsymbol{v}_i, \\boldsymbol{v}_j \\rangle = \\delta_{ij}\\) \uff08\u30af\u30ed\u30cd\u30c3\u30ab\u30fc\u306e\u30c7\u30eb\u30bf\uff09\u304c\u6210\u308a\u7acb\u3064\u3002</p> <p>\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306f\u3001\u8a08\u7b97\u4e0a\u306e\u5229\u4fbf\u6027\u3084\u6570\u5024\u7684\u5b89\u5b9a\u6027\u306e\u89b3\u70b9\u304b\u3089\u3001\u591a\u304f\u306e\u6570\u5b66\u7684\u304a\u3088\u3073\u5fdc\u7528\u7684\u6587\u8108\u3067\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/27-vector-space-and-inner-product/#41","title":"4.1 \u5185\u7a4d\u306e\u5177\u4f53\u4f8b\u3068\u8a08\u7b97","text":"<p>\u4f8b\u984c1: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{u} = (1, 2, 3)^T\\) \u3068 \\(\\boldsymbol{v} = (4, 5, 6)^T\\) \u306e\u5185\u7a4d\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002</p> <p>\u89e3\u7b54: \u6a19\u6e96\u5185\u7a4d\u306e\u5b9a\u7fa9\u306b\u3088\u308a\u3001 \\(\\(\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle = \\boldsymbol{u}^T \\boldsymbol{v} = \\sum_{i=1}^{3} u_i v_i = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32\\)\\)</p> <p>\u4f8b\u984c2: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{u} = (1, 2, 3)^T\\) \u3068 \\(\\boldsymbol{v} = (4, 5, 6)^T\\) \u306e\u306a\u3059\u89d2\u5ea6\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002</p> <p>\u89e3\u7b54: \u307e\u305a\u3001\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002 \\(\\(\\|\\boldsymbol{u}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}\\)\\) \\(\\(\\|\\boldsymbol{v}\\| = \\sqrt{4^2 + 5^2 + 6^2} = \\sqrt{77}\\)\\)</p> <p>\u5185\u7a4d\u306f\u4e0a\u306e\u4f8b\u3088\u308a \\(\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle = 32\\) \u3067\u3059\u3002</p> <p>\u30b3\u30b5\u30a4\u30f3\u516c\u5f0f\u3088\u308a\u3001 \\(\\(\\cos \\theta = \\frac{\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle}{\\|\\boldsymbol{u}\\| \\|\\boldsymbol{v}\\|} = \\frac{32}{\\sqrt{14} \\cdot \\sqrt{77}} = \\frac{32}{\\sqrt{1078}} \\approx 0.9747\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\theta \\approx 0.2268\\) \u30e9\u30b8\u30a2\u30f3\u3001\u307e\u305f\u306f\u7d04 \\(13.0\\) \u5ea6\u3067\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#42","title":"4.2 \u76f4\u4ea4\u57fa\u5e95\u3068\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306e\u4f8b","text":"<p>\u4f8b\u984c3: \\(\\mathbb{R}^3\\) \u306b\u304a\u3051\u308b\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u96c6\u5408\u304c\u76f4\u4ea4\u57fa\u5e95\u304b\u3069\u3046\u304b\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002\u307e\u305f\u3001\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3067\u306a\u3044\u5834\u5408\u306f\u3001\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306b\u5909\u63db\u3057\u306a\u3055\u3044\u3002</p> \\[\\boldsymbol{v}_1 = (1, 1, 1)^T, \\boldsymbol{v}_2 = (1, -1, 0)^T, \\boldsymbol{v}_3 = (1, 1, -2)^T\\] <p>\u89e3\u7b54: \u307e\u305a\u3001\u5185\u7a4d\u3092\u8a08\u7b97\u3057\u3066\u76f4\u4ea4\u6027\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> \\[\\langle \\boldsymbol{v}_1, \\boldsymbol{v}_2 \\rangle = 1 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 0 = 0$$ $$\\langle \\boldsymbol{v}_1, \\boldsymbol{v}_3 \\rangle = 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot (-2) = 0$$ $$\\langle \\boldsymbol{v}_2, \\boldsymbol{v}_3 \\rangle = 1 \\cdot 1 + (-1) \\cdot 1 + 0 \\cdot (-2) = 0\\] <p>\u3059\u3079\u3066\u306e\u5185\u7a4d\u304c0\u306a\u306e\u3067\u3001\u3053\u306e\u96c6\u5408\u306f\u76f4\u4ea4\u57fa\u5e95\u3067\u3059\u3002</p> <p>\u6b21\u306b\u3001\u5404\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[\\|\\boldsymbol{v}_1\\| = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}$$ $$\\|\\boldsymbol{v}_2\\| = \\sqrt{1^2 + (-1)^2 + 0^2} = \\sqrt{2}$$ $$\\|\\boldsymbol{v}_3\\| = \\sqrt{1^2 + 1^2 + (-2)^2} = \\sqrt{6}\\] <p>\u30ce\u30eb\u30e0\u304c1\u3067\u306a\u3044\u306e\u3067\u3001\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> <p>\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306b\u5909\u63db\u3059\u308b\u306b\u306f\u3001\u5404\u30d9\u30af\u30c8\u30eb\u3092\u305d\u306e\u30ce\u30eb\u30e0\u3067\u5272\u308a\u307e\u3059\u3002</p> \\[\\boldsymbol{e}_1 = \\frac{\\boldsymbol{v}_1}{\\|\\boldsymbol{v}_1\\|} = \\frac{1}{\\sqrt{3}}(1, 1, 1)^T = \\left(\\frac{1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}\\right)^T\\] \\[\\boldsymbol{e}_2 = \\frac{\\boldsymbol{v}_2}{\\|\\boldsymbol{v}_2\\|} = \\frac{1}{\\sqrt{2}}(1, -1, 0)^T = \\left(\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}, 0\\right)^T\\] \\[\\boldsymbol{e}_3 = \\frac{\\boldsymbol{v}_3}{\\|\\boldsymbol{v}_3\\|} = \\frac{1}{\\sqrt{6}}(1, 1, -2)^T = \\left(\\frac{1}{\\sqrt{6}}, \\frac{1}{\\sqrt{6}}, -\\frac{2}{\\sqrt{6}}\\right)^T\\] <p>\u3053\u308c\u3067 \\(\\{\\boldsymbol{e}_1, \\boldsymbol{e}_2, \\boldsymbol{e}_3\\}\\) \u306f \\(\\mathbb{R}^3\\) \u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3068\u306a\u308a\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#43","title":"4.3 \u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5","text":"<p>\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u306f\u3001\u4e0e\u3048\u3089\u308c\u305f\u7dda\u5f62\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u96c6\u5408\u304b\u3089\u76f4\u4ea4\u57fa\u5e95\u3092\u69cb\u6210\u3059\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u3059\u3002</p> <p>\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff08\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\uff09:</p> <p>\u5165\u529b: \u7dda\u5f62\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u96c6\u5408 \\(\\{\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_n\\}\\)</p> <p>\u51fa\u529b: \u76f4\u4ea4\u57fa\u5e95 \\(\\{\\boldsymbol{u}_1, \\boldsymbol{u}_2, \\ldots, \\boldsymbol{u}_n\\}\\)</p> <ol> <li>\\(\\boldsymbol{u}_1 = \\boldsymbol{v}_1\\)</li> <li>\\(k = 2, 3, \\ldots, n\\) \u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u3092\u8a08\u7b97\u3059\u308b:</li> </ol> <p>\\(\\(\\boldsymbol{u}_k = \\boldsymbol{v}_k - \\sum_{i=1}^{k-1} \\frac{\\langle \\boldsymbol{v}_k, \\boldsymbol{u}_i \\rangle}{\\langle \\boldsymbol{u}_i, \\boldsymbol{u}_i \\rangle} \\boldsymbol{u}_i\\)\\)</p> <p>\u3053\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u306f\u3001\u5404\u30b9\u30c6\u30c3\u30d7\u3067\u65b0\u3057\u3044\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{v}_k\\) \u304b\u3089\u3001\u305d\u308c\u307e\u3067\u306b\u5f97\u3089\u308c\u305f\u76f4\u4ea4\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{u}_1, \\boldsymbol{u}_2, \\ldots, \\boldsymbol{u}_{k-1}\\) \u306e\u65b9\u5411\u3078\u306e\u6210\u5206\u3092\u53d6\u308a\u9664\u3044\u3066\u3044\u307e\u3059\u3002</p> <p>\u76f4\u4ea4\u5316\u5f8c\u3001\u5404\u30d9\u30af\u30c8\u30eb\u3092\u305d\u306e\u30ce\u30eb\u30e0\u3067\u5272\u308b\u3053\u3068\u3067\u3001\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> \\[\\boldsymbol{e}_k = \\frac{\\boldsymbol{u}_k}{\\|\\boldsymbol{u}_k\\|}\\] <p>\u4f8b\u984c4: \\(\\mathbb{R}^3\\) \u306b\u304a\u3051\u308b\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u96c6\u5408\u306b\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u3092\u9069\u7528\u3057\u3001\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> \\[\\boldsymbol{v}_1 = (2, 0, 1)^T, \\boldsymbol{v}_2 = (1, 1, 0)^T, \\boldsymbol{v}_3 = (0, 1, 2)^T\\] <p>\u89e3\u7b54: \u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u5f93\u3063\u3066\u8a08\u7b97\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#1-boldsymbolu_1","title":"\u30b9\u30c6\u30c3\u30d71: \\(\\boldsymbol{u}_1\\) \u306e\u8a08\u7b97","text":"<p>\u6700\u521d\u306e\u30d9\u30af\u30c8\u30eb\u306f\u305d\u306e\u307e\u307e\u63a1\u7528\u3057\u307e\u3059\uff1a</p> \\[\\boldsymbol{u}_1 = \\boldsymbol{v}_1 = (2, 0, 1)^T\\]"},{"location":"lectures/LA/27-vector-space-and-inner-product/#2-boldsymbolu_2","title":"\u30b9\u30c6\u30c3\u30d72: \\(\\boldsymbol{u}_2\\) \u306e\u8a08\u7b97","text":"<p>\\(\\boldsymbol{u}_2\\)\u3092\u6c42\u3081\u308b\u305f\u3081\u306b\u3001\\(\\boldsymbol{v}_2\\)\u304b\u3089\\(\\boldsymbol{u}_1\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u6210\u5206\u3092\u5f15\u304d\u307e\u3059\u3002</p> <p>\u307e\u305a\u3001\\(\\boldsymbol{v}_2\\) \u3068 \\(\\boldsymbol{u}_1\\) \u306e\u5185\u7a4d\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\langle \\boldsymbol{v}_2, \\boldsymbol{u}_1 \\rangle = 1 \\cdot 2 + 1 \\cdot 0 + 0 \\cdot 1 = 2\\)\\)</p> <p>\u6b21\u306b\u3001\\(\\boldsymbol{u}_1\\) \u306e\u30ce\u30eb\u30e0\u306e\u4e8c\u4e57\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\langle \\boldsymbol{u}_1, \\boldsymbol{u}_1 \\rangle = 2^2 + 0^2 + 1^2 = 4 + 0 + 1 = 5\\)\\)</p> <p>\\(\\boldsymbol{u}_1\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u4fc2\u6570\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a \\(\\(\\frac{\\langle \\boldsymbol{v}_2, \\boldsymbol{u}_1 \\rangle}{\\langle \\boldsymbol{u}_1, \\boldsymbol{u}_1 \\rangle} = \\frac{2}{5}\\)\\)</p> <p>\\(\\boldsymbol{u}_1\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u306f\uff1a \\(\\(\\frac{\\langle \\boldsymbol{v}_2, \\boldsymbol{u}_1 \\rangle}{\\langle \\boldsymbol{u}_1, \\boldsymbol{u}_1 \\rangle} \\boldsymbol{u}_1 = \\frac{2}{5} \\cdot (2, 0, 1)^T = \\left(\\frac{4}{5}, 0, \\frac{2}{5}\\right)^T\\)\\)</p> <p>\u3053\u308c\u3092\\(\\boldsymbol{v}_2\\)\u304b\u3089\u5f15\u3044\u3066\\(\\boldsymbol{u}_2\\)\u3092\u6c42\u3081\u307e\u3059\uff1a \\(\\(\\begin{align*} \\boldsymbol{u}_2 &amp;= \\boldsymbol{v}_2 - \\frac{\\langle \\boldsymbol{v}_2, \\boldsymbol{u}_1 \\rangle}{\\langle \\boldsymbol{u}_1, \\boldsymbol{u}_1 \\rangle} \\boldsymbol{u}_1 \\\\ &amp;= (1, 1, 0)^T - \\left(\\frac{4}{5}, 0, \\frac{2}{5}\\right)^T \\\\ &amp;= \\left(1 - \\frac{4}{5}, 1 - 0, 0 - \\frac{2}{5}\\right)^T \\\\ &amp;= \\left(\\frac{5 - 4}{5}, 1, -\\frac{2}{5}\\right)^T \\\\ &amp;= \\left(\\frac{1}{5}, 1, -\\frac{2}{5}\\right)^T \\end{align*}\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\boldsymbol{u}_2 = \\left(\\frac{1}{5}, 1, -\\frac{2}{5}\\right)^T\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#3-boldsymbolu_3","title":"\u30b9\u30c6\u30c3\u30d73: \\(\\boldsymbol{u}_3\\) \u306e\u8a08\u7b97","text":"<p>\\(\\boldsymbol{u}_3\\)\u3092\u6c42\u3081\u308b\u305f\u3081\u306b\u3001\\(\\boldsymbol{v}_3\\)\u304b\u3089\\(\\boldsymbol{u}_1\\)\u65b9\u5411\u3068\\(\\boldsymbol{u}_2\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u6210\u5206\u3092\u5f15\u304d\u307e\u3059\u3002</p> <p>\u307e\u305a\u3001\\(\\boldsymbol{v}_3\\) \u3068 \\(\\boldsymbol{u}_1\\) \u306e\u5185\u7a4d\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\langle \\boldsymbol{v}_3, \\boldsymbol{u}_1 \\rangle = 0 \\cdot 2 + 1 \\cdot 0 + 2 \\cdot 1 = 0 + 0 + 2 = 2\\)\\)</p> <p>\\(\\boldsymbol{u}_1\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u4fc2\u6570\u306f\uff1a \\(\\(\\frac{\\langle \\boldsymbol{v}_3, \\boldsymbol{u}_1 \\rangle}{\\langle \\boldsymbol{u}_1, \\boldsymbol{u}_1 \\rangle} = \\frac{2}{5}\\)\\)</p> <p>\\(\\boldsymbol{u}_1\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u306f\uff1a \\(\\(\\frac{\\langle \\boldsymbol{v}_3, \\boldsymbol{u}_1 \\rangle}{\\langle \\boldsymbol{u}_1, \\boldsymbol{u}_1 \\rangle} \\boldsymbol{u}_1 = \\frac{2}{5} \\cdot (2, 0, 1)^T = \\left(\\frac{4}{5}, 0, \\frac{2}{5}\\right)^T\\)\\)</p> <p>\u6b21\u306b\u3001\\(\\boldsymbol{v}_3\\) \u3068 \\(\\boldsymbol{u}_2\\) \u306e\u5185\u7a4d\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\begin{align*} \\langle \\boldsymbol{v}_3, \\boldsymbol{u}_2 \\rangle &amp;= 0 \\cdot \\frac{1}{5} + 1 \\cdot 1 + 2 \\cdot \\left(-\\frac{2}{5}\\right) \\\\ &amp;= 0 + 1 + \\left(-\\frac{4}{5}\\right) \\\\ &amp;= 1 - \\frac{4}{5} \\\\ &amp;= \\frac{5 - 4}{5} \\\\ &amp;= \\frac{1}{5} \\end{align*}\\)\\)</p> <p>\u6b21\u306b\u3001\\(\\boldsymbol{u}_2\\) \u306e\u30ce\u30eb\u30e0\u306e\u4e8c\u4e57\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\begin{align*} \\langle \\boldsymbol{u}_2, \\boldsymbol{u}_2 \\rangle &amp;= \\left(\\frac{1}{5}\\right)^2 + 1^2 + \\left(-\\frac{2}{5}\\right)^2 \\\\ &amp;= \\frac{1}{25} + 1 + \\frac{4}{25} \\\\ &amp;= \\frac{1}{25} + \\frac{25}{25} + \\frac{4}{25} \\\\ &amp;= \\frac{1 + 25 + 4}{25} \\\\ &amp;= \\frac{30}{25} \\\\ &amp;= \\frac{6}{5} \\end{align*}\\)\\)</p> <p>\\(\\boldsymbol{u}_2\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u4fc2\u6570\u306f\uff1a \\(\\(\\frac{\\langle \\boldsymbol{v}_3, \\boldsymbol{u}_2 \\rangle}{\\langle \\boldsymbol{u}_2, \\boldsymbol{u}_2 \\rangle} = \\frac{\\frac{1}{5}}{\\frac{6}{5}} = \\frac{1}{6}\\)\\)</p> <p>\\(\\boldsymbol{u}_2\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u306f\uff1a \\(\\(\\begin{align*} \\frac{\\langle \\boldsymbol{v}_3, \\boldsymbol{u}_2 \\rangle}{\\langle \\boldsymbol{u}_2, \\boldsymbol{u}_2 \\rangle} \\boldsymbol{u}_2 &amp;= \\frac{1}{6} \\cdot \\left(\\frac{1}{5}, 1, -\\frac{2}{5}\\right)^T \\\\ &amp;= \\left(\\frac{1}{6} \\cdot \\frac{1}{5}, \\frac{1}{6} \\cdot 1, \\frac{1}{6} \\cdot \\left(-\\frac{2}{5}\\right)\\right)^T \\\\ &amp;= \\left(\\frac{1}{30}, \\frac{1}{6}, -\\frac{2}{30}\\right)^T \\\\ &amp;= \\left(\\frac{1}{30}, \\frac{1}{6}, -\\frac{1}{15}\\right)^T \\end{align*}\\)\\)</p> <p>\u3053\u308c\u3089\u306e\u5c04\u5f71\u30d9\u30af\u30c8\u30eb\u3092\\(\\boldsymbol{v}_3\\)\u304b\u3089\u5f15\u3044\u3066\\(\\boldsymbol{u}_3\\)\u3092\u6c42\u3081\u307e\u3059\uff1a \\(\\(\\begin{align*} \\boldsymbol{u}_3 &amp;= \\boldsymbol{v}_3 - \\frac{\\langle \\boldsymbol{v}_3, \\boldsymbol{u}_1 \\rangle}{\\langle \\boldsymbol{u}_1, \\boldsymbol{u}_1 \\rangle} \\boldsymbol{u}_1 - \\frac{\\langle \\boldsymbol{v}_3, \\boldsymbol{u}_2 \\rangle}{\\langle \\boldsymbol{u}_2, \\boldsymbol{u}_2 \\rangle} \\boldsymbol{u}_2 \\\\ &amp;= (0, 1, 2)^T - \\left(\\frac{4}{5}, 0, \\frac{2}{5}\\right)^T - \\left(\\frac{1}{30}, \\frac{1}{6}, -\\frac{1}{15}\\right)^T \\end{align*}\\)\\)</p> <p>\u7b2c1\u6210\u5206\uff1a \\(\\(\\begin{align*} 0 - \\frac{4}{5} - \\frac{1}{30} &amp;= -\\frac{4}{5} - \\frac{1}{30} \\\\ &amp;= -\\frac{24}{30} - \\frac{1}{30} \\\\ &amp;= -\\frac{25}{30} \\\\ &amp;= -\\frac{5}{6} \\end{align*}\\)\\)</p> <p>\u7b2c2\u6210\u5206\uff1a \\(\\(\\begin{align*} 1 - 0 - \\frac{1}{6} &amp;= 1 - \\frac{1}{6} \\\\ &amp;= \\frac{6}{6} - \\frac{1}{6} \\\\ &amp;= \\frac{5}{6} \\end{align*}\\)\\)</p> <p>\u7b2c3\u6210\u5206\uff1a \\(\\(\\begin{align*} 2 - \\frac{2}{5} - \\left(-\\frac{1}{15}\\right) &amp;= 2 - \\frac{2}{5} + \\frac{1}{15} \\\\ &amp;= \\frac{30}{15} - \\frac{6}{15} + \\frac{1}{15} \\\\ &amp;= \\frac{30 - 6 + 1}{15} \\\\ &amp;= \\frac{25}{15} \\\\ &amp;= \\frac{5}{3} \\end{align*}\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\boldsymbol{u}_3 = \\left(-\\frac{5}{6}, \\frac{5}{6}, \\frac{5}{3}\\right)^T\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#_1","title":"\u76f4\u4ea4\u57fa\u5e95","text":"<p>\u3053\u308c\u3067\u76f4\u4ea4\u57fa\u5e95 \\(\\{\\boldsymbol{u}_1, \\boldsymbol{u}_2, \\boldsymbol{u}_3\\}\\) \u304c\u5f97\u3089\u308c\u307e\u3057\u305f\uff1a</p> \\[\\boldsymbol{u}_1 = (2, 0, 1)^T, \\boldsymbol{u}_2 = \\left(\\frac{1}{5}, 1, -\\frac{2}{5}\\right)^T, \\boldsymbol{u}_3 = \\left(-\\frac{5}{6}, \\frac{5}{6}, \\frac{5}{3}\\right)^T\\]"},{"location":"lectures/LA/27-vector-space-and-inner-product/#_2","title":"\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306e\u8a08\u7b97","text":"<p>\u6b21\u306b\u3001\u5404\u30d9\u30af\u30c8\u30eb\u3092\u305d\u306e\u30ce\u30eb\u30e0\u3067\u5272\u3063\u3066\u6b63\u898f\u5316\u3057\u307e\u3059\u3002</p> <p>\\(\\boldsymbol{u}_1\\)\u306e\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\begin{align*} \\|\\boldsymbol{u}_1\\| &amp;= \\sqrt{2^2 + 0^2 + 1^2} \\\\ &amp;= \\sqrt{4 + 0 + 1} \\\\ &amp;= \\sqrt{5} \\end{align*}\\)\\)</p> <p>\\(\\boldsymbol{e}_1\\)\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\begin{align*} \\boldsymbol{e}_1 &amp;= \\frac{\\boldsymbol{u}_1}{\\|\\boldsymbol{u}_1\\|} \\\\ &amp;= \\frac{1}{\\sqrt{5}}(2, 0, 1)^T \\\\ &amp;= \\left(\\frac{2}{\\sqrt{5}}, 0, \\frac{1}{\\sqrt{5}}\\right)^T \\end{align*}\\)\\)</p> <p>\\(\\boldsymbol{u}_2\\)\u306e\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\begin{align*} \\|\\boldsymbol{u}_2\\| &amp;= \\sqrt{\\left(\\frac{1}{5}\\right)^2 + 1^2 + \\left(-\\frac{2}{5}\\right)^2} \\\\ &amp;= \\sqrt{\\frac{1}{25} + 1 + \\frac{4}{25}} \\\\ &amp;= \\sqrt{\\frac{1 + 25 + 4}{25}} \\\\ &amp;= \\sqrt{\\frac{30}{25}} \\\\ &amp;= \\sqrt{\\frac{6}{5}} \\end{align*}\\)\\)</p> <p>\\(\\boldsymbol{e}_2\\)\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\begin{align*} \\boldsymbol{e}_2 &amp;= \\frac{\\boldsymbol{u}_2}{\\|\\boldsymbol{u}_2\\|} \\\\ &amp;= \\frac{1}{\\sqrt{\\frac{6}{5}}}\\left(\\frac{1}{5}, 1, -\\frac{2}{5}\\right)^T \\\\ &amp;= \\sqrt{\\frac{5}{6}}\\left(\\frac{1}{5}, 1, -\\frac{2}{5}\\right)^T \\end{align*}\\)\\)</p> <p>\u7b2c1\u6210\u5206\uff1a \\(\\(\\begin{align*} \\sqrt{\\frac{5}{6}} \\cdot \\frac{1}{5} &amp;= \\frac{\\sqrt{5}}{\\sqrt{6}} \\cdot \\frac{1}{5} \\\\ &amp;= \\frac{\\sqrt{5}}{5\\sqrt{6}} \\\\ &amp;= \\frac{1}{5} \\cdot \\frac{\\sqrt{5}}{\\sqrt{6}} \\\\ &amp;= \\frac{1}{5} \\cdot \\frac{\\sqrt{30}}{\\sqrt{36}} \\\\ &amp;= \\frac{1}{5} \\cdot \\frac{\\sqrt{30}}{6} \\\\ &amp;= \\frac{\\sqrt{30}}{30} \\end{align*}\\)\\)</p> <p>\u7b2c2\u6210\u5206\uff1a \\(\\(\\begin{align*} \\sqrt{\\frac{5}{6}} \\cdot 1 &amp;= \\frac{\\sqrt{5}}{\\sqrt{6}} \\\\ &amp;= \\frac{\\sqrt{5}}{\\sqrt{6}} \\\\ &amp;= \\frac{\\sqrt{30}}{\\sqrt{36}} \\\\ &amp;= \\frac{\\sqrt{30}}{6} \\end{align*}\\)\\)</p> <p>\u7b2c3\u6210\u5206\uff1a \\(\\(\\begin{align*} \\sqrt{\\frac{5}{6}} \\cdot \\left(-\\frac{2}{5}\\right) &amp;= -\\frac{2}{5} \\cdot \\frac{\\sqrt{5}}{\\sqrt{6}} \\\\ &amp;= -\\frac{2\\sqrt{5}}{5\\sqrt{6}} \\\\ &amp;= -\\frac{2\\sqrt{30}}{5\\sqrt{36}} \\\\ &amp;= -\\frac{2\\sqrt{30}}{5 \\cdot 6} \\\\ &amp;= -\\frac{2\\sqrt{30}}{30} \\end{align*}\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\boldsymbol{e}_2 = \\left(\\frac{\\sqrt{30}}{30}, \\frac{\\sqrt{30}}{6}, -\\frac{2\\sqrt{30}}{30}\\right)^T\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\\(\\boldsymbol{u}_3\\)\u306e\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\begin{align*} \\|\\boldsymbol{u}_3\\| &amp;= \\sqrt{\\left(-\\frac{5}{6}\\right)^2 + \\left(\\frac{5}{6}\\right)^2 + \\left(\\frac{5}{3}\\right)^2} \\\\ &amp;= \\sqrt{\\frac{25}{36} + \\frac{25}{36} + \\frac{25}{9}} \\\\ &amp;= \\sqrt{\\frac{25}{36} \\cdot 2 + \\frac{25}{9}} \\\\ &amp;= \\sqrt{\\frac{25}{36} \\cdot 2 + \\frac{100}{36}} \\\\ &amp;= \\sqrt{\\frac{50}{36} + \\frac{100}{36}} \\\\ &amp;= \\sqrt{\\frac{150}{36}} \\\\ &amp;= \\sqrt{\\frac{25 \\cdot 6}{36}} \\\\ &amp;= \\frac{5}{\\sqrt{6}} \\end{align*}\\)\\)</p> <p>\\(\\boldsymbol{e}_3\\)\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\begin{align*} \\boldsymbol{e}_3 &amp;= \\frac{\\boldsymbol{u}_3}{\\|\\boldsymbol{u}_3\\|} \\\\ &amp;= \\frac{1}{\\frac{5}{\\sqrt{6}}}\\left(-\\frac{5}{6}, \\frac{5}{6}, \\frac{5}{3}\\right)^T \\\\ &amp;= \\frac{\\sqrt{6}}{5}\\left(-\\frac{5}{6}, \\frac{5}{6}, \\frac{5}{3}\\right)^T \\end{align*}\\)\\)</p> <p>\u7b2c1\u6210\u5206\uff1a \\(\\(\\begin{align*} \\frac{\\sqrt{6}}{5} \\cdot \\left(-\\frac{5}{6}\\right) &amp;= -\\frac{5\\sqrt{6}}{5 \\cdot 6} \\\\ &amp;= -\\frac{\\sqrt{6}}{6} \\end{align*}\\)\\)</p> <p>\u7b2c2\u6210\u5206\uff1a \\(\\(\\begin{align*} \\frac{\\sqrt{6}}{5} \\cdot \\frac{5}{6} &amp;= \\frac{5\\sqrt{6}}{5 \\cdot 6} \\\\ &amp;= \\frac{\\sqrt{6}}{6} \\end{align*}\\)\\)</p> <p>\u7b2c3\u6210\u5206\uff1a \\(\\(\\begin{align*} \\frac{\\sqrt{6}}{5} \\cdot \\frac{5}{3} &amp;= \\frac{5\\sqrt{6}}{5 \\cdot 3} \\\\ &amp;= \\frac{\\sqrt{6}}{3} \\end{align*}\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(\\boldsymbol{e}_3 = \\left(-\\frac{\\sqrt{6}}{6}, \\frac{\\sqrt{6}}{6}, \\frac{\\sqrt{6}}{3}\\right)^T\\) \u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#_3","title":"\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95","text":"<p>\u4ee5\u4e0a\u306e\u8a08\u7b97\u304b\u3089\u3001\u5f97\u3089\u308c\u305f\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95 \\(\\{\\boldsymbol{e}_1, \\boldsymbol{e}_2, \\boldsymbol{e}_3\\}\\) \u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> \\[\\boldsymbol{e}_1 = \\left(\\frac{2}{\\sqrt{5}}, 0, \\frac{1}{\\sqrt{5}}\\right)^T\\] \\[\\boldsymbol{e}_2 = \\left(\\frac{\\sqrt{30}}{30}, \\frac{\\sqrt{30}}{6}, -\\frac{2\\sqrt{30}}{30}\\right)^T\\] \\[\\boldsymbol{e}_3 = \\left(-\\frac{\\sqrt{6}}{6}, \\frac{\\sqrt{6}}{6}, \\frac{\\sqrt{6}}{3}\\right)^T\\]"},{"location":"lectures/LA/27-vector-space-and-inner-product/#_4","title":"\u691c\u8a3c","text":"<p>\u3053\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u304c\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306b\u3001\u5404\u30da\u30a2\u306e\u5185\u7a4d\u304c0\u3067\u3042\u308a\u3001\u5404\u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0\u304c1\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> \\[\\langle \\boldsymbol{e}_1, \\boldsymbol{e}_2 \\rangle = \\frac{2}{\\sqrt{5}} \\cdot \\frac{\\sqrt{30}}{30} + 0 \\cdot \\frac{\\sqrt{30}}{6} + \\frac{1}{\\sqrt{5}} \\cdot \\left(-\\frac{2\\sqrt{30}}{30}\\right) = 0\\] \\[\\langle \\boldsymbol{e}_1, \\boldsymbol{e}_3 \\rangle = \\frac{2}{\\sqrt{5}} \\cdot \\left(-\\frac{\\sqrt{6}}{6}\\right) + 0 \\cdot \\frac{\\sqrt{6}}{6} + \\frac{1}{\\sqrt{5}} \\cdot \\frac{\\sqrt{6}}{3} = 0\\] \\[\\langle \\boldsymbol{e}_2, \\boldsymbol{e}_3 \\rangle = \\frac{\\sqrt{30}}{30} \\cdot \\left(-\\frac{\\sqrt{6}}{6}\\right) + \\frac{\\sqrt{30}}{6} \\cdot \\frac{\\sqrt{6}}{6} + \\left(-\\frac{2\\sqrt{30}}{30}\\right) \\cdot \\frac{\\sqrt{6}}{3} = 0\\] \\[\\|\\boldsymbol{e}_1\\|^2 = \\left(\\frac{2}{\\sqrt{5}}\\right)^2 + 0^2 + \\left(\\frac{1}{\\sqrt{5}}\\right)^2 = \\frac{4}{5} + \\frac{1}{5} = 1\\] \\[\\|\\boldsymbol{e}_2\\|^2 = \\left(\\frac{\\sqrt{30}}{30}\\right)^2 + \\left(\\frac{\\sqrt{30}}{6}\\right)^2 + \\left(-\\frac{2\\sqrt{30}}{30}\\right)^2 = 1\\] \\[\\|\\boldsymbol{e}_3\\|^2 = \\left(-\\frac{\\sqrt{6}}{6}\\right)^2 + \\left(\\frac{\\sqrt{6}}{6}\\right)^2 + \\left(\\frac{\\sqrt{6}}{3}\\right)^2 = 1\\] <p>\u3053\u308c\u3089\u306e\u8a08\u7b97\u7d50\u679c\u304b\u3089\u3001\\(\\{\\boldsymbol{e}_1, \\boldsymbol{e}_2, \\boldsymbol{e}_3\\}\\) \u306f \\(\\mathbb{R}^3\\) \u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3067\u3042\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#_5","title":"\u307e\u3068\u3081","text":"<p>\u3053\u306e\u4f8b\u3067\u306f\u3001\\(\\mathbb{R}^3\\) \u306b\u304a\u3051\u308b\u7dda\u5f62\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u96c6\u5408\u306b\u5bfe\u3057\u3066\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u3092\u9069\u7528\u3057\u3001\u5404\u30b9\u30c6\u30c3\u30d7\u306e\u8a73\u7d30\u306a\u8a08\u7b97\u904e\u7a0b\u3092\u793a\u3057\u307e\u3057\u305f\u3002\u3053\u306e\u30d7\u30ed\u30bb\u30b9\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u6700\u521d\u306e\u30d9\u30af\u30c8\u30eb\\(\\boldsymbol{v}_1\\)\u3092\u63a1\u7528\u3057\u3066\\(\\boldsymbol{u}_1\\)\u3068\u3059\u308b</li> <li>\\(\\boldsymbol{v}_2\\)\u304b\u3089\\(\\boldsymbol{u}_1\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u3092\u9664\u53bb\u3057\u3066\\(\\boldsymbol{u}_2\\)\u3092\u5f97\u308b</li> <li>\\(\\boldsymbol{v}_3\\)\u304b\u3089\\(\\boldsymbol{u}_1\\)\u3068\\(\\boldsymbol{u}_2\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u3092\u9664\u53bb\u3057\u3066\\(\\boldsymbol{u}_3\\)\u3092\u5f97\u308b</li> <li>\u5404\\(\\boldsymbol{u}_i\\)\u3092\u305d\u306e\u30ce\u30eb\u30e0\u3067\u5272\u3063\u3066\\(\\boldsymbol{e}_i\\)\u3092\u5f97\u308b</li> </ol> <p>\u3053\u306e\u3088\u3046\u306b\u3057\u3066\u5f97\u3089\u308c\u305f\u57fa\u5e95\\(\\{\\boldsymbol{e}_1, \\boldsymbol{e}_2, \\boldsymbol{e}_3\\}\\)\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3057\u3001\u3059\u3079\u3066\u306e\u30d9\u30af\u30c8\u30eb\u306e\u9577\u3055\u304c1\u3067\u3042\u308b\u305f\u3081\u3001\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/27-vector-space-and-inner-product/#51","title":"5.1 \u5185\u7a4d\u3068\u89d2\u5ea6\u306e\u8a08\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\nu = np.array([1, 2, 3])\nv = np.array([4, 5, 6])\n\n# \u5185\u7a4d\u306e\u8a08\u7b97\ndot_product = np.dot(u, v)\nprint(f\"\u5185\u7a4d: {dot_product}\")\n\n# \u30ce\u30eb\u30e0\u306e\u8a08\u7b97\nnorm_u = np.linalg.norm(u)\nnorm_v = np.linalg.norm(v)\nprint(f\"u \u306e\u30ce\u30eb\u30e0: {norm_u}\")\nprint(f\"v \u306e\u30ce\u30eb\u30e0: {norm_v}\")\n\n# \u89d2\u5ea6\u306e\u8a08\u7b97\ncos_theta = dot_product / (norm_u * norm_v)\ntheta_rad = np.arccos(np.clip(cos_theta, -1.0, 1.0))  # \u6570\u5024\u8aa4\u5dee\u5bfe\u7b56\u306e\u305f\u3081clip\ntheta_deg = np.degrees(theta_rad)\nprint(f\"\u30d9\u30af\u30c8\u30eb\u9593\u306e\u89d2\u5ea6: {theta_rad:.4f} \u30e9\u30b8\u30a2\u30f3 ({theta_deg:.2f} \u5ea6)\")\n\n# 3D\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# \u539f\u70b9\norigin = np.zeros(3)\n\n# \u30d9\u30af\u30c8\u30eb\u306e\u53ef\u8996\u5316\nax.quiver(*origin, *u, color='r', label='u = [1, 2, 3]')\nax.quiver(*origin, *v, color='b', label='v = [4, 5, 6]')\n\n# \u30b0\u30e9\u30d5\u306e\u8a2d\u5b9a\nax.set_xlim([0, 6])\nax.set_ylim([0, 6])\nax.set_zlim([0, 6])\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.legend()\nax.set_title('3D \u30d9\u30af\u30c8\u30eb\u306e\u53ef\u8996\u5316')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#52","title":"5.2 \u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u306e\u5b9f\u88c5","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef gram_schmidt(vectors):\n    \"\"\"\n    \u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u306e\u5b9f\u88c5\n\n    Parameters:\n    vectors -- \u7dda\u5f62\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u96c6\u5408 (\u5404\u30d9\u30af\u30c8\u30eb\u306f\u884c\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u683c\u7d0d)\n\n    Returns:\n    orthogonal -- \u76f4\u4ea4\u57fa\u5e95\n    orthonormal -- \u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\n    \"\"\"\n    n = len(vectors)\n    orthogonal = np.zeros_like(vectors, dtype=float)\n\n    for i in range(n):\n        orthogonal[i] = vectors[i].copy()\n        for j in range(i):\n            # vectors[i] \u304b\u3089 orthogonal[j] \u65b9\u5411\u3078\u306e\u6210\u5206\u3092\u5f15\u304f\n            projection = np.dot(vectors[i], orthogonal[j]) / np.dot(orthogonal[j], orthogonal[j])\n            orthogonal[i] = orthogonal[i] - projection * orthogonal[j]\n\n    # \u6b63\u898f\u5316\n    orthonormal = np.zeros_like(orthogonal)\n    for i in range(n):\n        norm = np.linalg.norm(orthogonal[i])\n        orthonormal[i] = orthogonal[i] / norm\n\n    return orthogonal, orthonormal\n\n# \u4f8b\u984c4\u306e\u30d9\u30af\u30c8\u30eb\u96c6\u5408\nvectors = np.array([\n    [1, 0, 0],\n    [1, 1, 0],\n    [1, 1, 1]\n])\n\n# \u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u306e\u9069\u7528\northogonal, orthonormal = gram_schmidt(vectors)\n\n# \u7d50\u679c\u306e\u8868\u793a\nprint(\"\u5143\u306e\u30d9\u30af\u30c8\u30eb\u96c6\u5408:\")\nfor i, v in enumerate(vectors):\n    print(f\"v{i+1} = {v}\")\n\nprint(\"\\n\u76f4\u4ea4\u57fa\u5e95:\")\nfor i, u in enumerate(orthogonal):\n    print(f\"u{i+1} = {u}\")\n\nprint(\"\\n\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95:\")\nfor i, e in enumerate(orthonormal):\n    print(f\"e{i+1} = {e}\")\n\n# \u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u3092\u78ba\u8a8d\nprint(\"\\n\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306e\u5185\u7a4d\u884c\u5217:\")\ninner_products = np.zeros((3, 3))\nfor i in range(3):\n    for j in range(3):\n        inner_products[i, j] = np.dot(orthonormal[i], orthonormal[j])\nprint(inner_products)\n\n# 3D\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210\nfig = plt.figure(figsize=(15, 5))\n\n# \u5143\u306e\u30d9\u30af\u30c8\u30eb\nax1 = fig.add_subplot(131, projection='3d')\norigin = np.zeros(3)\nfor i, v in enumerate(vectors):\n    ax1.quiver(*origin, *v, label=f'v{i+1}')\nax1.set_xlim([-0.5, 1.5])\nax1.set_ylim([-0.5, 1.5])\nax1.set_zlim([-0.5, 1.5])\nax1.legend()\nax1.set_title('\u5143\u306e\u30d9\u30af\u30c8\u30eb')\n\n# \u76f4\u4ea4\u57fa\u5e95\nax2 = fig.add_subplot(132, projection='3d')\nfor i, u in enumerate(orthogonal):\n    ax2.quiver(*origin, *u, label=f'u{i+1}')\nax2.set_xlim([-0.5, 1.5])\nax2.set_ylim([-0.5, 1.5])\nax2.set_zlim([-0.5, 1.5])\nax2.legend()\nax2.set_title('\u76f4\u4ea4\u57fa\u5e95')\n\n# \u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\nax3 = fig.add_subplot(133, projection='3d')\nfor i, e in enumerate(orthonormal):\n    ax3.quiver(*origin, *e, label=f'e{i+1}')\nax3.set_xlim([-0.5, 1.5])\nax3.set_ylim([-0.5, 1.5])\nax3.set_zlim([-0.5, 1.5])\nax3.legend()\nax3.set_title('\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#53","title":"5.3 \u5fdc\u7528\u4f8b\uff1a\u5065\u5eb7\u30c7\u30fc\u30bf\u304b\u3089\u306e\u4e3b\u6210\u5206\u5206\u6790\u306b\u5411\u3051\u305f\u57fa\u5e95\u306e\u6e96\u5099","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# \u67b6\u7a7a\u306e\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u751f\u6210\nnp.random.seed(42)\nn_samples = 100\n\n# \u8840\u5727\uff08\u53ce\u7e2e\u671f\u3001\u62e1\u5f35\u671f\uff09\u3068\u5fc3\u62cd\u6570\u3001\u4f53\u91cd\u306e\u30c7\u30fc\u30bf\u3092\u751f\u6210\n# \u76f8\u95a2\u3092\u6301\u305f\u305b\u308b\u305f\u3081\u3001\u5171\u901a\u306e\u6f5c\u5728\u5909\u6570\u3092\u4f7f\u7528\nlatent = np.random.normal(0, 1, n_samples)\nweight = 70 + 10 * np.random.normal(0, 1, n_samples) + 5 * latent\nsystolic_bp = 120 + 15 * np.random.normal(0, 1, n_samples) + 8 * latent\ndiastolic_bp = 80 + 10 * np.random.normal(0, 1, n_samples) + 5 * latent\nheart_rate = 75 + 12 * np.random.normal(0, 1, n_samples) - 3 * latent\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210\nhealth_data = pd.DataFrame({\n    'Weight': weight,\n    'SystolicBP': systolic_bp,\n    'DiastolicBP': diastolic_bp,\n    'HeartRate': heart_rate\n})\n\nprint(health_data.head())\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(health_data)\ndata_scaled_df = pd.DataFrame(data_scaled, columns=health_data.columns)\n\n# \u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\ncov_matrix = np.cov(data_scaled.T)\n\n# \u5171\u5206\u6563\u884c\u5217\u304b\u3089\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n# \u56fa\u6709\u5024\u306e\u5927\u304d\u3055\u3067\u30bd\u30fc\u30c8\nidx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\n# \u7d50\u679c\u306e\u8868\u793a\nprint(\"\\n\u5171\u5206\u6563\u884c\u5217:\")\nprint(cov_matrix)\nprint(\"\\n\u56fa\u6709\u5024:\")\nprint(eigenvalues)\nprint(\"\\n\u56fa\u6709\u30d9\u30af\u30c8\u30eb (\u5217):\")\nprint(eigenvectors)\n\n# \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u6b63\u898f\u76f4\u4ea4\u6027\u3092\u78ba\u8a8d\nprint(\"\\n\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u884c\u5217:\")\ninner_products = np.zeros((4, 4))\nfor i in range(4):\n    for j in range(4):\n        inner_products[i, j] = np.dot(eigenvectors[:, i], eigenvectors[:, j])\nprint(np.round(inner_products, 10))\n\n# \u4e3b\u6210\u5206\u3078\u306e\u6295\u5f71\nprincipal_components = np.dot(data_scaled, eigenvectors)\nprincipal_df = pd.DataFrame(\n    principal_components, \n    columns=['PC1', 'PC2', 'PC3', 'PC4']\n)\n\n# \u6700\u521d\u306e2\u3064\u306e\u4e3b\u6210\u5206\u306b\u3088\u308b\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 8))\nplt.scatter(principal_df['PC1'], principal_df['PC2'], alpha=0.7)\nplt.xlabel('\u7b2c1\u4e3b\u6210\u5206')\nplt.ylabel('\u7b2c2\u4e3b\u6210\u5206')\nplt.title('\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790\uff08PC1 vs PC2\uff09')\nplt.grid(True)\n\n# \u5143\u306e\u7279\u5fb4\u91cf\u306e\u65b9\u5411\u3092\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u6295\u5f71\u3057\u3066\u53ef\u8996\u5316\nfeatures = health_data.columns\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, eigenvectors[i, 0]*3, eigenvectors[i, 1]*3, \n              head_width=0.2, head_length=0.2, fc='red', ec='red')\n    plt.text(eigenvectors[i, 0]*3.2, eigenvectors[i, 1]*3.2, feature, \n             color='red', fontsize=12)\n\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# \u5206\u6563\u8aac\u660e\u7387\u306e\u8a08\u7b97\u3068\u53ef\u8996\u5316\nexplained_variance_ratio = eigenvalues / np.sum(eigenvalues)\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, label='\u500b\u5225\u5bc4\u4e0e\u7387')\nplt.step(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, where='mid', label='\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.ylabel('\u5bc4\u4e0e\u7387')\nplt.xlabel('\u4e3b\u6210\u5206')\nplt.title('\u4e3b\u6210\u5206\u306e\u5bc4\u4e0e\u7387')\nplt.xticks(range(1, len(explained_variance_ratio) + 1))\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\u3053\u306ePython\u30b3\u30fc\u30c9\u3067\u306f\u3001\u5065\u5eb7\u30c7\u30fc\u30bf\uff08\u4f53\u91cd\u3001\u53ce\u7e2e\u671f\u8840\u5727\u3001\u62e1\u5f35\u671f\u8840\u5727\u3001\u5fc3\u62cd\u6570\uff09\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3057\u3001\u305d\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u57fa\u790e\u3068\u306a\u308b\u8a08\u7b97\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\u307e\u305a\u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\u3057\u3001\u305d\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3066\u3044\u307e\u3059\u3002\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308a\u3001\u3053\u308c\u304c\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u306e\u5fdc\u7528\u7d50\u679c\u3068\u540c\u7b49\u306e\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/27-vector-space-and-inner-product/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c1: \u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{u} = (2, -1, 3)^T\\) \u3068 \\(\\boldsymbol{v} = (1, 2, 2)^T\\) \u306b\u3064\u3044\u3066\u4ee5\u4e0b\u3092\u6c42\u3081\u306a\u3055\u3044\u3002 (a) \u5185\u7a4d \\(\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle\\) (b) \u30ce\u30eb\u30e0 \\(\\|\\boldsymbol{u}\\|\\) \u3068 \\(\\|\\boldsymbol{v}\\|\\) (c) \u30d9\u30af\u30c8\u30eb\u9593\u306e\u89d2\u5ea6 \\(\\theta\\)</p> <p>\u554f\u984c2: \\(\\mathbb{R}^3\\) \u306b\u304a\u3051\u308b\u30d9\u30af\u30c8\u30eb\u96c6\u5408 \\(\\{\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3\\}\\) \u304c\u76f4\u4ea4\u57fa\u5e95\u304b\u3069\u3046\u304b\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002\u305f\u3060\u3057\u3001 \\(\\(\\boldsymbol{v}_1 = (1, 1, 1)^T, \\boldsymbol{v}_2 = (1, -2, 1)^T, \\boldsymbol{v}_3 = (1, 0, -1)^T\\)\\)</p> <p>\u554f\u984c3: \u554f\u984c2\u306e\u30d9\u30af\u30c8\u30eb\u96c6\u5408\u304c\u76f4\u4ea4\u57fa\u5e95\u3067\u306a\u3044\u5834\u5408\u306f\u3001\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u3092\u9069\u7528\u3057\u3066\u76f4\u4ea4\u57fa\u5e95\u3092\u6c42\u3081\u306a\u3055\u3044\u3002\u307e\u305f\u3001\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3082\u6c42\u3081\u306a\u3055\u3044\u3002</p> <p>\u554f\u984c4: \u6b21\u306e\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{a}\\) \u3092\u3001\u76f4\u4ea4\u3059\u308b\u4e8c\u3064\u306e\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{v}_1\\) \u3068 \\(\\boldsymbol{v}_2\\) \u306b\u3088\u3063\u3066\u5f35\u3089\u308c\u308b\u7a7a\u9593\u3078\u306e\u5c04\u5f71\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{p}\\) \u3068\u3001\u305d\u308c\u306b\u76f4\u4ea4\u3059\u308b\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{r}\\) \u306b\u5206\u89e3\u3057\u306a\u3055\u3044\u3002 \\(\\(\\boldsymbol{a} = (3, 1, 2)^T, \\boldsymbol{v}_1 = (1, 0, 1)^T, \\boldsymbol{v}_2 = (0, 1, 0)^T\\)\\)</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c5: \\(n\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306b\u304a\u3051\u308b\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95 \\(\\{\\boldsymbol{e}_1, \\boldsymbol{e}_2, \\ldots, \\boldsymbol{e}_n\\}\\) \u3068\u3001\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{v}\\) \u306b\u3064\u3044\u3066\u3001\u30d5\u30fc\u30ea\u30a8\u4fc2\u6570 \\(c_i = \\langle \\boldsymbol{v}, \\boldsymbol{e}_i \\rangle\\) \u3092\u7528\u3044\u3066 \\(\\boldsymbol{v} = \\sum_{i=1}^{n} c_i \\boldsymbol{e}_i\\) \u3068\u8868\u305b\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u306a\u3055\u3044\u3002</p> <p>\u554f\u984c6: \u3042\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u4e3b\u6210\u5206\u5206\u6790\u3092\u9069\u7528\u3059\u308b\u524d\u306b\u3001\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u3092\u4f7f\u3063\u3066\u65b0\u3057\u3044\u57fa\u5e95\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3001\u3069\u306e\u3088\u3046\u306b\u30c7\u30fc\u30bf\u306e\u7406\u89e3\u306b\u5f79\u7acb\u3064\u304b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002\u5177\u4f53\u7684\u306a\u4f8b\u3092\u6319\u3052\u3066\u8aac\u660e\u3059\u308b\u3053\u3068\u3002</p> <p>\u554f\u984c7: \u5fc3\u62cd\u5909\u52d5\uff08HRV\uff09\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3059\u308b\u969b\u306b\u3001\u8907\u6570\u306e\u7279\u5fb4\u91cf\uff08SDNN\u3001RMSSD\u3001pNN50\u306a\u3069\uff09\u306e\u7dda\u5f62\u7d50\u5408\u3067\u65b0\u305f\u306a\u6307\u6a19\u3092\u4f5c\u308b\u3053\u3068\u3092\u8003\u3048\u307e\u3059\u3002\u3053\u306e\u969b\u3001\u7279\u5fb4\u91cf\u9593\u306e\u76f4\u4ea4\u6027\u304c\u91cd\u8981\u3067\u3042\u308b\u7406\u7531\u3092\u8aac\u660e\u3057\u3001\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u3092\u3069\u306e\u3088\u3046\u306b\u5fdc\u7528\u3067\u304d\u308b\u304b\u5177\u4f53\u7684\u306b\u63d0\u6848\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/27-vector-space-and-inner-product/#q1","title":"Q1: \u5185\u7a4d\u3068\u30ce\u30eb\u30e0\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A1: \u5185\u7a4d\u306f2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u9593\u306e\u6f14\u7b97\u3067\u3001\u30d9\u30af\u30c8\u30eb\u306e\u65b9\u5411\u306e\u985e\u4f3c\u6027\u3092\u6e2c\u308b\u6307\u6a19\u3067\u3059\u3002\u4e00\u65b9\u3001\u30ce\u30eb\u30e0\u306f\u30d9\u30af\u30c8\u30eb1\u3064\u306e\u300c\u9577\u3055\u300d\u3092\u6e2c\u308b\u6307\u6a19\u3067\u3059\u3002\u6570\u5b66\u7684\u306b\u306f\u3001\u30ce\u30eb\u30e0\u306f\u5185\u7a4d\u306e\u7279\u6b8a\u306a\u30b1\u30fc\u30b9\u3067\u3001\u30d9\u30af\u30c8\u30eb\u81ea\u8eab\u3068\u306e\u5185\u7a4d\u306e\u5e73\u65b9\u6839\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff08\\(\\|\\boldsymbol{v}\\| = \\sqrt{\\langle \\boldsymbol{v}, \\boldsymbol{v} \\rangle}\\)\uff09\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#q2","title":"Q2: \u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u4f7f\u3046\u30e1\u30ea\u30c3\u30c8\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A2: \u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u4f7f\u3046\u30e1\u30ea\u30c3\u30c8\u306f\u591a\u3005\u3042\u308a\u307e\u3059\u3002\u8a08\u7b97\u304c\u7c21\u7565\u5316\u3055\u308c\u308b\u70b9\u304c\u6700\u5927\u306e\u5229\u70b9\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u30d9\u30af\u30c8\u30eb\u306e\u5ea7\u6a19\u5909\u63db\u3001\u5c04\u5f71\u8a08\u7b97\u3001\u5185\u7a4d\u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u6570\u5024\u8a08\u7b97\u4e0a\u306e\u5b89\u5b9a\u6027\u304c\u5411\u4e0a\u3057\u3001\u69d8\u3005\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff08\u7279\u7570\u5024\u5206\u89e3\u3084\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\uff09\u306b\u304a\u3044\u3066\u3082\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#q3","title":"Q3: \u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u306f\u306a\u305c\u6a5f\u80fd\u3059\u308b\u306e\u3067\u3059\u304b\uff1f","text":"<p>A3: \u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u306f\u3001\u5404\u30b9\u30c6\u30c3\u30d7\u3067\u65b0\u3057\u3044\u30d9\u30af\u30c8\u30eb\u304b\u3089\u305d\u308c\u307e\u3067\u306b\u69cb\u7bc9\u3057\u305f\u76f4\u4ea4\u30d9\u30af\u30c8\u30eb\u306e\u65b9\u5411\u6210\u5206\u3092\u9010\u6b21\u7684\u306b\u53d6\u308a\u9664\u304f\u3053\u3068\u3067\u6a5f\u80fd\u3057\u307e\u3059\u3002\u3053\u306e\u30d7\u30ed\u30bb\u30b9\u306b\u3088\u308a\u3001\u6b8b\u3063\u305f\u30d9\u30af\u30c8\u30eb\u306f\u3059\u3067\u306b\u69cb\u7bc9\u3057\u305f\u76f4\u4ea4\u30d9\u30af\u30c8\u30eb\u3059\u3079\u3066\u306b\u76f4\u4ea4\u3059\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u6570\u5b66\u7684\u306b\u306f\u3001\u90e8\u5206\u7a7a\u9593\u3078\u306e\u5c04\u5f71\u3068\u3001\u305d\u306e\u5c04\u5f71\u306e\u6b8b\u5dee\u3092\u8a08\u7b97\u3057\u3066\u3044\u308b\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#q4","title":"Q4: \u5185\u7a4d\u7a7a\u9593\u306e\u5b9f\u4f8b\u306b\u306f\u3001\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A4: \u5185\u7a4d\u7a7a\u9593\u306e\u5b9f\u4f8b\u3068\u3057\u3066\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\uff1a - \\(\\mathbb{R}^n\\)\uff08\u6a19\u6e96\u5185\u7a4d\u3092\u6301\u3064n\u6b21\u5143\u5b9f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\uff09 - \u9023\u7d9a\u95a2\u6570\u306e\u7a7a\u9593\\(C[a,b]\\)\uff08\u5185\u7a4d\u306f\\(\\langle f,g \\rangle = \\int_a^b f(t)g(t)dt\\)\uff09 - \u78ba\u7387\u5909\u6570\u306e\u7a7a\u9593\uff08\u5185\u7a4d\u306f\u5171\u5206\u6563\u3084\u76f8\u95a2\u4fc2\u6570\u306b\u95a2\u9023\uff09 - \u91cf\u5b50\u529b\u5b66\u306b\u304a\u3051\u308b\u30d2\u30eb\u30d9\u30eb\u30c8\u7a7a\u9593 - \u4fe1\u53f7\u51e6\u7406\u306b\u304a\u3051\u308b\u30d5\u30fc\u30ea\u30a8\u57fa\u5e95\u3092\u6301\u3064\u95a2\u6570\u7a7a\u9593</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#q5","title":"Q5: \u4e3b\u6210\u5206\u5206\u6790\u3068\u5185\u7a4d\u306e\u95a2\u4fc2\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A5: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u65b9\u5411\uff08\u4e3b\u6210\u5206\uff09\u3092\u898b\u3064\u3051\u307e\u3059\u3002\u3053\u306e\u8a08\u7b97\u904e\u7a0b\u3067\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u307e\u3059\u304c\u3001\u3053\u308c\u3089\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3057\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001PCAs\u306f\u5185\u7a4d\u7a7a\u9593\u306b\u304a\u3051\u308b\u76f4\u4ea4\u57fa\u5e95\u306e\u63a2\u7d22\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4e3b\u6210\u5206\u306f\u5185\u7a4d\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u3092\u65b0\u3057\u3044\u5ea7\u6a19\u7cfb\u306b\u5909\u63db\u3059\u308b\u969b\u306e\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u6a5f\u80fd\u3057\u3001\u3053\u306e\u57fa\u5e95\u306f\u4e92\u3044\u306b\u7121\u76f8\u95a2\uff08\u76f4\u4ea4\uff09\u3068\u306a\u308b\u3088\u3046\u306b\u9078\u3070\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/27-vector-space-and-inner-product/#q6","title":"Q6: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u5185\u7a4d\u7a7a\u9593\u306e\u91cd\u8981\u6027\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A6: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u3001\u5185\u7a4d\u7a7a\u9593\u306e\u6982\u5ff5\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5074\u9762\u3067\u91cd\u8981\u3067\u3059\uff1a 1. \u7279\u5fb4\u91cf\u9593\u306e\u76f8\u95a2\u3084\u985e\u4f3c\u5ea6\u3092\u6e2c\u308b\uff08\u5185\u7a4d\u3084\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\uff09 2. \u6b21\u5143\u524a\u6e1b\u624b\u6cd5\uff08PCA\u3001SVD\u306a\u3069\uff09\u306e\u7406\u8ad6\u7684\u57fa\u76e4 3. \u5206\u985e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff08SVM\u306a\u3069\uff09\u3067\u306e\u30ab\u30fc\u30cd\u30eb\u95a2\u6570\u306e\u5b9a\u7fa9 4. \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306b\u304a\u3051\u308b\u8ddd\u96e2\u3084\u985e\u4f3c\u5ea6\u306e\u5b9a\u7fa9 5. \u4fe1\u53f7\u51e6\u7406\u3084\u753b\u50cf\u51e6\u7406\u306b\u304a\u3051\u308b\u57fa\u5e95\u5909\u63db\uff08\u30a6\u30a7\u30fc\u30d6\u30ec\u30c3\u30c8\u5909\u63db\u306a\u3069\uff09 6. \u63a8\u85a6\u30b7\u30b9\u30c6\u30e0\u306b\u304a\u3051\u308b\u9805\u76ee\u3084\u30e6\u30fc\u30b6\u30fc\u306e\u985e\u4f3c\u5ea6\u8a08\u7b97</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c28\u56de \u8b1b\u7fa9\u30ce\u30fc\u30c8\uff1a\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3068\u5c04\u5f71","text":""},{"location":"lectures/LA/28-vector-space-and-inner-product/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c28\u56de \u30c6\u30fc\u30de: \u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3068\u5c04\u5f71 \u95a2\u9023\u9805\u76ee: \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3001\u5185\u7a4d\u3001\u76f4\u4ea4\u57fa\u5e95\u3001\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb \u4e88\u7fd2\u9805\u76ee: - \u7b2c26\u56de\u306e\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u790e - \u7b2c27\u56de\u306e\u5185\u7a4d\u3068\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u30fb\u30b0\u30e9\u30e0\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> <ol> <li>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u76f4\u548c\u5206\u89e3\u3068\u76f4\u4ea4\u76f4\u548c\u5206\u89e3\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3057\u3001\u8aac\u660e\u3067\u304d\u308b</li> <li>\u76f4\u4ea4\u88dc\u7a7a\u9593\u306e\u5b9a\u7fa9\u3068\u6027\u8cea\u3092\u7406\u89e3\u3057\u3001\u5177\u4f53\u4f8b\u3067\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u76f4\u4ea4\u5c04\u5f71\u3068\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3057\u3001\u5c04\u5f71\u3092\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u5c04\u5f71\u884c\u5217\u30fb\u76f4\u4ea4\u5c04\u5f71\u884c\u5217\u306e\u6027\u8cea\u3092\u7406\u89e3\u3057\u3001\u69cb\u6210\u3067\u304d\u308b</li> <li>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3068\u76f4\u4ea4\u5c04\u5f71\u884c\u5217\u306e\u95a2\u9023\u3092\u8aac\u660e\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/28-vector-space-and-inner-product/#31","title":"3.1 \u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306e\u5fa9\u7fd2","text":"<p>\u524d\u56de\u5b66\u7fd2\u3057\u305f\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u5fa9\u7fd2\u3057\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95 \u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(V\\) \u306e\u57fa\u5e95 \\(\\{v_1, v_2, \\ldots, v_n\\}\\) \u304c\u4ee5\u4e0b\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u3068\u304d\u3001\u3053\u308c\u3092\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3068\u547c\u3073\u307e\u3059\u3002 1. \u5404\u30d9\u30af\u30c8\u30eb\u306e\u9577\u3055\u304c1\u3067\u3042\u308b\uff1a\\(\\|v_i\\| = 1\\) \uff08\u6b63\u898f\u6027\uff09 2. \u7570\u306a\u308b\u30d9\u30af\u30c8\u30eb\u540c\u58eb\u304c\u76f4\u4ea4\u3059\u308b\uff1a\\(v_i \\cdot v_j = 0\\) \uff08\\(i \\neq j\\)\uff09</p> <p>\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306e\u91cd\u8981\u306a\u6027\u8cea\uff1a - \u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308b - \u30d9\u30af\u30c8\u30eb\u306e\u5ea7\u6a19\u8868\u793a\u304c\u76f4\u611f\u7684\u306b\u306a\u308b - \u5c04\u5f71\u306e\u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308b</p> <p>\u4f8b: \\(\\mathbb{R}^3\\) \u306b\u304a\u3051\u308b\u6a19\u6e96\u57fa\u5e95 \\(\\{e_1, e_2, e_3\\}\\) \u306f\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3067\u3059\u3002 \\(\\(e_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, e_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, e_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\)\\)</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#32","title":"3.2 \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u76f4\u548c\u5206\u89e3","text":"<p>\u5b9a\u7fa9\uff1a\u90e8\u5206\u7a7a\u9593\u306e\u76f4\u548c \u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(V\\) \u306e2\u3064\u306e\u90e8\u5206\u7a7a\u9593 \\(U\\) \u3068 \\(W\\) \u306b\u3064\u3044\u3066\uff1a 1. \\(V = U + W\\) \uff08\\(U\\) \u3068 \\(W\\) \u3067 \\(V\\) \u3092\u751f\u6210\u3067\u304d\u308b\uff09 2. \\(U \\cap W = \\{0\\}\\) \uff08\u5171\u901a\u90e8\u5206\u306f\u96f6\u30d9\u30af\u30c8\u30eb\u306e\u307f\uff09</p> <p>\u3053\u306e\u3068\u304d\u3001\\(V\\) \u306f \\(U\\) \u3068 \\(W\\) \u306e\u76f4\u548c\u3067\u3042\u308b\u3068\u3044\u3044\u3001\\(V = U \\oplus W\\) \u3068\u8868\u8a18\u3057\u307e\u3059\u3002</p> <p>\u76f4\u548c\u306e\u91cd\u8981\u306a\u6027\u8cea\uff1a - \\(V\\) \u306e\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(v\\) \u306f\u3001\\(v = u + w\\) \uff08\\(u \\in U, w \\in W\\)\uff09\u3068\u4e00\u610f\u306b\u5206\u89e3\u3067\u304d\u308b - \\(\\dim(V) = \\dim(U) + \\dim(W)\\)</p> <p>\u5177\u4f53\u4f8b1: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001xy\u5e73\u9762\u3092\u8868\u3059\u90e8\u5206\u7a7a\u9593 \\(U = \\{(x, y, 0) \\mid x, y \\in \\mathbb{R}\\}\\) \u3068z\u8ef8\u3092\u8868\u3059\u90e8\u5206\u7a7a\u9593 \\(W = \\{(0, 0, z) \\mid z \\in \\mathbb{R}\\}\\) \u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\u3053\u306e\u5834\u5408\uff1a - \\(\\dim(U) = 2\\)\uff08\u57fa\u5e95\u306f \\(\\{(1,0,0), (0,1,0)\\}\\)\uff09 - \\(\\dim(W) = 1\\)\uff08\u57fa\u5e95\u306f \\(\\{(0,0,1)\\}\\)\uff09 - \\(U \\cap W = \\{(0,0,0)\\}\\)\uff08\u5171\u901a\u90e8\u5206\u306f\u539f\u70b9\u306e\u307f\uff09</p> <p>\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(v = (a, b, c) \\in \\mathbb{R}^3\\) \u306f\u3001\\(v = (a, b, 0) + (0, 0, c)\\) \u3068\u4e00\u610f\u306b\u5206\u89e3\u3067\u304d\u307e\u3059\u3002\u3088\u3063\u3066 \\(\\mathbb{R}^3 = U \\oplus W\\) \u3067\u3059\u3002</p> <p>\u5177\u4f53\u4f8b2: \\(\\mathbb{R}^2\\) \u306b\u304a\u3044\u3066\u3001x\u8ef8\u3092\u8868\u3059\u90e8\u5206\u7a7a\u9593 \\(U = \\{(x, 0) \\mid x \\in \\mathbb{R}\\}\\) \u3068y\u8ef8\u3092\u8868\u3059\u90e8\u5206\u7a7a\u9593 \\(W = \\{(0, y) \\mid y \\in \\mathbb{R}\\}\\) \u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\u3053\u306e\u3068\u304d\uff1a - \\(\\dim(U) = 1\\)\uff08\u57fa\u5e95\u306f \\(\\{(1,0)\\}\\)\uff09 - \\(\\dim(W) = 1\\)\uff08\u57fa\u5e95\u306f \\(\\{(0,1)\\}\\)\uff09 - \\(U \\cap W = \\{(0,0)\\}\\)\uff08\u5171\u901a\u90e8\u5206\u306f\u539f\u70b9\u306e\u307f\uff09</p> <p>\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(v = (a, b) \\in \\mathbb{R}^2\\) \u306f\u3001\\(v = (a, 0) + (0, b)\\) \u3068\u4e00\u610f\u306b\u5206\u89e3\u3067\u304d\u307e\u3059\u3002\u3088\u3063\u3066 \\(\\mathbb{R}^2 = U \\oplus W\\) \u3067\u3059\u3002</p> <p>\u76f4\u548c\u3067\u306a\u3044\u4f8b: \\(\\mathbb{R}^2\\) \u306b\u304a\u3044\u3066\u3001x\u8ef8\u3092\u8868\u3059\u90e8\u5206\u7a7a\u9593 \\(U = \\{(x, 0) \\mid x \\in \\mathbb{R}\\}\\) \u3068\u76f4\u7dda \\(W = \\{(x, x) \\mid x \\in \\mathbb{R}\\}\\) \u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\u3053\u306e\u5834\u5408\uff1a - \\(U \\cap W = \\{(0,0)\\}\\)\uff08\u5171\u901a\u90e8\u5206\u306f\u539f\u70b9\u306e\u307f\uff09 - \u3057\u304b\u3057\u3001\u30d9\u30af\u30c8\u30eb \\((0, 1) \\in \\mathbb{R}^2\\) \u306f \\(U\\) \u3068 \\(W\\) \u306e\u548c\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u307e\u305b\u3093 - \u3088\u3063\u3066 \\(\\mathbb{R}^2 \\neq U + W\\) \u3067\u3042\u308a\u3001\\(U \\oplus W\\) \u3067\u306f\u3042\u308a\u307e\u305b\u3093</p> <p>\u307e\u305f\u3001\u76f4\u7dda \\(U = \\{(t, 0, 0) \\mid t \\in \\mathbb{R}\\}\\) \u3068\u5e73\u9762 \\(W = \\{(s, t, 0) \\mid s, t \\in \\mathbb{R}\\}\\) \u3092\u8003\u3048\u308b\u3068\u3001\\(U \\subset W\\) \u306a\u306e\u3067 \\(U \\cap W = U \\neq \\{0\\}\\) \u3067\u3042\u308a\u3001\\(U \\oplus W\\) \u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#33","title":"3.3 \u76f4\u4ea4\u88dc\u7a7a\u9593","text":"<p>\u5b9a\u7fa9\uff1a\u76f4\u4ea4\u88dc\u7a7a\u9593 \u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(V\\) \u306e\u90e8\u5206\u7a7a\u9593 \\(U\\) \u306b\u5bfe\u3057\u3066\u3001\\(U\\) \u306e\u3059\u3079\u3066\u306e\u30d9\u30af\u30c8\u30eb\u3068\u76f4\u4ea4\u3059\u308b\u30d9\u30af\u30c8\u30eb\u306e\u96c6\u5408\u3092 \\(U\\) \u306e\u76f4\u4ea4\u88dc\u7a7a\u9593\u3068\u3044\u3044\u3001\\(U^{\\perp}\\) \u3068\u8868\u8a18\u3057\u307e\u3059\u3002</p> <p>\\(U^{\\perp} = \\{v \\in V \\mid \\langle v, u \\rangle = 0 \\text{ for all } u \\in U\\}\\)</p> <p>\u76f4\u4ea4\u88dc\u7a7a\u9593\u306e\u91cd\u8981\u306a\u6027\u8cea\uff1a - \\(U^{\\perp}\\) \u3082 \\(V\\) \u306e\u90e8\u5206\u7a7a\u9593\u3067\u3042\u308b - \\((U^{\\perp})^{\\perp} = U\\) - \\(\\dim(U) + \\dim(U^{\\perp}) = \\dim(V)\\) - \\(V = U \\oplus U^{\\perp}\\) \uff08\u76f4\u4ea4\u76f4\u548c\u5206\u89e3\uff09</p> <p>\u5177\u4f53\u4f8b1: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001xy\u5e73\u9762\u3092\u8868\u3059\u90e8\u5206\u7a7a\u9593 \\(U = \\{(x, y, 0) \\mid x, y \\in \\mathbb{R}\\}\\) \u306e\u76f4\u4ea4\u88dc\u7a7a\u9593\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\\(U\\) \u306e\u57fa\u5e95\u306f \\(\\{(1,0,0), (0,1,0)\\}\\) \u3067\u3059\u3002</p> <p>\\(U\\) \u306e\u76f4\u4ea4\u88dc\u7a7a\u9593 \\(U^{\\perp}\\) \u306f\u3001\u3053\u308c\u3089\u306e\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3068\u76f4\u4ea4\u3059\u308b\u30d9\u30af\u30c8\u30eb\u5168\u4f53\u306e\u96c6\u5408\u3067\u3059\u3002\u3064\u307e\u308a\uff1a - \\((1,0,0) \\cdot (a,b,c) = a = 0\\) - \\((0,1,0) \\cdot (a,b,c) = b = 0\\)</p> <p>\u3092\u6e80\u305f\u3059 \\((a,b,c)\\) \u306f \\((0,0,c)\\) \u306e\u5f62\u3092\u3068\u308a\u307e\u3059\u3002\u3088\u3063\u3066 \\(U^{\\perp} = \\{(0, 0, z) \\mid z \\in \\mathbb{R}\\}\\) \u3067\u3042\u308a\u3001\u3053\u308c\u306fz\u8ef8\u3092\u8868\u3057\u307e\u3059\u3002\u307e\u305f\uff1a - \\(\\dim(U) = 2\\) - \\(\\dim(U^{\\perp}) = 1\\) - \\(\\dim(U) + \\dim(U^{\\perp}) = 2 + 1 = 3 = \\dim(\\mathbb{R}^3)\\)</p> <p>\u304c\u6210\u308a\u7acb\u3061\u307e\u3059\u3002</p> <p>\u5177\u4f53\u4f8b2: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u76f4\u7dda \\(U = \\{(t, t, t) \\mid t \\in \\mathbb{R}\\}\\) \u306e\u76f4\u4ea4\u88dc\u7a7a\u9593\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\\(U\\) \u306e\u57fa\u5e95\u306f \\(\\{(1,1,1)\\}\\) \u3067\u3059\u3002</p> <p>\\(U\\) \u306e\u76f4\u4ea4\u88dc\u7a7a\u9593 \\(U^{\\perp}\\) \u306f\u3001\u3053\u306e\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3068\u76f4\u4ea4\u3059\u308b\u30d9\u30af\u30c8\u30eb\u5168\u4f53\u306e\u96c6\u5408\u3067\u3059\u3002\u3064\u307e\u308a\uff1a - \\((1,1,1) \\cdot (a,b,c) = a + b + c = 0\\)</p> <p>\u3092\u6e80\u305f\u3059 \\((a,b,c)\\) \u3067\u3059\u3002\u3053\u308c\u306f\u5e73\u9762 \\(a + b + c = 0\\) \u3092\u8868\u3057\u3001\u4f8b\u3048\u3070\u57fa\u5e95\u3068\u3057\u3066 \\(\\{(1,-1,0), (1,0,-1)\\}\\) \u3092\u53d6\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u307e\u305f\uff1a - \\(\\dim(U) = 1\\) - \\(\\dim(U^{\\perp}) = 2\\) - \\(\\dim(U) + \\dim(U^{\\perp}) = 1 + 2 = 3 = \\dim(\\mathbb{R}^3)\\)</p> <p>\u304c\u6210\u308a\u7acb\u3061\u307e\u3059\u3002</p> <p>\u5177\u4f53\u4f8b3: \\(\\mathbb{R}^4\\) \u306b\u304a\u3044\u3066\u3001\u90e8\u5206\u7a7a\u9593 \\(U = \\{(x, y, 0, 0) \\mid x, y \\in \\mathbb{R}\\}\\) \u306e\u76f4\u4ea4\u88dc\u7a7a\u9593\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\\(U\\) \u306e\u57fa\u5e95\u306f \\(\\{(1,0,0,0), (0,1,0,0)\\}\\) \u3067\u3059\u3002</p> <p>\\(U\\) \u306e\u76f4\u4ea4\u88dc\u7a7a\u9593 \\(U^{\\perp}\\) \u306f\u3001\u3053\u308c\u3089\u306e\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3068\u76f4\u4ea4\u3059\u308b\u30d9\u30af\u30c8\u30eb\u5168\u4f53\u306e\u96c6\u5408\u3067\u3059\u3002\u3064\u307e\u308a\uff1a - \\((1,0,0,0) \\cdot (a,b,c,d) = a = 0\\) - \\((0,1,0,0) \\cdot (a,b,c,d) = b = 0\\)</p> <p>\u3092\u6e80\u305f\u3059 \\((a,b,c,d)\\) \u306f \\((0,0,c,d)\\) \u306e\u5f62\u3092\u3068\u308a\u307e\u3059\u3002\u3088\u3063\u3066 \\(U^{\\perp} = \\{(0, 0, z, w) \\mid z, w \\in \\mathbb{R}\\}\\) \u3067\u3042\u308a\u3001\u307e\u305f\uff1a - \\(\\dim(U) = 2\\) - \\(\\dim(U^{\\perp}) = 2\\) - \\(\\dim(U) + \\dim(U^{\\perp}) = 2 + 2 = 4 = \\dim(\\mathbb{R}^4)\\)</p> <p>\u304c\u6210\u308a\u7acb\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#34","title":"3.4 \u76f4\u4ea4\u76f4\u548c\u5206\u89e3","text":"<p>\u5b9a\u7fa9\uff1a\u76f4\u4ea4\u76f4\u548c\u5206\u89e3 \u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(V\\) \u306e2\u3064\u306e\u90e8\u5206\u7a7a\u9593 \\(U\\) \u3068 \\(W\\) \u304c\u4ee5\u4e0b\u3092\u6e80\u305f\u3059\u3068\u304d\u3001\\(V\\) \u306f \\(U\\) \u3068 \\(W\\) \u306e\u76f4\u4ea4\u76f4\u548c\u3067\u3042\u308b\u3068\u3044\u3044\u3001\\(V = U \\oplus^{\\perp} W\\) \u3068\u8868\u8a18\u3057\u307e\u3059\u3002 1. \\(V = U \\oplus W\\) \uff08\u76f4\u548c\uff09 2. \\(U \\perp W\\) \uff08\u76f4\u4ea4\u6027\uff1a\\(\\langle u, w \\rangle = 0\\) for all \\(u \\in U, w \\in W\\)\uff09</p> <p>\u7279\u306b\u91cd\u8981\u306a\u306e\u306f\u3001\u90e8\u5206\u7a7a\u9593 \\(U\\) \u3068\u305d\u306e\u76f4\u4ea4\u88dc\u7a7a\u9593 \\(U^{\\perp}\\) \u306b\u3088\u308b\u76f4\u4ea4\u76f4\u548c\u5206\u89e3 \\(V = U \\oplus^{\\perp} U^{\\perp}\\) \u3067\u3059\u3002</p> <p>\u5177\u4f53\u4f8b1: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001xy\u5e73\u9762 \\(U = \\{(x, y, 0) \\mid x, y \\in \\mathbb{R}\\}\\) \u3068z\u8ef8 \\(W = \\{(0, 0, z) \\mid z \\in \\mathbb{R}\\}\\) \u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\u65e2\u306b\u78ba\u8a8d\u3057\u305f\u3088\u3046\u306b \\(W = U^{\\perp}\\) \u3067\u3042\u308a\u3001\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(v = (a, b, c) \\in \\mathbb{R}^3\\) \u306f \\(v = (a, b, 0) + (0, 0, c)\\) \u3068\u5206\u89e3\u3067\u304d\u307e\u3059\u3002\u3053\u3053\u3067 \\((a, b, 0) \\in U\\) \u3068 \\((0, 0, c) \\in W\\) \u306f\u76f4\u4ea4\u3057\u3066\u3044\u307e\u3059\uff08\u5185\u7a4d\u304c0\uff09\u3002\u3088\u3063\u3066 \\(\\mathbb{R}^3 = U \\oplus^{\\perp} W\\) \u306f\u76f4\u4ea4\u76f4\u548c\u3067\u3059\u3002</p> <p>\u3053\u306e\u76f4\u4ea4\u76f4\u548c\u5206\u89e3\u306e\u610f\u5473\u306f\u3001\u7a7a\u9593\u5185\u306e\u4efb\u610f\u306e\u70b9\u3092\u3001\u307e\u305axy\u5e73\u9762\u306b\u5782\u76f4\u306b\u5c04\u5f71\u3057\u3001\u305d\u3053\u304b\u3089z\u8ef8\u306b\u6cbf\u3063\u3066\u79fb\u52d5\u3059\u308b\u3053\u3068\u3067\u5230\u9054\u3067\u304d\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p> <p>\u5177\u4f53\u4f8b2: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u76f4\u7dda \\(U = \\{(t, t, t) \\mid t \\in \\mathbb{R}\\}\\) \u3068\u305d\u306e\u76f4\u4ea4\u88dc\u7a7a\u9593 \\(U^{\\perp} = \\{(x, y, z) \\mid x + y + z = 0\\}\\) \u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(v = (a, b, c) \\in \\mathbb{R}^3\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5206\u89e3\u3067\u304d\u307e\u3059\uff1a</p> <p>\u307e\u305a\u3001\\(v\\) \u3092 \\(U\\) \u3078\u5c04\u5f71\u3059\u308b\u305f\u3081\u306b\u3001\\(U\\) \u306e\u6b63\u898f\u5316\u3055\u308c\u305f\u65b9\u5411\u30d9\u30af\u30c8\u30eb \\(u_0 = \\frac{1}{\\sqrt{3}}(1, 1, 1)\\) \u3092\u7528\u3044\u3066\uff1a \\(\\(\\text{proj}_U(v) = \\langle v, u_0 \\rangle u_0 = \\frac{a + b + c}{3}(1, 1, 1)\\)\\)</p> <p>\u6b21\u306b\u3001\u6b8b\u308a\u306e\u6210\u5206\u306f \\(U^{\\perp}\\) \u306b\u5c5e\u3057\uff1a \\(\\(\\text{proj}_{U^{\\perp}}(v) = v - \\text{proj}_U(v) = (a, b, c) - \\frac{a + b + c}{3}(1, 1, 1)\\)\\)</p> <p>\u3053\u306e\u5206\u89e3\u306f\u76f4\u4ea4\u3057\u3066\u304a\u308a\uff08\u5185\u7a4d\u304c0\uff09\u3001\u4e00\u610f\u3067\u3059\u3002\u3088\u3063\u3066 \\(\\mathbb{R}^3 = U \\oplus^{\\perp} U^{\\perp}\\) \u306f\u76f4\u4ea4\u76f4\u548c\u3067\u3059\u3002</p> <p>\u3053\u306e\u76f4\u4ea4\u76f4\u548c\u5206\u89e3\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u306f\u3001\u7a7a\u9593\u5185\u306e\u4efb\u610f\u306e\u70b9\u3092\u3001\u307e\u305a\u76f4\u7dda \\(U\\) \uff08\u539f\u70b9\u3092\u901a\u308a \\((1,1,1)\\) \u65b9\u5411\u306e\u76f4\u7dda\uff09\u3078\u306e\u5782\u76f4\u5c04\u5f71\u3068\u3001\u305d\u3053\u304b\u3089\u306e\u5782\u76f4\u65b9\u5411\u306e\u79fb\u52d5\u306b\u5206\u89e3\u3067\u304d\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p> <p>\u5177\u4f53\u4f8b3: \\(\\mathbb{R}^4\\) \u306e\u90e8\u5206\u7a7a\u9593 \\(U = \\text{span}\\{(1,0,1,0), (0,1,0,1)\\}\\) \u3068\u305d\u306e\u76f4\u4ea4\u88dc\u7a7a\u9593 \\(U^{\\perp}\\) \u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\\(U^{\\perp}\\) \u3092\u6c42\u3081\u308b\u306b\u306f\u3001\\(U\\) \u306e\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3068\u76f4\u4ea4\u3059\u308b\u6761\u4ef6\u3092\u7acb\u3066\u307e\u3059\uff1a - \\((1,0,1,0) \\cdot (a,b,c,d) = a + c = 0\\) - \\((0,1,0,1) \\cdot (a,b,c,d) = b + d = 0\\)</p> <p>\u3053\u308c\u3088\u308a \\(U^{\\perp} = \\{(a, b, -a, -b) \\mid a, b \\in \\mathbb{R}\\}\\) \u3068\u306a\u308a\u3001\\(U^{\\perp}\\) \u306e\u57fa\u5e95\u306f \\(\\{(1, 0, -1, 0), (0, 1, 0, -1)\\}\\) \u3067\u3059\u3002</p> <p>\u3053\u3053\u3067\uff1a - \\(\\dim(U) = 2\\) - \\(\\dim(U^{\\perp}) = 2\\) - \\(\\dim(U) + \\dim(U^{\\perp}) = 2 + 2 = 4 = \\dim(\\mathbb{R}^4)\\)</p> <p>\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(v = (a, b, c, d) \\in \\mathbb{R}^4\\) \u306f\u6b21\u306e\u3088\u3046\u306b\u76f4\u4ea4\u5206\u89e3\u3067\u304d\u307e\u3059\uff1a \\(\\(v = \\frac{1}{2}(a+c, b+d, a+c, b+d) + \\frac{1}{2}(a-c, b-d, c-a, d-b)\\)\\)</p> <p>\u3053\u3053\u3067\u7b2c1\u9805\u306f \\(U\\) \u306e\u8981\u7d20\u3001\u7b2c2\u9805\u306f \\(U^{\\perp}\\) \u306e\u8981\u7d20\u3067\u3042\u308a\u3001\u3053\u308c\u3089\u306f\u76f4\u4ea4\u3057\u307e\u3059\u3002\u3088\u3063\u3066 \\(\\mathbb{R}^4 = U \\oplus^{\\perp} U^{\\perp}\\) \u306f\u76f4\u4ea4\u76f4\u548c\u3067\u3059\u3002</p> <p>\u76f4\u4ea4\u76f4\u548c\u3067\u306a\u3044\u4f8b: \\(\\mathbb{R}^2\\) \u306b\u304a\u3044\u3066\u3001\u76f4\u7dda \\(U = \\{(t, t) \\mid t \\in \\mathbb{R}\\}\\) \u3068\u76f4\u7dda \\(W = \\{(t, 0) \\mid t \\in \\mathbb{R}\\}\\) \u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\\(U\\) \u3068 \\(W\\) \u306f\u5171\u901a\u90e8\u5206\u304c\u539f\u70b9\u306e\u307f\u306a\u306e\u3067 \\(\\mathbb{R}^2 = U \\oplus W\\) \u306f\u76f4\u548c\u3067\u3059\u304c\u3001\\(U\\) \u306e\u65b9\u5411\u30d9\u30af\u30c8\u30eb \\((1,1)\\) \u3068 \\(W\\) \u306e\u65b9\u5411\u30d9\u30af\u30c8\u30eb \\((1,0)\\) \u306e\u5185\u7a4d\u306f \\(1 \\neq 0\\) \u306a\u306e\u3067\u3001\\(U \\perp W\\) \u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3088\u3063\u3066 \\(\\mathbb{R}^2 = U \\oplus W\\) \u306f\u76f4\u548c\u3067\u3059\u304c\u3001\u76f4\u4ea4\u76f4\u548c \\(U \\oplus^{\\perp} W\\) \u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/28-vector-space-and-inner-product/#41","title":"4.1 \u76f4\u4ea4\u5c04\u5f71\u3068\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306e\u95a2\u4fc2","text":"<p>\u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(V\\) \u306e\u90e8\u5206\u7a7a\u9593 \\(U\\) \u3078\u306e\u5c04\u5f71\u306f\u3001\u30d9\u30af\u30c8\u30eb \\(v \\in V\\) \u3092 \\(U\\) \u306e\u76f4\u4ea4\u76f4\u548c\u5206\u89e3 \\(v = u + w\\) (\\(u \\in U, w \\in U^{\\perp}\\)) \u306b\u304a\u3051\u308b \\(u\\) \u306b\u5bfe\u5fdc\u3055\u305b\u308b\u5199\u50cf\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u76f4\u4ea4\u5c04\u5f71 \u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(V\\) \u306e\u90e8\u5206\u7a7a\u9593 \\(U\\) \u3078\u306e\u76f4\u4ea4\u5c04\u5f71 \\(P_U: V \\rightarrow U\\) \u306f\u3001\u5404\u30d9\u30af\u30c8\u30eb \\(v \\in V\\) \u3092 \\(v\\) \u306e \\(U\\) \u3078\u306e\u6210\u5206 \\(u\\) \u306b\u5199\u3059\u7dda\u5f62\u5199\u50cf\u3067\u3059\u3002</p> <p>\u3053\u3053\u3067\u3001\\(v = u + w\\) (\\(u \\in U, w \\in U^{\\perp}\\)) \u306f \\(v\\) \u306e\u76f4\u4ea4\u76f4\u548c\u5206\u89e3\u3067\u3059\u3002</p> <p>\\(U\\) \u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95 \\(\\{u_1, u_2, \\ldots, u_k\\}\\) \u3092\u7528\u3044\u308b\u3068\u3001\\(v\\) \u306e \\(U\\) \u3078\u306e\u5c04\u5f71\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a</p> \\[P_U(v) = \\sum_{i=1}^{k} \\langle v, u_i \\rangle u_i\\] <p>\u8a08\u7b97\u4f8b1: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001xy\u5e73\u9762 \\(U = \\text{span}\\{e_1, e_2\\}\\) \u3078\u306e\u5c04\u5f71\u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\\(U\\) \u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306f \\(\\{e_1, e_2\\} = \\{(1,0,0), (0,1,0)\\}\\) \u3067\u3059\u3002</p> <p>\u30d9\u30af\u30c8\u30eb \\(v = (3, 4, 5)\\) \u306e \\(U\\) \u3078\u306e\u5c04\u5f71\u306f\uff1a \\(\\(P_U(v) = \\langle v, e_1 \\rangle e_1 + \\langle v, e_2 \\rangle e_2 = 3 \\cdot (1,0,0) + 4 \\cdot (0,1,0) = (3, 4, 0)\\)\\)</p> <p>\u6b8b\u308a\u306e\u6210\u5206 \\(v - P_U(v) = (3,4,5) - (3,4,0) = (0,0,5)\\) \u306f \\(U^{\\perp}\\) \u306b\u5c5e\u3057\u3001\\(P_U(v)\\) \u3068\u76f4\u4ea4\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p> <p>\u8a08\u7b97\u4f8b2: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u76f4\u7dda \\(U = \\text{span}\\{(1,1,1)\\}\\) \u3078\u306e\u5c04\u5f71\u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\\(U\\) \u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306f \\(\\{u_1\\} = \\{\\frac{1}{\\sqrt{3}}(1,1,1)\\}\\) \u3067\u3059\u3002</p> <p>\u30d9\u30af\u30c8\u30eb \\(v = (2, 3, 4)\\) \u306e \\(U\\) \u3078\u306e\u5c04\u5f71\u306f\uff1a \\(\\(P_U(v) = \\langle v, u_1 \\rangle u_1 = \\frac{2+3+4}{\\sqrt{3}} \\cdot \\frac{1}{\\sqrt{3}}(1,1,1) = 3 \\cdot (1,1,1) = (3,3,3)\\)\\)</p> <p>\u6b8b\u308a\u306e\u6210\u5206 \\(v - P_U(v) = (2,3,4) - (3,3,3) = (-1,0,1)\\) \u306f \\(U^{\\perp}\\) \u306b\u5c5e\u3057\u3001\\(P_U(v)\\) \u3068\u76f4\u4ea4\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\uff08\u5185\u7a4d \\(\\langle (3,3,3), (-1,0,1) \\rangle = -3 + 0 + 3 = 0\\)\uff09\u3002</p> <p>\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8: \u76f4\u4ea4\u5c04\u5f71\u306f\u3001\\(v\\) \u304b\u3089 \\(U\\) \u3078\u306e\u300c\u6700\u77ed\u8ddd\u96e2\u300d\u3092\u4e0e\u3048\u308b\u30dd\u30a4\u30f3\u30c8\u3092\u6c42\u3081\u308b\u3053\u3068\u3068\u540c\u7b49\u3067\u3059\u3002\u8a00\u3044\u63db\u3048\u308b\u3068\u3001\\(\\|v - P_U(v)\\|\\) \u304c\u6700\u5c0f\u3068\u306a\u308b\u3088\u3046\u306a \\(U\\) \u4e0a\u306e\u70b9 \\(P_U(v)\\) \u3092\u898b\u3064\u3051\u308b\u3053\u3068\u3067\u3059\u3002\u3053\u306e\u3053\u3068\u304b\u3089\u3001\\(v - P_U(v)\\) \u306f \\(U\\) \u306b\u5bfe\u3057\u3066\u5782\u76f4\uff08\u76f4\u4ea4\uff09\u306b\u306a\u308b\u3068\u3044\u3046\u6027\u8cea\u304c\u5c0e\u304b\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#42","title":"4.2 \u5c04\u5f71\u884c\u5217","text":"<p>\u76f4\u4ea4\u5c04\u5f71\u306f\u7dda\u5f62\u5199\u50cf\u306a\u306e\u3067\u3001\u884c\u5217\u3067\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9\uff1a\u5c04\u5f71\u884c\u5217 \u90e8\u5206\u7a7a\u9593 \\(U\\) \u3078\u306e\u76f4\u4ea4\u5c04\u5f71\u3092\u8868\u3059\u884c\u5217 \\(P\\) \u3092\u5c04\u5f71\u884c\u5217\u3068\u3044\u3044\u307e\u3059\u3002\\(U\\) \u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95 \\(\\{u_1, u_2, \\ldots, u_k\\}\\) \u3092\u7528\u3044\u308b\u3068\uff1a</p> \\[P = \\sum_{i=1}^{k} u_i u_i^T\\] <p>\u5c04\u5f71\u884c\u5217\u306e\u6027\u8cea\uff1a 1. \u51aa\u7b49\u6027: \\(P^2 = P\\) \uff08\u5c04\u5f71\u30922\u56de\u9069\u7528\u3057\u3066\u3082\u540c\u3058\uff09 2. \u5bfe\u79f0\u6027: \\(P^T = P\\) \uff08\u76f4\u4ea4\u5c04\u5f71\u306e\u5834\u5408\uff09 3. \u56fa\u6709\u5024: 0\u307e\u305f\u306f1\u306e\u307f 4. \u30c8\u30ec\u30fc\u30b9: \\(\\text{tr}(P) = \\dim(U)\\)</p> <p>\u8a08\u7b97\u4f8b1: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001xy\u5e73\u9762 \\(U = \\text{span}\\{e_1, e_2\\}\\) \u3078\u306e\u5c04\u5f71\u884c\u5217\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\\(U\\) \u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306f \\(\\{e_1, e_2\\} = \\{(1,0,0), (0,1,0)\\}\\) \u3067\u3059\u3002</p> <p>\u5c04\u5f71\u884c\u5217\u306f\uff1a \\(\\(P = e_1 e_1^T + e_2 e_2^T = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}\\)\\)</p> <p>\u3053\u306e\u5c04\u5f71\u884c\u5217\u306e\u6027\u8cea\u3092\u78ba\u8a8d\u3057\u307e\u3059\uff1a - \\(P^2 = P\\) \uff08\u51aa\u7b49\u6027\uff09 - \\(P^T = P\\) \uff08\u5bfe\u79f0\u6027\uff09 - \u56fa\u6709\u5024\u306f \\(\\{1, 1, 0\\}\\) \uff080\u30681\u306e\u307f\uff09 - \\(\\text{tr}(P) = 2 = \\dim(U)\\)</p> <p>\u8a08\u7b97\u4f8b2: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u76f4\u7dda \\(U = \\text{span}\\{(1,1,1)\\}\\) \u3078\u306e\u5c04\u5f71\u884c\u5217\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\\(U\\) \u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u306f \\(\\{u_1\\} = \\{\\frac{1}{\\sqrt{3}}(1,1,1)\\}\\) \u3067\u3059\u3002</p> <p>\u5c04\u5f71\u884c\u5217\u306f\uff1a \\(\\(P = u_1 u_1^T = \\frac{1}{3}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix}\\)\\)</p> <p>\u3053\u306e\u5c04\u5f71\u884c\u5217\u306e\u6027\u8cea\u3092\u78ba\u8a8d\u3057\u307e\u3059\uff1a - \\(P^2 = P\\) \uff08\u51aa\u7b49\u6027\uff09 - \\(P^T = P\\) \uff08\u5bfe\u79f0\u6027\uff09 - \u56fa\u6709\u5024\u306f \\(\\{1, 0, 0\\}\\) \uff080\u30681\u306e\u307f\uff09 - \\(\\text{tr}(P) = 1 = \\dim(U)\\)</p> <p>\u8a08\u7b97\u4f8b3: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u5e73\u9762 \\(U = \\{(x, y, z) \\mid x + y + z = 0\\}\\) \u3078\u306e\u5c04\u5f71\u884c\u5217\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\\(U\\) \u306e\u57fa\u5e95\u306f\u4f8b\u3048\u3070 \\(\\{(1,-1,0), (1,0,-1)\\}\\) \u3067\u3059\u304c\u3001\u3053\u308c\u306f\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u30b0\u30e9\u30e0\u30fb\u30b7\u30e5\u30df\u30c3\u30c8\u6cd5\u3067\u6b63\u898f\u76f4\u4ea4\u5316\u3057\u307e\u3059\uff1a</p> <p>\\(u_1 = \\frac{(1,-1,0)}{\\|(1,-1,0)\\|} = \\frac{1}{\\sqrt{2}}(1,-1,0)\\)</p> <p>\\(u_2\\) \u306f \\(u_1\\) \u3068 \\((1,0,-1)\\) \u304b\u3089\u6c42\u3081\u307e\u3059\uff1a \\(u_2' = (1,0,-1) - \\langle (1,0,-1), u_1 \\rangle u_1 = (1,0,-1) - \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}}(1,-1,0) = (1,0,-1) - \\frac{1}{2}(1,-1,0) = (\\frac{1}{2}, \\frac{1}{2}, -1)\\)</p> <p>\\(u_2 = \\frac{u_2'}{\\|u_2'\\|} = \\frac{(\\frac{1}{2}, \\frac{1}{2}, -1)}{\\sqrt{(\\frac{1}{2})^2 + (\\frac{1}{2})^2 + (-1)^2}} = \\frac{1}{\\sqrt{\\frac{1}{4} + \\frac{1}{4} + 1}} \\cdot (\\frac{1}{2}, \\frac{1}{2}, -1) = \\frac{1}{\\sqrt{\\frac{6}{4}}} \\cdot (\\frac{1}{2}, \\frac{1}{2}, -1) = \\frac{1}{\\sqrt{\\frac{3}{2}}} \\cdot (\\frac{1}{2}, \\frac{1}{2}, -1)\\)</p> <p>\\(u_2 = \\frac{1}{\\sqrt{6}}(1, 1, -2)\\)</p> <p>\u3088\u3063\u3066\u5c04\u5f71\u884c\u5217\u306f\uff1a \\(\\(P = u_1 u_1^T + u_2 u_2^T = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 &amp; -1 &amp; 0 \\end{pmatrix} + \\frac{1}{6}\\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 &amp; -2 \\end{pmatrix}\\)\\)</p> <p>\u3053\u308c\u3092\u8a08\u7b97\u3059\u308b\u3068\uff1a \\(\\(P = \\frac{1}{2}\\begin{pmatrix} 1 &amp; -1 &amp; 0 \\\\ -1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} + \\frac{1}{6}\\begin{pmatrix} 1 &amp; 1 &amp; -2 \\\\ 1 &amp; 1 &amp; -2 \\\\ -2 &amp; -2 &amp; 4 \\end{pmatrix} = \\frac{1}{6}\\begin{pmatrix} 4 &amp; -2 &amp; -2 \\\\ -2 &amp; 4 &amp; -2 \\\\ -2 &amp; -2 &amp; 4 \\end{pmatrix}\\)\\)</p> <p>\u3053\u306e\u5c04\u5f71\u884c\u5217\u306e\u6027\u8cea\u3092\u78ba\u8a8d\u3057\u307e\u3059\uff1a - \\(P^2 = P\\) \uff08\u51aa\u7b49\u6027\uff09 - \\(P^T = P\\) \uff08\u5bfe\u79f0\u6027\uff09 - \u56fa\u6709\u5024\u306f \\(\\{1, 1, 0\\}\\) \uff080\u30681\u306e\u307f\uff09 - \\(\\text{tr}(P) = 2 = \\dim(U)\\)</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#43","title":"4.3 \u76f4\u4ea4\u5c04\u5f71\u884c\u5217","text":"<p>\u5b9a\u7fa9\uff1a\u76f4\u4ea4\u5c04\u5f71\u884c\u5217 \u76f4\u4ea4\u5c04\u5f71\u3092\u8868\u3059\u884c\u5217 \\(P\\) \u306f\u4ee5\u4e0b\u306e\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\uff1a 1. \\(P^2 = P\\) \uff08\u51aa\u7b49\u6027\uff09 2. \\(P^T = P\\) \uff08\u5bfe\u79f0\u6027\uff09</p> <p>\u76f4\u4ea4\u5c04\u5f71\u884c\u5217\u3092\u7528\u3044\u308b\u3068\u3001\u90e8\u5206\u7a7a\u9593 \\(U\\) \u3078\u306e\u5c04\u5f71\u306f \\(P_U(v) = Pv\\) \u3068\u8868\u305b\u307e\u3059\u3002\u307e\u305f\u3001\\(U^{\\perp}\\) \u3078\u306e\u5c04\u5f71\u306f \\(P_{U^{\\perp}}(v) = (I-P)v\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u8a08\u7b97\u4f8b1: \\(\\mathbb{R}^2\\) \u306b\u304a\u3044\u3066\u3001\u76f4\u7dda \\(U = \\text{span}\\{(1, 1)\\}\\) \u3078\u306e\u5c04\u5f71\u884c\u5217\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\u6b63\u898f\u5316\u3059\u308b\u3068 \\(u_1 = \\frac{1}{\\sqrt{2}}(1, 1)\\) \u3068\u306a\u308a\u3001\u5c04\u5f71\u884c\u5217\u306f\uff1a \\(\\(P = u_1 u_1^T = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix}\\)\\)</p> <p>\u30d9\u30af\u30c8\u30eb \\(v = (3, 1)\\) \u306e \\(U\\) \u3078\u306e\u5c04\u5f71\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(Pv = \\frac{1}{2}\\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\\)\\)</p> <p>\\(U^{\\perp}\\) \u3078\u306e\u5c04\u5f71\u306f\uff1a \\(\\((I-P)v = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\)\\)</p> <p>\u5b9f\u969b\u306b \\(Pv\\) \u3068 \\((I-P)v\\) \u304c\u76f4\u4ea4\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\uff1a \\(\\(\\langle Pv, (I-P)v \\rangle = \\langle (2,2), (1,-1) \\rangle = 2 \\cdot 1 + 2 \\cdot (-1) = 0\\)\\)</p> <p>\u307e\u305f\u3001\\(v\\) \u304c \\(U\\) \u3068 \\(U^{\\perp}\\) \u306e\u76f4\u4ea4\u76f4\u548c\u3068\u3057\u3066\u8868\u3055\u308c\u308b\u3053\u3068\u3082\u78ba\u8a8d\u3067\u304d\u307e\u3059\uff1a \\(\\(v = Pv + (I-P)v = (2,2) + (1,-1) = (3,1)\\)\\)</p> <p>\u8a08\u7b97\u4f8b2: \\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u5e73\u9762 \\(U = \\{(x, y, z) \\mid x + 2y + 3z = 0\\}\\) \u3078\u306e\u5c04\u5f71\u884c\u5217\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\u5e73\u9762\u306e\u6cd5\u7dda\u30d9\u30af\u30c8\u30eb\u306f \\(n = (1, 2, 3)\\) \u3067\u3059\u3002\u6b63\u898f\u5316\u3059\u308b\u3068 \\(n_0 = \\frac{1}{\\sqrt{14}}(1, 2, 3)\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\\(U^{\\perp} = \\text{span}\\{n_0\\}\\) \u306a\u306e\u3067\u3001\\(U^{\\perp}\\) \u3078\u306e\u5c04\u5f71\u884c\u5217\u306f\uff1a \\(\\(P_{U^{\\perp}} = n_0 n_0^T = \\frac{1}{14}\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 6 \\\\ 3 &amp; 6 &amp; 9 \\end{pmatrix}\\)\\)</p> <p>\\(U\\) \u3078\u306e\u5c04\u5f71\u884c\u5217\u306f\uff1a \\(\\(P_U = I - P_{U^{\\perp}} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} - \\frac{1}{14}\\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 6 \\\\ 3 &amp; 6 &amp; 9 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 13 &amp; -2 &amp; -3 \\\\ -2 &amp; 10 &amp; -6 \\\\ -3 &amp; -6 &amp; 5 \\end{pmatrix}\\)\\)</p> <p>\u4f8b\u3048\u3070\u30d9\u30af\u30c8\u30eb \\(v = (1, 1, 1)\\) \u306e \\(U\\) \u3078\u306e\u5c04\u5f71\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(P_U v = \\frac{1}{14}\\begin{pmatrix} 13 &amp; -2 &amp; -3 \\\\ -2 &amp; 10 &amp; -6 \\\\ -3 &amp; -6 &amp; 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 8 \\\\ 2 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{7} \\\\ \\frac{1}{7} \\\\ -\\frac{2}{7} \\end{pmatrix}\\)\\)</p> <p>\u5b9f\u969b\u306b\u3053\u306e\u70b9\u304c\u5e73\u9762\u4e0a\u306b\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\uff1a \\(\\(1 \\cdot \\frac{4}{7} + 2 \\cdot \\frac{1}{7} + 3 \\cdot (-\\frac{2}{7}) = \\frac{4}{7} + \\frac{2}{7} - \\frac{6}{7} = 0\\)\\)</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#44","title":"4.4 \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3068\u76f4\u4ea4\u5c04\u5f71\u884c\u5217","text":"<p>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u3001\u30c7\u30fc\u30bf\u3092\u7279\u5b9a\u306e\u90e8\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u3059\u308b\u3053\u3068\u3067\u8fd1\u4f3c\u3059\u308b\u30e2\u30c7\u30eb\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> <p>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb: \\(y \\approx X\\beta\\) - \\(y\\): \u76ee\u7684\u5909\u6570\uff08\\(n\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb\uff09 - \\(X\\): \u8aac\u660e\u5909\u6570\u306e\u884c\u5217\uff08\\(n \\times p\\)\u884c\u5217\uff09 - \\(\\beta\\): \u56de\u5e30\u4fc2\u6570\uff08\\(p\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb\uff09</p> <p>\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b \\(\\beta\\) \u306e\u63a8\u5b9a\u306f\u3001\\(y\\) \u3092 \\(X\\) \u306e\u5217\u7a7a\u9593\u3078\u306e\u5c04\u5f71\u3068\u3057\u3066\u89e3\u91c8\u3067\u304d\u307e\u3059\uff1a</p> \\[\\hat{\\beta} = (X^TX)^{-1}X^Ty\\] <p>\u3053\u306e\u3068\u304d\u3001\u4e88\u6e2c\u5024 \\(\\hat{y} = X\\hat{\\beta} = X(X^TX)^{-1}X^Ty\\) \u306f\u3001\u5c04\u5f71\u884c\u5217 \\(P = X(X^TX)^{-1}X^T\\) \u3092\u7528\u3044\u3066 \\(\\hat{y} = Py\\) \u3068\u8868\u305b\u307e\u3059\u3002</p> <p>\u5177\u4f53\u4f8b: \u7c21\u5358\u306a\u5177\u4f53\u4f8b\u3067\u7dda\u5f62\u56de\u5e30\u3068\u5c04\u5f71\u306e\u95a2\u4fc2\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\u30c7\u30fc\u30bf\u70b9\u304c \\((x_1, y_1) = (1, 2)\\), \\((x_2, y_2) = (2, 3)\\), \\((x_3, y_3) = (3, 5)\\) \u306e3\u70b9\u304c\u3042\u308a\u3001\u76f4\u7dda \\(y = \\beta_0 + \\beta_1 x\\) \u3078\u306e\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\u3053\u306e\u3068\u304d\uff1a \\(\\(X = \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}\\)\\)</p> <p>\\(X^TX\\) \u3068 \\((X^TX)^{-1}\\) \u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(X^TX = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{pmatrix} = \\begin{pmatrix} 3 &amp; 6 \\\\ 6 &amp; 14 \\end{pmatrix}\\)\\)</p> \\[(X^TX)^{-1} = \\frac{1}{3 \\cdot 14 - 6 \\cdot 6} \\begin{pmatrix} 14 &amp; -6 \\\\ -6 &amp; 3 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 14 &amp; -6 \\\\ -6 &amp; 3 \\end{pmatrix}\\] <p>\u3053\u308c\u3092\u7528\u3044\u3066\u56de\u5e30\u4fc2\u6570\u3092\u6c42\u3081\u307e\u3059\uff1a \\(\\(\\hat{\\beta} = (X^TX)^{-1}X^Ty = \\frac{1}{6} \\begin{pmatrix} 14 &amp; -6 \\\\ -6 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}\\)\\)</p> \\[= \\frac{1}{6} \\begin{pmatrix} 14 &amp; -6 \\\\ -6 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 23 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 14 \\cdot 10 - 6 \\cdot 23 \\\\ -6 \\cdot 10 + 3 \\cdot 23 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 140 - 138 \\\\ -60 + 69 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 2 \\\\ 9 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{3}{2} \\end{pmatrix}\\] <p>\u3088\u3063\u3066\u56de\u5e30\u76f4\u7dda\u306f \\(y = \\frac{1}{3} + \\frac{3}{2}x\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u6b21\u306b\u5c04\u5f71\u884c\u5217\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(P = X(X^TX)^{-1}X^T = \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{pmatrix} \\frac{1}{6} \\begin{pmatrix} 14 &amp; -6 \\\\ -6 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 3 \\end{pmatrix}\\)\\)</p> <p>\u3053\u308c\u306f\u8a08\u7b97\u304c\u7169\u96d1\u306b\u306a\u308b\u305f\u3081\u3001\u76f4\u63a5 \\(\\hat{y} = X\\hat{\\beta}\\) \u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a \\(\\(\\hat{y} = \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} + \\frac{3}{2} \\cdot 1 \\\\ \\frac{1}{3} + \\frac{3}{2} \\cdot 2 \\\\ \\frac{1}{3} + \\frac{3}{2} \\cdot 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{11}{6} \\\\ \\frac{7}{2} \\\\ \\frac{29}{6} \\end{pmatrix} \\approx \\begin{pmatrix} 1.83 \\\\ 3.50 \\\\ 4.83 \\end{pmatrix}\\)\\)</p> <p>\u5b9f\u969b\u306e \\(y\\) \u3068\u4e88\u6e2c\u5024 \\(\\hat{y}\\) \u306e\u5dee\uff08\u6b8b\u5dee\uff09\u306f\uff1a \\(\\(y - \\hat{y} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} - \\begin{pmatrix} 1.83 \\\\ 3.50 \\\\ 4.83 \\end{pmatrix} = \\begin{pmatrix} 0.17 \\\\ -0.50 \\\\ 0.17 \\end{pmatrix}\\)\\)</p> <p>\u3053\u306e\u6b8b\u5dee\u30d9\u30af\u30c8\u30eb\u306f\u3001\\(X\\) \u306e\u5217\u7a7a\u9593\u3068\u76f4\u4ea4\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\uff1a \\(\\(X^T(y - \\hat{y}) = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 0.17 \\\\ -0.50 \\\\ 0.17 \\end{pmatrix} = \\begin{pmatrix} 0.17 - 0.50 + 0.17 \\\\ 0.17 - 1.00 + 0.51 \\end{pmatrix} \\approx \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\)\\)</p> <p>\u3053\u308c\u306f\u3001\\(y\\) \u3092 \\(X\\) \u306e\u5217\u7a7a\u9593\u306b\u5c04\u5f71\u3057\u305f\u3082\u306e\u304c \\(\\hat{y}\\) \u3067\u3042\u308a\u3001\u6b8b\u5dee \\(y - \\hat{y}\\) \u306f \\(X\\) \u306e\u5217\u7a7a\u9593\u306e\u76f4\u4ea4\u88dc\u7a7a\u9593\u306b\u5c5e\u3059\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u7dda\u5f62\u56de\u5e30\u306f\u3001\u30c7\u30fc\u30bf\u70b9 \\(y\\) \u3092\u8aac\u660e\u5909\u6570\u306b\u3088\u3063\u3066\u5f35\u3089\u308c\u308b\u90e8\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u3059\u308b\u3082\u306e\u3068\u89e3\u91c8\u3067\u304d\u308b\u306e\u3067\u3059\u3002</p> <p>\u5fdc\u7528\u4f8b: \u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306b\u304a\u3044\u3066\u3001\u76ee\u7684\u5909\u6570 \\(y\\) \u306b\u5bfe\u3059\u308b\u8907\u6570\u306e\u8aac\u660e\u5909\u6570 \\(X_1, X_2, \\ldots, X_p\\) \u306e\u5f71\u97ff\u3092\u5206\u6790\u3059\u308b\u969b\u3001\u5404\u8aac\u660e\u5909\u6570\u306e\u5bc4\u4e0e\u306f\u5c04\u5f71\u306e\u89b3\u70b9\u304b\u3089\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u8840\u5727\uff08\\(y\\)\uff09\u3092\u5e74\u9f62\uff08\\(X_1\\)\uff09\u3068BMI\uff08\\(X_2\\)\uff09\u3067\u4e88\u6e2c\u3059\u308b\u5834\u5408\u3001\u5c04\u5f71\u884c\u5217 \\(P = X(X^TX)^{-1}X^T\\) \u306f\u3001\u8840\u5727\u30c7\u30fc\u30bf\u3092\u5e74\u9f62\u3068BMI\u3067\u8aac\u660e\u53ef\u80fd\u306a\u90e8\u5206\u7a7a\u9593\u3078\u5c04\u5f71\u3057\u307e\u3059\u3002\u6b8b\u5dee\u5206\u6790\uff08\\(y - \\hat{y}\\)\uff09\u306f\u3001\u5e74\u9f62\u3068BMI\u3067\u8aac\u660e\u3067\u304d\u306a\u3044\u5909\u52d5\u3092\u8868\u3057\u3001\u4ed6\u306e\u8981\u56e0\uff08\u4f8b\uff1a\u5869\u5206\u6442\u53d6\u91cf\u3001\u904b\u52d5\u91cf\u306a\u3069\uff09\u306e\u5f71\u97ff\u3092\u793a\u5506\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/28-vector-space-and-inner-product/#51","title":"5.1 \u76f4\u4ea4\u88dc\u7a7a\u9593\u306e\u8a08\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u90e8\u5206\u7a7a\u9593\u306e\u57fa\u5e95\u3092\u5b9a\u7fa9\nU_basis = np.array([[1, 0, 0], [0, 1, 0]]).T  # xy\u5e73\u9762\u306e\u57fa\u5e95\n\n# \u76f4\u4ea4\u88dc\u7a7a\u9593\u3092\u8a08\u7b97\uff08\u30cc\u30eb\u7a7a\u9593\u3092\u6c42\u3081\u308b\uff09\nU_perp = np.linalg.null_space(U_basis.T)\n\nprint(\"U_basis =\\n\", U_basis)\nprint(\"U_perp =\\n\", U_perp)\n\n# 3D\u7a7a\u9593\u3067\u53ef\u8996\u5316\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# xy\u5e73\u9762\u306e\u8868\u793a\nxx, yy = np.meshgrid(range(-2, 3), range(-2, 3))\nz = np.zeros_like(xx)\nax.plot_surface(xx, yy, z, alpha=0.2, color='b')\n\n# \u76f4\u4ea4\u88dc\u7a7a\u9593\u306e\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a\nax.quiver(0, 0, 0, 0, 0, 1, color='r', arrow_length_ratio=0.1, length=2)\n\nax.set_xlim([-2, 2])\nax.set_ylim([-2, 2])\nax.set_zlim([-2, 2])\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('xy\u5e73\u9762(\u9752)\u3068\u305d\u306e\u76f4\u4ea4\u88dc\u7a7a\u9593(\u8d64)')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#52","title":"5.2 \u5c04\u5f71\u306e\u8a08\u7b97\u3068\u53ef\u8996\u5316","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u30d9\u30af\u30c8\u30eb\u3068\u305d\u306e\u90e8\u5206\u7a7a\u9593\u3078\u306e\u5c04\u5f71\u3092\u53ef\u8996\u5316\ndef plot_projection(v, u):\n    # u\u3092\u6b63\u898f\u5316\n    u_norm = u / np.linalg.norm(u)\n\n    # v\u306eu\u3078\u306e\u5c04\u5f71\n    proj = np.dot(v, u_norm) * u_norm\n\n    # v\u306eu_perp\u3078\u306e\u5c04\u5f71\uff08\u6b8b\u5dee\uff09\n    perp = v - proj\n\n    # \u30d7\u30ed\u30c3\u30c8\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # \u539f\u70b9\u3068\u5404\u30d9\u30af\u30c8\u30eb\u3092\u30d7\u30ed\u30c3\u30c8\n    ax.quiver(0, 0, u_norm[0], u_norm[1], angles='xy', scale_units='xy', scale=1, color='r', label='\u57fa\u5e95\u30d9\u30af\u30c8\u30eb')\n    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='b', label='\u30d9\u30af\u30c8\u30ebv')\n    ax.quiver(0, 0, proj[0], proj[1], angles='xy', scale_units='xy', scale=1, color='g', label='\u5c04\u5f71')\n    ax.quiver(proj[0], proj[1], perp[0], perp[1], angles='xy', scale_units='xy', scale=1, color='m', label='\u6b8b\u5dee')\n\n    # \u5c04\u5f71\u306e\u7dda\u3092\u70b9\u7dda\u3067\u8868\u793a\n    ax.plot([v[0], proj[0]], [v[1], proj[1]], 'k--')\n\n    # \u30d7\u30ed\u30c3\u30c8\u306e\u7bc4\u56f2\u8a2d\u5b9a\u3068\u8ef8\u306e\u8abf\u6574\n    max_val = max(np.max(np.abs(v)), np.max(np.abs(u_norm))) * 1.2\n    ax.set_xlim([-max_val, max_val])\n    ax.set_ylim([-max_val, max_val])\n    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n    ax.grid(True, alpha=0.3)\n\n    # \u30bf\u30a4\u30c8\u30eb\u3068\u51e1\u4f8b\n    ax.set_title('\u30d9\u30af\u30c8\u30eb\u306e\u5c04\u5f71')\n    ax.legend()\n\n    return proj, perp\n\n# \u4f8b\uff1a\u30d9\u30af\u30c8\u30ebv\u3068\u90e8\u5206\u7a7a\u9593\u306e\u57fa\u5e95u\nv = np.array([3, 4])\nu = np.array([1, 0])  # x\u8ef8\u65b9\u5411\n\n# \u5c04\u5f71\u3092\u8a08\u7b97\u3057\u53ef\u8996\u5316\nproj, perp = plot_projection(v, u)\n\nprint(f\"\u30d9\u30af\u30c8\u30ebv = {v}\")\nprint(f\"\u57fa\u5e95\u30d9\u30af\u30c8\u30ebu = {u}\")\nprint(f\"\u5c04\u5f71 = {proj}\")\nprint(f\"\u6b8b\u5dee = {perp}\")\nplt.show()\n</code></pre>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#53","title":"5.3 \u5c04\u5f71\u884c\u5217\u306e\u5b9f\u88c5\u3068\u78ba\u8a8d","text":"<pre><code>import numpy as np\n\n# \u5c04\u5f71\u884c\u5217\u306e\u69cb\u7bc9\u3068\u6027\u8cea\u78ba\u8a8d\ndef projection_matrix_properties(basis):\n    # \u57fa\u5e95\u306e\u6b63\u898f\u76f4\u4ea4\u5316\n    Q, _ = np.linalg.qr(basis)\n\n    # \u5c04\u5f71\u884c\u5217\u306e\u69cb\u7bc9\n    P = Q @ Q.T\n\n    # \u6027\u8cea\u306e\u78ba\u8a8d\n    print(\"\u5c04\u5f71\u884c\u5217P =\\n\", P)\n    print(\"\\n\u51aa\u7b49\u6027 P^2 = P?\")\n    print(np.allclose(P @ P, P))\n\n    print(\"\\n\u5bfe\u79f0\u6027 P^T = P?\")\n    print(np.allclose(P.T, P))\n\n    print(\"\\n\u56fa\u6709\u5024:\")\n    eigvals = np.linalg.eigvals(P)\n    print(np.round(eigvals, 10))\n\n    print(\"\\n\u30c8\u30ec\u30fc\u30b9:\", np.trace(P))\n\n    return P, Q\n\n# \u4f8b\uff1aR^3\u306e\u90e8\u5206\u7a7a\u9593\uff08xy\u5e73\u9762\uff09\u306e\u57fa\u5e95\nbasis = np.array([[1, 0, 0], [0, 1, 0]]).T\n\nP, Q = projection_matrix_properties(basis)\n\n# \u5c04\u5f71\u306e\u4f8b\nv = np.array([1, 2, 3])\nprojection = P @ v\nresidual = v - projection\n\nprint(\"\\n\u30d9\u30af\u30c8\u30ebv =\", v)\nprint(\"\u5c04\u5f71 =\", projection)\nprint(\"\u6b8b\u5dee =\", residual)\nprint(\"\u5c04\u5f71\u3068\u6b8b\u5dee\u306f\u76f4\u4ea4\u3057\u3066\u3044\u308b\u304b\uff1f\", np.allclose(np.dot(projection, residual), 0))\n</code></pre>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#54","title":"5.4 \u7dda\u5f62\u56de\u5e30\u3068\u5c04\u5f71\u306e\u95a2\u4fc2","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# \u30c7\u30fc\u30bf\u306e\u751f\u6210\nnp.random.seed(42)\nX = np.random.rand(50, 1) * 10\ny = 2 * X.squeeze() + 1 + np.random.randn(50) * 2\n\n# \u7dda\u5f62\u56de\u5e30\nX_with_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # \u5207\u7247\u306e\u305f\u3081\u306e\u5217\u3092\u8ffd\u52a0\nbeta = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\ny_pred = X_with_bias @ beta\n\n# \u5c04\u5f71\u884c\u5217\nP = X_with_bias @ np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T\ny_proj = P @ y\n\n# \u5c04\u5f71\u884c\u5217\u306e\u6027\u8cea\u78ba\u8a8d\nprint(\"P @ P \u2248 P?\", np.allclose(P @ P, P))\nprint(\"P.T \u2248 P?\", np.allclose(P.T, P))\nprint(\"P @ y \u2248 y_pred?\", np.allclose(P @ y, y_pred))\n\n# \u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, color='blue', label='\u30c7\u30fc\u30bf')\nplt.plot(X, y_pred, color='red', label='\u56de\u5e30\u76f4\u7dda')\n\n# \u30c7\u30fc\u30bf\u70b9\u304b\u3089\u56de\u5e30\u76f4\u7dda\u3078\u306e\u5c04\u5f71\u7dda\u3092\u63cf\u753b\nfor i in range(len(X)):\n    plt.plot([X[i], X[i]], [y[i], y_pred[i]], 'k--', alpha=0.3)\n\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('\u7dda\u5f62\u56de\u5e30\u3068\u5c04\u5f71\u306e\u95a2\u4fc2')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# \u6b8b\u5dee\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nresiduals = y - y_pred\nplt.stem(residuals)\nplt.axhline(y=0, color='r', linestyle='-')\nplt.xlabel('\u30c7\u30fc\u30bf\u70b9')\nplt.ylabel('\u6b8b\u5dee')\nplt.title('\u6b8b\u5dee\uff08y - P@y\uff09')\nplt.grid(alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/28-vector-space-and-inner-product/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\\(\\mathbb{R}^3\\) \u306b\u304a\u3044\u3066\u3001\u30d9\u30af\u30c8\u30eb \\(v = (2, 3, 4)\\) \u306e\u4ee5\u4e0b\u306e\u90e8\u5206\u7a7a\u9593\u3078\u306e\u5c04\u5f71\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    a) \\(U_1 = \\text{span}\\{(1, 0, 0)\\}\\) \uff08x\u8ef8\uff09    b) \\(U_2 = \\text{span}\\{(1, 1, 0), (0, 0, 1)\\}\\)</p> </li> <li> <p>\\(\\mathbb{R}^3\\) \u306b\u304a\u3051\u308b\u6b21\u306e\u90e8\u5206\u7a7a\u9593 \\(U = \\text{span}\\{(1, 1, 1), (1, 2, 0)\\}\\) \u306b\u3064\u3044\u3066\uff1a    a) \\(U\\) \u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    b) \\(U\\) \u306e\u5c04\u5f71\u884c\u5217 \\(P\\) \u3092\u6c42\u3081\u306a\u3055\u3044\u3002    c) \\(U^{\\perp}\\) \u306e\u57fa\u5e95\u3068\u5c04\u5f71\u884c\u5217 \\(I-P\\) \u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u6b21\u306e\u5c04\u5f71\u884c\u5217 \\(P = \\begin{pmatrix} 2/3 &amp; 1/3 &amp; 2/3 \\\\ 1/3 &amp; 2/3 &amp; 1/3 \\\\ 2/3 &amp; 1/3 &amp; 2/3 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\uff1a    a) \\(P\\) \u304c\u5c04\u5f71\u884c\u5217\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002    b) \\(P\\) \u306e\u968e\u6570\uff08\u30e9\u30f3\u30af\uff09\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    c) \\(P\\) \u306e\u50cf\u7a7a\u9593\uff08\u5217\u7a7a\u9593\uff09\u3092\u57fa\u5e95\u3067\u8868\u3057\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u30013\u3064\u306e\u5065\u5eb7\u6307\u6a19\uff08\u8840\u5727\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u3001\u8840\u7cd6\u5024\uff09\u304c\u6e2c\u5b9a\u3055\u308c\u305f\u30c7\u30fc\u30bf\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u6307\u6a19\u306f\u6a19\u6e96\u5316\u3055\u308c\u3066\u304a\u308a\u3001\u8840\u5727\u3068\u8840\u7cd6\u5024\u306b\u306f\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u3066\u3044\u307e\u3059\u3002</li> </ol> <p>a) \u8840\u5727\u3068\u8840\u7cd6\u5024\u3067\u5f35\u3089\u308c\u308b\u90e8\u5206\u7a7a\u9593 \\(U\\) \u3092\u8003\u3048\u3001\u3053\u306e\u90e8\u5206\u7a7a\u9593\u3078\u306e\u5c04\u5f71\u884c\u5217\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    b) \u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u304c \\(U\\) \u3068\u76f4\u4ea4\u3059\u308b\u3088\u3046\u306b\u30013\u3064\u306e\u5065\u5eb7\u6307\u6a19\u306e\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u69cb\u6210\u3057\u306a\u3055\u3044\u3002    c) \u3042\u308b\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8 \\((0.8, 0.5, 0.3)\\) \u306e \\(U\\) \u3078\u306e\u5c04\u5f71\u3068 \\(U^{\\perp}\\) \u3078\u306e\u5c04\u5f71\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    d) \u3053\u306e\u5c04\u5f71\u306e\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u89e3\u91c8\u3092\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> <ol> <li>\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb \\(y = X\\beta + \\varepsilon\\) \u306b\u304a\u3044\u3066\u3001\\(X\\) \u306f \\(n \\times p\\) \u884c\u5217\u3001\\(\\beta\\) \u306f \\(p\\) \u6b21\u5143\u30d1\u30e9\u30e1\u30fc\u30bf\u30d9\u30af\u30c8\u30eb\u3001\\(\\varepsilon\\) \u306f\u8aa4\u5dee\u9805\u3068\u3057\u307e\u3059\u3002</li> </ol> <p>a) \u6700\u5c0f\u4e8c\u4e57\u63a8\u5b9a\u91cf \\(\\hat{\\beta} = (X^TX)^{-1}X^Ty\\) \u304c\u3001\\(y\\) \u306e \\(X\\) \u306e\u5217\u7a7a\u9593\u3078\u306e\u5c04\u5f71\u3092\u610f\u5473\u3059\u308b\u3053\u3068\u3092\u793a\u3057\u306a\u3055\u3044\u3002    b) \u6b8b\u5dee \\(e = y - X\\hat{\\beta}\\) \u304c \\(X\\) \u306e\u5217\u7a7a\u9593\u3068\u76f4\u4ea4\u3059\u308b\u3053\u3068\u3092\u793a\u3057\u306a\u3055\u3044\u3002    c) \u30e2\u30c7\u30eb\u306b\u65b0\u3057\u3044\u8aac\u660e\u5909\u6570\u3092\u8ffd\u52a0\u3059\u308b\u3068\u3001\\(R^2\\) \u5024\uff08\u6c7a\u5b9a\u4fc2\u6570\uff09\u304c\u5e38\u306b\u5897\u52a0\u307e\u305f\u306f\u7dad\u6301\u3055\u308c\u308b\u7406\u7531\u3092\u3001\u5c04\u5f71\u306e\u89b3\u70b9\u304b\u3089\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/28-vector-space-and-inner-product/#q1","title":"Q1: \u76f4\u548c\u5206\u89e3\u3068\u76f4\u4ea4\u76f4\u548c\u5206\u89e3\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A1: \u76f4\u548c\u5206\u89e3 \\(V = U \\oplus W\\) \u306f\u3001\\(V\\) \u306e\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb\u304c \\(U\\) \u3068 \\(W\\) \u306e\u6210\u5206\u306b\u4e00\u610f\u306b\u5206\u89e3\u3067\u304d\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u76f4\u4ea4\u76f4\u548c\u5206\u89e3 \\(V = U \\oplus^{\\perp} W\\) \u306f\u3001\u76f4\u548c\u5206\u89e3\u306e\u6761\u4ef6\u306b\u52a0\u3048\u3066\u3001\\(U\\) \u3068 \\(W\\) \u304c\u4e92\u3044\u306b\u76f4\u4ea4\u3057\u3066\u3044\u308b\uff08\\(\\langle u, w \\rangle = 0\\) for all \\(u \\in U, w \\in W\\)\uff09\u3068\u3044\u3046\u6761\u4ef6\u3092\u6e80\u305f\u3059\u5834\u5408\u3067\u3059\u3002\u76f4\u4ea4\u76f4\u548c\u5206\u89e3\u306e\u5834\u5408\u3001\u5c04\u5f71\u304c\u8a08\u7b97\u3057\u3084\u3059\u304f\u306a\u308a\u3001\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u3082\u660e\u78ba\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#q2-01","title":"Q2: \u5c04\u5f71\u884c\u5217\u306e\u56fa\u6709\u5024\u306f\u306a\u305c0\u30681\u3060\u3051\u306a\u306e\u3067\u3059\u304b\uff1f","text":"<p>A2: \u5c04\u5f71\u884c\u5217 \\(P\\) \u306f\u51aa\u7b49\u6027 \\(P^2 = P\\) \u3092\u6301\u3061\u307e\u3059\u3002\\(P\\) \u306e\u56fa\u6709\u5024\u3092 \\(\\lambda\\) \u3068\u3059\u308b\u3068\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(v\\) \u306b\u5bfe\u3057\u3066 \\(Pv = \\lambda v\\) \u3067\u3059\u3002\u3053\u3053\u3067 \\(P(Pv) = P^2v = Pv = \\lambda v\\) \u3088\u308a\u3001\\(\\lambda^2 v = \\lambda v\\) \u3068\u306a\u308a\u307e\u3059\u3002\\(v \\neq 0\\) \u306a\u306e\u3067\u3001\\(\\lambda^2 = \\lambda\\) \u3068\u306a\u308a\u3001\u3053\u308c\u3092\u89e3\u304f\u3068 \\(\\lambda = 0\\) \u307e\u305f\u306f \\(\\lambda = 1\\) \u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u5c04\u5f71\u884c\u5217\u306e\u56fa\u6709\u5024\u306f0\u30681\u3060\u3051\u3067\u3059\u3002</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#q3","title":"Q3: \u7dda\u5f62\u56de\u5e30\u306b\u304a\u3051\u308b\u5c04\u5f71\u306e\u610f\u5473\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A3: \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb \\(y \\approx X\\beta\\) \u306b\u304a\u3044\u3066\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3067\u5f97\u3089\u308c\u308b \\(\\hat{y} = X\\hat{\\beta} = X(X^TX)^{-1}X^Ty\\) \u306f\u3001\\(y\\) \u3092 \\(X\\) \u306e\u5217\u7a7a\u9593\u306b\u5c04\u5f71\u3057\u305f\u3082\u306e\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u7dda\u5f62\u56de\u5e30\u306f\u76ee\u7684\u5909\u6570 \\(y\\) \u3092\u8aac\u660e\u5909\u6570 \\(X\\) \u3067\u5f35\u3089\u308c\u308b\u90e8\u5206\u7a7a\u9593\u306b\u6700\u3082\u8fd1\u3065\u3051\u308b\uff08\u4e8c\u4e57\u8aa4\u5dee\u3092\u6700\u5c0f\u5316\u3059\u308b\uff09\u3088\u3046\u306b\u5c04\u5f71\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u3002\u6b8b\u5dee \\(y - \\hat{y}\\) \u306f \\(X\\) \u306e\u5217\u7a7a\u9593\u3068\u76f4\u4ea4\u3057\u3066\u304a\u308a\u3001\u3053\u308c\u306f\u8aa4\u5dee\u304c\u8aac\u660e\u5909\u6570\u3068\u7121\u76f8\u95a2\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#q4","title":"Q4: \u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u4f7f\u3046\u30e1\u30ea\u30c3\u30c8\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A4: \u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u4f7f\u3046\u3068\u4ee5\u4e0b\u306e\u30e1\u30ea\u30c3\u30c8\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u5ea7\u6a19\u306e\u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308b\uff08\u5185\u7a4d\u3067\u76f4\u63a5\u6c42\u307e\u308b\uff09 2. \u5c04\u5f71\u306e\u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308b 3. \u5909\u63db\u884c\u5217\u304c\u76f4\u4ea4\u884c\u5217\u3068\u306a\u308a\u3001\u9006\u884c\u5217\u304c\u8ee2\u7f6e\u884c\u5217\u3068\u7b49\u3057\u304f\u306a\u308b 4. \u6570\u5024\u8a08\u7b97\u4e0a\u306e\u5b89\u5b9a\u6027\u304c\u5411\u4e0a\u3059\u308b 5. \u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u304c\u660e\u78ba\u306b\u306a\u308b</p>"},{"location":"lectures/LA/28-vector-space-and-inner-product/#q5","title":"Q5: \u30d9\u30af\u30c8\u30eb\u306e\u76f4\u4ea4\u5206\u89e3\u306f\u4e00\u610f\u306b\u6c7a\u307e\u308a\u307e\u3059\u304b\uff1f","text":"<p>A5: \u30d9\u30af\u30c8\u30eb \\(v \\in V\\) \u306e\u76f4\u4ea4\u5206\u89e3 \\(v = u + w\\) \uff08\\(u \\in U, w \\in U^{\\perp}\\)\uff09\u306f\u4e00\u610f\u306b\u6c7a\u307e\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u76f4\u4ea4\u88dc\u7a7a\u9593 \\(U^{\\perp}\\) \u304c\u4e00\u610f\u306b\u5b9a\u307e\u308b\u3053\u3068\u3068\u3001\u76f4\u4ea4\u76f4\u548c\u5206\u89e3 \\(V = U \\oplus^{\\perp} U^{\\perp}\\) \u306b\u304a\u3051\u308b\u5206\u89e3\u306e\u4e00\u610f\u6027\u306b\u3088\u308a\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u7570\u306a\u308b\u90e8\u5206\u7a7a\u9593 \\(U_1, U_2\\) \u306b\u95a2\u3057\u3066\u306f\u3001\u540c\u3058\u30d9\u30af\u30c8\u30eb \\(v\\) \u306e\u76f4\u4ea4\u5206\u89e3\u306f\u4e00\u822c\u306b\u7570\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II\uff1a\u7b2c32\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/32-eigen-value-vector/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c32\u56de \u30c6\u30fc\u30de: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u305f\u3081\u306e\u7dda\u5f62\u5909\u63db\u306e\u57fa\u790e \u95a2\u9023\u9805\u76ee: \u7dda\u5f62\u5909\u63db\u3001\u884c\u5217\u306b\u3088\u308b\u5909\u63db\u3001\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5c0e\u5165 \u4e88\u7fd2\u5185\u5bb9: \u884c\u5217\u306e\u6f14\u7b97\u3001\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u672c\u7684\u6027\u8cea\u3001\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3001\u5185\u7a4d\u306e\u6982\u5ff5</p>"},{"location":"lectures/LA/32-eigen-value-vector/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u7dda\u5f62\u5909\u63db\u306e\u6982\u5ff5\u3068\u6570\u5b66\u7684\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b</li> <li>\u884c\u5217\u306b\u3088\u308b\u7dda\u5f62\u5909\u63db\u306e\u8868\u73fe\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u4ee3\u8868\u7684\u306a\u7dda\u5f62\u5909\u63db\uff08\u56de\u8ee2\u3001\u62e1\u5927\u30fb\u7e2e\u5c0f\u3001\u305b\u3093\u65ad\u306a\u3069\uff09\u3068\u305d\u306e\u884c\u5217\u8868\u73fe\u3092\u5b66\u3076</li> <li>\u7dda\u5f62\u5909\u63db\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5b66\u7fd2\u306b\u5fc5\u8981\u306a\u7dda\u5f62\u5909\u63db\u306e\u57fa\u790e\u77e5\u8b58\u3092\u8eab\u306b\u3064\u3051\u308b</li> </ol>"},{"location":"lectures/LA/32-eigen-value-vector/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/32-eigen-value-vector/#31","title":"3.1 \u7dda\u5f62\u5909\u63db\u306e\u5b9a\u7fa9\u3068\u6027\u8cea","text":"<p>\u5b9a\u7fa9 3.1 (\u7dda\u5f62\u5909\u63db) \u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(V\\) \u304b\u3089 \u30d9\u30af\u30c8\u30eb\u7a7a\u9593 \\(W\\) \u3078\u306e\u5199\u50cf \\(T: V \\rightarrow W\\) \u304c\u6b21\u306e2\u3064\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u3068\u304d\u3001\\(T\\) \u3092\u7dda\u5f62\u5909\u63db\u3068\u3044\u3046\uff1a 1. \u52a0\u6cd5\u6027: \u3059\u3079\u3066\u306e \\(\\mathbf{u}, \\mathbf{v} \\in V\\) \u306b\u5bfe\u3057\u3066 \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) 2. \u6589\u6b21\u6027: \u3059\u3079\u3066\u306e \\(\\mathbf{v} \\in V\\) \u3068\u4efb\u610f\u306e\u30b9\u30ab\u30e9\u30fc \\(c\\) \u306b\u5bfe\u3057\u3066 \\(T(c \\mathbf{v}) = c T(\\mathbf{v})\\)</p> <p>\u3053\u308c\u306f\u3001\u7dda\u5f62\u5909\u63db\u304c\u300c\u30d9\u30af\u30c8\u30eb\u306e\u548c\u300d\u3068\u300c\u30b9\u30ab\u30e9\u30fc\u500d\u300d\u3068\u3044\u3046\u57fa\u672c\u7684\u306a\u6f14\u7b97\u3092\u4fdd\u5b58\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u8a00\u3044\u63db\u3048\u308b\u3068\u3001\u7dda\u5f62\u5909\u63db\u306f\u6b21\u306e\u3088\u3046\u306a\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\uff1a</p> <ul> <li>\u30d9\u30af\u30c8\u30eb\u3092\u8db3\u3057\u3066\u304b\u3089\u5909\u63db\u3057\u3066\u3082\u3001\u5909\u63db\u3057\u3066\u304b\u3089\u8db3\u3057\u3066\u3082\u7d50\u679c\u306f\u540c\u3058</li> <li>\u30d9\u30af\u30c8\u30eb\u3092\u30b9\u30ab\u30e9\u30fc\u500d\u3057\u3066\u304b\u3089\u5909\u63db\u3057\u3066\u3082\u3001\u5909\u63db\u3057\u3066\u304b\u3089\u30b9\u30ab\u30e9\u30fc\u500d\u3057\u3066\u3082\u7d50\u679c\u306f\u540c\u3058</li> </ul>"},{"location":"lectures/LA/32-eigen-value-vector/#32","title":"3.2 \u7dda\u5f62\u5909\u63db\u306e\u4f8b","text":"<p>\u7dda\u5f62\u5909\u63db\u306e\u4f8b\u3068\u3057\u3066\u4ee5\u4e0b\u306e\u3082\u306e\u304c\u6319\u3052\u3089\u308c\u307e\u3059\uff1a</p> <ol> <li>\u6052\u7b49\u5909\u63db \\(I(\\mathbf{v}) = \\mathbf{v}\\)\uff1a\u30d9\u30af\u30c8\u30eb\u3092\u5909\u5316\u3055\u305b\u306a\u3044</li> <li>\u96f6\u5909\u63db \\(O(\\mathbf{v}) = \\mathbf{0}\\)\uff1a\u3059\u3079\u3066\u306e\u30d9\u30af\u30c8\u30eb\u3092\u96f6\u30d9\u30af\u30c8\u30eb\u306b\u5199\u3059</li> <li>\u30b9\u30ab\u30e9\u30fc\u500d \\(T(\\mathbf{v}) = c\\mathbf{v}\\)\uff1a\u3059\u3079\u3066\u306e\u30d9\u30af\u30c8\u30eb\u3092\u5b9a\u6570 \\(c\\) \u500d\u3059\u308b</li> <li>\u53cd\u8ee2 \\(T(\\mathbf{v}) = -\\mathbf{v}\\)\uff1a\u3059\u3079\u3066\u306e\u30d9\u30af\u30c8\u30eb\u306e\u5411\u304d\u3092\u53cd\u5bfe\u306b\u3059\u308b</li> <li>\u5c04\u5f71 \\(P(\\mathbf{v})\\)\uff1a\u30d9\u30af\u30c8\u30eb\u3092\u90e8\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u3059\u308b</li> </ol> <p>\u4ee5\u4e0b\u306e\u5909\u63db\u3082\u7dda\u5f62\u5909\u63db\u3067\u3059\uff1a</p> <ol> <li>\u56de\u8ee2\uff1a\u30d9\u30af\u30c8\u30eb\u3092\u7279\u5b9a\u306e\u89d2\u5ea6\u3067\u56de\u8ee2\u3055\u305b\u308b</li> <li>\u62e1\u5927\u30fb\u7e2e\u5c0f\uff1a\u30d9\u30af\u30c8\u30eb\u306e\u9577\u3055\u3092\u7279\u5b9a\u306e\u6bd4\u7387\u3067\u62e1\u5927\u30fb\u7e2e\u5c0f\u3059\u308b</li> <li>\u305b\u3093\u65ad\uff1a\u30d9\u30af\u30c8\u30eb\u3092\u3042\u308b\u65b9\u5411\u306b\u6cbf\u3063\u3066\u305a\u3089\u3059</li> </ol> <p>\u7dda\u5f62\u3067\u306f\u306a\u3044\u5909\u63db\u306e\u4f8b\uff1a</p> <ul> <li>\\(T(\\mathbf{v}) = \\mathbf{v} + \\mathbf{b}\\) (\u305f\u3060\u3057 \\(\\mathbf{b} \\neq \\mathbf{0}\\))\uff1a\u5e73\u884c\u79fb\u52d5\u306f\u7dda\u5f62\u5909\u63db\u3067\u306f\u306a\u3044</li> <li>\\(T(\\mathbf{v}) = \\mathbf{v}^2\\)\uff1a\u4e8c\u4e57\u306f\u7dda\u5f62\u5909\u63db\u3067\u306f\u306a\u3044</li> </ul>"},{"location":"lectures/LA/32-eigen-value-vector/#33","title":"3.3 \u7dda\u5f62\u6027\u306e\u691c\u8a3c","text":"<p>\u5909\u63db\u304c\u7dda\u5f62\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3059\u308b\u306b\u306f\u3001\u52a0\u6cd5\u6027\u3068\u6589\u6b21\u6027\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> <p>\u4f8b 3.1\uff1a\\(\\mathbb{R}^2\\) \u4e0a\u306e\u5909\u63db \\(T(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) = \\begin{bmatrix} 2x \\\\ 3y \\end{bmatrix}\\) \u304c\u7dda\u5f62\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046\u3002</p> <ol> <li> <p>\u52a0\u6cd5\u6027\uff1a    \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} + \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}) = T(\\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\end{bmatrix}) = \\begin{bmatrix} 2(u_1 + v_1) \\\\ 3(u_2 + v_2) \\end{bmatrix} = \\begin{bmatrix} 2u_1 + 2v_1 \\\\ 3u_2 + 3v_2 \\end{bmatrix} = \\begin{bmatrix} 2u_1 \\\\ 3u_2 \\end{bmatrix} + \\begin{bmatrix} 2v_1 \\\\ 3v_2 \\end{bmatrix} = T(\\mathbf{u}) + T(\\mathbf{v})\\)</p> </li> <li> <p>\u6589\u6b21\u6027\uff1a    \\(T(c\\mathbf{v}) = T(c\\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}) = T(\\begin{bmatrix} cv_1 \\\\ cv_2 \\end{bmatrix}) = \\begin{bmatrix} 2(cv_1) \\\\ 3(cv_2) \\end{bmatrix} = \\begin{bmatrix} c(2v_1) \\\\ c(3v_2) \\end{bmatrix} = c\\begin{bmatrix} 2v_1 \\\\ 3v_2 \\end{bmatrix} = cT(\\mathbf{v})\\)</p> </li> </ol> <p>\u3088\u3063\u3066\u3001\u3053\u306e\u5909\u63db\u306f\u7dda\u5f62\u3067\u3059\u3002</p> <p>\u4f8b 3.2\uff1a\\(\\mathbb{R}^2\\) \u4e0a\u306e\u5909\u63db \\(T(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) = \\begin{bmatrix} x + 1 \\\\ y - 2 \\end{bmatrix}\\) \u304c\u7dda\u5f62\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046\u3002</p> <p>\u52a0\u6cd5\u6027\u3092\u78ba\u8a8d\u3057\u307e\u3059\uff1a \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} + \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}) = T(\\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\end{bmatrix}) = \\begin{bmatrix} (u_1 + v_1) + 1 \\\\ (u_2 + v_2) - 2 \\end{bmatrix} = \\begin{bmatrix} u_1 + v_1 + 1 \\\\ u_2 + v_2 - 2 \\end{bmatrix}\\)</p> <p>\u4e00\u65b9\u3001 \\(T(\\mathbf{u}) + T(\\mathbf{v}) = \\begin{bmatrix} u_1 + 1 \\\\ u_2 - 2 \\end{bmatrix} + \\begin{bmatrix} v_1 + 1 \\\\ v_2 - 2 \\end{bmatrix} = \\begin{bmatrix} u_1 + v_1 + 2 \\\\ u_2 + v_2 - 4 \\end{bmatrix}\\)</p> <p>\\(T(\\mathbf{u} + \\mathbf{v}) \\neq T(\\mathbf{u}) + T(\\mathbf{v})\\) \u306a\u306e\u3067\u3001\u3053\u306e\u5909\u63db\u306f\u7dda\u5f62\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> <p>\u3053\u308c\u306f\u5e73\u884c\u79fb\u52d5\u3092\u542b\u3080\u5909\u63db\u3067\u3042\u308a\u3001\u4e00\u822c\u306b\u5e73\u884c\u79fb\u52d5\u306f\u7dda\u5f62\u5909\u63db\u3067\u306f\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/32-eigen-value-vector/#41","title":"4.1 \u7dda\u5f62\u5909\u63db\u3068\u884c\u5217\u8868\u73fe","text":"<p>\u6709\u9650\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306b\u304a\u3051\u308b\u7dda\u5f62\u5909\u63db\u306f\u3001\u884c\u5217\u3092\u4f7f\u3063\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u306f\u7dda\u5f62\u4ee3\u6570\u306e\u6700\u3082\u91cd\u8981\u306a\u6982\u5ff5\u306e\u4e00\u3064\u3067\u3059\u3002</p> <p>\u5b9a\u7406 4.1 \\(V\\) \u3092 \\(n\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3001\\(W\\) \u3092 \\(m\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3068\u3057\u3066\u3001\\(T: V \\rightarrow W\\) \u3092\u7dda\u5f62\u5909\u63db\u3068\u3057\u307e\u3059\u3002\\(V\\) \u306e\u57fa\u5e95 \\(\\{v_1, v_2, \\ldots, v_n\\}\\) \u3068 \\(W\\) \u306e\u57fa\u5e95 \\(\\{w_1, w_2, \\ldots, w_m\\}\\) \u306b\u95a2\u3057\u3066\u3001\\(T\\) \u306f\u4e00\u610f\u7684\u306b \\(m \\times n\\) \u884c\u5217 \\(A\\) \u3067\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p> <p>\u5177\u4f53\u7684\u306b\u306f\u3001\u307e\u305a\u57fa\u5e95\u306b\u5bfe\u3059\u308b\u5909\u63db\u7d50\u679c\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a</p> <p>\\(T(v_1) = a_{11}w_1 + a_{21}w_2 + \\cdots + a_{m1}w_m\\) \\(T(v_2) = a_{12}w_1 + a_{22}w_2 + \\cdots + a_{m2}w_m\\) \\(\\vdots\\) \\(T(v_n) = a_{1n}w_1 + a_{2n}w_2 + \\cdots + a_{mn}w_m\\)</p> <p>\u3053\u306e\u3068\u304d\u3001\u5909\u63db \\(T\\) \u3092\u8868\u3059\u884c\u5217 \\(A\\) \u306f\uff1a</p> <p>\\(A = \\begin{bmatrix}  a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\)</p> <p>\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u7279\u306b \\(V = W = \\mathbb{R}^n\\) \u3067\u6a19\u6e96\u57fa\u5e95\u3092\u4f7f\u3046\u5834\u5408\u3001\u884c\u5217 \\(A\\) \u306e \\(j\\) \u5217\u76ee\u306f \\(T(e_j)\\) \u306e\u5ea7\u6a19\u3068\u306a\u308a\u307e\u3059\uff08\\(e_j\\) \u306f \\(j\\) \u756a\u76ee\u306e\u6a19\u6e96\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\uff09\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#42","title":"4.2 \u6a19\u6e96\u7684\u306a\u7dda\u5f62\u5909\u63db\u3068\u305d\u306e\u884c\u5217\u8868\u73fe","text":"<p>\\(\\mathbb{R}^2\\) \u306b\u304a\u3051\u308b\u6a19\u6e96\u7684\u306a\u7dda\u5f62\u5909\u63db\u306e\u884c\u5217\u8868\u73fe\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#1_1","title":"1. \u6052\u7b49\u5909\u63db","text":"<p>\u30d9\u30af\u30c8\u30eb\u3092\u5909\u5316\u3055\u305b\u306a\u3044\u5909\u63db\u3067\u3059\u3002 \\(I(\\mathbf{v}) = \\mathbf{v}\\)</p> <p>\u884c\u5217\u8868\u73fe: \\(I = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</p>"},{"location":"lectures/LA/32-eigen-value-vector/#2_1","title":"2. \u62e1\u5927\u30fb\u7e2e\u5c0f\u5909\u63db","text":"<p>\\(x\\) \u65b9\u5411\u306b \\(a\\) \u500d\u3001\\(y\\) \u65b9\u5411\u306b \\(b\\) \u500d\u3059\u308b\u5909\u63db\u3002 \\(T(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) = \\begin{bmatrix} ax \\\\ by \\end{bmatrix}\\)</p> <p>\u884c\u5217\u8868\u73fe: \\(A = \\begin{bmatrix} a &amp; 0 \\\\ 0 &amp; b \\end{bmatrix}\\)</p>"},{"location":"lectures/LA/32-eigen-value-vector/#3_1","title":"3. \u56de\u8ee2\u5909\u63db","text":"<p>\u539f\u70b9\u3092\u4e2d\u5fc3\u306b\u89d2\u5ea6 \\(\\theta\\) \u3060\u3051\u53cd\u6642\u8a08\u56de\u308a\u306b\u56de\u8ee2\u3055\u305b\u308b\u5909\u63db\u3002 \\(T(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\)</p> <p>\u884c\u5217\u8868\u73fe: \\(R_\\theta = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</p>"},{"location":"lectures/LA/32-eigen-value-vector/#4_1","title":"4. \u305b\u3093\u65ad\u5909\u63db (\u30b7\u30a2\u30fc\u5909\u63db)","text":"<p>\\(x\\) \u8ef8\u65b9\u5411\u306b \\(y\\) \u306e \\(k\\) \u500d\u3060\u3051\u305a\u3089\u3059\u5909\u63db\u3002 \\(T(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) = \\begin{bmatrix} x + ky \\\\ y \\end{bmatrix}\\)</p> <p>\u884c\u5217\u8868\u73fe: \\(S = \\begin{bmatrix} 1 &amp; k \\\\ 0 &amp; 1 \\end{bmatrix}\\)</p>"},{"location":"lectures/LA/32-eigen-value-vector/#5","title":"5. \u5bfe\u79f0\u5909\u63db (\u53cd\u5c04)","text":"<p>\u3042\u308b\u8ef8\u306b\u95a2\u3057\u3066\u5bfe\u79f0\u306b\u79fb\u3059\u5909\u63db\u3002\u4f8b\u3048\u3070 \\(x\\) \u8ef8\u306b\u95a2\u3059\u308b\u53cd\u5c04\u306f\uff1a \\(T(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) = \\begin{bmatrix} x \\\\ -y \\end{bmatrix}\\)</p> <p>\u884c\u5217\u8868\u73fe: \\(F_x = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\)</p> <p>\\(y\\) \u8ef8\u306b\u95a2\u3059\u308b\u53cd\u5c04\u306f\uff1a \\(T(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) = \\begin{bmatrix} -x \\\\ y \\end{bmatrix}\\)</p> <p>\u884c\u5217\u8868\u73fe: \\(F_y = \\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</p> <p>\u76f4\u7dda \\(y = x\\) \u306b\u95a2\u3059\u308b\u53cd\u5c04\u306f\uff1a \\(T(\\begin{bmatrix} x \\\\ y \\end{bmatrix}) = \\begin{bmatrix} y \\\\ x \\end{bmatrix}\\)</p> <p>\u884c\u5217\u8868\u73fe: \\(F_{y=x} = \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix}\\)</p>"},{"location":"lectures/LA/32-eigen-value-vector/#43","title":"4.3 \u5408\u6210\u5909\u63db","text":"<p>2\u3064\u306e\u7dda\u5f62\u5909\u63db \\(S: V \\rightarrow W\\) \u3068 \\(T: W \\rightarrow U\\) \u304c\u3042\u308b\u3068\u304d\u3001\u305d\u308c\u3089\u306e\u5408\u6210 \\(T \\circ S: V \\rightarrow U\\) \u3082\u7dda\u5f62\u5909\u63db\u3068\u306a\u308a\u307e\u3059\u3002\u884c\u5217\u8868\u73fe\u3067\u306f\u3001\u5408\u6210\u5909\u63db\u306f\u884c\u5217\u306e\u7a4d\u3068\u3057\u3066\u8868\u3055\u308c\u307e\u3059\u3002</p> <p>\\(S\\) \u306e\u884c\u5217\u8868\u73fe\u304c \\(A\\)\u3001\\(T\\) \u306e\u884c\u5217\u8868\u73fe\u304c \\(B\\) \u306e\u3068\u304d\u3001\\(T \\circ S\\) \u306e\u884c\u5217\u8868\u73fe\u306f \\(BA\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u4f8b 4.1\uff1a\\(\\mathbb{R}^2\\) \u4e0a\u3067\u3001\u307e\u305a\u89d2\u5ea6 \\(\\theta\\) \u306e\u56de\u8ee2\u3092\u884c\u3044\u3001\u6b21\u306b \\(x\\) \u65b9\u5411\u306b \\(a\\) \u500d\u3001\\(y\\) \u65b9\u5411\u306b \\(b\\) \u500d\u3059\u308b\u5909\u63db\u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\u56de\u8ee2\u884c\u5217\uff1a\\(R_\\theta = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\) \u62e1\u5927\u7e2e\u5c0f\u884c\u5217\uff1a\\(S = \\begin{bmatrix} a &amp; 0 \\\\ 0 &amp; b \\end{bmatrix}\\)</p> <p>\u5408\u6210\u5909\u63db\u306e\u884c\u5217\uff1a\\(SR_\\theta = \\begin{bmatrix} a &amp; 0 \\\\ 0 &amp; b \\end{bmatrix} \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix} = \\begin{bmatrix} a\\cos\\theta &amp; -a\\sin\\theta \\\\ b\\sin\\theta &amp; b\\cos\\theta \\end{bmatrix}\\)</p> <p>\u6ce8\u610f\u70b9\u3068\u3057\u3066\u3001\u884c\u5217\u306e\u7a4d\u306f\u4e00\u822c\u306b\u4ea4\u63db\u6cd5\u5247\u304c\u6210\u308a\u7acb\u3061\u307e\u305b\u3093\u3002\u3064\u307e\u308a\u3001\\(AB \\neq BA\\) \u3068\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u5909\u63db\u306e\u9806\u5e8f\u304c\u7d50\u679c\u306b\u5f71\u97ff\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#44","title":"4.4 \u7dda\u5f62\u5909\u63db\u306e\u6838\u3068\u50cf","text":"<p>\u7dda\u5f62\u5909\u63db \\(T: V \\rightarrow W\\) \u306b\u5bfe\u3057\u3066\u3001\u91cd\u8981\u306a\u90e8\u5206\u7a7a\u9593\u304c2\u3064\u3042\u308a\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9 4.2 (\u6838) \u7dda\u5f62\u5909\u63db \\(T: V \\rightarrow W\\) \u306e\u6838 (kernel) \u307e\u305f\u306f\u96f6\u7a7a\u9593 (null space) \u306f\u4ee5\u4e0b\u3067\u5b9a\u7fa9\u3055\u308c\u308b \\(V\\) \u306e\u90e8\u5206\u7a7a\u9593\u3067\u3059\uff1a \\(\\text{Ker}(T) = \\{\\mathbf{v} \\in V : T(\\mathbf{v}) = \\mathbf{0}\\}\\)</p> <p>\u5b9a\u7fa9 4.3 (\u50cf) \u7dda\u5f62\u5909\u63db \\(T: V \\rightarrow W\\) \u306e\u50cf (image) \u307e\u305f\u306f\u5024\u57df (range) \u306f\u4ee5\u4e0b\u3067\u5b9a\u7fa9\u3055\u308c\u308b \\(W\\) \u306e\u90e8\u5206\u7a7a\u9593\u3067\u3059\uff1a \\(\\text{Im}(T) = \\{T(\\mathbf{v}) : \\mathbf{v} \\in V\\}\\)</p> <p>\u3053\u308c\u3089\u306e\u6982\u5ff5\u306f\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5b66\u7fd2\u306b\u91cd\u8981\u3067\u3059\u3002\u7279\u306b\u3001\u6838\u306e\u6b21\u5143\u3068\u50cf\u306e\u6b21\u5143\u306b\u306f\u6b21\u306e\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u5b9a\u7406 4.2 (\u30e9\u30f3\u30af\u30fb\u96f6\u5ea6\u5b9a\u7406) \\(V\\) \u3092 \\(n\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3001\\(T: V \\rightarrow W\\) \u3092\u7dda\u5f62\u5909\u63db\u3068\u3059\u308b\u3068\uff1a \\(\\dim(\\text{Ker}(T)) + \\dim(\\text{Im}(T)) = \\dim(V) = n\\)</p> <p>\u884c\u5217 \\(A\\) \u304c\u7dda\u5f62\u5909\u63db \\(T\\) \u3092\u8868\u3059\u3068\u304d\u3001\\(\\dim(\\text{Ker}(T))\\) \u306f \\(A\\) \u306e\u96f6\u5ea6 (nullity)\u3001\\(\\dim(\\text{Im}(T))\\) \u306f \\(A\\) \u306e\u30e9\u30f3\u30af (rank) \u3068\u547c\u3070\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#45","title":"4.5 \u7dda\u5f62\u5909\u63db\u3068\u884c\u5217\u306e\u95a2\u4fc2\u306e\u307e\u3068\u3081","text":"<p>\u7dda\u5f62\u5909\u63db\u3068\u884c\u5217\u306e\u95a2\u4fc2\u3092\u307e\u3068\u3081\u308b\u3068\uff1a</p> <ol> <li>\\(n\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u304b\u3089 \\(m\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3078\u306e\u7dda\u5f62\u5909\u63db\u306f \\(m \\times n\\) \u884c\u5217\u3067\u8868\u3055\u308c\u308b</li> <li>\u884c\u5217\u306b\u3088\u308b\u6f14\u7b97 \\(A\\mathbf{x}\\) \u306f\u7dda\u5f62\u5909\u63db\u3092\u8868\u3059</li> <li>\u7dda\u5f62\u5909\u63db\u306e\u5408\u6210\u306f\u884c\u5217\u306e\u7a4d\u306b\u5bfe\u5fdc\u3059\u308b</li> <li>\u7dda\u5f62\u5909\u63db\u306e\u6838\u306f\u9023\u7acb\u65b9\u7a0b\u5f0f \\(A\\mathbf{x} = \\mathbf{0}\\) \u306e\u89e3\u7a7a\u9593</li> <li>\u7dda\u5f62\u5909\u63db\u306e\u50cf\u306f\u884c\u5217 \\(A\\) \u306e\u5217\u30d9\u30af\u30c8\u30eb\u306e\u5f35\u308b\u7a7a\u9593</li> <li>\u9006\u5909\u63db\u306e\u5b58\u5728\u306f\u884c\u5217\u304c\u9006\u884c\u5217\u3092\u6301\u3064\u3053\u3068\u3068\u540c\u5024</li> </ol>"},{"location":"lectures/LA/32-eigen-value-vector/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/32-eigen-value-vector/#51","title":"5.1 \u57fa\u672c\u7684\u306a\u7dda\u5f62\u5909\u63db\u306e\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306f\u3001\u7dda\u5f62\u5909\u63db\u3068\u305d\u306e\u52b9\u679c\u3092\u8996\u899a\u5316\u3059\u308b\u305f\u3081\u306ePython\u306e\u5b9f\u88c5\u4f8b\u3067\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\ndef plot_transformation(A, points=None, title=\"Linear Transformation\"):\n    \"\"\"\n    \u884c\u5217A\u3067\u8868\u3055\u308c\u308b\u7dda\u5f62\u5909\u63db\u3092\u53ef\u8996\u5316\u3059\u308b\u95a2\u6570\n    \"\"\"\n    # \u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u70b9\uff08\u5358\u4f4d\u6b63\u65b9\u5f62\u306e\u9802\u70b9\uff09\n    if points is None:\n        points = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n\n    # \u5909\u63db\u524d\u306e\u70b9\u3092\u4fdd\u5b58\n    original_points = points.copy()\n\n    # \u5909\u63db\u5f8c\u306e\u70b9\u3092\u8a08\u7b97\n    transformed_points = np.array([A @ point for point in points])\n\n    # \u30d7\u30ed\u30c3\u30c8\u9818\u57df\u306e\u8a2d\u5b9a\n    max_val = max(np.max(np.abs(original_points)), np.max(np.abs(transformed_points)))\n    limit = max(3, max_val * 1.2)\n\n    # \u56f3\u306e\u4f5c\u6210\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n    # \u5909\u63db\u524d\n    ax1.grid(True)\n    ax1.set_xlim(-limit, limit)\n    ax1.set_ylim(-limit, limit)\n    ax1.set_aspect('equal')\n    ax1.set_title('Original')\n\n    # x\u8ef8\u3068y\u8ef8\n    ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    ax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n    # \u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\n    ax1.arrow(0, 0, 1, 0, head_width=0.1, head_length=0.1, fc='r', ec='r')\n    ax1.arrow(0, 0, 0, 1, head_width=0.1, head_length=0.1, fc='g', ec='g')\n\n    # \u5909\u63db\u524d\u306e\u56f3\u5f62\u3092\u63cf\u753b\n    ax1.plot(original_points[:, 0], original_points[:, 1], 'b-', alpha=0.7)\n    ax1.fill(original_points[:, 0], original_points[:, 1], 'b', alpha=0.2)\n\n    # \u5909\u63db\u5f8c\n    ax2.grid(True)\n    ax2.set_xlim(-limit, limit)\n    ax2.set_ylim(-limit, limit)\n    ax2.set_aspect('equal')\n    ax2.set_title(f'After Transformation: {title}')\n\n    # x\u8ef8\u3068y\u8ef8\n    ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    ax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n    # \u5909\u63db\u5f8c\u306e\u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\n    e1 = A @ np.array([1, 0])\n    e2 = A @ np.array([0, 1])\n    ax2.arrow(0, 0, e1[0], e1[1], head_width=0.1, head_length=0.1, fc='r', ec='r')\n    ax2.arrow(0, 0, e2[0], e2[1], head_width=0.1, head_length=0.1, fc='g', ec='g')\n\n    # \u5909\u63db\u5f8c\u306e\u56f3\u5f62\u3092\u63cf\u753b\n    ax2.plot(transformed_points[:, 0], transformed_points[:, 1], 'b-', alpha=0.7)\n    ax2.fill(transformed_points[:, 0], transformed_points[:, 1], 'b', alpha=0.2)\n\n    plt.tight_layout()\n    plt.show()\n\n    return fig\n\n# \u5909\u63db\u306e\u4f8b\n\n# 1. \u6052\u7b49\u5909\u63db\nI = np.array([[1, 0], [0, 1]])\nplot_transformation(I, title=\"Identity\")\n\n# 2. \u62e1\u5927\u30fb\u7e2e\u5c0f\u5909\u63db\nS = np.array([[2, 0], [0, 0.5]])\nplot_transformation(S, title=\"Scaling (x2, y/2)\")\n\n# 3. \u56de\u8ee2\u5909\u63db (90\u5ea6)\ntheta = np.pi/2\nR = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\nplot_transformation(R, title=\"Rotation (90\u00b0)\")\n\n# 4. \u305b\u3093\u65ad\u5909\u63db\nSh = np.array([[1, 1], [0, 1]])\nplot_transformation(Sh, title=\"Shear\")\n\n# 5. \u53cd\u5c04\u5909\u63db\uff08x\u8ef8\u306b\u3064\u3044\u3066\uff09\nF = np.array([[1, 0], [0, -1]])\nplot_transformation(F, title=\"Reflection (x-axis)\")\n</code></pre>"},{"location":"lectures/LA/32-eigen-value-vector/#52","title":"5.2 \u884c\u5217\u306b\u3088\u308b\u8907\u5408\u5909\u63db","text":"<p>\u8907\u6570\u306e\u5909\u63db\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u4f8b\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code># \u56de\u8ee2\u3057\u3066\u304b\u3089\u62e1\u5927\u3059\u308b\u8907\u5408\u5909\u63db\ntheta = np.pi/4  # 45\u5ea6\u56de\u8ee2\nR = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\nS = np.array([[2, 0], [0, 0.5]])\n\n# \u8907\u5408\u5909\u63db\u306e\u884c\u5217\uff08\u62e1\u5927\u3057\u3066\u304b\u3089\u56de\u8ee2\uff09\nT1 = R @ S\nplot_transformation(T1, title=\"Rotation after Scaling\")\n\n# \u8907\u5408\u5909\u63db\u306e\u884c\u5217\uff08\u56de\u8ee2\u3057\u3066\u304b\u3089\u62e1\u5927\uff09\nT2 = S @ R\nplot_transformation(T2, title=\"Scaling after Rotation\")\n</code></pre>"},{"location":"lectures/LA/32-eigen-value-vector/#53","title":"5.3 \u7dda\u5f62\u5909\u63db\u306e\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3","text":"<p>\u7dda\u5f62\u5909\u63db\u3092\u9023\u7d9a\u7684\u306b\u53ef\u8996\u5316\u3059\u308b\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u6210\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code>from matplotlib.animation import FuncAnimation\n\ndef animate_transformation(A, frames=50, interval=100):\n    \"\"\"\n    \u5358\u4f4d\u6b63\u65b9\u5f62\u306b\u5bfe\u3059\u308b\u7dda\u5f62\u5909\u63db\u3092\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3\u5316\u3059\u308b\u95a2\u6570\n    \"\"\"\n    points = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n    I = np.eye(2)\n\n    # \u56f3\u306e\u521d\u671f\u8a2d\u5b9a\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.grid(True)\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-3, 3)\n    ax.set_aspect('equal')\n    ax.set_title('Linear Transformation Animation')\n\n    # x\u8ef8\u3068y\u8ef8\n    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n    # \u521d\u671f\u72b6\u614b\u306e\u56f3\u5f62\n    line, = ax.plot([], [], 'b-', alpha=0.7)\n    fill = ax.fill([], [], 'b', alpha=0.2)[0]\n\n    # \u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u7528\u306e\u77e2\u5370\n    arrow_e1, = ax.plot([], [], 'r-', lw=2)\n    arrow_e2, = ax.plot([], [], 'g-', lw=2)\n\n    def init():\n        line.set_data([], [])\n        fill.set_xy(np.zeros((5, 2)))\n        arrow_e1.set_data([], [])\n        arrow_e2.set_data([], [])\n        return line, fill, arrow_e1, arrow_e2\n\n    def update(frame):\n        t = frame / frames\n        # \u6052\u7b49\u5909\u63db\u304b\u3089\u76ee\u6a19\u306e\u5909\u63db\u3078\u3068\u5f90\u3005\u306b\u5909\u5316\n        current_matrix = I + t * (A - I)\n\n        # \u73fe\u5728\u306e\u5909\u63db\u3092\u9069\u7528\n        current_points = np.array([current_matrix @ point for point in points])\n\n        # \u56f3\u5f62\u306e\u66f4\u65b0\n        line.set_data(current_points[:, 0], current_points[:, 1])\n        fill.set_xy(current_points)\n\n        # \u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u306e\u66f4\u65b0\n        e1 = current_matrix @ np.array([1, 0])\n        e2 = current_matrix @ np.array([0, 1])\n        arrow_e1.set_data([0, e1[0]], [0, e1[1]])\n        arrow_e2.set_data([0, e2[0]], [0, e2[1]])\n\n        return line, fill, arrow_e1, arrow_e2\n\n    ani = FuncAnimation(fig, update, frames=frames, init_func=init,\n                        interval=interval, blit=True)\n    plt.close()  # IPython\u4e0a\u3067\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3\u304c\u4e8c\u91cd\u8868\u793a\u3055\u308c\u308b\u306e\u3092\u9632\u3050\n\n    return ani\n\n# 45\u5ea6\u56de\u8ee2\u306e\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3\ntheta = np.pi/4  # 45\u5ea6\nR = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\nani_rotation = animate_transformation(R)\n\n# \u62e1\u5927\u7e2e\u5c0f\u306e\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3\nS = np.array([[2, 0], [0, 0.5]])\nani_scaling = animate_transformation(S)\n\n# \u305b\u3093\u65ad\u5909\u63db\u306e\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3\nSh = np.array([[1, 1], [0, 1]])\nani_shear = animate_transformation(Sh)\n\n# HTML\u51fa\u529b\u3067\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3\u3092\u8868\u793a\nfrom IPython.display import HTML\nHTML(ani_rotation.to_jshtml())\n</code></pre>"},{"location":"lectures/LA/32-eigen-value-vector/#54","title":"5.4 \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306e\u7dda\u5f62\u5909\u63db\u306e\u5fdc\u7528\u4f8b","text":"<p>\u5065\u5eb7\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7dda\u5f62\u5909\u63db\u306e\u5fdc\u7528\u4f8b\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code># \u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u4f8b\uff1a(\u4f53\u91cd, \u8eab\u9577)\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\nnp.random.seed(42)\nheight = 170 + np.random.normal(0, 5, 100)  # \u8eab\u9577 (cm)\nweight = 0.5 * height - 35 + np.random.normal(0, 5, 100)  # \u4f53\u91cd (kg)\n\n# \u6b63\u898f\u5316\u306e\u305f\u3081\u306e\u7dda\u5f62\u5909\u63db\nheight_mean, height_std = np.mean(height), np.std(height)\nweight_mean, weight_std = np.mean(weight), np.std(weight)\n\n# \u6b63\u898f\u5316\u5909\u63db\u884c\u5217 (\u5bfe\u89d2\u884c\u5217)\nnormalize_matrix = np.array([[1/height_std, 0], [0, 1/weight_std]])\n\n# \u751f\u30c7\u30fc\u30bf\u3092\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(height, weight, alpha=0.7)\nplt.title('Original Data: Height vs Weight')\nplt.xlabel('Height (cm)')\nplt.ylabel('Weight (kg)')\nplt.grid(True)\n\n# \u30c7\u30fc\u30bf\u3092\u884c\u5217\u5f62\u5f0f\u306b\u5909\u63db\ndata = np.vstack((height, weight)).T\n\n# \u5e73\u5747\u3092\u5f15\u3044\u3066\u304b\u3089\u6b63\u898f\u5316\u5909\u63db\u3092\u9069\u7528\ncentered_data = data - np.array([height_mean, weight_mean])\nnormalized_data = np.array([normalize_matrix @ point for point in centered_data])\n\n# \u6b63\u898f\u5316\u5f8c\u306e\u30c7\u30fc\u30bf\u3092\u30d7\u30ed\u30c3\u30c8\nplt.subplot(1, 2, 2)\nplt.scatter(normalized_data[:, 0], normalized_data[:, 1], alpha=0.7)\nplt.title('Normalized Data')\nplt.xlabel('Normalized Height')\nplt.ylabel('Normalized Weight')\nplt.grid(True)\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/32-eigen-value-vector/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/32-eigen-value-vector/#61-20","title":"6.1 \u57fa\u672c\u554f\u984c (\u6f14\u7fd2\u6642\u9593: \u7d0420\u5206)","text":"<ol> <li>\u4ee5\u4e0b\u306e\u5199\u50cf\u304c\u7dda\u5f62\u5909\u63db\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002\u7dda\u5f62\u5909\u63db\u3067\u306a\u3044\u5834\u5408\u306f\u3001\u7dda\u5f62\u6027\u306e\u3069\u306e\u6027\u8cea\u304c\u6e80\u305f\u3055\u308c\u306a\u3044\u304b\u3092\u793a\u305b\u3002    a) \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2, T\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 3x \\\\ 2y-1 \\end{pmatrix}\\)</li> </ol> <p>b) \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2, T\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} x+y \\\\ x-y \\end{pmatrix}\\)</p> <p>c) \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}, T\\begin{pmatrix} x \\\\ y \\end{pmatrix} = xy\\)</p> <p>d) \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2, T\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} x^2 \\\\ y \\end{pmatrix}\\)</p> <ol> <li> <p>\u4ee5\u4e0b\u306e\u7dda\u5f62\u5909\u63db\u306e\u884c\u5217\u8868\u73fe\u3092\u6c42\u3081\u3088\u3002    a) \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) \u3067\u3001\u30d9\u30af\u30c8\u30eb\u3092 \\(x\\) \u8ef8\u306b\u95a2\u3057\u3066\u53cd\u5c04\u3055\u305b\u308b\u5909\u63db    b) \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) \u3067\u3001\u539f\u70b9\u3092\u4e2d\u5fc3\u306b \\(60^\\circ\\) \u53cd\u6642\u8a08\u56de\u308a\u306b\u56de\u8ee2\u3055\u305b\u308b\u5909\u63db    c) \\(T: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3\\) \u3067\u3001\\(z\\) \u8ef8\u65b9\u5411\u306b \\(2\\) \u500d\u306b\u62e1\u5927\u3057\u3001\\(x\\) \u8ef8\u3068 \\(y\\) \u8ef8\u65b9\u5411\u306f\u305d\u306e\u307e\u307e\u306b\u3059\u308b\u5909\u63db</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u3067\u8868\u3055\u308c\u308b\u7dda\u5f62\u5909\u63db\u306e\u6838\u3068\u50cf\u3092\u6c42\u3081\u3088\u3002    \\(A = \\begin{pmatrix} 1 &amp; 2 \\\\ 2 &amp; 4 \\end{pmatrix}\\)</p> </li> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} 2 &amp; 1 \\\\ -1 &amp; 3 \\end{pmatrix}\\) \u3068 \\(B = \\begin{pmatrix} 0 &amp; 2 \\\\ 1 &amp; -1 \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u3092\u8a08\u7b97\u305b\u3088\u3002    a) \\(AB\\) \u3068 \\(BA\\)    b) \\(A\\) \u3092\u884c\u5217\u3068\u3059\u308b\u7dda\u5f62\u5909\u63db\u3092 \\(T_A\\)\u3001\\(B\\) \u3092\u884c\u5217\u3068\u3059\u308b\u7dda\u5f62\u5909\u63db\u3092 \\(T_B\\) \u3068\u3057\u305f\u3068\u304d\u3001\u5408\u6210\u5909\u63db \\(T_B \\circ T_A\\) \u3068 \\(T_A \\circ T_B\\) \u306e\u884c\u5217\u8868\u73fe</p> </li> </ol>"},{"location":"lectures/LA/32-eigen-value-vector/#62-30","title":"6.2 \u5fdc\u7528\u554f\u984c (\u6f14\u7fd2\u6642\u9593: \u7d0430\u5206)","text":"<ol> <li> <p>\\(\\mathbb{R}^2\\) \u4e0a\u306e\u7dda\u5f62\u5909\u63db \\(T\\) \u304c\u884c\u5217 \\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 2 \\end{pmatrix}\\) \u3067\u8868\u3055\u308c\u308b\u3068\u304d\u3001\u30d9\u30af\u30c8\u30eb \\(\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\) \u306e\u50cf\u3092\u6c42\u3081\u3088\u3002\u307e\u305f\u3001\\(T\\) \u306e\u6838\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\\(\\mathbb{R}^2\\) \u306b\u304a\u3044\u3066\u3001\u70b9 \\((0, 0)\\), \\((1, 0)\\), \\((0, 1)\\), \\((1, 1)\\) \u304b\u3089\u306a\u308b\u5358\u4f4d\u6b63\u65b9\u5f62\u304c\u3042\u308b\u3068\u3059\u308b\u3002\u4ee5\u4e0b\u306e\u7dda\u5f62\u5909\u63db\u3092\u9806\u756a\u306b\u9069\u7528\u3057\u305f\u3068\u304d\u306e\u3001\u3053\u306e\u6b63\u65b9\u5f62\u306e\u5909\u63db\u5f8c\u306e\u9802\u70b9\u306e\u5ea7\u6a19\u3092\u6c42\u3081\u3088\u3002    a) \u307e\u305a \\(x\\) \u8ef8\u65b9\u5411\u306b \\(2\\) \u500d\u306b\u62e1\u5927\u3057\u3001\\(y\\) \u8ef8\u65b9\u5411\u306b \\(0.5\\) \u500d\u306b\u7e2e\u5c0f\u3059\u308b    b) \u6b21\u306b\u539f\u70b9\u3092\u4e2d\u5fc3\u306b \\(45^\\circ\\) \u53cd\u6642\u8a08\u56de\u308a\u306b\u56de\u8ee2\u3055\u305b\u308b</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u3067\u8868\u3055\u308c\u308b\u7dda\u5f62\u5909\u63db\u306b\u3064\u3044\u3066\u3001\u305d\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u8aac\u660e\u305b\u3088\u3002    a) \\(A = \\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{pmatrix}\\)    b) \\(B = \\begin{pmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix}\\)    c) \\(C = \\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix}\\)</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306e\u5fdc\u7528\u554f\u984c\uff1a\u3042\u308b\u7814\u7a76\u3067\u3001\u60a3\u8005\u306e\u8840\u5727\uff08\u53ce\u7e2e\u671f\u3001\u62e1\u5f35\u671f\uff09\u306e\u30c7\u30fc\u30bf\u304c\u96c6\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u3053\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u7dda\u5f62\u5909\u63db\u3092\u8003\u3048\u308b\u3002</p> </li> </ol> <p>\\(T\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} \\frac{x-120}{10} \\\\ \\frac{y-80}{5} \\end{pmatrix}\\)</p> <p>\u3053\u3053\u3067 \\(x\\) \u306f\u53ce\u7e2e\u671f\u8840\u5727\uff08mmHg\uff09\u3001\\(y\\) \u306f\u62e1\u5f35\u671f\u8840\u5727\uff08mmHg\uff09\u3067\u3042\u308b\u3002</p> <p>a) \u3053\u306e\u5909\u63db\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u8aac\u660e\u305b\u3088    b) \u6b63\u5e38\u8840\u5727\uff08120/80 mmHg\uff09\u3001\u8efd\u5ea6\u9ad8\u8840\u5727\uff08140/90 mmHg\uff09\u3001\u91cd\u5ea6\u9ad8\u8840\u5727\uff08160/100 mmHg\uff09\u304c\u5909\u63db\u5f8c\u3069\u306e\u3088\u3046\u306a\u70b9\u306b\u5199\u50cf\u3055\u308c\u308b\u304b\u3092\u8a08\u7b97\u305b\u3088    c) \u3053\u306e\u5909\u63db\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u8840\u5727\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3059\u308b\u969b\u306b\u3069\u306e\u3088\u3046\u306a\u5229\u70b9\u304c\u3042\u308b\u304b\u8aac\u660e\u305b\u3088</p>"},{"location":"lectures/LA/32-eigen-value-vector/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/32-eigen-value-vector/#q1","title":"Q1: \u7dda\u5f62\u5909\u63db\u3068\u884c\u5217\u306e\u95a2\u4fc2\u3092\u76f4\u611f\u7684\u306b\u7406\u89e3\u3059\u308b\u306b\u306f\uff1f","text":"<p>A1: \u7dda\u5f62\u5909\u63db\u3068\u306f\u3001\u30d9\u30af\u30c8\u30eb\u306e\u8db3\u3057\u7b97\u3068\u30b9\u30ab\u30e9\u30fc\u500d\u306e\u69cb\u9020\u3092\u4fdd\u5b58\u3059\u308b\u5199\u50cf\u3067\u3059\u3002\u884c\u5217\u306b\u3088\u308b\u6f14\u7b97\u306f\u307e\u3055\u306b\u3053\u306e\u6027\u8cea\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u7a4d \\(A\\mathbf{v}\\) \u3092\u8a08\u7b97\u3059\u308b\u3068\u304d\u3001\u5b9f\u306f\u884c\u5217 \\(A\\) \u306e\u5217\u30d9\u30af\u30c8\u30eb\u3092\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u306e\u50cf\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\\(2 \\times 2\\) \u884c\u5217\u306e\u5834\u5408\u3001\u7b2c1\u5217\u306f\u6a19\u6e96\u57fa\u5e95\u30d9\u30af\u30c8\u30eb \\((1,0)\\) \u306e\u5909\u63db\u5f8c\u306e\u4f4d\u7f6e\u3001\u7b2c2\u5217\u306f\u6a19\u6e96\u57fa\u5e95\u30d9\u30af\u30c8\u30eb \\((0,1)\\) \u306e\u5909\u63db\u5f8c\u306e\u4f4d\u7f6e\u3092\u8868\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u884c\u5217\u306f\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306e\u57fa\u5e95\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u63db\u3055\u308c\u308b\u304b\u3092\u8a18\u8ff0\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#q2","title":"Q2: \u306a\u305c\u884c\u5217\u306e\u7a4d\u306f\u4e00\u822c\u306b\u53ef\u63db\u3067\u306f\u306a\u3044\u306e\u3067\u3059\u304b\uff1f","text":"<p>A2: \u884c\u5217\u306e\u7a4d\u304c\u53ef\u63db\u3067\u306a\u3044\uff08\\(AB \\neq BA\\)\uff09\u306e\u306f\u3001\u5909\u63db\u306e\u9806\u5e8f\u304c\u7d50\u679c\u306b\u5f71\u97ff\u3059\u308b\u3068\u3044\u3046\u4e8b\u5b9f\u3092\u53cd\u6620\u3057\u3066\u3044\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u300c\u307e\u305a\u56de\u8ee2\u3057\u3066\u304b\u3089\u62e1\u5927\u3059\u308b\u300d\u3068\u3044\u3046\u64cd\u4f5c\u3068\u3001\u300c\u307e\u305a\u62e1\u5927\u3057\u3066\u304b\u3089\u56de\u8ee2\u3059\u308b\u300d\u3068\u3044\u3046\u64cd\u4f5c\u306f\u7570\u306a\u308b\u7d50\u679c\u3092\u751f\u3058\u307e\u3059\u3002\u5e7e\u4f55\u5b66\u7684\u306a\u5909\u63db\u3092\u9806\u756a\u306b\u9069\u7528\u3059\u308b\u3068\u304d\u3001\u305d\u306e\u9806\u5e8f\u304c\u91cd\u8981\u306a\u306e\u306f\u76f4\u611f\u7684\u306b\u3082\u7406\u89e3\u3067\u304d\u308b\u3067\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#q3","title":"Q3: \u7dda\u5f62\u5909\u63db\u306e\u6838\u3068\u50cf\u306f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u3069\u306e\u3088\u3046\u306b\u5f79\u7acb\u3061\u307e\u3059\u304b\uff1f","text":"<p>A3: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u6271\u3046\u3053\u3068\u304c\u591a\u3044\u3067\u3059\u3002\u7dda\u5f62\u5909\u63db\u306e\u6838\u306f\u3001\u300c\u5909\u63db\u306b\u3088\u3063\u3066\u540c\u3058\u70b9\u306b\u5199\u50cf\u3055\u308c\u308b\u5165\u529b\u306e\u96c6\u5408\u300d\u3092\u8868\u3057\u3001\u5909\u63db\u306b\u3088\u3063\u3066\u5931\u308f\u308c\u308b\u60c5\u5831\u306e\u65b9\u5411\u3092\u793a\u3057\u307e\u3059\u3002\u4e00\u65b9\u3001\u50cf\u306f\u300c\u5909\u63db\u306b\u3088\u3063\u3066\u5230\u9054\u53ef\u80fd\u306a\u51fa\u529b\u306e\u96c6\u5408\u300d\u3092\u8868\u3057\u3001\u30c7\u30fc\u30bf\u306e\u672c\u8cea\u7684\u306a\u69cb\u9020\u3084\u7279\u5fb4\u3092\u6349\u3048\u308b\u90e8\u5206\u7a7a\u9593\u3092\u8868\u3059\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u304c\u6700\u5927\u306b\u306a\u308b\u65b9\u5411\uff08\u4e3b\u6210\u5206\uff09\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u91cd\u8981\u306a\u7279\u5fb4\u3092\u62bd\u51fa\u3057\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u7dda\u5f62\u5909\u63db\u306e\u50cf\u3092\u6700\u9069\u5316\u3059\u308b\u554f\u984c\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#q4-beginpmatrix-costheta-sintheta-sintheta-costheta-endpmatrix","title":"Q4: \u56de\u8ee2\u884c\u5217\u306f\u306a\u305c \\(\\begin{pmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{pmatrix}\\) \u3068\u3044\u3046\u5f62\u306b\u306a\u308b\u306e\u3067\u3059\u304b\uff1f","text":"<p>A4: \u56de\u8ee2\u884c\u5217\u306e\u5f62\u306f\u3001\u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u304c\u56de\u8ee2\u5f8c\u306b\u3069\u306e\u3088\u3046\u306a\u5ea7\u6a19\u306b\u306a\u308b\u304b\u3092\u8003\u3048\u308b\u3068\u5c0e\u3051\u307e\u3059\u3002\u6a19\u6e96\u57fa\u5e95\u30d9\u30af\u30c8\u30eb \\((1,0)\\) \u3092\u89d2\u5ea6 \\(\\theta\\) \u3060\u3051\u56de\u8ee2\u3055\u305b\u308b\u3068\u3001\u65b0\u3057\u3044\u5ea7\u6a19\u306f \\((\\cos\\theta, \\sin\\theta)\\) \u306b\u306a\u308a\u307e\u3059\u3002\u540c\u69d8\u306b\u3001\u3082\u3046\u4e00\u3064\u306e\u6a19\u6e96\u57fa\u5e95\u30d9\u30af\u30c8\u30eb \\((0,1)\\) \u3092\u56de\u8ee2\u3055\u305b\u308b\u3068\u3001\u65b0\u3057\u3044\u5ea7\u6a19\u306f \\((-\\sin\\theta, \\cos\\theta)\\) \u306b\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u304c\u56de\u8ee2\u884c\u5217\u306e\u7b2c1\u5217\u3068\u7b2c2\u5217\u306b\u306a\u308a\u307e\u3059\u3002\u56de\u8ee2\u884c\u5217\u306e\u91cd\u8981\u306a\u7279\u5fb4\u306f\u3001\u305d\u308c\u304c\u76f4\u4ea4\u884c\u5217\uff08\\(R^TR = I\\)\uff09\u3067\u3042\u308b\u3053\u3068\u3067\u3001\u3053\u308c\u306f\u56de\u8ee2\u304c\u9577\u3055\u3068\u89d2\u5ea6\u3092\u4fdd\u5b58\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#q5","title":"Q5: \u7dda\u5f62\u5909\u63db\u3068\u7dda\u5f62\u6027\u306e\u6982\u5ff5\u306f\u4eca\u5f8c\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5b66\u7fd2\u306b\u3069\u3046\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u304b\uff1f","text":"<p>A5: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3001\u7dda\u5f62\u5909\u63db\u306e\u7279\u5225\u306a\u6319\u52d5\u3092\u8a18\u8ff0\u3057\u307e\u3059\u3002\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306f\u3001\u7dda\u5f62\u5909\u63db\u306b\u3088\u3063\u3066\u65b9\u5411\u304c\u5909\u308f\u3089\u306a\u3044\u30d9\u30af\u30c8\u30eb\uff08\u3064\u307e\u308a\u3001\u5909\u63db\u5f8c\u306e\u30d9\u30af\u30c8\u30eb\u304c\u5143\u306e\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u6570\u500d\u306b\u306a\u308b\uff09\u306e\u3053\u3068\u3067\u3059\u3002\u56fa\u6709\u5024\u306f\u305d\u306e\u300c\u5b9a\u6570\u500d\u300d\u306e\u5024\u3092\u6307\u3057\u307e\u3059\u3002\u7dda\u5f62\u5909\u63db\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3057\u3066\u3044\u308c\u3070\u3001\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u7dda\u5f62\u5909\u63db\u306e\u300c\u4e0d\u5909\u65b9\u5411\u300d\u3092\u898b\u3064\u3051\u308b\u3082\u306e\u3060\u3068\u3044\u3046\u76f4\u611f\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u306f\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u306e\u30c7\u30fc\u30bf\u5206\u6790\u624b\u6cd5\u306e\u7406\u8ad6\u7684\u57fa\u76e4\u3068\u306a\u3063\u3066\u304a\u308a\u3001\u30c7\u30fc\u30bf\u306e\u91cd\u8981\u306a\u7279\u5fb4\u3084\u69cb\u9020\u3092\u62bd\u51fa\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#q6","title":"Q6: \u884c\u5217\u3067\u8868\u305b\u306a\u3044\u7dda\u5f62\u5909\u63db\u306f\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A6: \u6709\u9650\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u9593\u306e\u7dda\u5f62\u5909\u63db\u306f\u3001\u9069\u5207\u306a\u57fa\u5e95\u3092\u9078\u3079\u3070\u5fc5\u305a\u884c\u5217\u3067\u8868\u73fe\u3067\u304d\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u7121\u9650\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\uff08\u95a2\u6570\u7a7a\u9593\u306a\u3069\uff09\u306b\u304a\u3051\u308b\u7dda\u5f62\u5909\u63db\u306f\u3001\u6709\u9650\u306e\u884c\u5217\u3067\u306f\u8868\u73fe\u3067\u304d\u306a\u3044\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u306f\u7dda\u5f62\u4f5c\u7528\u7d20\u3068\u547c\u3070\u308c\u3001\u5fae\u5206\u3084\u7a4d\u5206\u306a\u3069\u306e\u6f14\u7b97\u304c\u305d\u306e\u4f8b\u3067\u3059\u3002\u305f\u3060\u3057\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u5b9f\u8df5\u3067\u306f\u901a\u5e38\u3001\u6709\u9650\u6b21\u5143\u306e\u554f\u984c\u3092\u6271\u3046\u305f\u3081\u3001\u884c\u5217\u8868\u73fe\u3067\u5341\u5206\u3067\u3059\u3002</p>"},{"location":"lectures/LA/32-eigen-value-vector/#q7","title":"Q7: \u7dda\u5f62\u6027\u306e\u6761\u4ef6\uff08\u52a0\u6cd5\u6027\u3068\u6589\u6b21\u6027\uff09\u306f\u306a\u305c\u91cd\u8981\u306a\u306e\u3067\u3059\u304b\uff1f","text":"<p>A7: \u7dda\u5f62\u6027\u306e\u6761\u4ef6\u306f\u3001\u5909\u63db\u306e\u6319\u52d5\u3092\u5358\u7d14\u5316\u3057\u4e88\u6e2c\u53ef\u80fd\u306b\u3059\u308b\u305f\u3081\u306b\u91cd\u8981\u3067\u3059\u3002\u3053\u308c\u3089\u306e\u6761\u4ef6\u306b\u3088\u308a\u3001\u7dda\u5f62\u5909\u63db\u306f\u5c11\u6570\u306e\u30d9\u30af\u30c8\u30eb\uff08\u57fa\u5e95\uff09\u306e\u5909\u63db\u7d50\u679c\u3060\u3051\u3092\u77e5\u308c\u3070\u3001\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb\u306e\u5909\u63db\u7d50\u679c\u3092\u4e88\u6e2c\u3067\u304d\u308b\u3068\u3044\u3046\u5f37\u529b\u306a\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\u3002\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u8907\u96d1\u306a\u30b7\u30b9\u30c6\u30e0\u3092\u8fd1\u4f3c\u3059\u308b\u305f\u3081\u306b\u7dda\u5f62\u30e2\u30c7\u30eb\u304c\u5e83\u304f\u4f7f\u308f\u308c\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u7dda\u5f62\u30e2\u30c7\u30eb\u304c\u7406\u89e3\u3057\u3084\u3059\u304f\u3001\u8a08\u7b97\u304c\u52b9\u7387\u7684\u3067\u3001\u591a\u304f\u306e\u5834\u5408\u306b\u5341\u5206\u306a\u7cbe\u5ea6\u3092\u63d0\u4f9b\u3059\u308b\u305f\u3081\u3067\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II - \u7b2c33\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/33-eigen-value-vector/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c33\u56de \u30c6\u30fc\u30de: \u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb \u95a2\u9023\u9805\u76ee: \u7dda\u5f62\u5909\u63db\u3001\u7279\u6027\u65b9\u7a0b\u5f0f\u3001\u56fa\u6709\u7a7a\u9593\u3001\u56fa\u6709\u591a\u9805\u5f0f \u4e88\u7fd2\u5185\u5bb9: \u884c\u5217\u306e\u57fa\u672c\u6f14\u7b97\u3001\u884c\u5217\u5f0f\u306e\u8a08\u7b97\u3001\u7dda\u5f62\u5909\u63db\u306e\u6982\u5ff5</p>"},{"location":"lectures/LA/33-eigen-value-vector/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u3092\u901a\u3058\u3066\u3001\u4ee5\u4e0b\u306e\u76ee\u6a19\u306e\u9054\u6210\u3092\u76ee\u6307\u3057\u307e\u3059\uff1a</p> <ol> <li>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3057\u3001\u305d\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u3001\u56fa\u6709\u5024\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u308b</li> <li>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u3057\u3001\u305d\u306e\u6027\u8cea\u3092\u7406\u89e3\u3059\u308b</li> <li>\u56fa\u6709\u5024\u306e\u7dcf\u548c\u3068\u884c\u5217\u306e\u30c8\u30ec\u30fc\u30b9\u3001\u56fa\u6709\u5024\u306e\u7a4d\u3068\u884c\u5217\u5f0f\u306e\u95a2\u4fc2\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5fdc\u7528\u4f8b\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/33-eigen-value-vector/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/33-eigen-value-vector/#31","title":"3.1 \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9","text":"<p>\u884c\u5217\u306b\u3088\u308b\u7dda\u5f62\u5909\u63db\u3092\u8003\u3048\u308b\u3068\u304d\u3001\u65b9\u5411\u304c\u5909\u308f\u3089\u305a\u5927\u304d\u3055\u3060\u3051\u304c\u5909\u5316\u3059\u308b\u30d9\u30af\u30c8\u30eb\u304c\u5b58\u5728\u3057\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u7279\u5225\u306a\u30d9\u30af\u30c8\u30eb\u3092\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u547c\u3073\u3001\u305d\u306e\u62e1\u5927\u30fb\u7e2e\u5c0f\u7387\u3092\u56fa\u6709\u5024\u3068\u547c\u3073\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9: \\(n \\times n\\)\u884c\u5217\\(A\\)\u306b\u5bfe\u3057\u3066\u3001\u30bc\u30ed\u3067\u306a\u3044\u30d9\u30af\u30c8\u30eb\\(\\mathbf{v} \\in \\mathbb{R}^n\\)\u304c\u5b58\u5728\u3057\u3001\u3042\u308b\u5b9a\u6570\\(\\lambda \\in \\mathbb{R}\\)\uff08\u307e\u305f\u306f\\(\\mathbb{C}\\)\uff09\u306b\u5bfe\u3057\u3066</p> <p>\\(A\\mathbf{v} = \\lambda \\mathbf{v}\\)</p> <p>\u3092\u6e80\u305f\u3059\u3068\u304d\u3001\\(\\lambda\\)\u3092\u884c\u5217\\(A\\)\u306e\u56fa\u6709\u5024\u3001\\(\\mathbf{v}\\)\u3092\\(\\lambda\\)\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u547c\u3073\u307e\u3059\u3002</p> <p>\u3053\u306e\u5b9a\u7fa9\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u7406\u89e3\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a - \u884c\u5217\\(A\\)\u306b\u3088\u308b\u7dda\u5f62\u5909\u63db\u3092\u53d7\u3051\u3066\u3082\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\\(\\mathbf{v}\\)\u306f\u65b9\u5411\u304c\u5909\u308f\u3089\u305a - \u305d\u306e\u5927\u304d\u3055\u304c\\(\\lambda\\)\u500d\u306b\u306a\u308b</p>"},{"location":"lectures/LA/33-eigen-value-vector/#32","title":"3.2 \u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5e7e\u4f55\u7684\u89e3\u91c8","text":"<p>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u56fa\u6709\u5024\u306e\u5e7e\u4f55\u7684\u610f\u5473\u3092\u8003\u3048\u307e\u3057\u3087\u3046\u3002\u884c\u5217\\(A\\)\u306b\u3088\u308b\u7dda\u5f62\u5909\u63db\u3092\u8003\u3048\u305f\u3068\u304d\uff1a</p> <ol> <li>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3001\u5909\u63db\u5f8c\u3082\u5143\u306e\u65b9\u5411\u3092\u4fdd\u6301\u3059\u308b\u30d9\u30af\u30c8\u30eb</li> <li>\u56fa\u6709\u5024\u306f\u3001\u305d\u306e\u65b9\u5411\u306e\u30d9\u30af\u30c8\u30eb\u304c\u3069\u308c\u3060\u3051\u5f15\u304d\u4f38\u3070\u3055\u308c\u308b\uff08\u307e\u305f\u306f\u5727\u7e2e\u3055\u308c\u308b\uff09\u304b\u3092\u8868\u3059\u500d\u7387</li> <li>\u56fa\u6709\u5024\u304c\u6b63\u306a\u3089\u540c\u3058\u65b9\u5411\u306b\u5f15\u304d\u4f38\u3070\u3057\u3001\u8ca0\u306a\u3089\u53cd\u5bfe\u65b9\u5411\u306b\u5f15\u304d\u4f38\u3070\u3057</li> </ol> <p>\u4f8b\u3068\u3057\u3066\u30012\u6b21\u5143\u306e\u56de\u8ee2\u884c\u5217\u3092\u8003\u3048\u308b\u3068\u3001\u8907\u7d20\u56fa\u6709\u5024\u3092\u6301\u3061\u3001\u5b9f\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u5b58\u5728\u3057\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u56de\u8ee2\u306b\u3088\u3063\u3066\u3059\u3079\u3066\u306e\u65b9\u5411\u304c\u5909\u308f\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#33","title":"3.3 \u56fa\u6709\u591a\u9805\u5f0f\u3068\u7279\u6027\u65b9\u7a0b\u5f0f\u306e\u8a73\u7d30\u89e3\u8aac","text":"<p>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\u30d7\u30ed\u30bb\u30b9\u306f\u3001\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u91cd\u8981\u306a\u90e8\u5206\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u56fa\u6709\u591a\u9805\u5f0f\u3068\u7279\u6027\u65b9\u7a0b\u5f0f\u304c\u306a\u305c\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u306b\u4e0d\u53ef\u6b20\u306a\u306e\u304b\u3092\u8a73\u3057\u304f\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#_1","title":"\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u7684\u306a\u95a2\u4fc2","text":"<p>\u307e\u305a\u3001\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\u3092\u601d\u3044\u51fa\u3057\u307e\u3057\u3087\u3046\u3002\\(n \\times n\\)\u884c\u5217\\(A\\)\u306b\u5bfe\u3057\u3066\u3001\u30bc\u30ed\u3067\u306a\u3044\u30d9\u30af\u30c8\u30eb\\(\\mathbf{v}\\)\u3068\u6570\\(\\lambda\\)\u304c\u6b21\u306e\u95a2\u4fc2\u3092\u6e80\u305f\u3059\u3068\u304d\uff1a</p> \\[A\\mathbf{v} = \\lambda \\mathbf{v}\\] <p>\u3053\u306e\u3068\u304d\u3001\\(\\lambda\\)\u3092\u884c\u5217\\(A\\)\u306e\u56fa\u6709\u5024\u3001\\(\\mathbf{v}\\)\u3092\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u547c\u3073\u307e\u3059\u3002</p> <p>\u3053\u306e\u95a2\u4fc2\u5f0f\u3092\u6b21\u306e\u3088\u3046\u306b\u5909\u5f62\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> \\[A\\mathbf{v} - \\lambda \\mathbf{v} = \\mathbf{0}$$ $$\uff08A - \\lambda I\uff09\\mathbf{v} = \\mathbf{0}\\] <p>\u3053\u3053\u3067\\(I\\)\u306f\\(n \\times n\\)\u306e\u5358\u4f4d\u884c\u5217\u3067\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#_2","title":"\u540c\u6b21\u65b9\u7a0b\u5f0f\u306e\u975e\u81ea\u660e\u89e3\u306e\u6761\u4ef6","text":"<p>\u4e0a\u8a18\u306e\u5f0f \\((A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\) \u306f\u3001\u884c\u5217 \\((A - \\lambda I)\\) \u306b\u95a2\u3059\u308b\u540c\u6b21\u7dda\u5f62\u65b9\u7a0b\u5f0f\u3067\u3059\u3002\u3053\u306e\u65b9\u7a0b\u5f0f\u306b\u304a\u3044\u3066\u3001 \\(\\mathbf{v} \\neq \\mathbf{0}\\) \u3068\u3044\u3046\u975e\u81ea\u660e\u89e3\uff08\u30bc\u30ed\u3067\u306a\u3044\u89e3\uff09\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\u4f55\u3067\u3057\u3087\u3046\u304b\uff1f</p> <p>\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u57fa\u672c\u5b9a\u7406\u306b\u3088\u308c\u3070\u3001\u540c\u6b21\u65b9\u7a0b\u5f0f\\((A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\)\u304c\u975e\u81ea\u660e\u89e3\u3092\u6301\u3064\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\uff1a</p> <p>\u884c\u5217\\((A - \\lambda I)\\)\u304c\u6b63\u5247\u3067\u306a\u3044\uff08\u9006\u884c\u5217\u3092\u6301\u305f\u306a\u3044\uff09\u3053\u3068</p> <p>\u8a00\u3044\u63db\u3048\u308b\u3068\uff1a</p> <p>\u884c\u5217\\((A - \\lambda I)\\)\u306e\u30e9\u30f3\u30af\u304c\\(n\\)\u672a\u6e80\u3067\u3042\u308b\u3053\u3068</p> <p>\u307e\u305f\u306f\uff1a</p> <p>\u884c\u5217\\((A - \\lambda I)\\)\u306e\u884c\u5217\u5f0f\u304c\u30bc\u30ed\u3067\u3042\u308b\u3053\u3068</p> <p>\u3064\u307e\u308a\uff1a</p> \\[\\det(A - \\lambda I) = 0\\] <p>\u3053\u308c\u304c\u7279\u6027\u65b9\u7a0b\u5f0f\uff08\u56fa\u6709\u65b9\u7a0b\u5f0f\uff09\u3067\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#_3","title":"\u56fa\u6709\u591a\u9805\u5f0f\u306e\u5c0e\u5165","text":"<p>\\(\\det(A - \\lambda I)\\)\u3092\\(\\lambda\\)\u306e\u95a2\u6570\u3068\u898b\u306a\u3059\u3068\u3001\u3053\u308c\u306f\\(\\lambda\\)\u306b\u95a2\u3059\u308b\\(n\\)\u6b21\u591a\u9805\u5f0f\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u591a\u9805\u5f0f\u3092\u56fa\u6709\u591a\u9805\u5f0f\uff08\u307e\u305f\u306f\u7279\u6027\u591a\u9805\u5f0f\uff09\u3068\u547c\u3073\u307e\u3059\uff1a</p> \\[p_A(\\lambda) = \\det(A - \\lambda I)\\] <p>\u4f8b\u3048\u3070\u3001\\(2 \\times 2\\)\u884c\u5217\\(A = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\)\u306e\u5834\u5408\u3001\u56fa\u6709\u591a\u9805\u5f0f\u306f\uff1a</p> \\[p_A(\\lambda) = \\det\\begin{pmatrix} a-\\lambda &amp; b \\\\ c &amp; d-\\lambda \\end{pmatrix} = (a-\\lambda)(d-\\lambda) - bc\\] <p>\u3053\u308c\u3092\u5c55\u958b\u3059\u308b\u3068\uff1a</p> \\[p_A(\\lambda) = \\lambda^2 - (a+d)\\lambda + (ad-bc) = \\lambda^2 - \\text{tr}(A)\\lambda + \\det(A)\\] <p>\u3053\u306e\u3088\u3046\u306b\u3001\\(2 \\times 2\\)\u884c\u5217\u306e\u56fa\u6709\u591a\u9805\u5f0f\u306f\u3001\u884c\u5217\u306e\u30c8\u30ec\u30fc\u30b9\\(\\text{tr}(A) = a+d\\)\u3068\u884c\u5217\u5f0f\\(\\det(A) = ad-bc\\)\u3092\u7528\u3044\u3066\u7c21\u6f54\u306b\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#_4","title":"\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u7406\u7531","text":"<p>\u7279\u6027\u65b9\u7a0b\u5f0f\\(\\det(A - \\lambda I) = 0\\)\u3092\u89e3\u304f\u3053\u3068\u3067\u3001\u884c\u5217\\(A\\)\u306e\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u306a\u305c\u3067\u3057\u3087\u3046\u304b\uff1f</p> <ol> <li> <p>\u56fa\u6709\u5024\u306e\u5b9a\u7fa9\u306b\u623b\u308b: \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\)\u304c\u975e\u81ea\u660e\u89e3\\(\\mathbf{v} \\neq \\mathbf{0}\\)\u3092\u6301\u3064\u305f\u3081\u306b\u306f\u3001\\(\\det(A - \\lambda I) = 0\\)\u304c\u6210\u308a\u7acb\u3064\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u4ee3\u6570\u5b66\u306e\u57fa\u672c\u5b9a\u7406: \\(n\\)\u6b21\u591a\u9805\u5f0f\\(p_A(\\lambda)\\)\u306f\u3001\u91cd\u8907\u3082\u542b\u3081\u3066\u6b63\u78ba\u306b\\(n\\)\u500b\u306e\u8907\u7d20\u6839\u3092\u6301\u3061\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u6839\u304c\u884c\u5217\\(A\\)\u306e\u56fa\u6709\u5024\u3067\u3059\u3002</p> </li> <li> <p>\u56fa\u6709\u5024\u306e\u4ee3\u6570\u7684\u91cd\u8907\u5ea6: \\(p_A(\\lambda)\\)\u306b\u304a\u3051\u308b\u6839\\(\\lambda_i\\)\u306e\u91cd\u8907\u5ea6\u3092\u3001\u56fa\u6709\u5024\\(\\lambda_i\\)\u306e\u300c\u4ee3\u6570\u7684\u91cd\u8907\u5ea6\u300d\u3068\u547c\u3073\u307e\u3059\u3002</p> </li> </ol>"},{"location":"lectures/LA/33-eigen-value-vector/#_5","title":"\u8a08\u7b97\u306e\u5177\u4f53\u4f8b","text":"<p>\u5177\u4f53\u4f8b\u3092\u901a\u3058\u3066\u7406\u89e3\u3092\u6df1\u3081\u307e\u3057\u3087\u3046\u3002\u6b21\u306e\\(2 \\times 2\\)\u884c\u5217\u3092\u8003\u3048\u307e\u3059\uff1a</p> \\[A = \\begin{pmatrix} 3 &amp; 1 \\\\ 4 &amp; -1 \\end{pmatrix}\\] <p>\u7279\u6027\u591a\u9805\u5f0f\u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a</p> \\[p_A(\\lambda) = \\det(A - \\lambda I) = \\det\\begin{pmatrix} 3-\\lambda &amp; 1 \\\\ 4 &amp; -1-\\lambda \\end{pmatrix}$$ $$= (3-\\lambda)(-1-\\lambda) - 1 \\cdot 4$$ $$= (3-\\lambda)(-1-\\lambda) - 4$$ $$= -3-3\\lambda+\\lambda+\\lambda^2 - 4$$ $$= \\lambda^2 - 2\\lambda - 7\\] <p>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u308b\u3068\uff1a \\(\\(\\lambda^2 - 2\\lambda - 7 = 0\\)\\)</p> <p>\u3053\u306e2\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3068\uff1a \\(\\(\\lambda = \\frac{2 \\pm \\sqrt{4+28}}{2} = \\frac{2 \\pm \\sqrt{32}}{2} = \\frac{2 \\pm 4\\sqrt{2}}{2} = 1 \\pm 2\\sqrt{2}\\)\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u884c\u5217\\(A\\)\u306e\u56fa\u6709\u5024\u306f\uff1a \\(\\(\\lambda_1 = 1 + 2\\sqrt{2} \\approx 3.83\\)\\) \\(\\(\\lambda_2 = 1 - 2\\sqrt{2} \\approx -1.83\\)\\)</p>"},{"location":"lectures/LA/33-eigen-value-vector/#_6","title":"\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u3078","text":"<p>\u56fa\u6709\u5024\u304c\u6c42\u307e\u3063\u305f\u3089\u3001\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u56fa\u6709\u5024\\(\\lambda_i\\)\u306b\u5bfe\u3057\u3066\uff1a</p> <ol> <li>\u884c\u5217\\((A - \\lambda_i I)\\)\u3092\u8a08\u7b97\u3059\u308b</li> <li>\u540c\u6b21\u65b9\u7a0b\u5f0f\\((A - \\lambda_i I)\\mathbf{v} = \\mathbf{0}\\)\u3092\u89e3\u304f</li> <li>\u975e\u81ea\u660e\u89e3\u304c\u305d\u306e\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb</li> </ol> <p>\u4f8b\u3048\u3070\u3001\u4e0a\u306e\u4f8b\u3067\u56fa\u6709\u5024\\(\\lambda_1 = 1 + 2\\sqrt{2}\\)\u306b\u5bfe\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\u306b\u306f\uff1a</p> \\[A - \\lambda_1 I = \\begin{pmatrix} 3-(1+2\\sqrt{2}) &amp; 1 \\\\ 4 &amp; -1-(1+2\\sqrt{2}) \\end{pmatrix} = \\begin{pmatrix} 2-2\\sqrt{2} &amp; 1 \\\\ 4 &amp; -2-2\\sqrt{2} \\end{pmatrix}\\] <p>\u3053\u306e\u884c\u5217\u3092\u7528\u3044\u3066\u540c\u6b21\u65b9\u7a0b\u5f0f\\((A - \\lambda_1 I)\\mathbf{v} = \\mathbf{0}\\)\u3092\u89e3\u304d\u307e\u3059\u3002\u89e3\u3068\u3057\u3066\u5f97\u3089\u308c\u308b\u975e\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\\(\\mathbf{v}\\)\u304c\u3001\u56fa\u6709\u5024\\(\\lambda_1\\)\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/33-eigen-value-vector/#41","title":"4.1 \u56fa\u6709\u5024\u306e\u8a08\u7b97\u65b9\u6cd5","text":"<p>\\(n \\times n\\)\u884c\u5217\\(A\\)\u306e\u56fa\u6709\u5024\u3092\u6c42\u3081\u308b\u624b\u9806\uff1a</p> <ol> <li>\u7279\u6027\u591a\u9805\u5f0f\\(\\det(A - \\lambda I)\\)\u3092\u8a08\u7b97\u3059\u308b</li> <li>\u7279\u6027\u65b9\u7a0b\u5f0f\\(\\det(A - \\lambda I) = 0\\)\u3092\u89e3\u304f</li> <li>\u5f97\u3089\u308c\u305f\u89e3\\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\)\u304c\u56fa\u6709\u5024</li> </ol> <p>\u4f8b: 2\u00d72\u884c\u5217\u306e\u5834\u5408\u3001\u7279\u6027\u591a\u9805\u5f0f\u306f2\u6b21\u5f0f\u306b\u306a\u308a\u307e\u3059\uff1a \\(\\det\\begin{pmatrix} a-\\lambda &amp; b \\\\ c &amp; d-\\lambda \\end{pmatrix} = (a-\\lambda)(d-\\lambda) - bc = \\lambda^2 - (a+d)\\lambda + (ad-bc)\\)</p> <p>\u3064\u307e\u308a\u3001\\(\\lambda^2 - \\text{tr}(A)\\lambda + \\det(A) = 0\\)\u3092\u89e3\u304f\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#42","title":"4.2 \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u65b9\u6cd5","text":"<p>\u56fa\u6709\u5024\\(\\lambda\\)\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\u624b\u9806\uff1a</p> <ol> <li>\u884c\u5217\\((A - \\lambda I)\\)\u3092\u8a08\u7b97\u3059\u308b</li> <li>\u540c\u6b21\u65b9\u7a0b\u5f0f\\((A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\)\u3092\u89e3\u304f</li> <li>\u975e\u81ea\u660e\u89e3\uff08\u30bc\u30ed\u3067\u306a\u3044\u89e3\uff09\u304c\u305d\u306e\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb</li> </ol> <p>\u6ce8\u610f\u70b9:  - \u56fa\u6709\u5024\u304c\u91cd\u8907\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u6570\u306f\u56fa\u6709\u5024\u306e\u91cd\u8907\u5ea6\u4ee5\u4e0b - \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u30b9\u30ab\u30e9\u30fc\u500d\u3059\u308b\u3068\u4f9d\u7136\u3068\u3057\u3066\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u65b9\u5411\u306e\u307f\u304c\u91cd\u8981\uff09</p>"},{"location":"lectures/LA/33-eigen-value-vector/#43","title":"4.3 \u56fa\u6709\u7a7a\u9593","text":"<p>\u5b9a\u7fa9: \u56fa\u6709\u5024\\(\\lambda\\)\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u7a7a\u9593\\(E_\\lambda\\)\u306f\u3001\\(\\lambda\\)\u306b\u5bfe\u5fdc\u3059\u308b\u3059\u3079\u3066\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u96f6\u30d9\u30af\u30c8\u30eb\u304b\u3089\u306a\u308b\u90e8\u5206\u7a7a\u9593\uff1a</p> <p>\\(E_\\lambda = \\{\\mathbf{v} \\in \\mathbb{R}^n : A\\mathbf{v} = \\lambda \\mathbf{v}\\} = \\ker(A - \\lambda I)\\)</p> <p>\u56fa\u6709\u7a7a\u9593\u306e\u6b21\u5143\u306f\u3001\u305d\u306e\u56fa\u6709\u5024\u306e\u5e7e\u4f55\u7684\u91cd\u8907\u5ea6\u306b\u7b49\u3057\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#44","title":"4.4 \u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u7279\u5225\u306a\u6027\u8cea","text":"<p>\u5b9f\u5bfe\u79f0\u884c\u5217\\(A\\)\uff08\\(A = A^T\\)\uff09\u306e\u5834\u5408\uff1a</p> <ol> <li>\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u306f\u5b9f\u6570</li> <li>\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4</li> <li>\\(n\\)\u500b\u306e\u7dda\u5f62\u72ec\u7acb\u306a\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u5b58\u5728\uff08\u76f4\u4ea4\u5316\u53ef\u80fd\uff09</li> </ol> <p>\u3053\u308c\u3089\u306e\u6027\u8cea\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306a\u3069\u3067\u975e\u5e38\u306b\u91cd\u8981\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/33-eigen-value-vector/#51","title":"5.1 \u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[4, 2], \n              [1, 3]])\n\n# \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(\"\u884c\u5217A:\")\nprint(A)\nprint(\"\\n\u56fa\u6709\u5024:\")\nprint(eigenvalues)\nprint(\"\\n\u56fa\u6709\u30d9\u30af\u30c8\u30eb (\u5217\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066):\")\nprint(eigenvectors)\n\n# \u30c8\u30ec\u30fc\u30b9\u3068\u56fa\u6709\u5024\u306e\u548c\u306e\u78ba\u8a8d\nprint(\"\\n\u30c8\u30ec\u30fc\u30b9\u3068\u56fa\u6709\u5024\u306e\u548c:\")\nprint(f\"tr(A) = {np.trace(A)}\")\nprint(f\"\u56fa\u6709\u5024\u306e\u548c = {np.sum(eigenvalues)}\")\n\n# \u884c\u5217\u5f0f\u3068\u56fa\u6709\u5024\u306e\u7a4d\u306e\u78ba\u8a8d\nprint(\"\\n\u884c\u5217\u5f0f\u3068\u56fa\u6709\u5024\u306e\u7a4d:\")\nprint(f\"det(A) = {np.linalg.det(A)}\")\nprint(f\"\u56fa\u6709\u5024\u306e\u7a4d = {np.prod(eigenvalues)}\")\n</code></pre>"},{"location":"lectures/LA/33-eigen-value-vector/#52","title":"5.2 \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5e7e\u4f55\u7684\u89e3\u91c8\u306e\u53ef\u8996\u5316","text":"<pre><code># 2\u6b21\u5143\u5e73\u9762\u3067\u306e\u7dda\u5f62\u5909\u63db\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u53ef\u8996\u5316\ndef plot_transformation(A, eigenvalues, eigenvectors):\n    plt.figure(figsize=(12, 6))\n\n    # \u5358\u4f4d\u5186\u4e0a\u306e\u70b9\u7fa4\u3092\u4f5c\u6210\n    theta = np.linspace(0, 2*np.pi, 100)\n    x = np.cos(theta)\n    y = np.sin(theta)\n    circle_points = np.vstack([x, y])\n\n    # \u884c\u5217A\u306b\u3088\u308b\u5909\u63db\u5f8c\u306e\u70b9\u7fa4\n    transformed_points = A @ circle_points\n\n    # \u5143\u306e\u5358\u4f4d\u5186\u3068\u5909\u63db\u5f8c\u306e\u6955\u5186\u3092\u8868\u793a\n    plt.subplot(1, 2, 1)\n    plt.plot(x, y, 'b-', label='Unit Circle')\n    plt.plot(transformed_points[0, :], transformed_points[1, :], 'r-', \n             label='Transformed Circle')\n\n    # \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8868\u793a\n    for i in range(len(eigenvalues)):\n        vec = eigenvectors[:, i] * 2  # \u9577\u3055\u3092\u5f37\u8abf\n        transformed_vec = A @ vec      # \u5909\u63db\u5f8c\u306e\u30d9\u30af\u30c8\u30eb\n\n        plt.arrow(0, 0, vec[0], vec[1], head_width=0.1, \n                  head_length=0.1, fc=f'C{i}', ec=f'C{i}', \n                  label=f'Eigenvector {i+1}')\n        plt.arrow(0, 0, transformed_vec[0], transformed_vec[1], \n                  head_width=0.1, head_length=0.1, fc=f'C{i}', \n                  ec=f'C{i}', ls='--')\n\n    plt.grid(True)\n    plt.axis('equal')\n    plt.xlim(-3, 3)\n    plt.ylim(-3, 3)\n    plt.legend()\n    plt.title(\"Linear Transformation and Eigenvectors\")\n\n    # \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u65b9\u5411\u3068\u62e1\u5927\u7387\u3092\u8868\u793a\n    plt.subplot(1, 2, 2)\n    plt.quiver([0, 0], [0, 0], \n              [eigenvectors[0, 0], eigenvectors[0, 1]], \n              [eigenvectors[1, 0], eigenvectors[1, 1]], \n              angles='xy', scale_units='xy', scale=1, \n              color=['blue', 'green'])\n    plt.text(eigenvectors[0, 0]*1.1, eigenvectors[1, 0]*1.1, \n            f'\u03bb\u2081={eigenvalues[0]:.2f}')\n    plt.text(eigenvectors[0, 1]*1.1, eigenvectors[1, 1]*1.1, \n            f'\u03bb\u2082={eigenvalues[1]:.2f}')\n    plt.grid(True)\n    plt.axis('equal')\n    plt.xlim(-1, 1)\n    plt.ylim(-1, 1)\n    plt.title(\"Eigenvectors and their Eigenvalues\")\n\n    plt.tight_layout()\n    plt.show()\n\n# \u53ef\u8996\u5316\u95a2\u6570\u306e\u547c\u3073\u51fa\u3057\nplot_transformation(A, eigenvalues, eigenvectors)\n</code></pre>"},{"location":"lectures/LA/33-eigen-value-vector/#53","title":"5.3 \u7279\u6027\u65b9\u7a0b\u5f0f\u306e\u53ef\u8996\u5316","text":"<pre><code># \u7279\u6027\u591a\u9805\u5f0f\u3092\u8a08\u7b97\u3057\u3066\u53ef\u8996\u5316\ndef plot_characteristic_polynomial(A):\n    # \u30c8\u30ec\u30fc\u30b9\u3068\u884c\u5217\u5f0f\u306e\u8a08\u7b97\n    tr_A = np.trace(A)\n    det_A = np.linalg.det(A)\n\n    # \u7279\u6027\u591a\u9805\u5f0f: \u03bb^2 - tr(A)\u03bb + det(A)\n    x = np.linspace(-2, 7, 1000)\n    y = x**2 - tr_A*x + det_A\n\n    # \u56fa\u6709\u5024\n    eigenvalues = np.linalg.eigvals(A)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, 'b-', label='Characteristic Polynomial')\n    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    plt.grid(True)\n\n    # \u7279\u6027\u65b9\u7a0b\u5f0f\u306e\u89e3\uff08\u56fa\u6709\u5024\uff09\u3092\u30de\u30fc\u30af\u3059\u308b\n    for i, ev in enumerate(eigenvalues):\n        plt.plot(ev.real, 0, 'ro', markersize=8)\n        plt.text(ev.real, 0.5, f'\u03bb{i+1}={ev.real:.2f}')\n\n    plt.title(f\"Characteristic Polynomial: \u03bb^2 - {tr_A}\u03bb + {det_A}\")\n    plt.xlabel('\u03bb')\n    plt.ylabel('p(\u03bb)')\n    plt.legend()\n    plt.xlim(min(0, np.min(eigenvalues.real)-1), np.max(eigenvalues.real)+1)\n    plt.ylim(-5, 10)\n    plt.show()\n\n# \u7279\u6027\u591a\u9805\u5f0f\u306e\u53ef\u8996\u5316\nplot_characteristic_polynomial(A)\n</code></pre>"},{"location":"lectures/LA/33-eigen-value-vector/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/33-eigen-value-vector/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    \\(\\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 2 \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    \\(\\(B = \\begin{pmatrix} 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 4 \\\\ 0 &amp; 4 &amp; 3 \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u304c\u3059\u3079\u3066\u7570\u306a\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u306a\u3055\u3044\u3002    \\(\\(C = \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 4 &amp; 5 \\\\ 0 &amp; 0 &amp; 6 \\end{pmatrix}\\)\\)</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u30c8\u30ec\u30fc\u30b9\u3001\u884c\u5217\u5f0f\u306e\u95a2\u4fc2\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002    \\(\\(D = \\begin{pmatrix} 4 &amp; -1 &amp; 0 \\\\ -1 &amp; 4 &amp; -1 \\\\ 0 &amp; -1 &amp; 4 \\end{pmatrix}\\)\\)</p> </li> </ol>"},{"location":"lectures/LA/33-eigen-value-vector/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u5bfe\u79f0\u884c\u5217 \\(E = \\begin{pmatrix} 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 2 \\end{pmatrix}\\) \u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u884c\u5217 \\(F = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix}\\) \u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u3053\u306e\u884c\u5217\u304c\u8868\u3059\u7dda\u5f62\u5909\u63db\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u3042\u308b\u5171\u5206\u6563\u884c\u5217\u304c\u6b21\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u3066\u3044\u307e\u3059\u3002    \\(\\(\\Sigma = \\begin{pmatrix} 4 &amp; 2 \\\\ 2 &amp; 3 \\end{pmatrix}\\)\\)    \u3053\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u30c7\u30fc\u30bf\u306e\u4e3b\u306a\u5909\u52d5\u65b9\u5411\u3092\u7279\u5b9a\u3057\u306a\u3055\u3044\u3002\u3053\u308c\u306f\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u57fa\u672c\u539f\u7406\u3067\u3059\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u5fdc\u7528\u554f\u984c\uff1a\u60a3\u8005\u306e\u4f53\u91cd\uff08kg\uff09\u3068\u8eab\u9577\uff08cm\uff09\u306e\u30c7\u30fc\u30bf\u304b\u3089\u8a08\u7b97\u3055\u308c\u305f\u5171\u5206\u6563\u884c\u5217\u304c\u6b21\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u3066\u3044\u307e\u3059\u3002    \\(\\(\\Sigma_{\u5065\u5eb7} = \\begin{pmatrix} 25 &amp; 12 \\\\ 12 &amp; 9 \\end{pmatrix}\\)\\)    \u3053\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u305d\u308c\u3089\u304c\u8868\u3059\u8eab\u4f53\u6e2c\u5b9a\u5024\u306e\u4e3b\u8981\u306a\u5909\u52d5\u30d1\u30bf\u30fc\u30f3\u3092\u89e3\u91c8\u3057\u306a\u3055\u3044\u3002\u307e\u305f\u3001\u3053\u306e\u3088\u3046\u306a\u5206\u6790\u304c\u533b\u7642\u30c7\u30fc\u30bf\u89e3\u6790\u3067\u3069\u306e\u3088\u3046\u306b\u5f79\u7acb\u3064\u304b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/33-eigen-value-vector/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/33-eigen-value-vector/#q1","title":"Q1: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4f55\u306e\u5f79\u306b\u7acb\u3064\u306e\u3067\u3059\u304b\uff1f","text":"<p>A1: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u591a\u304f\u306e\u5fdc\u7528\u304c\u3042\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\uff1a - \u884c\u5217\u306e\u3079\u304d\u4e57(\\(A^n\\))\u306e\u52b9\u7387\u7684\u306a\u8a08\u7b97 - \u9023\u7acb\u5fae\u5206\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5 - \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306b\u3088\u308b\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b - \u753b\u50cf\u51e6\u7406\u306b\u304a\u3051\u308b\u7279\u5fb4\u62bd\u51fa - \u30b0\u30e9\u30d5\u7406\u8ad6\u306b\u304a\u3051\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5206\u6790 - \u91cf\u5b50\u529b\u5b66\u306b\u304a\u3051\u308b\u91cf\u5b50\u72b6\u614b\u306e\u89e3\u6790</p>"},{"location":"lectures/LA/33-eigen-value-vector/#q2","title":"Q2: \u884c\u5217\u306e\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u304c\u5b9f\u6570\u306b\u306a\u308b\u306e\u306f\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u3067\u3059\u304b\uff1f","text":"<p>A2: \u884c\u5217\u306e\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u304c\u5b9f\u6570\u306b\u306a\u308b\u306e\u306f\u4ee5\u4e0b\u306e\u5834\u5408\u3067\u3059\uff1a - \u5b9f\u5bfe\u79f0\u884c\u5217\uff08\\(A = A^T\\)\uff09 - \u30a8\u30eb\u30df\u30fc\u30c8\u884c\u5217\uff08\u8907\u7d20\u6570\u306e\u5834\u5408\u3001\\(A = A^*\\)\uff09 - \u4e09\u89d2\u884c\u5217\uff08\u3053\u306e\u5834\u5408\u3001\u56fa\u6709\u5024\u306f\u5bfe\u89d2\u6210\u5206\u306b\u4e00\u81f4\uff09</p>"},{"location":"lectures/LA/33-eigen-value-vector/#q3","title":"Q3: \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u4e00\u610f\u306b\u5b9a\u307e\u3089\u306a\u3044\u306e\u306f\u306a\u305c\u3067\u3059\u304b\uff1f","text":"<p>A3: \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u305d\u306e\u65b9\u5411\u306e\u307f\u304c\u91cd\u8981\u3067\u3001\u5927\u304d\u3055\u306f\u4efb\u610f\u3067\u3059\u3002\u3064\u307e\u308a\u3001\\(\\mathbf{v}\\)\u304c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306a\u3089\u3001\\(c\\mathbf{v}\\)\uff08\\(c \\neq 0\\)\uff09\u3082\u540c\u3058\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002\u307e\u305f\u3001\u56fa\u6709\u5024\u306e\u4ee3\u6570\u7684\u91cd\u8907\u5ea6\u304c\u5e7e\u4f55\u7684\u91cd\u8907\u5ea6\u3088\u308a\u5927\u304d\u3044\u5834\u5408\u3001\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u7a7a\u9593\u306e\u57fa\u5e95\u306e\u9078\u3073\u65b9\u306b\u81ea\u7531\u5ea6\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#q4","title":"Q4: \u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u306e\u304c\u96e3\u3057\u3044\u9ad8\u6b21\u5143\u884c\u5217\u306e\u5834\u5408\u306f\u3069\u3046\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A4: \u9ad8\u6b21\u5143\u306e\u5834\u5408\u306f\u6570\u5024\u8a08\u7b97\u624b\u6cd5\u3092\u7528\u3044\u307e\u3059\uff1a - NumPy\u306e <code>numpy.linalg.eig()</code> \u95a2\u6570 - \u3079\u304d\u4e57\u6cd5\uff08\u6700\u5927\u56fa\u6709\u5024\u3092\u6c42\u3081\u308b\u53cd\u5fa9\u6cd5\uff09 - QR\u6cd5\uff08\u5168\u3066\u306e\u56fa\u6709\u5024\u3092\u6c42\u3081\u308b\u52b9\u7387\u7684\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff09 - \u30e4\u30b3\u30d3\u6cd5\uff08\u5bfe\u79f0\u884c\u5217\u5411\u3051\u306e\u6570\u5024\u8a08\u7b97\u6cd5\uff09</p>"},{"location":"lectures/LA/33-eigen-value-vector/#q5","title":"Q5: \u56fa\u6709\u5024\u304c\u8907\u7d20\u6570\u306b\u306a\u308b\u5834\u5408\u3001\u305d\u306e\u5e7e\u4f55\u5b66\u7684\u306a\u610f\u5473\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A5: \u56fa\u6709\u5024\u304c\u8907\u7d20\u6570\u306b\u306a\u308b\u5834\u5408\u3001\u5bfe\u5fdc\u3059\u308b\u7dda\u5f62\u5909\u63db\u306f\u56de\u8ee2\u3092\u542b\u307f\u307e\u3059\uff1a - \u7d76\u5bfe\u5024\uff08\u30e2\u30b8\u30e5\u30e9\u30b9\uff09\u306f\u62e1\u5927\u30fb\u7e2e\u5c0f\u7387 - \u504f\u89d2\uff08\u5f15\u6570\uff09\u306f\u56de\u8ee2\u89d2\u5ea6 \u4f8b\u3048\u3070\u30012\u00d72\u56de\u8ee2\u884c\u5217\u306e\u56fa\u6709\u5024\u306f\\(e^{i\\theta}\\)\u3068\\(e^{-i\\theta}\\)\u306e\u5f62\uff08\\(\\theta\\)\u306f\u56de\u8ee2\u89d2\uff09\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/33-eigen-value-vector/#8","title":"8. \u4eca\u56de\u306e\u8b1b\u7fa9\u306e\u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u3044\u3046\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u91cd\u8981\u306a\u6982\u5ff5\u3092\u5b66\u3073\u307e\u3057\u305f\u3002</p> <ol> <li>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3001\u884c\u5217\u306b\u3088\u308b\u7dda\u5f62\u5909\u63db\u306b\u304a\u3044\u3066\u65b9\u5411\u304c\u4fdd\u5b58\u3055\u308c\u308b\u30d9\u30af\u30c8\u30eb\u3068\u305d\u306e\u62e1\u5927\u7387\u3092\u8868\u3059</li> <li>\u7279\u6027\u65b9\u7a0b\u5f0f(\\(\\det(A - \\lambda I) = 0\\))\u3092\u89e3\u304f\u3053\u3068\u3067\u56fa\u6709\u5024\u304c\u6c42\u307e\u308b</li> <li>\u5404\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3001\u540c\u6b21\u65b9\u7a0b\u5f0f\\((A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\)\u3092\u89e3\u304f\u3053\u3068\u3067\u6c42\u3081\u3089\u308c\u308b</li> <li>\u884c\u5217\u306e\u30c8\u30ec\u30fc\u30b9\u306f\u56fa\u6709\u5024\u306e\u548c\u3001\u884c\u5217\u5f0f\u306f\u56fa\u6709\u5024\u306e\u7a4d\u3068\u7b49\u3057\u3044</li> <li>\u5b9f\u5bfe\u79f0\u884c\u5217\u306f\u7279\u306b\u91cd\u8981\u306a\u6027\u8cea\uff08\u5b9f\u56fa\u6709\u5024\u3001\u76f4\u4ea4\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff09\u3092\u6301\u3064</li> </ol> <p>\u3053\u308c\u3089\u306e\u6982\u5ff5\u306f\u3001\u6b21\u56de\u5b66\u3076\u884c\u5217\u306e\u5bfe\u89d2\u5316\u3084\u3001\u5f8c\u306e\u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u624b\u6cd5\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5e7e\u4f55\u5b66\u7684\u7406\u89e3\u3092\u6df1\u3081\u308b\u3053\u3068\u3067\u3001\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u5206\u6790\u3084\u8907\u96d1\u306a\u7dda\u5f62\u5909\u63db\u306e\u7406\u89e3\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002\u7279\u306b\u5065\u5eb7\u30c7\u30fc\u30bf\u3084\u533b\u7642\u30c7\u30fc\u30bf\u306e\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u6b21\u5143\u524a\u6e1b\u3084\u91cd\u8981\u306a\u5909\u52d5\u30d1\u30bf\u30fc\u30f3\u306e\u767a\u898b\u306b\u5f79\u7acb\u3064\u57fa\u790e\u77e5\u8b58\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u57fa\u790e \u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/34-eigen-value-vector/#34","title":"\u7b2c34\u56de\uff1a\u7279\u6027\u65b9\u7a0b\u5f0f\u3068\u5bfe\u89d2\u5316","text":""},{"location":"lectures/LA/34-eigen-value-vector/#_1","title":"\u8b1b\u7fa9\u60c5\u5831","text":"<ul> <li>\u8b1b\u7fa9\u56de: \u7b2c34\u56de</li> <li>\u95a2\u9023\u9805\u76ee: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb</li> <li>\u4e88\u7fd2\u5185\u5bb9: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\u3001\u7dda\u5f62\u5909\u63db\u306e\u57fa\u790e</li> </ul>"},{"location":"lectures/LA/34-eigen-value-vector/#_2","title":"\u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306e\u5b9a\u7fa9\u3068\u610f\u5473\u3092\u7406\u89e3\u3059\u308b</li> <li>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u3044\u3066\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u884c\u5217\u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308b\u305f\u3081\u306e\u6761\u4ef6\u3092\u7406\u89e3\u3059\u308b</li> <li>\u5bfe\u89d2\u5316\u306e\u8a08\u7b97\u624b\u9806\u3092\u5b9f\u884c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u5bfe\u89d2\u5316\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u5fdc\u7528\u610f\u7fa9\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/34-eigen-value-vector/#1","title":"1. \u57fa\u672c\u6982\u5ff5\uff1a\u884c\u5217\u306e\u5bfe\u89d2\u5316","text":""},{"location":"lectures/LA/34-eigen-value-vector/#11","title":"1.1 \u5bfe\u89d2\u5316\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \\(n \\times n\\)\u884c\u5217\\(A\\)\u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308b\u3068\u306f\u3001\u53ef\u9006\u884c\u5217\\(P\\)\u3068\u5bfe\u89d2\u884c\u5217\\(D\\)\u304c\u5b58\u5728\u3057\u3066\u3001\\(P^{-1}AP = D\\)\u3068\u8868\u305b\u308b\u3053\u3068\u3092\u3044\u3046\u3002</p> <p>\u5bfe\u89d2\u5316\u3068\u306f\u3001\u884c\u5217\\(A\\)\u306b\u5bfe\u3057\u3066\u3001\u9069\u5207\u306a\u57fa\u5e95\u5909\u63db\u3092\u884c\u3046\u3053\u3068\u3067\u3001\u3088\u308a\u5358\u7d14\u306a\u5f62\uff08\u5bfe\u89d2\u884c\u5217\uff09\u306b\u5909\u5f62\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u5bfe\u89d2\u884c\u5217\u306f\u5bfe\u89d2\u6210\u5206\u4ee5\u5916\u304c\u3059\u3079\u30660\u3067\u3042\u308b\u884c\u5217\u3067\u3001\u8a08\u7b97\u3084\u6027\u8cea\u306e\u5206\u6790\u304c\u975e\u5e38\u306b\u7c21\u5358\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#12","title":"1.2 \u5bfe\u89d2\u5316\u306e\u610f\u5473","text":"<p>\u5bfe\u89d2\u5316\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u610f\u5473\u3092\u6301\u3061\u307e\u3059\uff1a</p> <ol> <li>\u7dda\u5f62\u5909\u63db\u306e\u5358\u7d14\u5316: \u884c\u5217\\(A\\)\u3092\u7dda\u5f62\u5909\u63db\u3068\u3057\u3066\u898b\u305f\u3068\u304d\u3001\u9069\u5207\u306a\u57fa\u5e95\u3092\u9078\u3076\u3053\u3068\u3067\u3001\u5404\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u306f\u305d\u306e\u65b9\u5411\u306b\u3060\u3051\u4f38\u7e2e\u3055\u308c\u308b\uff08\u56de\u8ee2\u3092\u4f34\u308f\u306a\u3044\uff09\u5909\u63db\u306b\u306a\u308b</li> <li>\u8a08\u7b97\u306e\u7c21\u7565\u5316: \u884c\u5217\u306e\u51aa\u4e57\\(A^k\\)\u3084\u95a2\u6570\\(f(A)\\)\u306e\u8a08\u7b97\u304c\u5bb9\u6613\u306b\u306a\u308b</li> <li>\u30b7\u30b9\u30c6\u30e0\u306e\u89e3\u6790: \u52d5\u7684\u30b7\u30b9\u30c6\u30e0\u3084\u5fae\u5206\u65b9\u7a0b\u5f0f\u306e\u89e3\u6790\u304c\u7c21\u5358\u306b\u306a\u308b</li> </ol>"},{"location":"lectures/LA/34-eigen-value-vector/#13","title":"1.3 \u5bfe\u89d2\u884c\u5217\u306e\u6027\u8cea","text":"<p>\u5bfe\u89d2\u884c\u5217\\(D = \\text{diag}(d_1, d_2, \\ldots, d_n)\\)\u306f\u4ee5\u4e0b\u306e\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\uff1a</p> <ol> <li>\\(D^k = \\text{diag}(d_1^k, d_2^k, \\ldots, d_n^k)\\)</li> <li>\\(\\det(D) = d_1 \\cdot d_2 \\cdot \\ldots \\cdot d_n\\)</li> <li>\\(\\text{tr}(D) = d_1 + d_2 + \\ldots + d_n\\)</li> </ol>"},{"location":"lectures/LA/34-eigen-value-vector/#2","title":"2. \u7406\u8ad6\u3068\u624b\u6cd5\uff1a\u7279\u6027\u65b9\u7a0b\u5f0f\u3068\u56fa\u6709\u5024","text":""},{"location":"lectures/LA/34-eigen-value-vector/#21","title":"2.1 \u7279\u6027\u65b9\u7a0b\u5f0f","text":"<p>\u5b9a\u7fa9: \\(n \\times n\\)\u884c\u5217\\(A\\)\u306e\u7279\u6027\u65b9\u7a0b\u5f0f\uff08\u56fa\u6709\u65b9\u7a0b\u5f0f\uff09\u3068\u306f\u3001\\(\\det(A - \\lambda I) = 0\\)\u306e\u3053\u3068\u3067\u3042\u308b\u3002</p> <p>\u3053\u306e\u65b9\u7a0b\u5f0f\u306e\u89e3\\(\\lambda\\)\u304c\u884c\u5217\\(A\\)\u306e\u56fa\u6709\u5024\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u7279\u6027\u591a\u9805\u5f0f\\(p_A(\\lambda) = \\det(A - \\lambda I)\\)\u306f\\(\\lambda\\)\u306b\u95a2\u3059\u308b\\(n\\)\u6b21\u591a\u9805\u5f0f\u306b\u306a\u308a\u3001\u6700\u5927\u3067\\(n\\)\u500b\u306e\u56fa\u6709\u5024\u3092\u6301\u3061\u307e\u3059\uff08\u91cd\u8907\u3082\u542b\u3080\uff09\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#22","title":"2.2 \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u624b\u9806","text":"<ol> <li>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u308b: \\(\\det(A - \\lambda I) = 0\\)</li> <li>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f: \u591a\u9805\u5f0f\u306e\u6839\u3068\u3057\u3066\u56fa\u6709\u5024\\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\)\u3092\u6c42\u3081\u308b</li> <li>\u5404\u56fa\u6709\u5024\u306b\u5bfe\u3059\u308b\u56fa\u6709\u7a7a\u9593\u3092\u6c42\u3081\u308b: \u5404\\(\\lambda_i\\)\u306b\u5bfe\u3057\u3066\u3001\\((A - \\lambda_i I)x = 0\\)\u3092\u6e80\u305f\u3059\u975e\u96f6\u30d9\u30af\u30c8\u30eb\\(x\\)\u3092\u6c42\u3081\u308b</li> <li>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u5e95\u3092\u69cb\u6210\u3059\u308b: \u5404\u56fa\u6709\u7a7a\u9593\u306e\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b</li> </ol>"},{"location":"lectures/LA/34-eigen-value-vector/#23","title":"2.3 \u56fa\u6709\u5024\u306e\u4ee3\u6570\u7684\u91cd\u8907\u5ea6\u3068\u5e7e\u4f55\u7684\u91cd\u8907\u5ea6","text":"<ul> <li>\u4ee3\u6570\u7684\u91cd\u8907\u5ea6: \u7279\u6027\u591a\u9805\u5f0f\u306b\u304a\u3051\u308b\u56fa\u6709\u5024\u306e\u91cd\u8907\u5ea6</li> <li>\u5e7e\u4f55\u7684\u91cd\u8907\u5ea6: \u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u7a7a\u9593\u306e\u6b21\u5143</li> </ul> <p>\u4f8b\u3048\u3070\u3001\\(\\lambda = 3\\)\u304c\u7279\u6027\u591a\u9805\u5f0f\u306e\u4e8c\u91cd\u6839\uff08\u4ee3\u6570\u7684\u91cd\u8907\u5ea62\uff09\u3067\u3042\u308b\u3068\u304d\u3001\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u7a7a\u9593\u306e\u6b21\u5143\uff08\u5e7e\u4f55\u7684\u91cd\u8907\u5ea6\uff09\u306f1\u307e\u305f\u306f2\u306b\u306a\u308a\u3048\u307e\u3059\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#3","title":"3. \u5bfe\u89d2\u5316\u306e\u7406\u8ad6","text":""},{"location":"lectures/LA/34-eigen-value-vector/#31","title":"3.1 \u5bfe\u89d2\u5316\u53ef\u80fd\u6761\u4ef6","text":"<p>\u5b9a\u7406: \\(n \\times n\\)\u884c\u5217\\(A\\)\u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\u3001\\(A\\)\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3067\\(n\\)\u6b21\u5143\u7a7a\u9593\u306e\u57fa\u5e95\u304c\u69cb\u6210\u3067\u304d\u308b\u3053\u3068\u3067\u3042\u308b\u3002</p> <p>\u3053\u308c\u306f\u6b21\u306e\u6761\u4ef6\u3068\u540c\u5024\u3067\u3059\uff1a</p> <ol> <li>\\(A\\)\u306e\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u306b\u5bfe\u3057\u3066\u3001\u4ee3\u6570\u7684\u91cd\u8907\u5ea6\u3068\u5e7e\u4f55\u7684\u91cd\u8907\u5ea6\u304c\u7b49\u3057\u3044</li> <li>\\(A\\)\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\\(n\\)\u500b\u7dda\u5f62\u72ec\u7acb\u306b\u5b58\u5728\u3059\u308b</li> </ol>"},{"location":"lectures/LA/34-eigen-value-vector/#32","title":"3.2 \u5bfe\u89d2\u5316\u306e\u624b\u9806","text":"<ol> <li>\\(A\\)\u306e\u56fa\u6709\u5024\\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\)\u3092\u6c42\u3081\u308b</li> <li>\u5404\u56fa\u6709\u5024\\(\\lambda_i\\)\u306b\u5bfe\u3057\u3066\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\\(v_i\\)\u3092\u6c42\u3081\u308b</li> <li>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u5217\u306b\u4e26\u3079\u305f\u884c\u5217\\(P = [v_1, v_2, \\ldots, v_n]\\)\u3092\u69cb\u6210\u3059\u308b</li> <li>\u5bfe\u89d2\u884c\u5217\\(D = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)\\)\u3092\u69cb\u6210\u3059\u308b</li> <li>\\(P^{-1}AP = D\\)\u3092\u78ba\u8a8d\u3059\u308b</li> </ol>"},{"location":"lectures/LA/34-eigen-value-vector/#33","title":"3.3 \u5bfe\u89d2\u5316\u304c\u53ef\u80fd\u3067\u306a\u3044\u5834\u5408","text":"<p>\u4ee5\u4e0b\u306e\u5834\u5408\u306f\u5bfe\u89d2\u5316\u3067\u304d\u307e\u305b\u3093\uff1a</p> <ol> <li>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u6570\u304c\u8db3\u308a\u306a\u3044\uff08\u7dda\u5f62\u72ec\u7acb\u306a\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\\(n\\)\u500b\u306a\u3044\uff09</li> <li>\u8907\u7d20\u56fa\u6709\u5024\u3092\u6301\u3064\u5b9f\u884c\u5217\uff08\u3053\u306e\u5834\u5408\u3001\u5b9f\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\u306f\u3067\u304d\u306a\u3044\u304c\u3001\u8907\u7d20\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\u306f\u53ef\u80fd\uff09</li> </ol>"},{"location":"lectures/LA/34-eigen-value-vector/#4","title":"4. \u5177\u4f53\u4f8b\u306b\u3088\u308b\u5bfe\u89d2\u5316\u306e\u8a08\u7b97","text":""},{"location":"lectures/LA/34-eigen-value-vector/#122","title":"\u4f8b\u984c1\uff1a2\u00d72\u884c\u5217\u306e\u5bfe\u89d2\u5316","text":"<p>\u884c\u5217 \\(A = \\begin{bmatrix} 3 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix}\\) \u3092\u5bfe\u89d2\u5316\u3057\u307e\u3057\u3087\u3046\u3002</p> <p>Step 1: \u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u308b \\(\\det(A - \\lambda I) = \\det\\begin{bmatrix} 3-\\lambda &amp; 1 \\\\ 1 &amp; 3-\\lambda \\end{bmatrix} = (3-\\lambda)^2 - 1 = (3-\\lambda)^2 - 1 = 0\\)</p> <p>Step 2: \u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f \\((3-\\lambda)^2 = 1\\) \\(3-\\lambda = \\pm 1\\) \\(\\lambda = 3 \\pm 1 = 2, 4\\)</p> <p>\u5f93\u3063\u3066\u3001\u56fa\u6709\u5024\u306f \\(\\lambda_1 = 2\\) \u3068 \\(\\lambda_2 = 4\\) \u3067\u3059\u3002</p> <p>Step 3: \u5404\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b</p> <p>\\(\\lambda_1 = 2\\) \u306e\u5834\u5408\uff1a \\((A - 2I)v = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)</p> <p>\u3053\u308c\u306f \\(v_1 + v_2 = 0\\) \u3092\u610f\u5473\u3057\u307e\u3059\u3002\u5f93\u3063\u3066\u3001\\(v_1 = -v_2\\) \u3068\u306a\u308a\u3001\u4f8b\u3048\u3070 \\(v_1 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\) \u304c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\\(\\lambda_2 = 4\\) \u306e\u5834\u5408\uff1a \\((A - 4I)v = \\begin{bmatrix} -1 &amp; 1 \\\\ 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)</p> <p>\u3053\u308c\u306f \\(-v_1 + v_2 = 0\\) \u3092\u610f\u5473\u3057\u307e\u3059\u3002\u5f93\u3063\u3066\u3001\\(v_1 = v_2\\) \u3068\u306a\u308a\u3001\u4f8b\u3048\u3070 \\(v_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) \u304c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>Step 4: \u5bfe\u89d2\u5316\u884c\u5217 \\(P\\) \u3068\u5bfe\u89d2\u884c\u5217 \\(D\\) \u3092\u69cb\u6210\u3059\u308b</p> <p>\\(P = \\begin{bmatrix} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{bmatrix}\\), \\(D = \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 4 \\end{bmatrix}\\)</p> <p>Step 5: \u691c\u8a3c</p> <p>\\(P^{-1}AP = D\\) \u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> <p>\\(P^{-1} = \\frac{1}{\\det(P)}\\begin{bmatrix} 1 &amp; -1 \\\\ 1 &amp; 1 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 1 &amp; -1 \\\\ 1 &amp; 1 \\end{bmatrix}\\)</p> <p>\\(P^{-1}AP = \\frac{1}{2}\\begin{bmatrix} 1 &amp; -1 \\\\ 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 3 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 4 \\end{bmatrix} = D\\)</p> <p>\u4ee5\u4e0a\u306b\u3088\u308a\u3001\u884c\u5217 \\(A\\) \u306f\u5bfe\u89d2\u5316\u3055\u308c\u3001\\(P^{-1}AP = D\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#233","title":"\u4f8b\u984c2\uff1a3\u00d73\u884c\u5217\u306e\u5bfe\u89d2\u5316","text":"<p>\u884c\u5217 \\(A = \\begin{bmatrix} 4 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 4 \\end{bmatrix}\\) \u3092\u5bfe\u89d2\u5316\u3057\u307e\u3057\u3087\u3046\u3002</p> <p>Step 1: \u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u308b \\(\\det(A - \\lambda I) = \\det\\begin{bmatrix} 4-\\lambda &amp; 0 &amp; 1 \\\\ 0 &amp; 1-\\lambda &amp; 0 \\\\ 1 &amp; 0 &amp; 4-\\lambda \\end{bmatrix}\\)</p> <p>\u3053\u308c\u3092\u5c55\u958b\u3059\u308b\u3068\uff1a \\(= (1-\\lambda)[(4-\\lambda)(4-\\lambda) - 1]\\) \\(= (1-\\lambda)[(4-\\lambda)^2 - 1]\\) \\(= (1-\\lambda)[(16 - 8\\lambda + \\lambda^2) - 1]\\) \\(= (1-\\lambda)(15 - 8\\lambda + \\lambda^2)\\)</p> <p>Step 2: \u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f \\((1-\\lambda)(15 - 8\\lambda + \\lambda^2) = 0\\)</p> <p>\u5f93\u3063\u3066\u3001\\(\\lambda_1 = 1\\) \u3068 \\((15 - 8\\lambda + \\lambda^2) = 0\\) \u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <p>\u4e8c\u6b21\u65b9\u7a0b\u5f0f \\(\\lambda^2 - 8\\lambda + 15 = 0\\) \u3092\u89e3\u304f\u3068\uff1a \\(\\lambda = \\frac{8 \\pm \\sqrt{64 - 60}}{2} = \\frac{8 \\pm 2}{2} = 4 \\pm 1\\)</p> <p>\u3088\u3063\u3066\u3001\u56fa\u6709\u5024\u306f \\(\\lambda_1 = 1\\), \\(\\lambda_2 = 3\\), \\(\\lambda_3 = 5\\) \u3067\u3059\u3002</p> <p>Step 3: \u5404\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b</p> <p>\\(\\lambda_1 = 1\\) \u306e\u5834\u5408\uff1a \\((A - I)v = \\begin{bmatrix} 3 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 3 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\)</p> <p>\u3053\u308c\u306f \\(3v_1 + v_3 = 0\\) \u304a\u3088\u3073 \\(v_1 + 3v_3 = 0\\) \u3092\u610f\u5473\u3057\u307e\u3059\u304c\u3001\u3053\u308c\u3089\u306f\u540c\u3058\u6761\u4ef6\u3067\u3059\u3002\u307e\u305f\u3001\\(v_2\\) \u306f\u81ea\u7531\u306b\u9078\u3079\u307e\u3059\u3002 \u5f93\u3063\u3066\u3001\\(v_1 = -v_3\\), \\(v_2\\) \u306f\u4efb\u610f\u3068\u306a\u308a\u3001\u4f8b\u3048\u3070 \\(v_1 = \\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) \u304c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\\(\\lambda_2 = 3\\) \u306e\u5834\u5408\uff1a \\((A - 3I)v = \\begin{bmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; -2 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\)</p> <p>\u3053\u308c\u3088\u308a\u3001\\(v_1 + v_3 = 0\\), \\(-2v_2 = 0\\), \\(v_1 + v_3 = 0\\) \u3068\u306a\u308a\u307e\u3059\u3002\u5f93\u3063\u3066\u3001\\(v_2 = 0\\), \\(v_1 = -v_3\\) \u3068\u306a\u308a\u3001\u4f8b\u3048\u3070 \\(v_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}\\) \u304c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\\(\\lambda_3 = 5\\) \u306e\u5834\u5408\uff1a \\((A - 5I)v = \\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ 0 &amp; -4 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\)</p> <p>\u3053\u308c\u3088\u308a\u3001\\(-v_1 + v_3 = 0\\), \\(-4v_2 = 0\\), \\(v_1 - v_3 = 0\\) \u3068\u306a\u308a\u307e\u3059\u3002\u5f93\u3063\u3066\u3001\\(v_2 = 0\\), \\(v_1 = v_3\\) \u3068\u306a\u308a\u3001\u4f8b\u3048\u3070 \\(v_3 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\) \u304c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>Step 4: \u5bfe\u89d2\u5316\u884c\u5217 \\(P\\) \u3068\u5bfe\u89d2\u884c\u5217 \\(D\\) \u3092\u69cb\u6210\u3059\u308b</p> <p>\\(P = \\begin{bmatrix} -1 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; -1 &amp; 1 \\end{bmatrix}\\), \\(D = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 5 \\end{bmatrix}\\)</p> <p>Step 5: \u691c\u8a3c</p> <p>\\(P^{-1}AP = D\\) \u3092\u78ba\u8a8d\u3057\u307e\u3059\uff08\u8a08\u7b97\u306f\u7701\u7565\uff09\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#5","title":"5. \u5bfe\u89d2\u5316\u306e\u5fdc\u7528","text":""},{"location":"lectures/LA/34-eigen-value-vector/#51","title":"5.1 \u884c\u5217\u306e\u51aa\u4e57\u8a08\u7b97","text":"<p>\u5bfe\u89d2\u5316\u3055\u308c\u305f\u884c\u5217 \\(A = PDP^{-1}\\) \u306b\u5bfe\u3057\u3066\u3001\\(A^k = PD^kP^{-1}\\) \u304c\u6210\u308a\u7acb\u3061\u307e\u3059\u3002\u5bfe\u89d2\u884c\u5217\u306e\u51aa\u4e57\u8a08\u7b97\u306f\u5404\u5bfe\u89d2\u6210\u5206\u3092\u51aa\u4e57\u3059\u308b\u3060\u3051\u306a\u306e\u3067\u3001\u8907\u96d1\u306a\u884c\u5217\u306e\u51aa\u4e57\u8a08\u7b97\u304c\u7c21\u5358\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u4f8b: \\(A = \\begin{bmatrix} 3 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix}\\) \u306e10\u4e57\u3092\u8a08\u7b97\u3059\u308b\u3002</p> <p>\u5148\u307b\u3069\u306e\u4f8b\u984c1\u3088\u308a\u3001\\(A = PDP^{-1}\\) \u3067 \\(P = \\begin{bmatrix} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{bmatrix}\\), \\(D = \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 4 \\end{bmatrix}\\) \u3067\u3059\u3002</p> <p>\\(A^{10} = PD^{10}P^{-1} = P\\begin{bmatrix} 2^{10} &amp; 0 \\\\ 0 &amp; 4^{10} \\end{bmatrix}P^{-1}\\) \\(= P\\begin{bmatrix} 1024 &amp; 0 \\\\ 0 &amp; 1048576 \\end{bmatrix}P^{-1}\\)</p> <p>\\(P^{-1} = \\frac{1}{2}\\begin{bmatrix} 1 &amp; -1 \\\\ 1 &amp; 1 \\end{bmatrix}\\)</p> <p>\u8a08\u7b97\u3059\u308b\u3068\u3001\\(A^{10} = \\begin{bmatrix} 524800 &amp; 523776 \\\\ 523776 &amp; 524800 \\end{bmatrix}\\) \u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#52","title":"5.2 \u7dda\u5f62\u6f38\u5316\u5f0f\u306e\u4e00\u822c\u9805","text":"<p>\\(a_{n+1} = c_1a_n + c_2a_{n-1} + \\ldots + c_ka_{n-k+1}\\) \u3068\u3044\u3046\u7dda\u5f62\u6f38\u5316\u5f0f\u306f\u3001\u9069\u5207\u306a\u884c\u5217 \\(A\\) \u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067\u3001\\(\\vec{x}_{n+1} = A\\vec{x}_n\\) \u3068\u3044\u3046\u5f62\u306b\u66f8\u304d\u63db\u3048\u3089\u308c\u307e\u3059\u3002\u5bfe\u89d2\u5316\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u4e00\u822c\u9805\u3092\u7c21\u5358\u306b\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#53","title":"5.3 \u5fae\u5206\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5","text":"<p>\u5fae\u5206\u65b9\u7a0b\u5f0f\u7cfb \\(\\frac{d\\vec{x}}{dt} = A\\vec{x}\\) \u306e\u89e3\u306f\u3001\\(A\\) \u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u306a\u3089 \\(\\vec{x}(t) = Pe^{Dt}P^{-1}\\vec{x}(0)\\) \u3068\u8868\u305b\u307e\u3059\u3002\u3053\u3053\u3067 \\(e^{Dt} = \\text{diag}(e^{\\lambda_1 t}, e^{\\lambda_2 t}, \\ldots, e^{\\lambda_n t})\\) \u3067\u3059\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#6-python","title":"6. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u4ee5\u4e0b\u306b\u3001\u5bfe\u89d2\u5316\u306e\u8a08\u7b97\u3068\u305d\u306e\u53ef\u8996\u5316\u3092Python\u3067\u5b9f\u88c5\u3057\u305f\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import eig, inv\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[3, 1], [1, 3]])\n\n# \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\neigenvalues, eigenvectors = eig(A)\n\nprint(\"\u56fa\u6709\u5024:\")\nprint(eigenvalues)\n\nprint(\"\\n\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u5217\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u793a\uff09:\")\nprint(eigenvectors)\n\n# \u5bfe\u89d2\u5316\u306e\u691c\u8a3c\nP = eigenvectors\nD = np.diag(eigenvalues)\nP_inv = inv(P)\n\n# P^(-1) * A * P = D \u3092\u691c\u8a3c\nresult = P_inv @ A @ P\nprint(\"\\n\u691c\u8a3c: P^(-1) * A * P\")\nprint(result)\nprint(\"\u3053\u308c\u306f\u5bfe\u89d2\u884c\u5217 D \u306b\u8fd1\u4f3c\u7684\u306b\u7b49\u3057\u3044\")\n\n# \u5bfe\u89d2\u5316\u3092\u7528\u3044\u305f\u884c\u5217\u306e\u51aa\u4e57\u8a08\u7b97\ndef matrix_power_diag(A, power):\n    eigenvalues, eigenvectors = eig(A)\n    P = eigenvectors\n    D_power = np.diag(eigenvalues ** power)\n    P_inv = inv(P)\n    return P @ D_power @ P_inv\n\n# A^10 \u3092\u8a08\u7b97\nA_power_10 = matrix_power_diag(A, 10)\nprint(\"\\nA^10 (\u5bfe\u89d2\u5316\u3092\u4f7f\u7528):\")\nprint(A_power_10)\n\n# \u76f4\u63a5\u8a08\u7b97\u3067\u691c\u8a3c\nA_power_10_direct = np.linalg.matrix_power(A, 10)\nprint(\"\\nA^10 (\u76f4\u63a5\u8a08\u7b97):\")\nprint(A_power_10_direct)\n\n# \u5bfe\u89d2\u5316\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u3092\u53ef\u8996\u5316\ndef plot_transformation(A):\n    # \u5358\u4f4d\u5186\u4e0a\u306e\u70b9\u3092\u751f\u6210\n    theta = np.linspace(0, 2*np.pi, 100)\n    circle_x = np.cos(theta)\n    circle_y = np.sin(theta)\n    circle_points = np.vstack((circle_x, circle_y))\n\n    # \u884c\u5217\u5909\u63db\u3092\u9069\u7528\n    transformed_points = A @ circle_points\n\n    # \u30d7\u30ed\u30c3\u30c8\n    plt.figure(figsize=(10, 5))\n\n    # \u5143\u306e\u5358\u4f4d\u5186\n    plt.subplot(1, 2, 1)\n    plt.plot(circle_x, circle_y, 'b-')\n    plt.grid(True)\n    plt.axis('equal')\n    plt.title('Original Unit Circle')\n\n    # \u5909\u63db\u5f8c\n    plt.subplot(1, 2, 2)\n    plt.plot(transformed_points[0], transformed_points[1], 'r-')\n    plt.grid(True)\n    plt.axis('equal')\n    plt.title('Transformed by Matrix A')\n\n    # \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3082\u63cf\u753b\n    for i in range(len(eigenvalues)):\n        v = eigenvectors[:, i]\n        transformed_v = A @ v\n        plt.subplot(1, 2, 1)\n        plt.arrow(0, 0, v[0], v[1], head_width=0.05, head_length=0.1, fc='green', ec='green')\n        plt.subplot(1, 2, 2)\n        plt.arrow(0, 0, transformed_v[0], transformed_v[1], head_width=0.05, head_length=0.1, fc='green', ec='green')\n\n    plt.tight_layout()\n    plt.show()\n\n# \u5909\u63db\u3092\u53ef\u8996\u5316\nplot_transformation(A)\n</code></pre>"},{"location":"lectures/LA/34-eigen-value-vector/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/34-eigen-value-vector/#_3","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u884c\u5217\u3092\u5bfe\u89d2\u5316\u305b\u3088\u3002    a) \\(\\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix}\\)    b) \\(\\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix}\\)    c) \\(\\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 0 \\\\ 0 &amp; 1 &amp; 2 \\end{bmatrix}\\)</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u3057\u3001\u53ef\u80fd\u306a\u3089\u3070\u5bfe\u89d2\u5316\u305b\u3088\u3002    a) \\(\\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}\\)    b) \\(\\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}\\)</p> </li> <li> <p>\u884c\u5217 \\(A = \\begin{bmatrix} 1 &amp; 2 \\\\ 4 &amp; 3 \\end{bmatrix}\\) \u3092\u5bfe\u89d2\u5316\u3057\u3001\\(A^{20}\\) \u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\u5bfe\u89d2\u5316\u3092\u7528\u3044\u3066\u3001\u6f38\u5316\u5f0f \\(a_{n+2} = 6a_{n+1} - 9a_n\\) (\\(a_0 = 1\\), \\(a_1 = 3\\)) \u306e\u4e00\u822c\u9805\u3092\u6c42\u3081\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/34-eigen-value-vector/#_4","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u6b63\u65b9\u884c\u5217 \\(A\\) \u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308b\u3068\u304d\u3001\u7dda\u5f62\u5199\u50cf \\(f(x) = e^{At}x\\) \u306f\u4f55\u3092\u8868\u3057\u3066\u3044\u308b\u304b\u8003\u5bdf\u305b\u3088\u3002\u307e\u305f\u3001\\(A\\) \u304c\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u5834\u5408\u3001\\(t=1\\) \u3067\u306e\u5199\u50cf\u3092\u53ef\u8996\u5316\u305b\u3088\u3002    \\(A = \\begin{bmatrix} -1 &amp; 1 \\\\ -1 &amp; -1 \\end{bmatrix}\\)</p> </li> <li> <p>3\u00d73\u306e\u5bfe\u79f0\u884c\u5217 \\(A = \\begin{bmatrix} 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 2 \\end{bmatrix}\\) \u306b\u3064\u3044\u3066\u3001\u5bfe\u89d2\u5316\u3092\u884c\u3044\u3001\u305d\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u3092\u8003\u5bdf\u305b\u3088\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u3053\u306e\u884c\u5217\u304c\u8868\u3059\u4e8c\u6b21\u5f62\u5f0f \\(q(x,y,z) = x^TAx\\) \u306e\u5f62\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306e\u5fdc\u7528\u554f\u984c\uff1a\u60a3\u8005\u306e\u8840\u5727\u30c7\u30fc\u30bf\u304b\u3089\u306a\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \\(X\\) \u306b\u3064\u3044\u3066\u3001\u5171\u5206\u6563\u884c\u5217 \\(\\Sigma = X^TX\\) \u3092\u8003\u3048\u308b\u3002\u3053\u306e\u5171\u5206\u6563\u884c\u5217\u3092\u5bfe\u89d2\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u4e3b\u8981\u306a\u5909\u52d5\u65b9\u5411\u3092\u6c42\u3081\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u305b\u3088\u3002\u307e\u305f\u3001\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u6301\u3064\u610f\u5473\u3092\u89e3\u91c8\u305b\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/34-eigen-value-vector/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/34-eigen-value-vector/#q1","title":"Q1: \u884c\u5217\u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u304b\u3069\u3046\u304b\u3092\u7c21\u5358\u306b\u5224\u5b9a\u3059\u308b\u65b9\u6cd5\u306f\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A1: \u4e00\u822c\u7684\u306b\u306f\u56fa\u6709\u5024\u3068\u56fa\u6709\u7a7a\u9593\u306e\u6b21\u5143\u3092\u8abf\u3079\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u4ee5\u4e0b\u306e\u30b1\u30fc\u30b9\u3067\u306f\u5224\u5b9a\u304c\u7c21\u5358\u306b\u306a\u308a\u307e\u3059\uff1a - \u5bfe\u79f0\u884c\u5217\u306f\u5e38\u306b\u5b9f\u5bfe\u89d2\u5316\u53ef\u80fd - \u56fa\u6709\u5024\u304c\u3059\u3079\u3066\u7570\u306a\u308b\u884c\u5217\u306f\u5bfe\u89d2\u5316\u53ef\u80fd - \u4e09\u89d2\u884c\u5217\u306e\u5834\u5408\u3001\u5bfe\u89d2\u6210\u5206\u304c\u56fa\u6709\u5024\u306b\u306a\u308b\u306e\u3067\u3001\u5bfe\u89d2\u6210\u5206\u306b\u91cd\u8907\u304c\u306a\u3051\u308c\u3070\u5bfe\u89d2\u5316\u53ef\u80fd - \u51aa\u96f6\u884c\u5217\uff08\\(A^n = 0\\) \u3068\u306a\u308b\u3088\u3046\u306a\u884c\u5217\uff09\u306f\u4e00\u822c\u306b\u5bfe\u89d2\u5316\u4e0d\u53ef\u80fd</p>"},{"location":"lectures/LA/34-eigen-value-vector/#q2","title":"Q2: \u56fa\u6709\u5024\u304c\u3059\u3079\u3066\u540c\u3058\u5024\u306b\u306a\u308b\u884c\u5217\u306f\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3059\u304b\uff1f","text":"<p>A2: \u56fa\u6709\u5024\u304c\u3059\u3079\u3066\u540c\u3058\u5024 \\(\\lambda\\) \u306b\u306a\u308b\u884c\u5217\u306f\u3001\u4ee5\u4e0b\u306e2\u3064\u306e\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u305d\u306e\u884c\u5217\u304c \\(\\lambda I\\) \u306e\u5f62\uff08\u30b9\u30ab\u30e9\u30fc\u884c\u5217\uff09\u306e\u5834\u5408\uff1a\u65e2\u306b\u5bfe\u89d2\u5f62\u5f0f\u306a\u306e\u3067\u5bfe\u89d2\u5316\u53ef\u80fd 2. \\(\\lambda I\\) \u3067\u306a\u3044\u5834\u5408\uff1a\u4e00\u822c\u306b\u5bfe\u89d2\u5316\u4e0d\u53ef\u80fd\uff08\u30e8\u30eb\u30c0\u30f3\u6a19\u6e96\u5f62\u304c\u5fc5\u8981\uff09</p> <p>\u4f8b\u3048\u3070 \\(\\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}\\) \u306f\u56fa\u6709\u5024\u304c\u4e21\u65b9\u3068\u30821\u3067\u3059\u304c\u5bfe\u89d2\u5316\u3067\u304d\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#q3","title":"Q3: \u5b9f\u884c\u5217\u3067\u8907\u7d20\u56fa\u6709\u5024\u304c\u3042\u308b\u5834\u5408\u306f\u3069\u3046\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A3: \u5b9f\u884c\u5217\u3067\u8907\u7d20\u56fa\u6709\u5024\u304c\u73fe\u308c\u308b\u5834\u5408\u3001\u8907\u7d20\u56fa\u6709\u5024\u306f\u5171\u5f79\u30da\u30a2\u3067\u73fe\u308c\u307e\u3059\u3002\u3053\u306e\u5834\u5408\uff1a 1. \u8907\u7d20\u6570\u4f53\u4e0a\u3067\u306f\u901a\u5e38\u901a\u308a\u5bfe\u89d2\u5316\u3067\u304d\u307e\u3059 2. \u5b9f\u6570\u4f53\u4e0a\u3067\u306f\u300c\u5b9f\u30e8\u30eb\u30c0\u30f3\u6a19\u6e96\u5f62\u300d\u3068\u3044\u3046\u5225\u306e\u5f62\u306b\u5909\u63db\u3067\u304d\u307e\u3059\uff08\u30d6\u30ed\u30c3\u30af\u5bfe\u89d2\u5f62\u5f0f\uff09 3. \u5b9f\u5bfe\u79f0\u884c\u5217\u306e\u5834\u5408\u306f\u5e38\u306b\u5b9f\u56fa\u6709\u5024\u306e\u307f\u3092\u6301\u3064\u306e\u3067\u3001\u3053\u306e\u554f\u984c\u306f\u767a\u751f\u3057\u307e\u305b\u3093</p>"},{"location":"lectures/LA/34-eigen-value-vector/#q4-pca","title":"Q4: \u5bfe\u89d2\u5316\u3068\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u95a2\u4fc2\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A4: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u3092\u5bfe\u89d2\u5316\u3057\u307e\u3059\u3002\u5171\u5206\u6563\u884c\u5217\u306f\u5bfe\u79f0\u884c\u5217\u306a\u306e\u3067\u5e38\u306b\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3059\u3002\u5bfe\u89d2\u5316\u3057\u305f\u969b\u306e\uff1a - \u56fa\u6709\u5024\uff1a\u4e3b\u6210\u5206\u306e\u5206\u6563\uff08\u91cd\u8981\u5ea6\uff09\u3092\u8868\u3059 - \u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff1a\u4e3b\u6210\u5206\u306e\u65b9\u5411\uff08\u65b0\u3057\u3044\u5ea7\u6a19\u8ef8\uff09\u3092\u8868\u3059</p> <p>PCA\u306f\u672c\u8cea\u7684\u306b\u3001\u30c7\u30fc\u30bf\u306e\u5909\u52d5\u3092\u6700\u3082\u3088\u304f\u8868\u73fe\u3059\u308b\u65b0\u3057\u3044\u5ea7\u6a19\u7cfb\u3092\u898b\u3064\u3051\u308b\u624b\u6cd5\u3067\u3059\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#q5","title":"Q5: \u5bfe\u89d2\u5316\u3067\u304d\u306a\u3044\u884c\u5217\u306f\u3069\u306e\u3088\u3046\u306b\u6271\u3044\u307e\u3059\u304b\uff1f","text":"<p>A5: \u5bfe\u89d2\u5316\u3067\u304d\u306a\u3044\u884c\u5217\u306b\u5bfe\u3057\u3066\u306f\u3001\u3088\u308a\u4e00\u822c\u7684\u306a\u30e8\u30eb\u30c0\u30f3\u6a19\u6e96\u5f62\u3092\u7528\u3044\u307e\u3059\u3002\u307e\u305f\u306f\u3001\u7279\u5b9a\u306e\u7528\u9014\u306b\u5fdc\u3058\u3066\uff1a 1. \u4e09\u89d2\u884c\u5217\u5316\uff08\u30b7\u30e5\u30fc\u30eb\u5206\u89e3\uff09 2. \u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09 3. \u5b9f\u30e8\u30eb\u30c0\u30f3\u6a19\u6e96\u5f62\uff08\u5b9f\u884c\u5217\u306e\u5834\u5408\uff09 \u306a\u3069\u306e\u65b9\u6cd5\u304c\u4f7f\u3048\u307e\u3059\u3002</p>"},{"location":"lectures/LA/34-eigen-value-vector/#9","title":"9. \u53c2\u8003\u6587\u732e\u3068\u8ffd\u52a0\u30ea\u30bd\u30fc\u30b9","text":"<ol> <li>Gilbert Strang. \"Linear Algebra and Its Applications\"</li> <li>David C. Lay. \"Linear Algebra and Its Applications\"</li> <li>Python NumPy\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8: https://numpy.org/doc/stable/reference/routines.linalg.html</li> <li>Khan Academy\u7dda\u5f62\u4ee3\u6570\u8b1b\u5ea7: https://www.khanacademy.org/math/linear-algebra</li> </ol> <p>\u6b21\u56de\u306e\u6388\u696d\u3067\u306f\u3001\u5bfe\u79f0\u884c\u5217\u306e\u76f4\u4ea4\u5bfe\u89d2\u5316\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002\u5bfe\u79f0\u884c\u5217\u306f\u5e38\u306b\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308a\u3001\u3055\u3089\u306b\u76f4\u4ea4\u884c\u5217\u306b\u3088\u3063\u3066\u5bfe\u89d2\u5316\u3067\u304d\u308b\u7279\u5225\u306a\u6027\u8cea\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u975e\u5e38\u306b\u91cd\u8981\u306a\u6027\u8cea\u3067\u3001\u7279\u306b\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II","text":""},{"location":"lectures/LA/35-eigen-value-vector/#35","title":"\u7b2c35\u56de: \u5bfe\u79f0\u884c\u5217\u3068\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316","text":""},{"location":"lectures/LA/35-eigen-value-vector/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c35\u56de \u95a2\u9023\u9805\u76ee: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3001\u5bfe\u79f0\u884c\u5217\u3001\u76f4\u4ea4\u884c\u5217\u3001\u5bfe\u89d2\u5316 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: - \u884c\u5217\u306e\u5bfe\u89d2\u5316\u306e\u6982\u5ff5\uff08\u7b2c34\u56de\uff09 - \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u65b9\u6cd5\uff08\u7b2c33\u56de\uff09 - \u5185\u7a4d\u3068\u76f4\u4ea4\u6027\u306e\u6982\u5ff5\uff08\u7b2c27\u56de\uff09</p>"},{"location":"lectures/LA/35-eigen-value-vector/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u5bfe\u79f0\u884c\u5217\u306e\u5b9a\u7fa9\u3068\u7279\u6027\u3092\u7406\u89e3\u3059\u308b</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u304c\u5b9f\u6570\u3067\u3042\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u7406\u89e3\u3059\u308b</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u76f4\u4ea4\u6027\u3092\u7406\u89e3\u3059\u308b</li> <li>\u76f4\u4ea4\u884c\u5217\u306e\u5b9a\u7fa9\u3068\u6027\u8cea\u3092\u7406\u89e3\u3059\u308b</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\uff08\u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\uff09\u306e\u624b\u9806\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u91cd\u8981\u6027\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/35-eigen-value-vector/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/35-eigen-value-vector/#31","title":"3.1 \u5bfe\u79f0\u884c\u5217\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \u6b63\u65b9\u884c\u5217 \\(A\\) \u304c\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u3068\u306f\u3001\u305d\u306e\u8ee2\u7f6e\u884c\u5217\u304c\u5143\u306e\u884c\u5217\u306b\u7b49\u3057\u3044\u5834\u5408\u3001\u3059\u306a\u308f\u3061 \\(A^T = A\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3092\u3044\u3046\u3002</p> <p>\u5bfe\u79f0\u884c\u5217\u306e\u5177\u4f53\u4f8b:</p> \\[A = \\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 6 \\end{pmatrix}\\] <p>\u3053\u306e\u884c\u5217\u306f\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u3002\u306a\u305c\u306a\u3089\u3001\\(a_{ij} = a_{ji}\\) \u304c\u5168\u3066\u306e \\(i, j\\) \u306b\u3064\u3044\u3066\u6210\u308a\u7acb\u3064\u304b\u3089\u3067\u3042\u308b\u3002</p> <p>\u5bfe\u79f0\u884c\u5217\u3067\u306a\u3044\u884c\u5217\u306e\u4f8b:</p> \\[B = \\begin{pmatrix}  1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{pmatrix}\\] <p>\u3053\u306e\u884c\u5217\u306f\u5bfe\u79f0\u884c\u5217\u3067\u306f\u306a\u3044\u3002\u4f8b\u3048\u3070 \\(b_{12} = 2\\) \u3060\u304c \\(b_{21} = 4\\) \u3067\u3042\u308a\u3001\\(b_{12} \\neq b_{21}\\) \u3068\u306a\u308b\u304b\u3089\u3067\u3042\u308b\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#32","title":"3.2 \u5bfe\u79f0\u884c\u5217\u306e\u6027\u8cea","text":"<p>\u5bfe\u79f0\u884c\u5217\u306f\u4ee5\u4e0b\u306e\u91cd\u8981\u306a\u6027\u8cea\u3092\u6301\u3064:</p> <ol> <li>\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u306f\u5b9f\u6570\u3067\u3042\u308b</li> <li>\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b</li> <li>\\(n\\) \u6b21\u5bfe\u79f0\u884c\u5217\u306f \\(n\\) \u500b\u306e\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6301\u3064</li> </ol> <p>\u3053\u308c\u3089\u306e\u6027\u8cea\u306b\u3064\u3044\u3066\u3001\u6b21\u7bc0\u3067\u8a73\u3057\u304f\u8aac\u660e\u3059\u308b\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#33","title":"3.3 \u76f4\u4ea4\u884c\u5217\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \u6b63\u65b9\u884c\u5217 \\(Q\\) \u304c\u76f4\u4ea4\u884c\u5217\u3067\u3042\u308b\u3068\u306f\u3001\\(Q^T Q = Q Q^T = I\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3092\u3044\u3046\u3002\u3053\u3053\u3067 \\(I\\) \u306f\u5358\u4f4d\u884c\u5217\u3067\u3042\u308b\u3002</p> <p>\u76f4\u4ea4\u884c\u5217\u306e\u6027\u8cea:</p> <ol> <li>\u76f4\u4ea4\u884c\u5217\u306e\u5217\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b\u5358\u4f4d\u30d9\u30af\u30c8\u30eb\uff08\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\uff09\u3067\u3042\u308b</li> <li>\u76f4\u4ea4\u884c\u5217\u306e\u884c\u30d9\u30af\u30c8\u30eb\u3082\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b\u5358\u4f4d\u30d9\u30af\u30c8\u30eb\uff08\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\uff09\u3067\u3042\u308b</li> <li>\u76f4\u4ea4\u884c\u5217\u306e\u9006\u884c\u5217\u306f\u3001\u305d\u306e\u8ee2\u7f6e\u884c\u5217\u306b\u7b49\u3057\u3044: \\(Q^{-1} = Q^T\\)</li> <li>\u76f4\u4ea4\u884c\u5217\u306e\u884c\u5217\u5f0f\u306e\u7d76\u5bfe\u5024\u306f 1 \u3067\u3042\u308b: \\(|\\det(Q)| = 1\\)</li> </ol> <p>\u76f4\u4ea4\u884c\u5217\u306e\u4f8b:</p> \\[Q = \\begin{pmatrix}  \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} \\end{pmatrix}\\] <p>\u5b9f\u969b\u306b\u78ba\u8a8d\u3057\u3066\u307f\u308b\u3068:</p> \\[Q^T Q = \\begin{pmatrix}  \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} \\end{pmatrix}^T  \\begin{pmatrix}  \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} \\end{pmatrix} =  \\begin{pmatrix}  1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} = I\\]"},{"location":"lectures/LA/35-eigen-value-vector/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/35-eigen-value-vector/#41","title":"4.1 \u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u304c\u5b9f\u6570\u3067\u3042\u308b\u3053\u3068\u306e\u8a3c\u660e","text":"<p>\u5bfe\u79f0\u884c\u5217 \\(A\\) \u306e\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u306f\u5b9f\u6570\u3067\u3042\u308b\u3002\u3053\u308c\u3092\u8a3c\u660e\u3059\u308b:</p> <p>\u8a3c\u660e: \\(A\\) \u3092\u5bfe\u79f0\u884c\u5217\u3068\u3057\u3001\\(\\lambda\\) \u3092\u305d\u306e\u56fa\u6709\u5024\u3001\\(\\mathbf{v}\\) \u3092\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u3059\u308b\u3002 \u3053\u3053\u3067 \\(\\mathbf{v}\\) \u306f\u4e00\u822c\u306b\u8907\u7d20\u6570\u6210\u5206\u3092\u6301\u3064\u53ef\u80fd\u6027\u304c\u3042\u308b\u3068\u4eee\u5b9a\u3059\u308b\u3002 \u307e\u305f\u3001\\(\\mathbf{v}^*\\) \u3092 \\(\\mathbf{v}\\) \u306e\u8907\u7d20\u5171\u5f79\u8ee2\u7f6e\u3068\u3059\u308b\u3002</p> <p>\u56fa\u6709\u5024\u306e\u5b9a\u7fa9\u3088\u308a: \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\)</p> <p>\u4e21\u8fba\u306b\u5de6\u304b\u3089 \\(\\mathbf{v}^*\\) \u3092\u304b\u3051\u308b: \\(\\mathbf{v}^* A \\mathbf{v} = \\lambda \\mathbf{v}^* \\mathbf{v}\\)</p> <p>\\(A\\) \u304c\u5bfe\u79f0\u884c\u5217\u306a\u306e\u3067 \\(A^T = A\\) \u3067\u3042\u308a\u3001\u3057\u305f\u304c\u3063\u3066 \\(A = A^*\\) (\u8907\u7d20\u5171\u5f79\u8ee2\u7f6e\u306b\u7b49\u3057\u3044)</p> <p>\u4e00\u65b9\u3067\u3001\u6b21\u306e\u5f0f\u3082\u6210\u308a\u7acb\u3064: \\(\\mathbf{v}^* A \\mathbf{v} = \\mathbf{v}^* A^* \\mathbf{v} = (A\\mathbf{v})^* \\mathbf{v} = (\\lambda\\mathbf{v})^* \\mathbf{v} = \\lambda^* \\mathbf{v}^* \\mathbf{v}\\)</p> <p>\u3088\u3063\u3066: \\(\\lambda \\mathbf{v}^* \\mathbf{v} = \\lambda^* \\mathbf{v}^* \\mathbf{v}\\)</p> <p>\\(\\mathbf{v} \\neq \\mathbf{0}\\) \u306a\u306e\u3067 \\(\\mathbf{v}^* \\mathbf{v} &gt; 0\\) \u3067\u3042\u308a\u3001\u4e21\u8fba\u3092 \\(\\mathbf{v}^* \\mathbf{v}\\) \u3067\u5272\u308b\u3068: \\(\\lambda = \\lambda^*\\)</p> <p>\u3053\u308c\u306f \\(\\lambda\\) \u304c\u5b9f\u6570\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#42","title":"4.2 \u5bfe\u79f0\u884c\u5217\u306e\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u76f4\u4ea4\u6027","text":"<p>\u5bfe\u79f0\u884c\u5217 \\(A\\) \u306b\u3064\u3044\u3066\u3001\u7570\u306a\u308b\u56fa\u6709\u5024 \\(\\lambda_1 \\neq \\lambda_2\\) \u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_1\\) \u3068 \\(\\mathbf{v}_2\\) \u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b\u3002</p> <p>\u8a3c\u660e: \\(A\\mathbf{v}_1 = \\lambda_1\\mathbf{v}_1\\) \u304b\u3064 \\(A\\mathbf{v}_2 = \\lambda_2\\mathbf{v}_2\\) \u3068\u3059\u308b\u3002</p> <p>\\(\\mathbf{v}_1\\) \u3068 \\(A\\mathbf{v}_2\\) \u306e\u5185\u7a4d\u3092\u8003\u3048\u308b: \\(\\mathbf{v}_1^T (A\\mathbf{v}_2) = \\mathbf{v}_1^T (\\lambda_2\\mathbf{v}_2) = \\lambda_2 \\mathbf{v}_1^T \\mathbf{v}_2\\)</p> <p>\u4e00\u65b9\u3001\\(A\\) \u304c\u5bfe\u79f0\u884c\u5217\u306a\u306e\u3067: \\(\\mathbf{v}_1^T (A\\mathbf{v}_2) = (A^T\\mathbf{v}_1)^T \\mathbf{v}_2 = (A\\mathbf{v}_1)^T \\mathbf{v}_2 = (\\lambda_1\\mathbf{v}_1)^T \\mathbf{v}_2 = \\lambda_1 \\mathbf{v}_1^T \\mathbf{v}_2\\)</p> <p>\u3088\u3063\u3066: \\(\\lambda_2 \\mathbf{v}_1^T \\mathbf{v}_2 = \\lambda_1 \\mathbf{v}_1^T \\mathbf{v}_2\\) \\((\\lambda_2 - \\lambda_1) \\mathbf{v}_1^T \\mathbf{v}_2 = 0\\)</p> <p>\\(\\lambda_1 \\neq \\lambda_2\\) \u3068\u3044\u3046\u4eee\u5b9a\u304b\u3089: \\(\\mathbf{v}_1^T \\mathbf{v}_2 = 0\\)</p> <p>\u3053\u308c\u306f\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_1\\) \u3068 \\(\\mathbf{v}_2\\) \u304c\u76f4\u4ea4\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#43","title":"4.3 \u5bfe\u79f0\u884c\u5217\u306e\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\uff08\u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\uff09","text":"<p>\u5bfe\u79f0\u884c\u5217 \\(A\\) \u306f\u76f4\u4ea4\u884c\u5217 \\(Q\\) \u3092\u7528\u3044\u3066\u5bfe\u89d2\u5316\u3067\u304d\u308b:</p> <p>\u5b9a\u7406\uff08\u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\uff09: \\(n\\) \u6b21\u5bfe\u79f0\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\u76f4\u4ea4\u884c\u5217 \\(Q\\) \u3068\u5bfe\u89d2\u884c\u5217 \\(D\\) \u304c\u5b58\u5728\u3057\u3001\\(A = QDQ^T\\) \u3068\u8868\u305b\u308b\u3002\u3053\u3053\u3067 \\(D\\) \u306e\u5bfe\u89d2\u6210\u5206\u306f \\(A\\) \u306e\u56fa\u6709\u5024\u3067\u3042\u308a\u3001\\(Q\\) \u306e\u5217\u306f\u5bfe\u5fdc\u3059\u308b\u6b63\u898f\u5316\u3055\u308c\u305f\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b\u3002</p> <p>\u5bfe\u89d2\u5316\u306e\u624b\u9806:</p> <ol> <li>\u5bfe\u79f0\u884c\u5217 \\(A\\) \u306e\u56fa\u6709\u5024 \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\) \u3068\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) \u3092\u6c42\u3081\u308b</li> <li>\u540c\u3058\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304b\u3089\u306a\u308b\u90e8\u5206\u7a7a\u9593\u3067\u3001\u30b0\u30e9\u30e0\u30fb\u30b7\u30e5\u30df\u30c3\u30c8\u306e\u76f4\u4ea4\u5316\u6cd5\u3092\u7528\u3044\u3066\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u69cb\u6210\u3059\u308b</li> <li>\u5168\u3066\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6b63\u898f\u5316\u3057\u3066\u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u306b\u3059\u308b</li> <li>\u6b63\u898f\u5316\u3055\u308c\u305f\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u5217\u3068\u3059\u308b\u884c\u5217 \\(Q\\) \u3092\u4f5c\u308b</li> <li>\u56fa\u6709\u5024\u3092\u5bfe\u89d2\u6210\u5206\u3068\u3059\u308b\u5bfe\u89d2\u884c\u5217 \\(D\\) \u3092\u4f5c\u308b</li> </ol> <p>\u3053\u306e\u6642\u3001\\(A = QDQ^T\\) \u304c\u6210\u308a\u7acb\u3064\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#44","title":"4.4 \u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\u306e\u610f\u5473","text":"<p>\\(A = QDQ^T\\) \u3068\u3044\u3046\u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\u306f\u3001\u884c\u5217 \\(A\\) \u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u73fe\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u308b:</p> \\[A = \\sum_{i=1}^{n} \\lambda_i \\mathbf{q}_i \\mathbf{q}_i^T\\] <p>\u3053\u3053\u3067 \\(\\lambda_i\\) \u306f\u56fa\u6709\u5024\u3001\\(\\mathbf{q}_i\\) \u306f\u5bfe\u5fdc\u3059\u308b\u6b63\u898f\u5316\u3055\u308c\u305f\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\\(Q\\) \u306e \\(i\\) \u5217\u76ee\uff09\u3067\u3042\u308b\u3002</p> <p>\u3053\u306e\u8868\u73fe\u306f\u3001\u5bfe\u79f0\u884c\u5217 \\(A\\) \u304c\u3001\u5404\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u65b9\u5411\u306e\u300crank-1\u300d\u5c04\u5f71\u884c\u5217 \\(\\mathbf{q}_i \\mathbf{q}_i^T\\) \u306e\u56fa\u6709\u5024\u306b\u3088\u308b\u91cd\u307f\u4ed8\u304d\u548c\u3068\u3057\u3066\u5206\u89e3\u3067\u304d\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#5-python","title":"5. \u5177\u4f53\u7684\u306a\u8a08\u7b97\u4f8b\u3068Python\u5b9f\u88c5","text":""},{"location":"lectures/LA/35-eigen-value-vector/#51","title":"5.1 \u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306e\u4f8b","text":"<p>\u6b21\u306e\u5bfe\u79f0\u884c\u5217\u306b\u3064\u3044\u3066\u8003\u3048\u308b:</p> \\[A = \\begin{pmatrix}  2 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 2 \\end{pmatrix}\\] <p>\u30b9\u30c6\u30c3\u30d71: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b</p> <p>\u7279\u6027\u65b9\u7a0b\u5f0f: \\(\\det(A - \\lambda I) = 0\\)</p> \\[\\det\\begin{pmatrix}  2-\\lambda &amp; 1 &amp; 1 \\\\ 1 &amp; 2-\\lambda &amp; 1 \\\\ 1 &amp; 1 &amp; 2-\\lambda \\end{pmatrix} = 0\\] <p>\u8a08\u7b97\u3059\u308b\u3068: \\((2-\\lambda)^3 - 3(2-\\lambda) + 2 = 0\\) \\(-(2-\\lambda)^3 + 3(2-\\lambda) - 2 = 0\\) \\(-(\\lambda-2)^3 + 3(\\lambda-2) - 2 = 0\\) \\(-(\\lambda-2)^3 + 3(\\lambda-2) - 2 = 0\\)</p> <p>\u56e0\u6570\u5206\u89e3\u3059\u308b\u3068: \\(-(\\lambda-2)(\\lambda-2)^2 + 3(\\lambda-2) - 2 = 0\\) \\(-(\\lambda-2)((\\lambda-2)^2 - 3) - 2 = 0\\) \\(-(\\lambda-2)((\\lambda-2)^2 - 3) - 2 = 0\\) \\(-(\\lambda-2)((\\lambda-2)^2 - 3) - 2 = 0\\) \\(-(\\lambda-2)((\\lambda-2)^2 - 3) - 2 = 0\\) \\(-(\\lambda-2)((\\lambda-2)^2 - 3) - 2 = 0\\)</p> <p>\u7c21\u5358\u306a\u8a08\u7b97\u65b9\u6cd5\u3068\u3057\u3066\u3001\u3053\u306e\u884c\u5217\u306f\u7279\u6b8a\u306a\u5f62\u3092\u3057\u3066\u304a\u308a\u3001\u56fa\u6709\u591a\u9805\u5f0f\u306f: \\((\\lambda - 4)(\\lambda - 1)^2 = 0\\)</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u56fa\u6709\u5024\u306f: \\(\\lambda_1 = 4\\) (\u591a\u91cd\u5ea61) \\(\\lambda_2 = 1\\) (\u591a\u91cd\u5ea62)</p> <p>\u56fa\u6709\u5024 \\(\\lambda_1 = 4\\) \u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b: \\((A - 4I)\\mathbf{v}_1 = \\mathbf{0}\\)</p> \\[\\begin{pmatrix}  -2 &amp; 1 &amp; 1 \\\\ 1 &amp; -2 &amp; 1 \\\\ 1 &amp; 1 &amp; -2 \\end{pmatrix}\\mathbf{v}_1 = \\mathbf{0}\\] <p>\u3053\u308c\u3092\u89e3\u304f\u3068\u3001\\(\\mathbf{v}_1 = (1, 1, 1)^T\\) \u304c\u5f97\u3089\u308c\u308b\uff08\u6b63\u898f\u5316\u524d\uff09\u3002</p> <p>\u6b63\u898f\u5316\u3059\u308b\u3068: \\(\\mathbf{q}_1 = \\frac{\\mathbf{v}_1}{||\\mathbf{v}_1||} = \\frac{1}{\\sqrt{3}}(1, 1, 1)^T\\)</p> <p>\u56fa\u6709\u5024 \\(\\lambda_2 = 1\\) \u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b: \\((A - I)\\mathbf{v}_2 = \\mathbf{0}\\)</p> \\[\\begin{pmatrix}  1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix}\\mathbf{v} = \\mathbf{0}\\] <p>\u3053\u306e\u884c\u5217\u306f\u30e9\u30f3\u30af1\u306a\u306e\u3067\u3001\u56fa\u6709\u7a7a\u9593\u306e\u6b21\u5143\u306f2\u3067\u3059\u30022\u3064\u306e\u7dda\u5f62\u72ec\u7acb\u306a\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u3001\u4f8b\u3048\u3070: \\(\\mathbf{v}_2 = (1, -1, 0)^T\\) \u3068 \\(\\mathbf{v}_3 = (1, 0, -1)^T\\) \u3092\u9078\u3079\u307e\u3059\u3002</p> <p>\u3053\u308c\u3089\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3001\\(\\mathbf{q}_1\\) \u306b\u76f4\u4ea4\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059: \\(\\mathbf{q}_1^T \\mathbf{v}_2 = \\frac{1}{\\sqrt{3}}(1 + (-1) + 0) = 0\\) \\(\\mathbf{q}_1^T \\mathbf{v}_3 = \\frac{1}{\\sqrt{3}}(1 + 0 + (-1)) = 0\\)</p> <p>\u3057\u304b\u3057\u3001\\(\\mathbf{v}_2\\) \u3068 \\(\\mathbf{v}_3\\) \u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3057\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u30b0\u30e9\u30e0\u30fb\u30b7\u30e5\u30df\u30c3\u30c8\u306e\u76f4\u4ea4\u5316\u6cd5\u3092\u9069\u7528\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u305f\u3060\u3001\u7c21\u5358\u306e\u305f\u3081\u306b\u5225\u306e\u65b9\u6cd5\u30672\u3064\u306e\u76f4\u4ea4\u3059\u308b\u30d9\u30af\u30c8\u30eb\u3092\u9078\u3073\u307e\u3057\u3087\u3046: \\(\\mathbf{v}_2 = (1, -1, 0)^T\\) \u3068 \\(\\mathbf{v}_3 = (1, 1, -2)^T\\)</p> <p>\u3053\u306e\u4e8c\u3064\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059: \\(\\mathbf{v}_2^T \\mathbf{v}_3 = 1 \\cdot 1 + (-1) \\cdot 1 + 0 \\cdot (-2) = 0\\)</p> <p>\u3053\u308c\u3089\u3092\u6b63\u898f\u5316\u3059\u308b\u3068: \\(\\mathbf{q}_2 = \\frac{\\mathbf{v}_2}{||\\mathbf{v}_2||} = \\frac{1}{\\sqrt{2}}(1, -1, 0)^T\\) \\(\\mathbf{q}_3 = \\frac{\\mathbf{v}_3}{||\\mathbf{v}_3||} = \\frac{1}{\\sqrt{6}}(1, 1, -2)^T\\)</p> <p>\u30b9\u30c6\u30c3\u30d73: \u76f4\u4ea4\u884c\u5217 \\(Q\\) \u3068\u5bfe\u89d2\u884c\u5217 \\(D\\) \u3092\u69cb\u6210\u3059\u308b</p> \\[Q = \\begin{pmatrix}  \\frac{1}{\\sqrt{3}} &amp; \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} &amp; -\\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} &amp; 0 &amp; -\\frac{2}{\\sqrt{6}} \\end{pmatrix}\\] \\[D = \\begin{pmatrix}  4 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] <p>\u30b9\u30c6\u30c3\u30d74: \\(A = QDQ^T\\) \u3092\u78ba\u8a8d\u3059\u308b</p> <p>\u8a08\u7b97\u306b\u3088\u308a\u3001\\(QDQ^T = A\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#52-python","title":"5.2 Python\u306b\u3088\u308b\u5b9f\u88c5","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u5bfe\u79f0\u884c\u5217\u306e\u4f8b\nA = np.array([\n    [2, 1, 1],\n    [1, 2, 1],\n    [1, 1, 2]\n])\n\nprint(\"\u5bfe\u79f0\u884c\u5217 A:\")\nprint(A)\n\n# \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\neigvals, eigvecs = np.linalg.eigh(A)  # eigh \u306f\u5bfe\u79f0\u884c\u5217\u7528\u306e\u95a2\u6570\n\nprint(\"\\n\u56fa\u6709\u5024:\")\nprint(eigvals)\n\nprint(\"\\n\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u5217\u30d9\u30af\u30c8\u30eb\uff09:\")\nprint(eigvecs)\n\n# \u5bfe\u89d2\u884c\u5217\u306e\u69cb\u6210\nD = np.diag(eigvals)\nprint(\"\\n\u5bfe\u89d2\u884c\u5217 D:\")\nprint(D)\n\n# \u76f4\u4ea4\u884c\u5217 Q\nQ = eigvecs\nprint(\"\\n\u76f4\u4ea4\u884c\u5217 Q:\")\nprint(Q)\n\n# \u78ba\u8a8d: Q^T Q = I\nprint(\"\\nQ^T Q:\")\nprint(np.round(Q.T @ Q, 10))  # \u4e38\u3081\u8aa4\u5dee\u3092\u8003\u616e\n\n# \u78ba\u8a8d: A = Q D Q^T\nA_reconstructed = Q @ D @ Q.T\nprint(\"\\nQ D Q^T:\")\nprint(np.round(A_reconstructed, 10))  # \u4e38\u3081\u8aa4\u5dee\u3092\u8003\u616e\n\n# \u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\u306e\u8996\u899a\u5316\ndef plot_spectral_decomposition():\n    # \u5358\u4f4d\u7403\u306e\u70b9\u3092\u751f\u6210\n    phi = np.linspace(0, np.pi, 20)\n    theta = np.linspace(0, 2*np.pi, 20)\n    phi, theta = np.meshgrid(phi, theta)\n\n    x = np.sin(phi) * np.cos(theta)\n    y = np.sin(phi) * np.sin(theta)\n    z = np.cos(phi)\n\n    # \u7403\u9762\u4e0a\u306e\u70b9\u3092\u884c\u5217\u5f62\u5f0f\u306b\u5909\u63db\n    points = np.vstack([x.flatten(), y.flatten(), z.flatten()]).T\n\n    # \u5404\u70b9\u3092\u5909\u63db\n    transformed_points = np.array([A @ p for p in points])\n\n    # \u53ef\u8996\u5316\n    fig = plt.figure(figsize=(12, 6))\n\n    # \u5143\u306e\u5358\u4f4d\u7403\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax1.scatter(points[:,0], points[:,1], points[:,2], c='b', alpha=0.2)\n    ax1.set_title(\"\u5143\u306e\u5358\u4f4d\u7403\")\n    ax1.set_xlim([-4, 4])\n    ax1.set_ylim([-4, 4])\n    ax1.set_zlim([-4, 4])\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_zlabel('Z')\n\n    # A \u306b\u3088\u308b\u5909\u63db\u5f8c\n    ax2 = fig.add_subplot(122, projection='3d')\n    ax2.scatter(transformed_points[:,0], transformed_points[:,1], transformed_points[:,2], c='r', alpha=0.2)\n\n    # \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\n    for i in range(3):\n        eigen_vec = eigvecs[:, i] * eigvals[i] * 3  # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u3066\u898b\u3084\u3059\u304f\n        ax2.quiver(0, 0, 0, eigen_vec[0], eigen_vec[1], eigen_vec[2], color='g', arrow_length_ratio=0.1)\n\n    ax2.set_title(\"A \u306b\u3088\u308b\u5909\u63db\u5f8c\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\")\n    ax2.set_xlim([-4, 4])\n    ax2.set_ylim([-4, 4])\n    ax2.set_zlim([-4, 4])\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_zlabel('Z')\n\n    plt.tight_layout()\n    plt.show()\n\n# \u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\u306e\u8996\u899a\u5316\u5b9f\u884c\nplot_spectral_decomposition()\n</code></pre>"},{"location":"lectures/LA/35-eigen-value-vector/#6","title":"6. \u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u5fdc\u7528","text":""},{"location":"lectures/LA/35-eigen-value-vector/#61-pca","title":"6.1 \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09","text":"<p>\u5bfe\u79f0\u884c\u5217\u306e\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\u306f\u3001\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u7406\u8ad6\u7684\u57fa\u790e\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002PCA\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\uff08\u3053\u308c\u306f\u5bfe\u79f0\u884c\u5217\u3067\u3059\uff09\u3092\u5bfe\u89d2\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u3092\u898b\u3064\u3051\u307e\u3059\u3002</p> <p>\u4e3b\u6210\u5206\u5206\u6790\u306e\u30b9\u30c6\u30c3\u30d7:</p> <ol> <li>\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u306e\u5404\u5217\uff08\u5909\u6570\uff09\u306b\u3064\u3044\u3066\u4e2d\u5fc3\u5316\u3092\u884c\u3046</li> <li>\u5171\u5206\u6563\u884c\u5217 \\(C = \\frac{1}{n-1}X^T X\\) \u3092\u8a08\u7b97\u3059\u308b\uff08\u3053\u308c\u306f\u5bfe\u79f0\u884c\u5217\uff09</li> <li>\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u3059\u308b</li> <li>\u56fa\u6709\u5024\u306e\u5927\u304d\u3044\u9806\u306b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u4e26\u3079\u308b\uff08\u3053\u308c\u3089\u304c\u4e3b\u6210\u5206\uff09</li> <li>\u5143\u306e\u30c7\u30fc\u30bf\u3092\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u3059\u308b: \\(Z = X W\\)\uff08\u3053\u3053\u3067 \\(W\\) \u306f\u9078\u629e\u3057\u305f\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u5217\u3068\u3059\u308b\u884c\u5217\uff09</li> </ol>"},{"location":"lectures/LA/35-eigen-value-vector/#62","title":"6.2 \u753b\u50cf\u51e6\u7406\u306b\u304a\u3051\u308b\u5fdc\u7528","text":"<p>\u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306f\u3001\u753b\u50cf\u51e6\u7406\u3067\u3082\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u753b\u50cf\u306e\u5727\u7e2e\u3084\u7279\u5fb4\u62bd\u51fa\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5fdc\u7528\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ul> <li>\u753b\u50cf\u5727\u7e2e: \u56fa\u6709\u5024\u306e\u5c0f\u3055\u306a\u6210\u5206\u3092\u5207\u308a\u6368\u3066\u308b\u3053\u3068\u3067\u3001\u60c5\u5831\u3092\u307b\u3068\u3093\u3069\u5931\u308f\u305a\u306b\u753b\u50cf\u306e\u30b5\u30a4\u30ba\u3092\u524a\u6e1b</li> <li>\u9854\u8a8d\u8b58: \u9854\u753b\u50cf\u306e\u4e3b\u6210\u5206\uff08\u56fa\u6709\u9854\uff09\u3092\u62bd\u51fa\u3057\u3066\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u5229\u7528</li> <li>\u30ce\u30a4\u30ba\u9664\u53bb: \u56fa\u6709\u5024\u306e\u5c0f\u3055\u306a\u6210\u5206\u3092\u53d6\u308a\u9664\u304f\u3053\u3068\u3067\u3001\u753b\u50cf\u304b\u3089\u30ce\u30a4\u30ba\u3092\u524a\u6e1b</li> </ul>"},{"location":"lectures/LA/35-eigen-value-vector/#63","title":"6.3 \u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u5fdc\u7528\u4f8b","text":"<p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u3067\u306f\u3001\u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30b7\u30ca\u30ea\u30aa\u3067\u5229\u7528\u3055\u308c\u307e\u3059\uff1a</p> <ol> <li>\u591a\u6b21\u5143\u5065\u5eb7\u6307\u6a19\u306e\u6b21\u5143\u524a\u6e1b:</li> <li>\u8840\u5727\u3001\u5fc3\u62cd\u6570\u3001\u8840\u7cd6\u5024\u306a\u3069\u306e\u8907\u6570\u306e\u5065\u5eb7\u6307\u6a19\u306e\u76f8\u95a2\u69cb\u9020\u3092\u5206\u6790</li> <li>\u4e3b\u6210\u5206\u5206\u6790\u3092\u7528\u3044\u3066\u91cd\u8981\u306a\u5909\u52d5\u306e\u65b9\u5411\u3092\u7279\u5b9a</li> <li> <p>\u5197\u9577\u6027\u3092\u6e1b\u3089\u3057\u305f\u65b0\u3057\u3044\u5065\u5eb7\u30b9\u30b3\u30a2\u306e\u958b\u767a</p> </li> <li> <p>\u533b\u7642\u753b\u50cf\u51e6\u7406:</p> </li> <li>MRI\u3084CT\u30b9\u30ad\u30e3\u30f3\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u62bd\u51fa</li> <li>\u7570\u5e38\u691c\u51fa\u306e\u305f\u3081\u306e\u6b63\u5e38\u30d1\u30bf\u30fc\u30f3\u306e\u5b66\u7fd2</li> <li>\u753b\u50cf\u306e\u5727\u7e2e\u3068\u518d\u69cb\u7bc9</li> </ol> <pre><code># \u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790\u306e\u4f8b\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# \u30b5\u30f3\u30d7\u30eb\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u751f\u6210\uff08\u6a21\u64ec\u30c7\u30fc\u30bf\uff09\nnp.random.seed(42)\nn_samples = 100\n\n# \u53ce\u7e2e\u671f\u8840\u5727(SBP)\u3001\u62e1\u5f35\u671f\u8840\u5727(DBP)\u3001\u5fc3\u62cd\u6570(HR)\u3001\u8840\u7cd6\u5024(BS)\u3001\u4f53\u6e29(BT)\nhealth_data = pd.DataFrame({\n    'SBP': np.random.normal(120, 15, n_samples),\n    'DBP': np.random.normal(80, 10, n_samples),\n    'HR': np.random.normal(70, 10, n_samples),\n    'BS': np.random.normal(100, 20, n_samples),\n    'BT': np.random.normal(36.5, 0.5, n_samples)\n})\n\n# \u76f8\u95a2\u3092\u6301\u305f\u305b\u308b\nhealth_data['SBP'] = health_data['SBP'] + health_data['DBP'] * 0.5\nhealth_data['HR'] = health_data['HR'] + health_data['BS'] * 0.2\n\nprint(\"\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u30b5\u30f3\u30d7\u30eb:\")\nprint(health_data.head())\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\nscaler = StandardScaler()\nhealth_data_scaled = scaler.fit_transform(health_data)\n\n# \u5171\u5206\u6563\u884c\u5217\u306e\u8a08\u7b97\uff08\u3053\u308c\u306f\u5bfe\u79f0\u884c\u5217\uff09\ncov_matrix = np.cov(health_data_scaled.T)\nprint(\"\\n\u5171\u5206\u6563\u884c\u5217:\")\nprint(cov_matrix)\n\n# \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\neigvals, eigvecs = np.linalg.eigh(cov_matrix)\n\n# \u56fa\u6709\u5024\u3092\u964d\u9806\u306b\u4e26\u3079\u66ff\u3048\nidx = eigvals.argsort()[::-1]\neigvals = eigvals[idx]\neigvecs = eigvecs[:, idx]\n\nprint(\"\\n\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\uff08\u964d\u9806\uff09:\")\nprint(eigvals)\n\nprint(\"\\n\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u5217\u30d9\u30af\u30c8\u30eb\u3001\u964d\u9806\uff09:\")\nprint(eigvecs)\n\n# \u4e3b\u6210\u5206\u5206\u6790\u306e\u5b9f\u65bd\npca = PCA()\nhealth_data_pca = pca.fit_transform(health_data_scaled)\n\n# \u5bc4\u4e0e\u7387\uff08\u5404\u4e3b\u6210\u5206\u304c\u3069\u308c\u3060\u3051\u5143\u306e\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u8aac\u660e\u3059\u308b\u304b\uff09\nprint(\"\\n\u5404\u4e3b\u6210\u5206\u306e\u5bc4\u4e0e\u7387:\")\nprint(pca.explained_variance_ratio_)\n\nprint(\"\\n\u7d2f\u7a4d\u5bc4\u4e0e\u7387:\")\nprint(np.cumsum(pca.explained_variance_ratio_))\n\n# \u53ef\u8996\u5316: \u7b2c1\u4e3b\u6210\u5206\u3068\u7b2c2\u4e3b\u6210\u5206\u3078\u306e\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.scatter(health_data_pca[:, 0], health_data_pca[:, 1], alpha=0.7)\nplt.xlabel('\u7b2c1\u4e3b\u6210\u5206')\nplt.ylabel('\u7b2c2\u4e3b\u6210\u5206')\nplt.title('\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790')\nplt.grid(True)\n\n# \u5143\u306e\u7279\u5fb4\u91cf\u306e\u4e3b\u6210\u5206\u3078\u306e\u5bc4\u4e0e\u3092\u793a\u3059\u77e2\u5370\u3092\u8ffd\u52a0\nfor i, (x, y) in enumerate(zip(eigvecs[0, :2], eigvecs[1, :2])):\n    plt.arrow(0, 0, x*3, y*3, head_width=0.15, head_length=0.2, fc='red', ec='red')\n    plt.text(x*3.2, y*3.2, health_data.columns[i], fontsize=12)\n\nplt.tight_layout()\nplt.show()\n\n# \u30d0\u30a4\u30d7\u30ed\u30c3\u30c8: \u4e3b\u6210\u5206\u7a7a\u9593\u306b\u304a\u3051\u308b\u5909\u6570\u3068\u89b3\u6e2c\u5024\u306e\u540c\u6642\u8868\u793a\ndef create_biplot(score, coeff, labels):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n\n    plt.figure(figsize=(12, 8))\n    plt.scatter(xs, ys, alpha=0.7)\n\n    # \u4e3b\u6210\u5206\u7a7a\u9593\u306b\u304a\u3051\u308b\u5909\u6570\u306e\u65b9\u5411\u3092\u77e2\u5370\u3067\u8868\u793a\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0]*5, coeff[i,1]*5, color='r', alpha=0.5, head_width=0.1)\n        plt.text(coeff[i,0]*5.2, coeff[i,1]*5.2, labels[i], color='g', fontsize=12)\n\n    plt.xlabel('\u7b2c1\u4e3b\u6210\u5206')\n    plt.ylabel('\u7b2c2\u4e3b\u6210\u5206')\n    plt.title('\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n# \u4e3b\u6210\u5206\u4fc2\u6570\uff08\u8ca0\u8377\u91cf\uff09\u3092\u53d6\u5f97\nloadings = pca.components_.T\n\n# \u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210\ncreate_biplot(health_data_pca[:, :2], loadings[:, :2], health_data.columns)\n</code></pre>"},{"location":"lectures/LA/35-eigen-value-vector/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/35-eigen-value-vector/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u884c\u5217\u304c\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002    a) \\(\\begin{pmatrix} 3 &amp; 2 \\\\ 2 &amp; 5 \\end{pmatrix}\\)    b) \\(\\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 6 &amp; 9 \\end{pmatrix}\\)    c) \\(\\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix}\\)</p> </li> <li> <p>\u6b21\u306e\u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\u3092\u884c\u3048\u3002    \\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 1 &amp; 3 \\end{pmatrix}\\)</p> </li> <li> <p>\u6b21\u306e\u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u3092\u6c42\u3081\u3088\u3002    \\(B = \\begin{pmatrix} 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 4 \\\\ 0 &amp; 4 &amp; 3 \\end{pmatrix}\\)</p> </li> <li> <p>\u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u304c\u5168\u3066\u6b63\u3067\u3042\u308b\u3068\u304d\u3001\u305d\u306e\u884c\u5217\u306f\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3068\u3044\u3046\u3002\u6b21\u306e\u884c\u5217\u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002    \\(C = \\begin{pmatrix} 2 &amp; -1 &amp; 0 \\\\ -1 &amp; 2 &amp; -1 \\\\ 0 &amp; -1 &amp; 2 \\end{pmatrix}\\)</p> </li> <li> <p>\u76f4\u4ea4\u884c\u5217 \\(Q = \\begin{pmatrix} \\frac{3}{5} &amp; \\frac{4}{5} \\\\ \\frac{4}{5} &amp; -\\frac{3}{5} \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\\(Q^TQ = I\\) \u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u305b\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/35-eigen-value-vector/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li>3\u00d73\u306e\u5bfe\u79f0\u884c\u5217 \\(A\\) \u306b\u3064\u3044\u3066\u3001\u305d\u306e\u7279\u6027\u591a\u9805\u5f0f\u304c \\(\\lambda^3 - 6\\lambda^2 + 11\\lambda - 6 = 0\\) \u3067\u3042\u308b\u3068\u304d\u3001\\(A\\) \u306e\u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\u3092\u6c42\u3081\u3088\u3002\u305f\u3060\u3057\u3001\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u6b21\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u3066\u3044\u308b\u3068\u3059\u308b\u3002</li> <li>\\(\\lambda_1 = 1\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{v}_1 = (1, 1, 1)^T\\)</li> <li>\\(\\lambda_2 = 2\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{v}_2 = (1, 0, -1)^T\\)</li> <li> <p>\\(\\lambda_3 = 3\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{v}_3 = (1, -2, 1)^T\\)</p> </li> <li> <p>\u5bfe\u79f0\u884c\u5217 \\(A = \\begin{pmatrix} 4 &amp; 0 &amp; 0 \\\\ 0 &amp; 4 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\u3053\u306e\u884c\u5217\u306e\u4efb\u610f\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304b\u3089\u4f5c\u3089\u308c\u308b\u76f4\u4ea4\u884c\u5217 \\(Q\\) \u3092\u69cb\u6210\u3057\u3001\\(A = QDQ^T\\) \u306e\u5f62\u3067\u8868\u305b\u3002</p> </li> <li> <p>2\u6b21\u5143\u30c7\u30fc\u30bf\u70b9 \\((1,2)\\), \\((2,3)\\), \\((3,5)\\), \\((4,4)\\), \\((5,6)\\) \u306b\u3064\u3044\u3066\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u4e3b\u6210\u5206\u5206\u6790\u3092\u884c\u3048\u3002    a) \u30c7\u30fc\u30bf\u306e\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3088\u3002    b) \u5e73\u5747\u3092\u5f15\u3044\u305f\u4e2d\u5fc3\u5316\u30c7\u30fc\u30bf\u3092\u6c42\u3081\u3088\u3002    c) \u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\u305b\u3088\u3002    d) \u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3088\u3002    e) \u4e3b\u6210\u5206\u65b9\u5411\u3092\u7279\u5b9a\u3057\u3001\u30c7\u30fc\u30bf\u3092\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u305b\u3088\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306e\u6587\u8108\u306b\u304a\u3044\u3066\u3001\u7570\u306a\u308b\u6e2c\u5b9a\u5024\uff08\u8840\u5727\u3001\u5fc3\u62cd\u6570\u3001\u4f53\u6e29\u3001\u8840\u7cd6\u5024\u306a\u3069\uff09\u306e\u5171\u5206\u6563\u884c\u5217\u304c\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u7406\u7531\u3092\u8aac\u660e\u3057\u3001\u3053\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3067\u304d\u308b\u304b\u3092\u8003\u5bdf\u305b\u3088\u3002\u307e\u305f\u3001\u3053\u308c\u3089\u306e\u6e2c\u5b9a\u5024\u9593\u306b\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3001\u4e3b\u6210\u5206\u5206\u6790\u3092\u7528\u3044\u305f\u6b21\u5143\u524a\u6e1b\u304c\u3069\u306e\u3088\u3046\u306b\u6709\u52b9\u304b\u3092\u8aac\u660e\u305b\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/35-eigen-value-vector/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/35-eigen-value-vector/#q1","title":"Q1: \u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u304c\u5e38\u306b\u5b9f\u6570\u3067\u3042\u308b\u7406\u7531\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A1: \u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u304c\u5e38\u306b\u5b9f\u6570\u3067\u3042\u308b\u7406\u7531\u306f\u3001\u5bfe\u79f0\u884c\u5217 \\(A\\) \u306e\u4efb\u610f\u306e\u56fa\u6709\u5024 \\(\\lambda\\) \u3068\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}\\) \u306b\u3064\u3044\u3066\u3001\\(\\mathbf{v}^* A \\mathbf{v} = \\lambda \\mathbf{v}^* \\mathbf{v}\\) \u3068 \\(\\mathbf{v}^* A \\mathbf{v} = \\lambda^* \\mathbf{v}^* \\mathbf{v}\\) \u306e\u4e21\u65b9\u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u304b\u3089\u5c0e\u304b\u308c\u307e\u3059\u3002\u3053\u306e\u4e8c\u3064\u306e\u5f0f\u304b\u3089 \\(\\lambda = \\lambda^*\\) \u304c\u5c0e\u304b\u308c\u3001\u3053\u308c\u306f\u56fa\u6709\u5024\u304c\u5b9f\u6570\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u76f4\u611f\u7684\u306b\u306f\u3001\u5bfe\u79f0\u884c\u5217\u306f\u300c\u30d0\u30e9\u30f3\u30b9\u306e\u53d6\u308c\u305f\u300d\u5909\u63db\u3092\u8868\u3057\u3001\u305d\u306e\u3088\u3046\u306a\u5909\u63db\u306e\u4e3b\u8981\u306a\u65b9\u5411\uff08\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff09\u306b\u6cbf\u3063\u305f\u4f38\u7e2e\u7387\uff08\u56fa\u6709\u5024\uff09\u306f\u5b9f\u6570\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#q2","title":"Q2: \u5bfe\u79f0\u884c\u5217\u306e\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u76f4\u4ea4\u3059\u308b\u7406\u7531\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A2: \u5bfe\u79f0\u884c\u5217 \\(A\\) \u306e\u7570\u306a\u308b\u56fa\u6709\u5024 \\(\\lambda_1 \\neq \\lambda_2\\) \u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_1\\) \u3068 \\(\\mathbf{v}_2\\) \u306b\u3064\u3044\u3066\u3001\\(\\mathbf{v}_1^T A \\mathbf{v}_2 = \\lambda_2 \\mathbf{v}_1^T \\mathbf{v}_2\\) \u304b\u3064 \\(\\mathbf{v}_1^T A \\mathbf{v}_2 = \\lambda_1 \\mathbf{v}_1^T \\mathbf{v}_2\\) \u304c\u6210\u308a\u7acb\u3061\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a \\((\\lambda_2 - \\lambda_1) \\mathbf{v}_1^T \\mathbf{v}_2 = 0\\) \u3068\u306a\u308a\u3001\\(\\lambda_1 \\neq \\lambda_2\\) \u306a\u306e\u3067 \\(\\mathbf{v}_1^T \\mathbf{v}_2 = 0\\) \u3068\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u4e8c\u3064\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u76f4\u4ea4\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u5bfe\u79f0\u884c\u5217\u306f\u4e92\u3044\u306b\u72ec\u7acb\u3057\u305f\u76f4\u4ea4\u3059\u308b\u65b9\u5411\u306b\u6cbf\u3063\u3066\u4f38\u7e2e\u3059\u308b\u5909\u63db\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#q3","title":"Q3: \u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316\u3068\u4e00\u822c\u306e\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A3: \u5bfe\u79f0\u884c\u5217\u3068\u4e00\u822c\u306e\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306e\u4e3b\u306a\u9055\u3044\u306f\u4ee5\u4e0b\u306e\u70b9\u3067\u3059\uff1a 1. \u5bfe\u79f0\u884c\u5217\u306f\u5fc5\u305a\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3059\u304c\u3001\u4e00\u822c\u306e\u884c\u5217\u306f\u5bfe\u89d2\u5316\u3067\u304d\u306a\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002 2. \u5bfe\u79f0\u884c\u5217\u306f\u76f4\u4ea4\u884c\u5217\uff08\\(Q^TQ = I\\)\uff09\u306b\u3088\u3063\u3066\u5bfe\u89d2\u5316\u3067\u304d\u307e\u3059\u304c\u3001\u4e00\u822c\u306e\u884c\u5217\u306f\u5fc5\u305a\u3057\u3082\u76f4\u4ea4\u884c\u5217\u3067\u306f\u5bfe\u89d2\u5316\u3067\u304d\u307e\u305b\u3093\u3002 3. \u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u306f\u5168\u3066\u5b9f\u6570\u3067\u3059\u304c\u3001\u4e00\u822c\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u306f\u8907\u7d20\u6570\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002 4. \u5bfe\u79f0\u884c\u5217\u306e\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u76f4\u4ea4\u3057\u307e\u3059\u304c\u3001\u4e00\u822c\u306e\u884c\u5217\u3067\u306f\u305d\u306e\u3088\u3046\u306a\u4fdd\u8a3c\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#q4-pca","title":"Q4: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3068\u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306f\u3069\u306e\u3088\u3046\u306b\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u304b\uff1f","text":"<p>A4: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\uff08\u5bfe\u79f0\u884c\u5217\uff09\u306e\u5bfe\u89d2\u5316\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\uff1a 1. \u30c7\u30fc\u30bf\u884c\u5217\u304b\u3089\u8a08\u7b97\u3055\u308c\u308b\u5171\u5206\u6563\u884c\u5217\u306f\u5bfe\u79f0\u884c\u5217\u3067\u3059\u3002 2. \u3053\u306e\u5171\u5206\u6563\u884c\u5217\u3092\u5bfe\u89d2\u5316\u3059\u308b\u3068\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u4e3b\u6210\u5206\u306e\u65b9\u5411\u3092\u8868\u3057\u3001\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u5024\u304c\u305d\u306e\u65b9\u5411\u306e\u5206\u6563\u3092\u8868\u3057\u307e\u3059\u3002 3. \u56fa\u6709\u5024\u304c\u5927\u304d\u3044\u9806\u306b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u4e3b\u6210\u5206\uff09\u3092\u4e26\u3079\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u5909\u52d5\u3092\u6700\u3082\u3088\u304f\u6349\u3048\u308b\u8ef8\u3092\u7279\u5b9a\u3067\u304d\u307e\u3059\u3002 4. \u5c11\u6570\u306e\u4e3b\u8981\u306a\u4e3b\u6210\u5206\u3060\u3051\u3092\u9078\u3076\u3053\u3068\u3067\u3001\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u4f4e\u6b21\u5143\u306b\u5727\u7e2e\u3057\u306a\u304c\u3089\u3001\u91cd\u8981\u306a\u60c5\u5831\u3092\u4fdd\u6301\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/35-eigen-value-vector/#q5","title":"Q5: \u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\u306e\u5fdc\u7528\u4f8b\u3068\u3057\u3066\u4ed6\u306b\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A5: \u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\u306e\u5fdc\u7528\u4f8b\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u4fe1\u53f7\u51e6\u7406\uff1a\u30ce\u30a4\u30ba\u9664\u53bb\u3084\u4fe1\u53f7\u306e\u7279\u5fb4\u62bd\u51fa 2. \u91cf\u5b50\u529b\u5b66\uff1a\u91cf\u5b50\u72b6\u614b\u3084\u89b3\u6e2c\u91cf\u306e\u89e3\u6790 3. \u30b0\u30e9\u30d5\u7406\u8ad6\uff1a\u30b0\u30e9\u30d5\u306e\u30e9\u30d7\u30e9\u30b7\u30a2\u30f3\u884c\u5217\u304b\u3089\u5f97\u3089\u308c\u308b\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306b\u3088\u308b\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u691c\u51fa 4. \u632f\u52d5\u89e3\u6790\uff1a\u69cb\u9020\u7269\u306e\u56fa\u6709\u632f\u52d5\u30e2\u30fc\u30c9\u306e\u7279\u5b9a 5. \u63a8\u85a6\u30b7\u30b9\u30c6\u30e0\uff1a\u5354\u8abf\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u306b\u304a\u3051\u308b\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c 6. \u6a5f\u68b0\u5b66\u7fd2\uff1a\u30ab\u30fc\u30cd\u30eb\u6cd5\uff08\u30ab\u30fc\u30cd\u30eb\u4e3b\u6210\u5206\u5206\u6790\uff09 7. \u753b\u50cf\u51e6\u7406\uff1a\u753b\u50cf\u5727\u7e2e\u3084\u7279\u5fb4\u62bd\u51fa</p>"},{"location":"lectures/LA/35-eigen-value-vector/#q6","title":"Q6: \u591a\u91cd\u56fa\u6709\u5024\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u5bfe\u89d2\u5316\u306f\u3069\u306e\u3088\u3046\u306b\u884c\u3044\u307e\u3059\u304b\uff1f","text":"<p>A6: \u591a\u91cd\u56fa\u6709\u5024\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306e\u5bfe\u89d2\u5316\u624b\u9806\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a 1. \u5404\u56fa\u6709\u5024\u3068\u305d\u306e\u591a\u91cd\u5ea6\u3092\u6c42\u3081\u307e\u3059\u3002 2. \u5404\u56fa\u6709\u5024\u306b\u3064\u3044\u3066\u3001\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u7a7a\u9593\uff08\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5f35\u308b\u7a7a\u9593\uff09\u306e\u57fa\u5e95\u3092\u6c42\u3081\u307e\u3059\u3002 3. \u540c\u3058\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u30bb\u30c3\u30c8\u306b\u306f\u3001\u30b0\u30e9\u30e0\u30fb\u30b7\u30e5\u30df\u30c3\u30c8\u306e\u76f4\u4ea4\u5316\u6cd5\u3092\u9069\u7528\u3057\u3066\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u5f97\u307e\u3059\u3002 4. \u5168\u3066\u306e\u56fa\u6709\u5024\u306b\u5bfe\u3057\u3066\u5f97\u3089\u308c\u305f\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u5217\u3068\u3057\u3066\u4e26\u3079\u305f\u884c\u5217 \\(Q\\) \u3068\u3001\u56fa\u6709\u5024\u3092\u5bfe\u89d2\u6210\u5206\u306b\u6301\u3064\u5bfe\u89d2\u884c\u5217 \\(D\\) \u3092\u69cb\u6210\u3057\u307e\u3059\u3002 5. \\(A = QDQ^T\\) \u306b\u3088\u3063\u3066\u5bfe\u89d2\u5316\u304c\u5b8c\u4e86\u3057\u307e\u3059\u3002</p> <p>\u91cd\u8981\u306a\u306e\u306f\u3001\u5bfe\u79f0\u884c\u5217\u306e\u5834\u5408\u3001\u591a\u91cd\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3082\u76f4\u4ea4\u5316\u304c\u53ef\u80fd\u3067\u3001\u5fc5\u305a \\(n\\) \u500b\u306e\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u5b58\u5728\u3059\u308b\u3068\u3044\u3046\u70b9\u3067\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II\u3000\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/36-eigen-value-vector/#36-2","title":"\u7b2c36\u56de: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u305f\u3081\u306e2\u6b21\u5f62\u5f0f\u3068\u57fa\u790e","text":""},{"location":"lectures/LA/36-eigen-value-vector/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c36\u56de \u95a2\u9023\u9805\u76ee: 2\u6b21\u5f62\u5f0f\u3001\u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024\u884c\u5217\u3001\u5206\u6563\u5171\u5206\u6563\u884c\u5217 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u884c\u5217\u306e\u5bfe\u89d2\u5316\u3001\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3001\u5bfe\u79f0\u884c\u5217</p>"},{"location":"lectures/LA/36-eigen-value-vector/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\uff1a</p> <ol> <li>2\u6b21\u5f62\u5f0f\u306e\u5b9a\u7fa9\u3092\u7406\u89e3\u3057\u3001\u884c\u5217\u8868\u73fe\u304c\u3067\u304d\u308b</li> <li>\u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024\u884c\u5217\u306e\u6982\u5ff5\u3068\u5224\u5b9a\u6cd5\u3092\u7406\u89e3\u3059\u308b</li> <li>\u6b63\u5b9a\u5024\u6027\u3068\u56fa\u6709\u5024\u306e\u95a2\u4fc2\u3092\u7406\u89e3\u3057\u3001\u5224\u5b9a\u306b\u5fdc\u7528\u3067\u304d\u308b</li> <li>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b2\u6b21\u5f62\u5f0f\u3068\u6b63\u5b9a\u5024\u884c\u5217\u306e\u5f79\u5272\u3092\u7406\u89e3\u3059\u308b</li> <li>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u6b63\u5b9a\u5024\u6027\u3092\u7406\u89e3\u3057\u3001\u30c7\u30fc\u30bf\u5206\u6790\u306b\u5fdc\u7528\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/36-eigen-value-vector/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/36-eigen-value-vector/#31-2","title":"3.1 2\u6b21\u5f62\u5f0f\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \\(n\\)\u500b\u306e\u5909\u6570 \\(x_1, x_2, \\ldots, x_n\\) \u306b\u95a2\u3059\u308b2\u6b21\u306e\u591a\u9805\u5f0f\u3067\u3001\u5404\u9805\u304c\u5909\u6570\u306e2\u4e57\u307e\u305f\u306f\u7a4d\u306e\u5f62\u3092\u3057\u3066\u3044\u308b\u3082\u306e\u30922\u6b21\u5f62\u5f0f\u3068\u3044\u3046\u3002\u4e00\u822c\u7684\u306b\u3001\\(n\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)^T\\) \u306b\u5bfe\u3057\u3066\u3001\\(n \\times n\\) \u884c\u5217 \\(A\\) \u3092\u7528\u3044\u3066 \\(Q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\) \u3068\u8868\u73fe\u3055\u308c\u308b\u3002</p> <p>2\u6b21\u5f62\u5f0f\u306f\u3001\\(n\\)\u500b\u306e\u5909\u6570 \\(x_1, x_2, \\ldots, x_n\\) \u306e2\u6b21\u306e\u591a\u9805\u5f0f\u3067\u3001\u4ee5\u4e0b\u306e\u5f62\u3067\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[ Q(x_1, x_2, \\ldots, x_n) = \\sum_{i=1}^{n}\\sum_{j=1}^{n} a_{ij}x_i x_j \\] <p>\u3053\u3053\u3067\u3001\\(a_{ij}\\) \u306f\u4fc2\u6570\u3067\u3059\u3002\u884c\u5217\u8868\u8a18\u3067\u306f\uff1a</p> \\[ Q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\] <p>\u3053\u3053\u3067\u3001\\(A = (a_{ij})\\) \u306f \\(n \\times n\\) \u306e\u4fc2\u6570\u884c\u5217\u3067\u3059\u3002</p> <p>\u91cd\u8981\u306a\u30dd\u30a4\u30f3\u30c8\uff1a\u884c\u5217 \\(A\\) \u306f\u5e38\u306b\u5bfe\u79f0\u884c\u5217\u3068\u3057\u3066\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u306a\u305c\u306a\u3089\u3001\u4efb\u610f\u306e2\u6b21\u5f62\u5f0f\u306b\u304a\u3044\u3066 \\(a_{ij}x_i x_j + a_{ji}x_j x_i = (a_{ij} + a_{ji})x_i x_j\\) \u3068\u306a\u308b\u305f\u3081\u3001\\(A\\) \u3092\u5bfe\u79f0\u884c\u5217 \\(\\frac{1}{2}(A + A^T)\\) \u306b\u7f6e\u304d\u63db\u3048\u3066\u3082\u3001\u540c\u30582\u6b21\u5f62\u5f0f\u3092\u8868\u3059\u304b\u3089\u3067\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#32-2","title":"3.2 2\u6b21\u5f62\u5f0f\u306e\u6a19\u6e96\u5f62\u3068\u5bfe\u89d2\u5316","text":"<p>\u5bfe\u79f0\u884c\u5217 \\(A\\) \u306f\u76f4\u4ea4\u884c\u5217 \\(P\\) \u3092\u7528\u3044\u3066\u5bfe\u89d2\u5316\u3067\u304d\u308b\u305f\u3081\uff1a</p> \\[ A = PDP^T \\] <p>\u3053\u3053\u3067\u3001\\(D\\) \u306f\u56fa\u6709\u5024\u3092\u5bfe\u89d2\u6210\u5206\u306b\u3082\u3064\u5bfe\u89d2\u884c\u5217\u3067\u3059\u3002\u3053\u308c\u30922\u6b21\u5f62\u5f0f\u306b\u9069\u7528\u3059\u308b\u3068\uff1a</p> \\[ Q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} = \\mathbf{x}^T PDP^T \\mathbf{x} = (P^T\\mathbf{x})^T D (P^T\\mathbf{x}) = \\mathbf{y}^T D \\mathbf{y} \\] <p>\u3053\u3053\u3067\u3001\\(\\mathbf{y} = P^T\\mathbf{x}\\) \u306f\u5909\u6570\u306e\u76f4\u4ea4\u5909\u63db\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u9069\u5207\u306a\u5909\u6570\u5909\u63db\u306b\u3088\u308a\u30012\u6b21\u5f62\u5f0f\u306f\u4ee5\u4e0b\u306e\u6a19\u6e96\u5f62\u3067\u8868\u305b\u307e\u3059\uff1a</p> \\[ Q(\\mathbf{y}) = \\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\cdots + \\lambda_n y_n^2 \\] <p>\u3053\u3053\u3067\u3001\\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\) \u306f\u884c\u5217 \\(A\\) \u306e\u56fa\u6709\u5024\u3067\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/36-eigen-value-vector/#41","title":"4.1 \u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024\u884c\u5217","text":"<p>\u5b9a\u7fa9: \u5bfe\u79f0\u884c\u5217 \\(A\\) \u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3068\u306f\u3001\u4efb\u610f\u306e\u975e\u96f6\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} \\neq \\mathbf{0}\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3092\u3044\u3046\u3002</p> <p>\u5b9a\u7fa9: \u5bfe\u79f0\u884c\u5217 \\(A\\) \u304c\u534a\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3068\u306f\u3001\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{x}^T A \\mathbf{x} \\geq 0\\) \u304c\u6210\u308a\u7acb\u3061\u3001\u304b\u3064\u3042\u308b\u975e\u96f6\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} \\neq \\mathbf{0}\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{x}^T A \\mathbf{x} = 0\\) \u3068\u306a\u308b\u3053\u3068\u3092\u3044\u3046\u3002</p> <p>\u6b63\u5b9a\u5024\u884c\u5217\u306f\u30012\u6b21\u5f62\u5f0f\u304c\u3069\u306e\u3088\u3046\u306a\u975e\u96f6\u5165\u529b\u306b\u5bfe\u3057\u3066\u3082\u5e38\u306b\u6b63\u306e\u5024\u3092\u53d6\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u91cd\u8981\u306a\u6027\u8cea\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u4f8b: \u884c\u5217 \\(A = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{pmatrix}\\) \u3092\u8003\u3048\u307e\u3059\u3002\u3053\u306e\u884c\u5217\u304c\u6b63\u5b9a\u5024\u304b\u3069\u3046\u304b\u3092\u8abf\u3079\u308b\u305f\u3081\u306b\u3001\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} = (x, y)^T\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{x}^T A \\mathbf{x}\\) \u3092\u8a08\u7b97\u3057\u307e\u3059\uff1a</p> \\[ \\mathbf{x}^T A \\mathbf{x} = (x, y) \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = 2x^2 + 2xy + 3y^2 \\] <p>\u3053\u308c\u304c\u4efb\u610f\u306e\u975e\u96f6\u30d9\u30af\u30c8\u30eb \\((x,y) \\neq (0,0)\\) \u306b\u5bfe\u3057\u3066\u6b63\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#42","title":"4.2 \u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024\u3068\u56fa\u6709\u5024\u306e\u95a2\u4fc2","text":"<p>\u5b9a\u7406: \u5bfe\u79f0\u884c\u5217 \\(A\\) \u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3053\u3068\u3068\u3001\\(A\\) \u306e\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u304c\u6b63\u3067\u3042\u308b\u3053\u3068\u306f\u540c\u5024\u3067\u3042\u308b\u3002</p> <p>\u5b9a\u7406: \u5bfe\u79f0\u884c\u5217 \\(A\\) \u304c\u534a\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3053\u3068\u3068\u3001\\(A\\) \u306e\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u304c\u975e\u8ca0\u3067\u3042\u308b\u3053\u3068\u306f\u540c\u5024\u3067\u3042\u308b\u3002</p> <p>\u3053\u306e\u5b9a\u7406\u306b\u3088\u308a\u3001\u884c\u5217\u306e\u6b63\u5b9a\u5024\u6027\u306e\u5224\u5b9a\u306f\u3001\u56fa\u6709\u5024\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u884c\u3048\u307e\u3059\u3002</p> <p>\u8a3c\u660e\u306e\u6982\u7565: \u5bfe\u79f0\u884c\u5217 \\(A\\) \u304c\u76f4\u4ea4\u884c\u5217 \\(P\\) \u3067\u5bfe\u89d2\u5316\u3067\u304d\u308b\u3053\u3068\u3092\u5229\u7528\u3057\u307e\u3059\u3002\\(A = PDP^T\\) \u3068\u3059\u308b\u3068\u3001\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u306b\u5bfe\u3057\u3066\uff1a</p> \\[ \\mathbf{x}^T A \\mathbf{x} = \\mathbf{x}^T PDP^T \\mathbf{x} = \\mathbf{y}^T D \\mathbf{y} = \\sum_{i=1}^{n} \\lambda_i y_i^2 \\] <p>\u3053\u3053\u3067\u3001\\(\\mathbf{y} = P^T\\mathbf{x}\\) \u3068\u7f6e\u304d\u307e\u3057\u305f\u3002\\(\\mathbf{x} \\neq \\mathbf{0}\\) \u306e\u3068\u304d \\(\\mathbf{y} \\neq \\mathbf{0}\\) \u3067\u3042\u308a\u3001\\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) \u3067\u3042\u308b\u305f\u3081\u306b\u306f\u3001\u3059\u3079\u3066\u306e \\(\\lambda_i &gt; 0\\) \u304c\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#43","title":"4.3 \u6b63\u5b9a\u5024\u6027\u306e\u5224\u5b9a\u6cd5","text":"<p>\u6b63\u5b9a\u5024\u884c\u5217\u3092\u5224\u5b9a\u3059\u308b\u65b9\u6cd5\u306f\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li> <p>\u56fa\u6709\u5024\u306b\u3088\u308b\u5224\u5b9a\uff1a\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u304c\u6b63\u3067\u3042\u308c\u3070\u3001\u884c\u5217\u306f\u6b63\u5b9a\u5024\u3067\u3059\u3002</p> </li> <li> <p>\u4e3b\u5ea7\u5c0f\u884c\u5217\u5f0f\u306b\u3088\u308b\u5224\u5b9a\uff08\u30b7\u30eb\u30d9\u30b9\u30bf\u30fc\u306e\u5224\u5b9a\u6cd5\uff09\uff1a    \\(n \\times n\\) \u884c\u5217 \\(A\\) \u306b\u3064\u3044\u3066\u3001\u5de6\u4e0a\u306e \\(k \\times k\\) \u4e3b\u5ea7\u5c0f\u884c\u5217\u5f0f\u3092 \\(D_k\\) \u3068\u3059\u308b\u3068\u304d\u3001\u3059\u3079\u3066\u306e \\(D_k &gt; 0\\) (\\(k = 1,2,\\ldots,n\\)) \u3067\u3042\u308c\u3070\u3001\\(A\\) \u306f\u6b63\u5b9a\u5024\u3067\u3059\u3002</p> </li> </ol> <p>\u4f8b\u3048\u3070\u3001\\(A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{pmatrix}\\) \u306e\u5834\u5408\uff1a    - \\(D_1 = a_{11} &gt; 0\\)    - \\(D_2 = \\begin{vmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{vmatrix} &gt; 0\\)    - \\(D_3 = \\det(A) &gt; 0\\)</p> <ol> <li>\u30b3\u30ec\u30b9\u30ad\u30fc\u5206\u89e3\uff1a\u6b63\u5b9a\u5024\u884c\u5217 \\(A\\) \u306f\u3001\u4e0b\u4e09\u89d2\u884c\u5217 \\(L\\) \u3092\u7528\u3044\u3066 \\(A = LL^T\\) \u3068\u5206\u89e3\u3067\u304d\u307e\u3059\u3002</li> </ol>"},{"location":"lectures/LA/36-eigen-value-vector/#44","title":"4.4 \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3068\u6b63\u5b9a\u5024\u6027","text":"<p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\\(n\\)\u6b21\u5143\u306e\u78ba\u7387\u5909\u6570 \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_n)^T\\) \u306b\u5bfe\u3059\u308b\u5206\u6563\u5171\u5206\u6563\u884c\u5217 \\(\\Sigma\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[ \\Sigma = \\begin{pmatrix}  \\text{Var}(X_1) &amp; \\text{Cov}(X_1, X_2) &amp; \\cdots &amp; \\text{Cov}(X_1, X_n) \\\\ \\text{Cov}(X_2, X_1) &amp; \\text{Var}(X_2) &amp; \\cdots &amp; \\text{Cov}(X_2, X_n) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\text{Cov}(X_n, X_1) &amp; \\text{Cov}(X_n, X_2) &amp; \\cdots &amp; \\text{Var}(X_n) \\end{pmatrix} \\] <p>\u7406\u8ad6\u7684\u306b\u306f\u3001\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306f\u5e38\u306b\u534a\u6b63\u5b9a\u5024\u3067\u3059\u3002\u5909\u6570\u9593\u306b\u5b8c\u5168\u306a\u7dda\u5f62\u95a2\u4fc2\u304c\u306a\u3051\u308c\u3070\u3001\u6b63\u5b9a\u5024\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306f\u30012\u6b21\u5f62\u5f0f\u306e\u8a08\u7b97\u3068\u53ef\u8996\u5316\u3001\u304a\u3088\u3073\u884c\u5217\u306e\u6b63\u5b9a\u5024\u6027\u306e\u5224\u5b9a\u3092\u884c\u3046\u3082\u306e\u3067\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom scipy.linalg import eigh\n\n# 1. 2\u6b21\u5f62\u5f0f\u306e\u8a08\u7b97\u3068\u53ef\u8996\u5316\ndef visualize_quadratic_form(A, title=\"2\u6b21\u5f62\u5f0f\u306e\u53ef\u8996\u5316\"):\n    # \u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\n    A = (A + A.T) / 2\n\n    # \u30e1\u30c3\u30b7\u30e5\u30b0\u30ea\u30c3\u30c9\u306e\u4f5c\u6210\n    x = np.linspace(-2, 2, 100)\n    y = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(x, y)\n\n    # 2\u6b21\u5f62\u5f0f\u306e\u8a08\u7b97 Q(x,y) = [x y] A [x; y]\n    Z = np.zeros_like(X)\n    for i in range(len(x)):\n        for j in range(len(y)):\n            xy = np.array([X[i, j], Y[i, j]])\n            Z[i, j] = xy.dot(A).dot(xy)\n\n    # 3D\u30d7\u30ed\u30c3\u30c8\n    fig = plt.figure(figsize=(12, 6))\n\n    # 3D\u66f2\u9762\u30d7\u30ed\u30c3\u30c8\n    ax1 = fig.add_subplot(121, projection='3d')\n    surf = ax1.plot_surface(X, Y, Z, cmap=cm.coolwarm, alpha=0.8)\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('Q(x,y)')\n    ax1.set_title(title)\n\n    # \u7b49\u9ad8\u7dda\u30d7\u30ed\u30c3\u30c8\n    ax2 = fig.add_subplot(122)\n    contour = ax2.contour(X, Y, Z, 20, cmap=cm.coolwarm)\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title(\"\u7b49\u9ad8\u7dda\u56f3\")\n    plt.colorbar(contour, ax=ax2)\n\n    plt.tight_layout()\n    plt.show()\n\n    # \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    print(\"\u56fa\u6709\u5024:\", eigenvalues)\n    print(\"\u56fa\u6709\u30d9\u30af\u30c8\u30eb:\")\n    for i in range(len(eigenvalues)):\n        print(f\"\u03bb_{i+1} = {eigenvalues[i]:.4f}\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb: {eigenvectors[:, i]}\")\n\n    # \u884c\u5217\u306e\u6b63\u5b9a\u5024\u6027\u306e\u5224\u5b9a\n    if np.all(eigenvalues &gt; 0):\n        print(\"\u884c\u5217\u306f\u6b63\u5b9a\u5024\u3067\u3059\")\n    elif np.all(eigenvalues &gt;= 0):\n        print(\"\u884c\u5217\u306f\u534a\u6b63\u5b9a\u5024\u3067\u3059\")\n    else:\n        print(\"\u884c\u5217\u306f\u6b63\u5b9a\u5024\u3067\u3082\u534a\u6b63\u5b9a\u5024\u3067\u3082\u3042\u308a\u307e\u305b\u3093\")\n\n# 2. \u69d8\u3005\u306a\u884c\u5217\u3067\u306e2\u6b21\u5f62\u5f0f\u306e\u53ef\u8996\u5316\n# \u6b63\u5b9a\u5024\u884c\u5217\u306e\u4f8b\nA_positive_definite = np.array([[2, 1], [1, 3]])\nvisualize_quadratic_form(A_positive_definite, \"\u6b63\u5b9a\u5024\u884c\u5217\u306e2\u6b21\u5f62\u5f0f\")\n\n# \u534a\u6b63\u5b9a\u5024\u884c\u5217\u306e\u4f8b\nA_positive_semidefinite = np.array([[1, 1], [1, 1]])\nvisualize_quadratic_form(A_positive_semidefinite, \"\u534a\u6b63\u5b9a\u5024\u884c\u5217\u306e2\u6b21\u5f62\u5f0f\")\n\n# \u4e0d\u5b9a\u5024\u884c\u5217\u306e\u4f8b\nA_indefinite = np.array([[1, 2], [2, -3]])\nvisualize_quadratic_form(A_indefinite, \"\u4e0d\u5b9a\u5024\u884c\u5217\u306e2\u6b21\u5f62\u5f0f\")\n\n# 3. \u30c7\u30fc\u30bf\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3068\u6b63\u5b9a\u5024\u6027\ndef analyze_covariance_matrix(X):\n    # \u30c7\u30fc\u30bf\u306e\u4e2d\u5fc3\u5316\n    X_centered = X - np.mean(X, axis=0)\n\n    # \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u8a08\u7b97\n    n = X.shape[0]\n    cov_matrix = (X_centered.T @ X_centered) / n\n\n    print(\"\u5206\u6563\u5171\u5206\u6563\u884c\u5217:\")\n    print(cov_matrix)\n\n    # \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    print(\"\\n\u56fa\u6709\u5024:\", eigenvalues)\n\n    # \u6b63\u5b9a\u5024\u6027\u306e\u5224\u5b9a\n    if np.all(eigenvalues &gt; 1e-10):  # \u6570\u5024\u8aa4\u5dee\u3092\u8003\u616e\u3057\u3066\u5224\u5b9a\n        print(\"\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306f\u6b63\u5b9a\u5024\u3067\u3059\")\n    elif np.all(eigenvalues &gt;= -1e-10):\n        print(\"\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306f\u534a\u6b63\u5b9a\u5024\u3067\u3059\")\n    else:\n        print(\"\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306f\u6b63\u5b9a\u5024\u3067\u3082\u534a\u6b63\u5b9a\u5024\u3067\u3082\u3042\u308a\u307e\u305b\u3093\")\n\n    # 2\u6b21\u5f62\u5f0f\u306e\u53ef\u8996\u5316\n    visualize_quadratic_form(cov_matrix, \"\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e2\u6b21\u5f62\u5f0f\")\n\n    return cov_matrix, eigenvalues, eigenvectors\n\n# \u30e9\u30f3\u30c0\u30e0\u306a\u30c7\u30fc\u30bf\u3092\u751f\u6210\nnp.random.seed(42)\n# 2\u6b21\u5143\u306e\u76f8\u95a2\u306e\u3042\u308b\u30c7\u30fc\u30bf\u3092\u751f\u6210\nX = np.random.multivariate_normal(mean=[0, 0], cov=[[2, 1], [1, 3]], size=100)\n\n# \u30c7\u30fc\u30bf\u3092\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(8, 6))\nplt.scatter(X[:, 0], X[:, 1], alpha=0.7)\nplt.title(\"2\u6b21\u5143\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u30c7\u30fc\u30bf\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\n# \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u5206\u6790\ncov_matrix, eigenvalues, eigenvectors = analyze_covariance_matrix(X)\n\n# \u4e3b\u8ef8\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(8, 6))\nplt.scatter(X[:, 0], X[:, 1], alpha=0.7)\n\n# \u30c7\u30fc\u30bf\u306e\u4e2d\u5fc3\nmean = np.mean(X, axis=0)\n\n# \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u3066\u4e3b\u8ef8\u3068\u3057\u3066\u8868\u793a\nfor i in range(2):\n    plt.arrow(mean[0], mean[1], \n              eigenvalues[i] * eigenvectors[0, i], \n              eigenvalues[i] * eigenvectors[1, i],\n              head_width=0.1, head_length=0.1, fc='red', ec='red')\n\nplt.title(\"\u30c7\u30fc\u30bf\u3068\u4e3b\u8ef8\uff08\u56fa\u6709\u30d9\u30af\u30c8\u30eb*\u56fa\u6709\u5024\uff09\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n</code></pre> <p>\u3053\u306e\u30b3\u30fc\u30c9\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u3092\u884c\u3063\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\u69d8\u3005\u306a\u7a2e\u985e\u306e\u884c\u5217\uff08\u6b63\u5b9a\u5024\u3001\u534a\u6b63\u5b9a\u5024\u3001\u4e0d\u5b9a\u5024\uff09\u306e2\u6b21\u5f62\u5f0f\u3092\u8a08\u7b97\u3057\u30013D\u66f2\u9762\u3068\u7b49\u9ad8\u7dda\u56f3\u3067\u53ef\u8996\u5316</li> <li>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u304b\u3089\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\u3057\u3001\u305d\u306e\u6b63\u5b9a\u5024\u6027\u3092\u5224\u5b9a</li> <li>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u3057\u3001\u30c7\u30fc\u30bf\u306e\u4e3b\u8ef8\u3068\u3057\u3066\u53ef\u8996\u5316</li> </ol>"},{"location":"lectures/LA/36-eigen-value-vector/#6-2","title":"6. \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b2\u6b21\u5f62\u5f0f\u3068\u6b63\u5b9a\u5024\u884c\u5217\u306e\u5fdc\u7528","text":""},{"location":"lectures/LA/36-eigen-value-vector/#61","title":"6.1 \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u3068\u5206\u6563\u5171\u5206\u6563\u884c\u5217","text":"<p>\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[ f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right) \\] <p>\u3053\u3053\u3067\u3001\\(\\boldsymbol{\\mu}\\) \u306f\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u3001\\(\\Sigma\\) \u306f\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3067\u3059\u3002\u5206\u6563\u5171\u5206\u6563\u884c\u5217 \\(\\Sigma\\) \u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3053\u3068\u306f\u3001\u78ba\u7387\u5206\u5e03\u304c\u9069\u5207\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u305f\u3081\u306b\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#62","title":"6.2 \u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3068\u7570\u5e38\u691c\u51fa","text":"<p>\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306f\u3001\u591a\u5909\u91cf\u7a7a\u9593\u3067\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u8ddd\u96e2\u3092\u6e2c\u308b\u6307\u6a19\u3067\u3001\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[ d_M(\\mathbf{x}, \\boldsymbol{\\mu}) = \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})} \\] <p>\u3053\u308c\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u69cb\u9020\u3092\u8003\u616e\u3057\u305f\u8ddd\u96e2\u5c3a\u5ea6\u3067\u3042\u308a\u3001\u7570\u5e38\u691c\u51fa\u306a\u3069\u306b\u5229\u7528\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u5f0f\u306e\u4e2d\u5fc3\u90e8\u5206 \\((\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\) \u306f2\u6b21\u5f62\u5f0f\u3067\u3042\u308a\u3001\\(\\Sigma^{-1}\\) \u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#63","title":"6.3 \u6700\u9069\u5316\u554f\u984c\u306b\u304a\u3051\u308b\u6b63\u5b9a\u5024\u884c\u5217","text":"<p>\u51f8\u6700\u9069\u5316\u554f\u984c\u3067\u306f\u3001\u76ee\u7684\u95a2\u6570\u306e\u30d8\u30c3\u30bb\u884c\u5217\u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3053\u3068\u304c\u3001\u305d\u306e\u95a2\u6570\u304c\u51f8\u95a2\u6570\u3067\u3042\u308b\u5341\u5206\u6761\u4ef6\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u56de\u5e30\u5206\u6790\u3067\u306f\u3001\u76ee\u7684\u95a2\u6570\u306e\u30d8\u30c3\u30bb\u884c\u5217\u304c\u6b63\u5b9a\u5024\u3067\u3042\u308c\u3070\u3001\u4e00\u610f\u7684\u306a\u6700\u5c0f\u5024\u304c\u5b58\u5728\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#64-pca","title":"6.4 \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3068\u56fa\u6709\u5024","text":"<p>\u4e3b\u6210\u5206\u5206\u6790\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306f\u534a\u6b63\u5b9a\u5024\u3067\u3042\u308a\u3001\u305d\u306e\u56fa\u6709\u5024\u306f\u975e\u8ca0\u3067\u3059\u3002\u56fa\u6709\u5024\u306e\u5927\u304d\u3055\u306f\u3001\u5bfe\u5fdc\u3059\u308b\u4e3b\u6210\u5206\u306e\u91cd\u8981\u5ea6\uff08\u8aac\u660e\u3055\u308c\u308b\u5206\u6563\u306e\u91cf\uff09\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/36-eigen-value-vector/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li>\u6b21\u306e\u884c\u5217\u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u304b\u3001\u534a\u6b63\u5b9a\u5024\u3067\u3042\u308b\u304b\u3001\u307e\u305f\u306f\u4e0d\u5b9a\u5024\u3067\u3042\u308b\u304b\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002</li> </ol> <p>(a) \\(A = \\begin{pmatrix} 4 &amp; 2 \\\\ 2 &amp; 3 \\end{pmatrix}\\)</p> <p>(b) \\(B = \\begin{pmatrix} 1 &amp; 2 &amp; 0 \\\\ 2 &amp; 5 &amp; 1 \\\\ 0 &amp; 1 &amp; 3 \\end{pmatrix}\\)</p> <p>(c) \\(C = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix}\\)</p> <p>(d) \\(D = \\begin{pmatrix} 2 &amp; -1 \\\\ -1 &amp; 0 \\end{pmatrix}\\)</p> <ol> <li>\u884c\u5217 \\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\uff1a</li> </ol> <p>(a) \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> <p>(b) 2\u6b21\u5f62\u5f0f \\(Q(x,y) = 3x^2 + 2xy + 2y^2\\) \u3092\u6a19\u6e96\u5f62\uff08\u56fa\u6709\u5024\u3092\u7528\u3044\u305f\u5f62\uff09\u306b\u5909\u63db\u3057\u306a\u3055\u3044\u3002</p> <p>(c) \u3053\u306e2\u6b21\u5f62\u5f0f\u304c\u8868\u3059\u66f2\u9762\u3092\u5206\u985e\u3057\u306a\u3055\u3044\u3002</p> <ol> <li>\u6b21\u306e2\u6b21\u5f62\u5f0f\u306b\u3064\u3044\u3066\u3001\u5bfe\u5fdc\u3059\u308b\u5bfe\u79f0\u884c\u5217\u3092\u6c42\u3081\u3001\u305d\u306e\u56fa\u6709\u5024\u304b\u30892\u6b21\u5f62\u5f0f\u306e\u6027\u8cea\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002</li> </ol> <p>(a) \\(Q(x,y,z) = 2x^2 + 3y^2 + 4z^2 + 2xy + 4yz\\)</p> <p>(b) \\(Q(x,y) = x^2 - y^2 + 4xy\\)</p> <ol> <li>\u30b7\u30eb\u30d9\u30b9\u30bf\u30fc\u306e\u5224\u5b9a\u6cd5\u3092\u7528\u3044\u3066\u3001\u6b21\u306e\u884c\u5217\u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002</li> </ol> <p>\\(A = \\begin{pmatrix} 2 &amp; -1 &amp; 0 \\\\ -1 &amp; 2 &amp; -1 \\\\ 0 &amp; -1 &amp; 2 \\end{pmatrix}\\)</p>"},{"location":"lectures/LA/36-eigen-value-vector/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li>\u6b21\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u304b\u3089\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3001\u8996\u899a\u5316\u3057\u306a\u3055\u3044\u3002\u307e\u305f\u3001\u305d\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u30c7\u30fc\u30bf\u306e\u4e3b\u8ef8\u3068\u3057\u3066\u53ef\u8996\u5316\u3057\u306a\u3055\u3044\u3002</li> </ol> <p>\\(\\Sigma = \\begin{pmatrix} 4 &amp; 2 \\\\ 2 &amp; 3 \\end{pmatrix}\\)</p> <ol> <li>2\u6b21\u5143\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8 \\((1,2)\\), \\((2,3)\\), \\((3,5)\\), \\((4,6)\\), \\((5,8)\\) \u306b\u3064\u3044\u3066\uff1a</li> </ol> <p>(a) \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002</p> <p>(b) \u8a08\u7b97\u3057\u305f\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> <p>(c) \u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u3092\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8ef8\u3068\u3059\u308b\u65b0\u3057\u3044\u5ea7\u6a19\u7cfb\u306b\u5909\u63db\u3057\u306a\u3055\u3044\u3002</p> <p>(d) \u5143\u306e\u30c7\u30fc\u30bf\u3068\u5909\u63db\u5f8c\u306e\u30c7\u30fc\u30bf\u3092\u53ef\u8996\u5316\u3057\u3001\u6bd4\u8f03\u3057\u306a\u3055\u3044\u3002</p> <ol> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u79d1\u5b66\u5fdc\u7528\u554f\u984c\uff1a\u5fc3\u62cd\u6570\u3068\u8840\u5727\u306e\u95a2\u4fc2\u3092\u8abf\u67fb\u3059\u308b\u305f\u3081\u306b\u300110\u4eba\u306e\u88ab\u9a13\u8005\u304b\u3089\u6e2c\u5b9a\u3057\u305f\u30c7\u30fc\u30bf\u304c\u3042\u308a\u307e\u3059\u3002\u5404\u88ab\u9a13\u8005\u306e\u5b89\u9759\u6642\u5fc3\u62cd\u6570\uff08\u62cd/\u5206\uff09\u3068\u53ce\u7e2e\u671f\u8840\u5727\uff08mmHg\uff09\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</li> </ol> \u88ab\u9a13\u8005 \u5fc3\u62cd\u6570 \u53ce\u7e2e\u671f\u8840\u5727 1 62 120 2 65 124 3 68 130 4 70 135 5 71 132 6 72 138 7 74 140 8 76 142 9 78 145 10 80 148 <p>(a) \u3053\u306e\u30c7\u30fc\u30bf\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002</p> <p>(b) \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> <p>(c) \u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3092\u7528\u3044\u3066\u3001\u5404\u88ab\u9a13\u8005\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u304c\u7570\u5e38\u5024\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u3057\u306a\u3055\u3044\u3002\uff08\u95be\u5024\u3068\u3057\u3066\u3001\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u5e73\u65b9 &gt; 5.99 \u3092\u7570\u5e38\u5024\u3068\u3059\u308b\uff09</p> <p>(d) \u5065\u5eb7\u8a55\u4fa1\u6307\u6a19\u3068\u3057\u3066\u3001\u5fc3\u62cd\u6570\u3068\u8840\u5727\u306e\u7dda\u5f62\u7d50\u5408\u3067\u8868\u3055\u308c\u308b\u65b0\u3057\u3044\u5909\u6570\u3092\u8003\u3048\u307e\u3059\u3002\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u3088\u3046\u306a\u7dda\u5f62\u7d50\u5408\u306e\u4fc2\u6570\u3092\u6c42\u3081\u3001\u305d\u306e\u6307\u6a19\u306e\u610f\u5473\u3092\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/36-eigen-value-vector/#q1","title":"Q1: \u6b63\u5b9a\u5024\u884c\u5217\u3068\u6b63\u5247\u884c\u5217\uff08\u53ef\u9006\u884c\u5217\uff09\u306f\u540c\u3058\u3067\u3059\u304b\uff1f","text":"<p>A1: \u3044\u3044\u3048\u3001\u6b63\u5b9a\u5024\u884c\u5217\u306f\u5fc5\u305a\u6b63\u5247\u3067\u3059\u304c\u3001\u9006\u306f\u5fc5\u305a\u3057\u3082\u6210\u308a\u7acb\u3061\u307e\u305b\u3093\u3002\u4f8b\u3048\u3070\u3001\\(A = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix}\\) \u306f\u6b63\u5247\u3067\u3059\u304c\u3001\u6b63\u5b9a\u5024\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff08\u56fa\u6709\u5024\u306e\u4e00\u3064\u304c\u8ca0\u3067\u3059\uff09\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#q2","title":"Q2: \u534a\u6b63\u5b9a\u5024\u884c\u5217\u3068\u306f\u3069\u306e\u3088\u3046\u306a\u884c\u5217\u3067\u3059\u304b\uff1f","text":"<p>A2: \u534a\u6b63\u5b9a\u5024\u884c\u5217\u306f\u30012\u6b21\u5f62\u5f0f \\(\\mathbf{x}^T A \\mathbf{x} \\geq 0\\) \u304c\u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u306b\u5bfe\u3057\u3066\u6210\u308a\u7acb\u3064\u5bfe\u79f0\u884c\u5217\u3067\u3059\u3002\u4f8b\u3048\u3070\u30010\u3067\u306f\u306a\u3044\u884c\u5217 \\(B\\) \u306b\u5bfe\u3057\u3066 \\(A = B^T B\\) \u3068\u3059\u308b\u3068\u3001\\(A\\) \u306f\u534a\u6b63\u5b9a\u5024\u306b\u306a\u308a\u307e\u3059\u3002\u56fa\u6709\u5024\u306f\u5168\u3066\u975e\u8ca0\u3067\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#q3","title":"Q3: \u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3067\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u304c\u6b63\u5b9a\u5024\u3067\u306a\u3044\u5834\u5408\u306f\u3069\u3046\u3059\u308c\u3070\u3044\u3044\u3067\u3059\u304b\uff1f","text":"<p>A3: \u3053\u308c\u306f\u300c\u591a\u91cd\u5171\u7dda\u6027\u300d\u306e\u554f\u984c\u3068\u3057\u3066\u77e5\u3089\u308c\u3066\u3044\u307e\u3059\u3002\u89e3\u6c7a\u7b56\u3068\u3057\u3066\u306f\uff1a 1. \u554f\u984c\u306e\u3042\u308b\u5909\u6570\u3092\u53d6\u308a\u9664\u304f 2. \u6b63\u5247\u5316\u624b\u6cd5\uff08\u30ea\u30c3\u30b8\u56de\u5e30\u306a\u3069\uff09\u3092\u9069\u7528\u3059\u308b 3. \u4e3b\u6210\u5206\u5206\u6790\u306a\u3069\u306e\u6b21\u5143\u524a\u6e1b\u3092\u884c\u3046 \u306a\u3069\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#q4-2-qmathbfx-mathbfxt-a-mathbfx","title":"Q4: 2\u6b21\u5f62\u5f0f \\(Q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\) \u304c\u8868\u3059\u5e7e\u4f55\u5b66\u7684\u306a\u5f62\u72b6\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A4: 2\u6b21\u5143\u306e\u5834\u5408\u3001\\(A\\) \u306e\u56fa\u6709\u5024\u306e\u7b26\u53f7\u306b\u3088\u3063\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5206\u985e\u3055\u308c\u307e\u3059\uff1a - \u4e21\u65b9\u306e\u56fa\u6709\u5024\u304c\u6b63 \u2192 \u6955\u5186\uff08\u6b63\u5b9a\u5024\uff09 - \u4e21\u65b9\u306e\u56fa\u6709\u5024\u304c\u8ca0 \u2192 \u6955\u5186\uff08\u8ca0\u5b9a\u5024\uff09 - \u4e00\u65b9\u304c\u6b63\u3001\u4e00\u65b9\u304c\u8ca0 \u2192 \u53cc\u66f2\u7dda\uff08\u4e0d\u5b9a\u5024\uff09 - \u4e00\u65b9\u304c0\u3001\u4e00\u65b9\u304c\u975e0 \u2192 \u653e\u7269\u7dda\uff08\u534a\u5b9a\u5024\uff09</p>"},{"location":"lectures/LA/36-eigen-value-vector/#q5","title":"Q5: \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306f\u306a\u305c\u534a\u6b63\u5b9a\u5024\u306a\u306e\u3067\u3059\u304b\uff1f","text":"<p>A5: \u4efb\u610f\u306e\u30d9\u30af\u30c8\u30eb \\(\\mathbf{a}\\) \u306b\u5bfe\u3057\u3066\u3001\\(\\mathbf{a}^T \\Sigma \\mathbf{a}\\) \u306f\u7dda\u5f62\u7d50\u5408 \\(\\sum_i a_i X_i\\) \u306e\u5206\u6563\u3092\u8868\u3057\u307e\u3059\u3002\u5206\u6563\u306f\u5e38\u306b\u975e\u8ca0\u3067\u3042\u308b\u305f\u3081\u3001\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306f\u534a\u6b63\u5b9a\u5024\u3067\u3059\u3002\u3055\u3089\u306b\u3001\u5b8c\u5168\u306a\u7dda\u5f62\u5f93\u5c5e\u95a2\u4fc2\u304c\u306a\u3044\u5834\u5408\u306f\u6b63\u5b9a\u5024\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/36-eigen-value-vector/#9","title":"9. \u53c2\u8003\u6587\u732e","text":"<ol> <li>Gilbert Strang, \"Linear Algebra and Its Applications\", 4th Edition, 2006.</li> <li>Roger A. Horn and Charles R. Johnson, \"Matrix Analysis\", 2nd Edition, Cambridge University Press, 2012.</li> <li>Trevor Hastie, Robert Tibshirani, and Jerome Friedman, \"The Elements of Statistical Learning\", 2nd Edition, Springer, 2009.</li> <li>David A. Harville, \"Matrix Algebra From a Statistician's Perspective\", Springer, 1997.</li> </ol> <p>\u3053\u306e\u8b1b\u7fa9\u30ce\u30fc\u30c8\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u7dda\u5f62\u4ee3\u6570\u306e\u91cd\u8981\u306a\u6982\u5ff5\u3067\u3042\u308b2\u6b21\u5f62\u5f0f\u3068\u6b63\u5b9a\u5024\u884c\u5217\u306b\u3064\u3044\u3066\u8a73\u8aac\u3057\u307e\u3057\u305f\u3002\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u6982\u5ff5\u3092\u767a\u5c55\u3055\u305b\u3001\u4e3b\u6210\u5206\u5206\u6790\u306e\u7406\u8ad6\u7684\u57fa\u790e\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"lectures/LA/37-exercise/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II","text":""},{"location":"lectures/LA/37-exercise/#37","title":"\u7b2c37\u56de\u8b1b\u7fa9\uff1a\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u7dcf\u5408\u6f14\u7fd2","text":""},{"location":"lectures/LA/37-exercise/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c37\u56de \u95a2\u9023\u9805\u76ee: \u56fa\u6709\u5024\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3001\u5bfe\u89d2\u5316\u30012\u6b21\u5f62\u5f0f\u3001\u6b63\u5b9a\u5024\u884c\u5217 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9:  - \u7b2c33\u56de\u300c\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u300d - \u7b2c34\u56de\u300c\u7279\u6027\u65b9\u7a0b\u5f0f\u3068\u5bfe\u89d2\u5316\u300d - \u7b2c35\u56de\u300c\u5bfe\u79f0\u884c\u5217\u3068\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\u300d - \u7b2c36\u56de\u300c\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u305f\u3081\u306e2\u6b21\u5f62\u5f0f\u3068\u57fa\u790e\u300d</p>"},{"location":"lectures/LA/37-exercise/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u4ee5\u4e0b\u306e\u80fd\u529b\u3092\u8eab\u306b\u3064\u3051\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3057\u307e\u3059\uff1a</p> <ol> <li>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u3092\u6b63\u78ba\u306b\u884c\u3048\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u884c\u5217\u306e\u5bfe\u89d2\u5316\u3092\u5b9f\u884c\u3057\u3001\u305d\u306e\u6027\u8cea\u3092\u7406\u89e3\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u5bfe\u89d2\u5316\u3092\u5229\u7528\u3057\u305f\u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97\u3084\u6570\u5217\u306e\u4e00\u822c\u9805\u306e\u5c0e\u51fa\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>2\u6b21\u5f62\u5f0f\u3068\u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024\u884c\u5217\u306e\u6027\u8cea\u3092\u7406\u89e3\u3057\u5224\u5b9a\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u3053\u308c\u3089\u306e\u6982\u5ff5\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u5fdc\u7528\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/37-exercise/#3","title":"3. \u57fa\u672c\u6982\u5ff5\u306e\u5fa9\u7fd2","text":""},{"location":"lectures/LA/37-exercise/#31","title":"3.1 \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb","text":"<p>\u5b9a\u7fa9: \u6b63\u65b9\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v} \\neq \\mathbf{0}\\) \u3068\u6570 \\(\\lambda\\) \u304c</p> \\[A\\mathbf{v} = \\lambda\\mathbf{v}\\] <p>\u3092\u6e80\u305f\u3059\u3068\u304d\u3001\\(\\lambda\\) \u3092\u884c\u5217 \\(A\\) \u306e\u56fa\u6709\u5024\u3001\\(\\mathbf{v}\\) \u3092\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u547c\u3073\u307e\u3059\u3002</p> <p>\u56fa\u6709\u5024\u306e\u6c42\u3081\u65b9: 1. \u7279\u6027\u65b9\u7a0b\u5f0f \\(\\det(A - \\lambda I) = 0\\) \u3092\u7acb\u3066\u308b 2. \u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u3044\u3066\u56fa\u6709\u5024 \\(\\lambda\\) \u3092\u6c42\u3081\u308b 3. \u5404\u56fa\u6709\u5024\u306b\u3064\u3044\u3066 \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\) \u3092\u6e80\u305f\u3059\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}\\) \u3092\u6c42\u3081\u308b</p> <p>\u91cd\u8981\u306a\u6027\u8cea: - \\(n\\) \u6b21\u6b63\u65b9\u884c\u5217\u306f\u6700\u5927 \\(n\\) \u500b\u306e\u56fa\u6709\u5024\u3092\u6301\u3064 - \u56fa\u6709\u5024\u306e\u7dcf\u548c\u306f\u884c\u5217\u306e\u30c8\u30ec\u30fc\u30b9\uff08\u5bfe\u89d2\u6210\u5206\u306e\u548c\uff09\u306b\u7b49\u3057\u3044: \\(\\sum_{i=1}^n \\lambda_i = \\mathrm{tr}(A)\\) - \u56fa\u6709\u5024\u306e\u7a4d\u306f\u884c\u5217\u5f0f\u306b\u7b49\u3057\u3044: \\(\\prod_{i=1}^n \\lambda_i = \\det(A)\\)</p>"},{"location":"lectures/LA/37-exercise/#32","title":"3.2 \u884c\u5217\u306e\u5bfe\u89d2\u5316","text":"<p>\u5b9a\u7fa9: \u6b63\u65b9\u884c\u5217 \\(A\\) \u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308b\u3068\u306f\u3001\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u884c\u5217 \\(P\\) \u3068\u5bfe\u89d2\u884c\u5217 \\(D\\) \u304c\u5b58\u5728\u3057\u3066\u3001</p> \\[P^{-1}AP = D\\] <p>\u3068\u8868\u305b\u308b\u3053\u3068\u3092\u8a00\u3044\u307e\u3059\u3002\u3053\u306e\u3068\u304d\u3001\\(D\\) \u306e\u5bfe\u89d2\u6210\u5206\u306f \\(A\\) \u306e\u56fa\u6709\u5024\u3067\u3042\u308a\u3001\\(P\\) \u306e\u5404\u5217\u30d9\u30af\u30c8\u30eb\u306f\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p> <p>\u5bfe\u89d2\u5316\u306e\u624b\u9806: 1. \u884c\u5217 \\(A\\) \u306e\u56fa\u6709\u5024 \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\) \u3092\u6c42\u3081\u308b 2. \u5404\u56fa\u6709\u5024 \\(\\lambda_i\\) \u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_i\\) \u3092\u6c42\u3081\u308b 3. \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u5217\u3068\u3059\u308b\u884c\u5217 \\(P = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n]\\) \u3092\u4f5c\u308b 4. \u5bfe\u89d2\u884c\u5217 \\(D = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)\\) \u3092\u4f5c\u308b</p> <p>\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308b\u305f\u3081\u306e\u6761\u4ef6: - \u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c \\(n\\) \u500b\u306e\u7dda\u5f62\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u3092\u5f62\u6210\u3059\u308b\u3053\u3068 - \u7279\u306b\u3001\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u5fc5\u305a\u7dda\u5f62\u72ec\u7acb - \u91cd\u8907\u56fa\u6709\u5024\u3092\u6301\u3064\u5834\u5408\u3001\u305d\u306e\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u7a7a\u9593\u306e\u6b21\u5143\u304c\u56fa\u6709\u5024\u306e\u91cd\u8907\u5ea6\u3068\u4e00\u81f4\u3059\u308c\u3070\u5bfe\u89d2\u5316\u53ef\u80fd</p>"},{"location":"lectures/LA/37-exercise/#33","title":"3.3 \u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316","text":"<p>\u5b9a\u7406: \u5b9f\u5bfe\u79f0\u884c\u5217 \\(A\\) \uff08\u3064\u307e\u308a \\(A^T = A\\)\uff09\u306f\u5e38\u306b\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308a\u3001\u3055\u3089\u306b\u76f4\u4ea4\u884c\u5217 \\(Q\\) \uff08\u3064\u307e\u308a \\(Q^T Q = QQ^T = I\\)\uff09\u306b\u3088\u3063\u3066\u5bfe\u89d2\u5316\u3067\u304d\u307e\u3059\u3002</p> \\[Q^T A Q = D\\] <p>\u5bfe\u79f0\u884c\u5217\u306e\u6027\u8cea: - \u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u306f\u5b9f\u6570 - \u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4 - \u540c\u3058\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304b\u3089\u30b0\u30e9\u30e0\u30fb\u30b7\u30e5\u30df\u30c3\u30c8\u76f4\u4ea4\u5316\u6cd5\u306b\u3088\u3063\u3066\u6b63\u898f\u76f4\u4ea4\u57fa\u5e95\u3092\u69cb\u6210\u3067\u304d\u308b</p>"},{"location":"lectures/LA/37-exercise/#34-2","title":"3.4 2\u6b21\u5f62\u5f0f\u3068\u6b63\u5b9a\u5024\u884c\u5217","text":"<p>\u5b9a\u7fa9: \\(n\\) \u6b21\u5143\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u3068 \\(n \\times n\\) \u5b9f\u5bfe\u79f0\u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001</p> \\[f(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\] <p>\u306e\u5f62\u306e\u95a2\u6570\u30922\u6b21\u5f62\u5f0f\u3068\u547c\u3073\u307e\u3059\u3002</p> <p>\u884c\u5217\u306e\u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024: - \\(A\\) \u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3068\u306f\u3001\u4efb\u610f\u306e \\(\\mathbf{x} \\neq \\mathbf{0}\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068 - \\(A\\) \u304c\u534a\u6b63\u5b9a\u5024\u3067\u3042\u308b\u3068\u306f\u3001\u4efb\u610f\u306e \\(\\mathbf{x}\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{x}^T A \\mathbf{x} \\geq 0\\) \u304c\u6210\u308a\u7acb\u3064\u3053\u3068 - \u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024\u306e\u5224\u5b9a\u306f\u3001\u884c\u5217 \\(A\\) \u306e\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u304c\u6b63\u30fb\u975e\u8ca0\u3067\u3042\u308b\u3053\u3068\u3068\u540c\u5024</p>"},{"location":"lectures/LA/37-exercise/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/37-exercise/#41","title":"4.1 \u5bfe\u89d2\u5316\u3092\u5229\u7528\u3057\u305f\u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97","text":"<p>\u884c\u5217 \\(A\\) \u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u3067 \\(P^{-1}AP = D\\) \u3067\u3042\u308b\u3068\u304d\u3001\\(A\\) \u306e \\(k\\) \u4e57\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a</p> \\[A^k = PD^kP^{-1}\\] <p>\u3053\u3053\u3067 \\(D\\) \u306f\u5bfe\u89d2\u884c\u5217\u306a\u306e\u3067\u3001\u305d\u306e\u3079\u304d\u4e57 \\(D^k\\) \u306f\u5bfe\u89d2\u6210\u5206\u3092\u305d\u308c\u305e\u308c \\(k\\) \u4e57\u3057\u305f\u3082\u306e\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[D^k = \\begin{pmatrix} \\lambda_1^k &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2^k &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_n^k \\end{pmatrix}\\] <p>\u3053\u308c\u306b\u3088\u308a\u3001\\(A^k\\) \u306e\u8a08\u7b97\u304c\u8457\u3057\u304f\u7c21\u5358\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/37-exercise/#42","title":"4.2 \u6570\u5217\u306e\u4e00\u822c\u9805\u306e\u6c42\u3081\u65b9","text":"<p>\u6f38\u5316\u5f0f\u3067\u5b9a\u7fa9\u3055\u308c\u308b\u6570\u5217\u306e\u4e00\u822c\u9805\u3092\u884c\u5217\u306e\u3079\u304d\u4e57\u3092\u7528\u3044\u3066\u6c42\u3081\u308b\u65b9\u6cd5\u3092\u8003\u3048\u307e\u3059\u3002</p> <p>\u7dda\u5f62\u6f38\u5316\u5f0f \\(a_{n+2} = pa_{n+1} + qa_n\\) \uff08\\(p\\), \\(q\\) \u306f\u5b9a\u6570\uff09\u306f\u4ee5\u4e0b\u306e\u884c\u5217\u3092\u7528\u3044\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\uff1a</p> \\[\\begin{pmatrix} a_{n+2} \\\\ a_{n+1} \\end{pmatrix} = \\begin{pmatrix} p &amp; q \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} a_{n+1} \\\\ a_n \\end{pmatrix}\\] <p>\u3053\u308c\u3092 \\(n\\) \u56de\u7e70\u308a\u8fd4\u3059\u3068\uff1a</p> \\[\\begin{pmatrix} a_{n+1} \\\\ a_n \\end{pmatrix} = \\begin{pmatrix} p &amp; q \\\\ 1 &amp; 0 \\end{pmatrix}^n \\begin{pmatrix} a_1 \\\\ a_0 \\end{pmatrix}\\] <p>\u3053\u3053\u3067\u884c\u5217\u306e\u3079\u304d\u4e57\u3092\u5bfe\u89d2\u5316\u306b\u3088\u3063\u3066\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u3001\u6570\u5217\u306e\u4e00\u822c\u9805\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/37-exercise/#43-2","title":"4.3 2\u6b21\u5f62\u5f0f\u306e\u6a19\u6e96\u5f62","text":"<p>\u5b9f\u5bfe\u79f0\u884c\u5217 \\(A\\) \u304c\u76f4\u4ea4\u884c\u5217 \\(Q\\) \u306b\u3088\u3063\u3066\u5bfe\u89d2\u5316\u3055\u308c\u308b\u3068\u304d\uff08\\(Q^T A Q = D\\)\uff09\u30012\u6b21\u5f62\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6a19\u6e96\u5f62\u306b\u5909\u63db\u3067\u304d\u307e\u3059\uff1a</p> \\[\\mathbf{x}^T A \\mathbf{x} = \\mathbf{y}^T D \\mathbf{y} = \\sum_{i=1}^n \\lambda_i y_i^2\\] <p>\u3053\u3053\u3067 \\(\\mathbf{y} = Q^T \\mathbf{x}\\) \u3067\u3059\u3002\u3053\u306e\u6a19\u6e96\u5f62\u3092\u4f7f\u3046\u3068\u30012\u6b21\u5f62\u5f0f\u306e\u5e7e\u4f55\u5b66\u7684\u610f\u5473\u304c\u660e\u78ba\u306b\u306a\u308a\u307e\u3059\u3002\u4f8b\u3048\u30702\u6b21\u5143\u306e\u5834\u5408\u3001\u6955\u5186\u3001\u53cc\u66f2\u7dda\u3001\u653e\u7269\u7dda\u306a\u3069\u306e\u30b0\u30e9\u30d5\u3092\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/37-exercise/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/37-exercise/#51","title":"5.1 \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy import linalg as LA\n\n# \u4f8b\u3068\u3057\u3066\u5bfe\u79f0\u884c\u5217\u3092\u751f\u6210\nA = np.array([[4, 1], \n              [1, 3]])\n\n# \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\neigenvalues, eigenvectors = LA.eig(A)\n\nprint(\"\u884c\u5217A:\")\nprint(A)\nprint(\"\\n\u56fa\u6709\u5024:\")\nprint(eigenvalues)\nprint(\"\\n\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u5404\u5217\u304c\u5404\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\uff09:\")\nprint(eigenvectors)\n\n# \u691c\u8a3c: A*v = \u03bb*v\nfor i in range(len(eigenvalues)):\n    v = eigenvectors[:, i]\n    Av = A @ v\n    lv = eigenvalues[i] * v\n    print(f\"\\n\u56fa\u6709\u5024 \u03bb_{i+1} = {eigenvalues[i]:.4f} \u306e\u691c\u8a3c:\")\n    print(f\"A\u30fbv_{i+1} = {Av}\")\n    print(f\"\u03bb_{i+1}\u30fbv_{i+1} = {lv}\")\n    print(f\"\u5dee\u306e\u7d76\u5bfe\u5024: {np.abs(Av - lv).sum():.10f}\")\n</code></pre>"},{"location":"lectures/LA/37-exercise/#52","title":"5.2 \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u53ef\u8996\u5316","text":"<pre><code>def plot_eigenvectors(A):\n    eigenvalues, eigenvectors = LA.eig(A)\n\n    # \u539f\u70b9\u304b\u3089\u59cb\u307e\u308b\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a\u3059\u308b\u305f\u3081\u306e\u6e96\u5099\n    origin = np.zeros(2)\n\n    # \u30b0\u30ea\u30c3\u30c9\u306e\u4f5c\u6210\n    x = np.linspace(-3, 3, 20)\n    y = np.linspace(-3, 3, 20)\n    X, Y = np.meshgrid(x, y)\n\n    # 2\u6b21\u5f62\u5f0f\u306e\u5024\u3092\u8a08\u7b97\n    Z = np.zeros_like(X)\n    for i in range(len(x)):\n        for j in range(len(y)):\n            point = np.array([X[i, j], Y[i, j]])\n            Z[i, j] = point @ A @ point\n\n    plt.figure(figsize=(10, 8))\n\n    # 2\u6b21\u5f62\u5f0f\u306e\u7b49\u9ad8\u7dda\u3092\u30d7\u30ed\u30c3\u30c8\n    contour = plt.contour(X, Y, Z, levels=10, cmap='viridis')\n    plt.colorbar(contour, label='2\u6b21\u5f62\u5f0f\u306e\u5024')\n\n    # \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8868\u793a\uff08\u9577\u3055\u306f\u56fa\u6709\u5024\u306b\u6bd4\u4f8b\uff09\n    for i in range(len(eigenvalues)):\n        scaled_vec = eigenvectors[:, i] * np.sqrt(abs(eigenvalues[i]))\n        plt.arrow(origin[0], origin[1], scaled_vec[0], scaled_vec[1], \n                 head_width=0.1, head_length=0.2, fc=f'C{i}', ec=f'C{i}',\n                 label=f'\u56fa\u6709\u30d9\u30af\u30c8\u30eb {i+1}, \u56fa\u6709\u5024={eigenvalues[i]:.2f}')\n\n    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n    plt.grid(alpha=0.3)\n    plt.legend()\n    plt.title('\u884c\u5217\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u30682\u6b21\u5f62\u5f0f\u306e\u7b49\u9ad8\u7dda')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.axis('equal')\n    plt.show()\n\n# \u4f8b\u3068\u3057\u3066\u6b63\u5b9a\u5024\u884c\u5217\u3092\u53ef\u8996\u5316\nA_positive = np.array([[2, 0.5], \n                       [0.5, 1]])\nplot_eigenvectors(A_positive)\n\n# \u4f8b\u3068\u3057\u3066\u4e0d\u5b9a\u5024\u884c\u5217\u3092\u53ef\u8996\u5316\nA_indefinite = np.array([[1, 2], \n                         [2, -1]])\nplot_eigenvectors(A_indefinite)\n</code></pre>"},{"location":"lectures/LA/37-exercise/#53","title":"5.3 \u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97","text":"<pre><code>def matrix_power_by_diagonalization(A, k):\n    \"\"\"\u5bfe\u89d2\u5316\u306b\u3088\u308b\u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97\"\"\"\n    eigenvalues, P = LA.eig(A)\n\n    # \u5bfe\u89d2\u884c\u5217D^k\u3092\u4f5c\u6210\n    D_k = np.diag(eigenvalues ** k)\n\n    # A^k = P D^k P^(-1)\n    A_k = P @ D_k @ LA.inv(P)\n\n    return A_k\n\n# \u4f8b\u3068\u3057\u3066\nA = np.array([[3, 1], \n              [1, 3]])\nk = 10\n\n# \u5bfe\u89d2\u5316\u306b\u3088\u308bA^k\u306e\u8a08\u7b97\nA_k_diag = matrix_power_by_diagonalization(A, k)\nprint(f\"\u5bfe\u89d2\u5316\u306b\u3088\u308b A^{k}:\")\nprint(A_k_diag)\n\n# \u76f4\u63a5\u8a08\u7b97\u306b\u3088\u308bA^k\u306e\u8a08\u7b97\uff08\u6bd4\u8f03\u7528\uff09\nA_k_direct = np.linalg.matrix_power(A, k)\nprint(f\"\\n\u76f4\u63a5\u8a08\u7b97\u306b\u3088\u308b A^{k}:\")\nprint(A_k_direct)\n\n# \u5dee\u306e\u78ba\u8a8d\nprint(f\"\\n\u4e21\u8005\u306e\u5dee\u306e\u7d76\u5bfe\u5024\u306e\u7dcf\u548c: {np.abs(A_k_diag - A_k_direct).sum():.10f}\")\n</code></pre>"},{"location":"lectures/LA/37-exercise/#54","title":"5.4 \u6570\u5217\u306e\u4e00\u822c\u9805\u306e\u8a08\u7b97\u4f8b\uff1a\u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u5217","text":"<pre><code>def fibonacci_matrix(n):\n    \"\"\"\u884c\u5217\u306b\u3088\u308b\u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u5217\u306en\u9805\u76ee\u306e\u8a08\u7b97\"\"\"\n    if n == 0:\n        return 0\n    if n == 1:\n        return 1\n\n    # \u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u5217\u306e\u6f38\u5316\u5f0f\u3092\u8868\u3059\u884c\u5217\n    A = np.array([[1, 1], \n                  [1, 0]])\n\n    # \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\n    eigenvalues, P = LA.eig(A)\n\n    # \u5bfe\u89d2\u884c\u5217D^(n-1)\u3092\u4f5c\u6210\n    D_pow = np.diag(eigenvalues ** (n-1))\n\n    # A^(n-1) = P D^(n-1) P^(-1)\n    A_pow = P @ D_pow @ LA.inv(P)\n\n    # [F_n, F_{n-1}] = A^(n-1) [F_1, F_0]\n    result = A_pow @ np.array([1, 0])\n\n    return result[0]\n\n# \u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u5217\u306e\u8a08\u7b97\nfor n in range(20):\n    fib_n = fibonacci_matrix(n)\n    print(f\"F_{n} = {fib_n:.0f}\")\n</code></pre>"},{"location":"lectures/LA/37-exercise/#55-2","title":"5.5 2\u6b21\u5f62\u5f0f\u3068\u6b63\u5b9a\u5024\u884c\u5217\u306e\u5224\u5b9a","text":"<pre><code>def check_matrix_definiteness(A):\n    \"\"\"\u884c\u5217\u306e\u6b63\u5b9a\u5024\u6027\u3001\u534a\u6b63\u5b9a\u5024\u6027\u306a\u3069\u3092\u5224\u5b9a\"\"\"\n    # \u5bfe\u79f0\u5316\uff08\u5bfe\u79f0\u3067\u306a\u3044\u5834\u5408\u306e\u305f\u3081\uff09\n    A_sym = (A + A.T) / 2\n\n    # \u56fa\u6709\u5024\u3092\u8a08\u7b97\n    eigenvalues = LA.eigvals(A_sym)\n\n    print(f\"\u884c\u5217:\\n{A}\")\n    print(f\"\u56fa\u6709\u5024: {eigenvalues}\")\n\n    if np.all(eigenvalues &gt; 0):\n        print(\"\u5224\u5b9a\u7d50\u679c: \u6b63\u5b9a\u5024\u884c\u5217\")\n    elif np.all(eigenvalues &gt;= 0):\n        print(\"\u5224\u5b9a\u7d50\u679c: \u534a\u6b63\u5b9a\u5024\u884c\u5217\")\n    elif np.all(eigenvalues &lt; 0):\n        print(\"\u5224\u5b9a\u7d50\u679c: \u8ca0\u5b9a\u5024\u884c\u5217\")\n    elif np.all(eigenvalues &lt;= 0):\n        print(\"\u5224\u5b9a\u7d50\u679c: \u534a\u8ca0\u5b9a\u5024\u884c\u5217\")\n    else:\n        print(\"\u5224\u5b9a\u7d50\u679c: \u4e0d\u5b9a\u5024\u884c\u5217\")\n\n    return eigenvalues\n\n# \u3044\u304f\u3064\u304b\u306e\u884c\u5217\u3067\u8a66\u3059\nmatrices = [\n    np.array([[2, 0], [0, 3]]),                  # \u6b63\u5b9a\u5024\n    np.array([[1, 0], [0, 0]]),                  # \u534a\u6b63\u5b9a\u5024\n    np.array([[-2, 0], [0, -3]]),                # \u8ca0\u5b9a\u5024\n    np.array([[0, 0], [0, -1]]),                 # \u534a\u8ca0\u5b9a\u5024\n    np.array([[1, 0], [0, -1]])                  # \u4e0d\u5b9a\u5024\n]\n\nfor i, A in enumerate(matrices):\n    print(f\"\\n\u4f8b {i+1}:\")\n    check_matrix_definiteness(A)\n</code></pre>"},{"location":"lectures/LA/37-exercise/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/37-exercise/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c1: \u4ee5\u4e0b\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3088\u3002 \\(\\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 1 &amp; 3 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c2: \u4ee5\u4e0b\u306e\u884c\u5217\u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\u3057\u3001\u53ef\u80fd\u306a\u3089\u3070\u5bfe\u89d2\u5316\u305b\u3088\u3002 \\(\\(B = \\begin{pmatrix} 2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c3: \u4ee5\u4e0b\u306e\u884c\u5217\u306e4\u4e57 \\(C^4\\) \u3092\u5bfe\u89d2\u5316\u3092\u5229\u7528\u3057\u3066\u6c42\u3081\u3088\u3002 \\(\\(C = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c4: \u4ee5\u4e0b\u306e2\u6b21\u5f62\u5f0f\u304c\u6b63\u5b9a\u5024\u3067\u3042\u308b\u304b\u3001\u534a\u6b63\u5b9a\u5024\u3067\u3042\u308b\u304b\u3001\u4e0d\u5b9a\u5024\u3067\u3042\u308b\u304b\u3092\u5224\u5b9a\u305b\u3088\u3002 \\(\\(f(x, y) = 2x^2 + 2xy + 2y^2\\)\\)</p> <p>\u554f\u984c5: \u6f38\u5316\u5f0f \\(a_{n+2} = 5a_{n+1} - 6a_n\\) \u306b\u3064\u3044\u3066\u3001\u521d\u671f\u5024 \\(a_0 = 1\\), \\(a_1 = 2\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u3001\\(a_{10}\\) \u306e\u5024\u3092\u5bfe\u89d2\u5316\u3092\u5229\u7528\u3057\u3066\u6c42\u3081\u3088\u3002</p>"},{"location":"lectures/LA/37-exercise/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c6: \u4ee5\u4e0b\u306e\u884c\u5217 \\(D\\) \u306f\u7279\u7570\uff08\\(\\det(D) = 0\\)\uff09\u3067\u3042\u308a\u3001\u30e9\u30f3\u30af\u304c2\u3067\u3042\u308b\u3002\u305d\u306e2\u3064\u306e\u975e\u30bc\u30ed\u56fa\u6709\u5024\u3068\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u7279\u7570\u5024\u5206\u89e3\u306e\u6982\u5ff5\u3092\u4f7f\u3063\u3066 \\(D\\) \u3092\u5206\u89e3\u305b\u3088\u3002 \\(\\(D = \\begin{pmatrix} 1 &amp; 2 &amp; 2 \\\\ 2 &amp; 1 &amp; -1 \\\\ 2 &amp; -1 &amp; 1 \\end{pmatrix}\\)\\)</p> <p>\u554f\u984c7: \u4ee5\u4e0b\u306e\u6f38\u5316\u5f0f\u3067\u5b9a\u7fa9\u3055\u308c\u308b\u6570\u5217 \\(\\{a_n\\}\\) \u306e\u4e00\u822c\u9805\u3092\u6c42\u3081\u3088\u3002 \\(\\(a_{n+3} = 3a_{n+2} - 3a_{n+1} + a_n, \\quad a_0 = 1, a_1 = 2, a_2 = 3\\)\\)</p> <p>\u554f\u984c8: \u4e3b\u6210\u5206\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u679c\u305f\u3059\u5f79\u5272\u306b\u3064\u3044\u3066\u8aac\u660e\u305b\u3088\u3002\u7279\u306b\u3001\u7b2c\u4e00\u4e3b\u6210\u5206\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u95a2\u4fc2\u3092\u8ff0\u3079\u3088\u3002</p> <p>\u554f\u984c9 (\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5fdc\u7528):  \u3042\u308b\u5065\u5eb7\u8a3a\u65ad\u30c7\u30fc\u30bf\u3067\u306f\u3001\u60a3\u8005\u306e\u8eab\u9577\u3001\u4f53\u91cd\u3001\u8840\u5727\u3001\u8840\u7cd6\u5024\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u306e5\u3064\u306e\u6307\u6a19\u3092\u6e2c\u5b9a\u3057\u3066\u3044\u308b\u3002\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u3092\u6a19\u6e96\u5316\u3057\u305f\u5f8c\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u304c [2.5, 1.2, 0.8, 0.3, 0.2] \u3060\u3063\u305f\u3068\u3059\u308b\u3002\u4e3b\u6210\u5206\u5206\u6790\u3092\u9069\u7528\u3059\u308b\u5834\u5408\u3001\u4f55\u500b\u306e\u4e3b\u6210\u5206\u3092\u63a1\u7528\u3059\u308b\u3079\u304d\u304b\u3001\u305d\u306e\u7406\u7531\u3068\u5171\u306b\u8ff0\u3079\u3088\u3002\u307e\u305f\u3001\u56fa\u6709\u5024\u304b\u3089\u8a08\u7b97\u3055\u308c\u308b\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u3092\u6c42\u3081\u3088\u3002</p>"},{"location":"lectures/LA/37-exercise/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":"<p>Q1: \u884c\u5217\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u91cd\u8907\u3059\u308b\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306b\u5bfe\u89d2\u5316\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f</p> <p>A1: \u56fa\u6709\u5024\u304c\u91cd\u8907\u3059\u308b\u5834\u5408\uff08\u591a\u91cd\u56fa\u6709\u5024\uff09\u3001\u305d\u306e\u56fa\u6709\u7a7a\u9593\u306e\u6b21\u5143\u304c\u56fa\u6709\u5024\u306e\u91cd\u8907\u5ea6\u3068\u4e00\u81f4\u3059\u308c\u3070\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u5404\u56fa\u6709\u5024 \\(\\lambda_i\\) \u306b\u5bfe\u3057\u3066\u3001\\((A - \\lambda_i I)\\mathbf{v} = \\mathbf{0}\\) \u306e\u4e00\u822c\u89e3\u3092\u6c42\u3081\u3001\u7dda\u5f62\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u3092\u91cd\u8907\u5ea6\u5206\u3060\u3051\u898b\u3064\u3051\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30d9\u30af\u30c8\u30eb\u3092\u5bfe\u89d2\u5316\u884c\u5217 \\(P\\) \u306e\u5217\u3068\u3057\u3066\u4f7f\u7528\u3057\u307e\u3059\u3002</p> <p>Q2: \u5bfe\u79f0\u884c\u5217\u3067\u306a\u3044\u884c\u5217\u3082\u5bfe\u89d2\u5316\u3067\u304d\u307e\u3059\u304b\uff1f</p> <p>A2: \u306f\u3044\u3001\u5bfe\u89d2\u5316\u306e\u6761\u4ef6\u306f\u300c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u57fa\u5e95\u3092\u5f62\u6210\u3059\u308b\u3053\u3068\u300d\u3067\u3059\u3002\u5bfe\u79f0\u884c\u5217\u306f\u5fc5\u305a\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3059\u304c\u3001\u975e\u5bfe\u79f0\u884c\u5217\u3067\u3082\u6761\u4ef6\u3092\u6e80\u305f\u305b\u3070\u5bfe\u89d2\u5316\u3067\u304d\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u8907\u7d20\u56fa\u6709\u5024\u304c\u73fe\u308c\u308b\u5834\u5408\u306f\u3001\u8907\u7d20\u6570\u306e\u8a08\u7b97\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u5bfe\u79f0\u884c\u5217\u3068\u9055\u3063\u3066\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\u304c\u3067\u304d\u306a\u3044\u5834\u5408\u3082\u3042\u308a\u307e\u3059\u3002</p> <p>Q3: \u5bfe\u89d2\u5316\u3092\u4f7f\u3063\u3066\u884c\u5217\u306e\u3079\u304d\u4e57\u3092\u8a08\u7b97\u3059\u308b\u5229\u70b9\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A3: \u901a\u5e38\u306e\u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97\u306f\u8a08\u7b97\u91cf\u304c \\(O(n^3 \\cdot k)\\) \uff08\\(n\\) \u306f\u884c\u5217\u306e\u6b21\u5143\u3001\\(k\\) \u306f\u3079\u304d\u6570\uff09\u3067\u3059\u304c\u3001\u5bfe\u89d2\u5316\u3092\u4f7f\u3048\u3070 \\(O(n^3 + n \\cdot k)\\) \u306b\u524a\u6e1b\u3067\u304d\u307e\u3059\u3002\u7279\u306b \\(k\\) \u304c\u5927\u304d\u3044\u5834\u5408\u3001\u8a08\u7b97\u52b9\u7387\u304c\u683c\u6bb5\u306b\u5411\u4e0a\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u6570\u5217\u306e\u4e00\u822c\u9805\u3084\u6f38\u5316\u5f0f\u306e\u89e3\u3092\u6c42\u3081\u308b\u969b\u306b\u3082\u975e\u5e38\u306b\u6709\u7528\u3067\u3059\u3002</p> <p>Q4: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3069\u306e\u3088\u3046\u306b\u4f7f\u308f\u308c\u307e\u3059\u304b\uff1f</p> <p>A4: \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5834\u9762\u3067\u4f7f\u308f\u308c\u307e\u3059\uff1a - \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09: \u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u65b9\u5411\uff08\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff09\u3092\u898b\u3064\u3051\u308b - \u30b9\u30da\u30af\u30c8\u30e9\u30eb\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0: \u30b0\u30e9\u30d5\u30e9\u30d7\u30e9\u30b7\u30a2\u30f3\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0 - \u63a8\u85a6\u30b7\u30b9\u30c6\u30e0: \u884c\u5217\u5206\u89e3\u306b\u3088\u308b\u6f5c\u5728\u56e0\u5b50\u306e\u62bd\u51fa - \u753b\u50cf\u51e6\u7406: \u7279\u5fb4\u62bd\u51fa\u3084\u5727\u7e2e\uff08\u56fa\u6709\u9854\u306a\u3069\uff09 - \u81ea\u7136\u8a00\u8a9e\u51e6\u7406: \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u524a\u6e1b\u3084\u6f5c\u5728\u610f\u5473\u89e3\u6790</p> <p>Q5: \u6b63\u5b9a\u5024\u884c\u5217\u306e\u5fdc\u7528\u4f8b\u306b\u306f\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u304b\uff1f</p> <p>A5: \u6b63\u5b9a\u5024\u884c\u5217\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5fdc\u7528\u304c\u3042\u308a\u307e\u3059\uff1a - \u6a5f\u68b0\u5b66\u7fd2\u306b\u304a\u3051\u308b\u640d\u5931\u95a2\u6570\u306e\u51f8\u6027\u4fdd\u8a3c - \u6700\u9069\u5316\u554f\u984c\u306e\u5236\u7d04\u6761\u4ef6 - \u7d71\u8a08\u5b66\u3067\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\uff08\u5e38\u306b\u534a\u6b63\u5b9a\u5024\uff09 - \u30ab\u30fc\u30cd\u30eb\u6cd5\u306b\u304a\u3051\u308b\u30b0\u30e9\u30e0\u884c\u5217\uff08\u534a\u6b63\u5b9a\u5024\uff09 - \u7269\u7406\u5b66\u3067\u306e\u5b89\u5b9a\u6027\u89e3\u6790</p>"},{"location":"lectures/LA/37-exercise/#8","title":"8. \u8ab2\u984c\u306e\u89e3\u7b54\u4f8b","text":"<p>\u554f\u984c1\u306e\u89e3\u7b54:  \u884c\u5217 \\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 1 &amp; 3 \\end{pmatrix}\\) \u306e\u7279\u6027\u65b9\u7a0b\u5f0f\u306f\uff1a</p> \\[\\det(A - \\lambda I) = \\begin{vmatrix} 3-\\lambda &amp; 1 \\\\ 1 &amp; 3-\\lambda \\end{vmatrix} = (3-\\lambda)^2 - 1 = \\lambda^2 - 6\\lambda + 8 = 0\\] <p>\u3053\u308c\u3092\u89e3\u304f\u3068\u3001\\(\\lambda = 2, 4\\) \u304c\u56fa\u6709\u5024\u3068\u3057\u3066\u5f97\u3089\u308c\u307e\u3059\u3002</p> <p>\u56fa\u6709\u5024 \\(\\lambda = 2\\) \u306b\u5bfe\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\uff1a \\(\\((A - 2I)\\mathbf{v} = \\mathbf{0} \\Rightarrow \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix}\\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\)\\)</p> <p>\u3053\u308c\u304b\u3089 \\(v_1 + v_2 = 0\\) \u3068\u306a\u308b\u306e\u3067\u3001\u4f8b\u3048\u3070 \\(\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\) \u304c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u540c\u69d8\u306b\u3001\u56fa\u6709\u5024 \\(\\lambda = 4\\) \u306b\u5bfe\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\uff1a \\(\\((A - 4I)\\mathbf{v} = \\mathbf{0} \\Rightarrow \\begin{pmatrix} -1 &amp; 1 \\\\ 1 &amp; -1 \\end{pmatrix}\\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\)\\)</p> <p>\u3053\u308c\u304b\u3089 \\(v_1 = v_2\\) \u3068\u306a\u308b\u306e\u3067\u3001\u4f8b\u3048\u3070 \\(\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) \u304c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u554f\u984c3\u306e\u89e3\u7b54:  \u884c\u5217 \\(C = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix}\\) \u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\u7279\u6027\u65b9\u7a0b\u5f0f\uff1a\\(\\det(C - \\lambda I) = (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 = 0\\)</p> <p>\u89e3\u304f\u3068 \\(\\lambda = 1, 3\\) \u304c\u56fa\u6709\u5024\u3068\u3057\u3066\u5f97\u3089\u308c\u307e\u3059\u3002</p> <p>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\u3068\u3001\\(\\lambda = 1\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\)\u3001\\(\\lambda = 3\\) \u306b\u5bfe\u3057\u3066 \\(\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) \u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <p>\u5bfe\u89d2\u5316\u884c\u5217\u306f \\(P = \\begin{pmatrix} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{pmatrix}\\) \u3068\u306a\u308a\u3001 \\(P^{-1} = \\frac{1}{2}\\begin{pmatrix} 1 &amp; -1 \\\\ 1 &amp; 1 \\end{pmatrix}\\) \u3067\u3059\u3002</p> <p>\u3088\u3063\u3066\u3001\\(C^4 = P D^4 P^{-1} = P \\begin{pmatrix} 1^4 &amp; 0 \\\\ 0 &amp; 3^4 \\end{pmatrix} P^{-1} = P \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 81 \\end{pmatrix} P^{-1}\\)</p> <p>\u8a08\u7b97\u3059\u308b\u3068\u3001\\(C^4 = \\begin{pmatrix} 41 &amp; 40 \\\\ 40 &amp; 41 \\end{pmatrix}\\) \u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u554f\u984c4\u306e\u89e3\u7b54:  2\u6b21\u5f62\u5f0f \\(f(x, y) = 2x^2 + 2xy + 2y^2\\) \u306b\u5bfe\u5fdc\u3059\u308b\u5bfe\u79f0\u884c\u5217\u306f \\(\\(A = \\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix}\\)\\)</p> <p>\u3053\u306e\u884c\u5217\u306e\u56fa\u6709\u5024\u3092\u6c42\u3081\u308b\u305f\u3081\u3001\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u307e\u3059\uff1a \\(\\(\\det(A - \\lambda I) = \\begin{vmatrix} 2-\\lambda &amp; 1 \\\\ 1 &amp; 2-\\lambda \\end{vmatrix} = (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 = 0\\)\\)</p> <p>\u3053\u308c\u3092\u89e3\u304f\u3068\u3001\\(\\lambda = 1, 3\\) \u304c\u56fa\u6709\u5024\u3068\u3057\u3066\u5f97\u3089\u308c\u307e\u3059\u3002</p> <p>\u4e21\u65b9\u306e\u56fa\u6709\u5024\u304c\u6b63\u3067\u3042\u308b\u305f\u3081\u3001\u884c\u5217 \\(A\\) \u306f\u6b63\u5b9a\u5024\u3067\u3042\u308a\u30012\u6b21\u5f62\u5f0f \\(f(x, y)\\) \u3082\u6b63\u5b9a\u5024\u3067\u3059\u3002\u3053\u308c\u306f\u5e7e\u4f55\u5b66\u7684\u306b\u306f\u3001\u539f\u70b9\u3092\u4e2d\u5fc3\u3068\u3059\u308b\u6955\u5186\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u554f\u984c5\u306e\u89e3\u7b54:  \u6f38\u5316\u5f0f \\(a_{n+2} = 5a_{n+1} - 6a_n\\) \u3092\u884c\u5217\u5f62\u5f0f\u3067\u8868\u73fe\u3057\u307e\u3059\uff1a \\(\\(\\begin{pmatrix} a_{n+2} \\\\ a_{n+1} \\end{pmatrix} = \\begin{pmatrix} 5 &amp; -6 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} a_{n+1} \\\\ a_n \\end{pmatrix}\\)\\)</p> <p>\u884c\u5217 \\(A = \\begin{pmatrix} 5 &amp; -6 \\\\ 1 &amp; 0 \\end{pmatrix}\\) \u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\u7279\u6027\u65b9\u7a0b\u5f0f\uff1a\\(\\det(A - \\lambda I) = (5-\\lambda)(0-\\lambda) - (-6)(1) = \\lambda^2 - 5\\lambda + 6 = 0\\)</p> <p>\u89e3\u304f\u3068 \\(\\lambda = 2, 3\\) \u304c\u56fa\u6709\u5024\u3068\u3057\u3066\u5f97\u3089\u308c\u307e\u3059\u3002</p> <p>\u56fa\u6709\u5024 \\(\\lambda = 2\\) \u306b\u5bfe\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\uff1a \\(\\((A - 2I)\\mathbf{v} = \\mathbf{0} \\Rightarrow \\begin{pmatrix} 3 &amp; -6 \\\\ 1 &amp; -2 \\end{pmatrix}\\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\)\\)</p> <p>\u3053\u308c\u304b\u3089 \\(3v_1 - 6v_2 = 0\\) \u3068\u306a\u308b\u306e\u3067\u3001\\(v_1 = 2v_2\\) \u3067\u3059\u3002\u4f8b\u3048\u3070 \\(\\mathbf{v}_1 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\) \u304c\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u56fa\u6709\u5024 \\(\\lambda = 3\\) \u306b\u5bfe\u3057\u3066\u3082\u540c\u69d8\u306b\u8a08\u7b97\u3059\u308b\u3068\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{v}_2 = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\) \u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <p>\u3053\u308c\u3089\u304b\u3089\u5bfe\u89d2\u5316\u884c\u5217 \\(P = \\begin{pmatrix} 2 &amp; 3 \\\\ 1 &amp; 1 \\end{pmatrix}\\) \u304c\u5f97\u3089\u308c\u307e\u3059\u3002 \u307e\u305f\u3001\\(P^{-1} = \\begin{pmatrix} -1 &amp; 3 \\\\ 1 &amp; -2 \\end{pmatrix}\\) \u3067\u3059\u3002</p> <p>\u521d\u671f\u5024\u304b\u3089\u59cb\u3081\u3066 \\(n\\) \u56de\u306e\u9077\u79fb\u3092\u8003\u3048\u308b\u3068\uff1a \\(\\(\\begin{pmatrix} a_{n+1} \\\\ a_n \\end{pmatrix} = A^n \\begin{pmatrix} a_1 \\\\ a_0 \\end{pmatrix} = P D^n P^{-1} \\begin{pmatrix} a_1 \\\\ a_0 \\end{pmatrix} = P \\begin{pmatrix} 2^n &amp; 0 \\\\ 0 &amp; 3^n \\end{pmatrix} P^{-1} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\)\\)</p> <p>\u8a08\u7b97\u3059\u308b\u3068\uff1a \\(\\(\\begin{pmatrix} a_{n+1} \\\\ a_n \\end{pmatrix} = P \\begin{pmatrix} 2^n &amp; 0 \\\\ 0 &amp; 3^n \\end{pmatrix} \\begin{pmatrix} -1 \\cdot 2 + 3 \\cdot 1 \\\\ 1 \\cdot 2 - 2 \\cdot 1 \\end{pmatrix} = P \\begin{pmatrix} 2^n &amp; 0 \\\\ 0 &amp; 3^n \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = P \\begin{pmatrix} 2^n \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 2^n \\\\ 1 \\cdot 2^n \\end{pmatrix}\\)\\)</p> <p>\u3088\u3063\u3066\u3001\\(a_n = 2^n\\) \u3068\u306a\u308a\u307e\u3059\u3002\u7279\u306b \\(a_{10} = 2^{10} = 1024\\) \u3067\u3059\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c38\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/38-midterm-exam/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c38\u56de \u30c6\u30fc\u30de: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306b\u95a2\u3059\u308b\u4e2d\u9593\u8a66\u9a13 \u95a2\u9023\u9805\u76ee: \u56fa\u6709\u5024\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3001\u5bfe\u89d2\u5316\u30012\u6b21\u5f62\u5f0f \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9:  - \u7b2c32\u56de\u301c\u7b2c37\u56de\u306e\u8b1b\u7fa9\u5185\u5bb9\uff08\u7279\u306b\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u65b9\u6cd5\u3068\u5bfe\u89d2\u5316\u306e\u624b\u9806\uff09 - \u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97\u3068\u6570\u5217\u306e\u4e00\u822c\u9805\u306e\u5c0e\u51fa\u65b9\u6cd5</p>"},{"location":"lectures/LA/38-midterm-exam/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u4e2d\u9593\u8a66\u9a13\u3092\u901a\u3058\u3066\u3001\u4ee5\u4e0b\u306e\u80fd\u529b\u3092\u78ba\u8a8d\u3057\u307e\u3059\uff1a</p> <ol> <li>\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6b63\u78ba\u306b\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u5bfe\u89d2\u5316\u53ef\u80fd\u306a\u884c\u5217\u306b\u5bfe\u3057\u3066\u3001\u5bfe\u89d2\u5316\u306e\u624b\u9806\u3092\u9069\u5207\u306b\u5b9f\u884c\u3067\u304d\u308b</li> <li>\u5bfe\u89d2\u5316\u3092\u5229\u7528\u3057\u3066\u884c\u5217\u306e\u3079\u304d\u4e57\uff08A^n\uff09\u3092\u52b9\u7387\u7684\u306b\u8a08\u7b97\u3067\u304d\u308b</li> <li>\u884c\u5217\u306b\u95a2\u9023\u3059\u308b\u6570\u5217\u306e\u4e00\u822c\u9805\u3092\u5c0e\u51fa\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/38-midterm-exam/#3","title":"3. \u8a66\u9a13\u306e\u6982\u8981","text":"<p>\u4e2d\u9593\u8a66\u9a13\u306f\u4ee5\u4e0b\u306e\u69cb\u6210\u3067\u5b9f\u65bd\u3055\u308c\u307e\u3059\uff1a</p> <ul> <li>\u8a66\u9a13\u6642\u9593: 60\u5206</li> <li>\u554f\u984c\u6570: 4\u554f</li> <li>\u554f\u984c1-2: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u3092\u4e2d\u5fc3\u3068\u3057\u305f\u554f\u984c</li> <li>\u554f\u984c3-4: \u5bfe\u89d2\u5316\u3092\u5229\u7528\u3057\u305f A^n \u3084\u6570\u5217\u306e\u4e00\u822c\u9805\u306e\u8a08\u7b97\u554f\u984c</li> <li>\u914d\u70b9: \u5404\u554f25\u70b9\uff08\u5408\u8a08100\u70b9\uff09</li> <li>\u6301\u3061\u8fbc\u307f: \u81ea\u7b46\u306e\u8b1b\u7fa9\u30ce\u30fc\u30c8\u306e\u307f\u8a31\u53ef\uff08\u96fb\u5b50\u6a5f\u5668\u3001\u6559\u79d1\u66f8\u3001\u30d7\u30ea\u30f3\u30c8\u985e\u306f\u4e0d\u53ef\uff09</li> </ul>"},{"location":"lectures/LA/38-midterm-exam/#4","title":"4. \u8a66\u9a13\u5bfe\u7b56\u30dd\u30a4\u30f3\u30c8","text":""},{"location":"lectures/LA/38-midterm-exam/#41","title":"4.1 \u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97","text":"<p>\u5b9a\u7fa9: \u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\u30bc\u30ed\u3067\u306a\u3044\u30d9\u30af\u30c8\u30eb \\(v\\) \u3068\u3001\u30b9\u30ab\u30e9\u30fc \\(\\lambda\\) \u304c</p> <p>\\(Av = \\lambda v\\)</p> <p>\u3092\u6e80\u305f\u3059\u3068\u304d\u3001\\(\\lambda\\) \u3092\u884c\u5217 \\(A\\) \u306e\u56fa\u6709\u5024\u3001\\(v\\) \u3092\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u547c\u3076\u3002</p> <p>\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\u624b\u9806\uff1a</p> <ol> <li>\u7279\u6027\u65b9\u7a0b\u5f0f \\(det(A - \\lambda I) = 0\\) \u3092\u7acb\u3066\u308b</li> <li>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u3044\u3066\u56fa\u6709\u5024 \\(\\lambda\\) \u3092\u6c42\u3081\u308b</li> <li>\u5404\u56fa\u6709\u5024 \\(\\lambda\\) \u306b\u3064\u3044\u3066\u3001\\((A - \\lambda I)v = 0\\) \u3092\u89e3\u3044\u3066\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(v\\) \u3092\u6c42\u3081\u308b</li> </ol>"},{"location":"lectures/LA/38-midterm-exam/#1-22","title":"\u4f8b\u984c1: 2\u00d72\u884c\u5217\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb","text":"<p>\u884c\u5217 \\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 1 &amp; 3 \\end{pmatrix}\\) \u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3088\u3002</p> <p>\u89e3\u7b54:</p> <ol> <li> <p>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u308b\uff1a    \\(\\(det(A - \\lambda I) = det\\begin{pmatrix} 3-\\lambda &amp; 1 \\\\ 1 &amp; 3-\\lambda \\end{pmatrix} = (3-\\lambda)^2 - 1 = 0\\)\\)</p> </li> <li> <p>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\uff1a    \\((3-\\lambda)^2 = 1\\)\u3088\u308a\u3001\\(\\lambda = 2\\) \u307e\u305f\u306f \\(\\lambda = 4\\)</p> </li> <li> <p>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\uff1a</p> </li> </ol> <p>\\(\\lambda = 2\\) \u306e\u3068\u304d\uff1a    \\((A - 2I)v = \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix}v = 0\\)    \u3053\u306e\u540c\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3068\u3001\\(v = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\\) \uff08\u307e\u305f\u306f\u5b9a\u6570\u500d\uff09\u304c\u5f97\u3089\u308c\u308b\u3002</p> <p>\\(\\lambda = 4\\) \u306e\u3068\u304d\uff1a    \\((A - 4I)v = \\begin{pmatrix} -1 &amp; 1 \\\\ 1 &amp; -1 \\end{pmatrix}v = 0\\)    \u3053\u306e\u540c\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3068\u3001\\(v = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) \uff08\u307e\u305f\u306f\u5b9a\u6570\u500d\uff09\u304c\u5f97\u3089\u308c\u308b\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#2-33","title":"\u4f8b\u984c2: 3\u00d73\u884c\u5217\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb","text":"<p>\u884c\u5217 \\(A = \\begin{pmatrix} 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 4 \\\\ 0 &amp; 1 &amp; 2 \\end{pmatrix}\\) \u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3088\u3002</p> <p>\u89e3\u7b54:</p> <ol> <li> <p>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u308b\uff1a    \\(\\(det(A - \\lambda I) = det\\begin{pmatrix} 2-\\lambda &amp; 0 &amp; 0 \\\\ 0 &amp; 3-\\lambda &amp; 4 \\\\ 0 &amp; 1 &amp; 2-\\lambda \\end{pmatrix}    = (2-\\lambda)\u00b7det\\begin{pmatrix} 3-\\lambda &amp; 4 \\\\ 1 &amp; 2-\\lambda \\end{pmatrix}    = (2-\\lambda)\u00b7((3-\\lambda)(2-\\lambda) - 4)    = (2-\\lambda)\u00b7((3-\\lambda)(2-\\lambda) - 4\u00b71)    = (2-\\lambda)\u00b7(6-3\\lambda-2\\lambda+\\lambda^2 - 4)    = (2-\\lambda)\u00b7(\\lambda^2 - 5\\lambda + 2)    = 0\\)\\)</p> </li> <li> <p>\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\uff1a    \\((2-\\lambda) = 0 \u307e\u305f\u306f \\lambda^2 - 5\\lambda + 2 = 0    \\lambda = 2\\) \u307e\u305f\u306f \\((\\lambda - 2)(\\lambda - 1) = 0\\)    \u3088\u3063\u3066\u3001\\(\\lambda = 2\\)\uff08\u91cd\u6839\uff09\u307e\u305f\u306f \\(\\lambda = 1\\)</p> </li> <li> <p>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\uff1a</p> </li> </ol> <p>\\(\\(\\lambda = 2 \u306e\u3068\u304d\uff1a    (A - 2I)v = \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 4 \\\\ 0 &amp; 1 &amp; 0 \\end{pmatrix}v = 0\\)\\)</p> <p>\u3053\u306e\u540c\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3068\u3001\\(\\(v_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \u3068 v_2 = \\begin{pmatrix} 0 \\\\ -4 \\\\ 1 \\end{pmatrix}\\)\\) \uff08\u307e\u305f\u306f\u305d\u308c\u3089\u306e\u7dda\u5f62\u7d50\u5408\uff09\u304c\u5f97\u3089\u308c\u308b\u3002</p> <p>\\(\\lambda = 1\\) \u306e\u3068\u304d\uff1a    \\(\\((A - I)v = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 4 \\\\ 0 &amp; 1 &amp; 1 \\end{pmatrix}v = 0\\)\\)</p> <p>\u3053\u306e\u540c\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3068\u3001\\(\\(v_3 = \\begin{pmatrix} 0 \\\\ -2 \\\\ 1 \\end{pmatrix}\\)\\) \uff08\u307e\u305f\u306f\u5b9a\u6570\u500d\uff09\u304c\u5f97\u3089\u308c\u308b\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#42","title":"4.2 \u884c\u5217\u306e\u5bfe\u89d2\u5316","text":"<p>\u5b9a\u7fa9: \\(n\u00d7n\\) \u884c\u5217 \\(A\\) \u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308b\u3068\u306f\u3001\u6b63\u5247\u884c\u5217 \\(P\\) \u3068\u5bfe\u89d2\u884c\u5217 \\(D\\) \u304c\u5b58\u5728\u3057\u3066\u3001</p> <p>\\(P^{-1}AP = D\\)</p> <p>\u3068\u8868\u305b\u308b\u3053\u3068\u3092\u3044\u3046\u3002\u3053\u306e\u3068\u304d\u3001\\(D\\) \u306e\u5bfe\u89d2\u6210\u5206\u306f \\(A\\) \u306e\u56fa\u6709\u5024\u3067\u3042\u308a\u3001\\(P\\) \u306e\u5217\u30d9\u30af\u30c8\u30eb\u306f \\(A\\) \u306e\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b\u3002</p> <p>\u5bfe\u89d2\u5316\u306e\u624b\u9806\uff1a</p> <ol> <li>\u884c\u5217 A \u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b</li> <li>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u5217\u3068\u3059\u308b\u884c\u5217 P \u3092\u4f5c\u308b</li> <li>P^{-1}AP = D \u3068\u306a\u308b\u5bfe\u89d2\u884c\u5217 D \u3092\u6c42\u3081\u308b\uff08D \u306e\u5bfe\u89d2\u6210\u5206\u306f A \u306e\u56fa\u6709\u5024\uff09</li> </ol>"},{"location":"lectures/LA/38-midterm-exam/#3_1","title":"\u4f8b\u984c3: \u884c\u5217\u306e\u5bfe\u89d2\u5316","text":"<p>\u884c\u5217 A = \\begin{pmatrix} 4 &amp; -3 \\ 2 &amp; -1 \\end{pmatrix} \u3092\u5bfe\u89d2\u5316\u305b\u3088\u3002</p> <p>\u89e3\u7b54:</p> <ol> <li>\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b\uff1a</li> </ol> <p>\u7279\u6027\u65b9\u7a0b\u5f0f\uff1a\\(det(A - \\lambda I) = det\\begin{pmatrix} 4-\\lambda &amp; -3 \\\\ 2 &amp; -1-\\lambda \\end{pmatrix}    = (4-\\lambda)(-1-\\lambda) - (-3)(2)    = -4-4\\lambda+\\lambda+\\lambda^2 - (-6)    = \\lambda^2 - 3\\lambda - 2 = 0\\)</p> <p>\u89e3\u304f\u3068\u3001\\(\\lambda = -1, 2\\)</p> <p>\\(\\lambda = -1\\) \u306e\u3068\u304d\uff1a    $$(A - (-1)I)v = \\begin{pmatrix} 5 &amp; -3 \\ 2 &amp; 0 \\end{pmatrix}v = 0    $$    \u3053\u306e\u540c\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3068\u3001\\(v_1 = \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix}\\) \uff08\u307e\u305f\u306f\u5b9a\u6570\u500d\uff09\u304c\u5f97\u3089\u308c\u308b\u3002</p> <p>\\(\\lambda = 2\\) \u306e\u3068\u304d\uff1a    \\((A - 2I)v = \\begin{pmatrix} 2 &amp; -3 \\\\ 2 &amp; -3 \\end{pmatrix}v = 0\\)</p> <p>\u3053\u306e\u540c\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3068\u3001\\(v_2 = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\\) \uff08\u307e\u305f\u306f\u5b9a\u6570\u500d\uff09\u304c\u5f97\u3089\u308c\u308b\u3002</p> <ol> <li> <p>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u5217\u3068\u3059\u308b\u884c\u5217 P \u3092\u4f5c\u308b\uff1a    \\(P = \\begin{pmatrix} 3 &amp; 3 \\\\ 5 &amp; 2 \\end{pmatrix}\\)</p> </li> <li> <p>\u5bfe\u89d2\u884c\u5217 D \u3092\u6c42\u3081\u308b\uff1a    \\(D = P^{-1}AP = \\begin{pmatrix} -1 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix}\\)</p> </li> </ol>"},{"location":"lectures/LA/38-midterm-exam/#43","title":"4.3 \u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97","text":"<p>\u5bfe\u89d2\u5316\u53ef\u80fd\u306a\u884c\u5217 A = PDP^{-1} \u306b\u5bfe\u3057\u3066\u3001A \u306e n \u4e57\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3067\u304d\u308b\uff1a</p> <p>A^n = (PDP^{-1})^n = PD^nP^{-1}</p> <p>\u3053\u3053\u3067\u3001D^n \u306f\u5bfe\u89d2\u884c\u5217\u3067\u3042\u308b\u305f\u3081\u3001\u5404\u5bfe\u89d2\u6210\u5206\u306e n \u4e57\u3092\u8a08\u7b97\u3059\u308b\u3060\u3051\u3067\u5f97\u3089\u308c\u308b\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#4_1","title":"\u4f8b\u984c4: \u884c\u5217\u306e\u3079\u304d\u4e57","text":"<p>\u524d\u554f\u3067\u5bfe\u89d2\u5316\u3057\u305f\u884c\u5217 \\(A = \\begin{pmatrix} 4 &amp; -3 \\\\ 2 &amp; -1 \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u3001\\(A^{10}\\) \u3092\u6c42\u3081\u3088\u3002</p> <p>\u89e3\u7b54:</p> <p>\\(A = PDP^{-1}\\) \u3068\u5bfe\u89d2\u5316\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001 \\(A^{10} = PD^{10}P^{-1}\\)</p> \\[D^{10} = \\begin{pmatrix} (-1)^{10} &amp; 0 \\\\ 0 &amp; 2^{10} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1024 \\end{pmatrix}\\] <p>\\(P = \\begin{pmatrix} 3 &amp; 3 \\\\ 5 &amp; 2 \\end{pmatrix}\u3001P^{-1}\\) \u3092\u8a08\u7b97\u3059\u308b\u3068\u3001 \\(P^{-1} = \\frac{1}{det(P)} \\begin{pmatrix} 2 &amp; -3 \\\\ -5 &amp; 3 \\end{pmatrix} = \\frac{1}{-9} \\begin{pmatrix} 2 &amp; -3 \\\\ -5 &amp; 3 \\end{pmatrix}\\)</p> <p>\u5f93\u3063\u3066\u3001 \\(A^{10} = P \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1024 \\end{pmatrix} P^{-1}       = \\begin{pmatrix} 3 &amp; 3 \\\\ 5 &amp; 2 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1024 \\end{pmatrix} \\frac{1}{-9} \\begin{pmatrix} 2 &amp; -3 \\\\ -5 &amp; 3 \\end{pmatrix}\\)</p> <p>\u884c\u5217\u306e\u7a4d\u3092\u8a08\u7b97\u3059\u308b\u3068\u3001\u6700\u7d42\u7684\u306b \\(A^{10}\\) \u304c\u5f97\u3089\u308c\u308b\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#44","title":"4.4 \u6570\u5217\u306e\u4e00\u822c\u9805\u306e\u8a08\u7b97","text":"<p>\u884c\u5217\u3092\u7528\u3044\u3066\u8868\u3055\u308c\u308b\u6f38\u5316\u5f0f\u304b\u3089\u6570\u5217\u306e\u4e00\u822c\u9805\u3092\u6c42\u3081\u308b\u554f\u984c\u3082\u3088\u304f\u51fa\u984c\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#5","title":"\u4f8b\u984c5: \u6570\u5217\u306e\u4e00\u822c\u9805","text":"<p>\u6f38\u5316\u5f0f \\(a_{n+2} = 5a_{n+1} - 6a_n\\) \u304c\u4e0e\u3048\u3089\u308c\u3001\u521d\u671f\u5024 \\(a_0 = 2, a_1 = 5\\) \u306e\u3068\u304d\u3001\u4e00\u822c\u9805$ a_n $\u3092\u6c42\u3081\u3088\u3002</p> <p>\u89e3\u7b54:</p> <ol> <li>\u884c\u5217\u5f62\u5f0f\u3067\u8868\u73fe\u3059\u308b\uff1a    \\(\\begin{pmatrix} a_{n+1} \\\\ a_{n+2} \\end{pmatrix} = \\begin{pmatrix} 0 &amp; 1 \\\\ 6 &amp; 5 \\end{pmatrix} \\begin{pmatrix} a_n \\\\ a_{n+1} \\end{pmatrix}\\)</li> </ol> <p>\u3059\u306a\u308f\u3061\u3001\\(\\begin{pmatrix} a_{n+1} \\\\ a_{n+2} \\end{pmatrix} = A \\begin{pmatrix} a_n \\\\ a_{n+1} \\end{pmatrix} \uff08A = \\begin{pmatrix} 0 &amp; 1 \\\\ 6 &amp; 5 \\end{pmatrix}\uff09\\)</p> <ol> <li> <p>\u521d\u671f\u5024\u304b\u3089\u3001\\(\\begin{pmatrix} a_n \\\\ a_{n+1} \\end{pmatrix} = A^n \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix} = A^n \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}\\)</p> </li> <li> <p>\u884c\u5217 \\(A\\) \u3092\u5bfe\u89d2\u5316\u3059\u308b\uff1a    \u7279\u6027\u65b9\u7a0b\u5f0f\uff1a    $$ det(A - \\lambda I) = det \\begin{pmatrix} -\\lambda &amp; 1 \\ 6 &amp; 5 - \\lambda \\end{pmatrix}    = (-\\lambda)(5-\\lambda) - 1\u00b76    = -5\\lambda + \\lambda^2 - 6    = \\lambda^2 - 5\\lambda - 6 = 0$$</p> </li> </ol> <p>\u89e3\u304f\u3068\u3001\\(\\lambda = 6, -1\\)</p> <p>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u5bfe\u89d2\u5316\u884c\u5217\u3092\u69cb\u6210\uff1a    \\(P = \\begin{pmatrix} 1 &amp; 1 \\\\ 6 &amp; -1 \\end{pmatrix}\u3001D = \\begin{pmatrix} 6 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix}\\)</p> <ol> <li> <p>\\(A^n\\) \u3092\u8a08\u7b97\uff1a    \\(A^n = PD^nP^{-1} = P \\begin{pmatrix} 6^n &amp; 0 \\\\ 0 &amp; (-1)^n \\end{pmatrix} P^{-1}\\)</p> </li> <li> <p>\u4e00\u822c\u9805 \\(a_n\\) \u306e\u5c0e\u51fa\uff1a    \\(\\begin{pmatrix} a_n \\\\ a_{n+1} \\end{pmatrix} = A^n \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix}\\)</p> </li> </ol> <p>\u8a08\u7b97\u3092\u9032\u3081\u308b\u3068\u3001\\(a_n = \\frac{1}{7}(2\u00b76^n + 5\u00b7(-1)^n)\\) \u304c\u5f97\u3089\u308c\u308b\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#5_1","title":"5. \u8a66\u9a13\u554f\u984c\u30b5\u30f3\u30d7\u30eb","text":"<p>\u4ee5\u4e0b\u306b\u3001\u4e2d\u9593\u8a66\u9a13\u3067\u51fa\u984c\u3055\u308c\u308b\u3088\u3046\u306a\u554f\u984c\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u793a\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#1_1","title":"\u554f\u984c1: \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97","text":"<p>\u884c\u5217 A = \\begin{pmatrix} 1 &amp; 2 &amp; 0 \\ 2 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 3 \\end{pmatrix} \u306e\u56fa\u6709\u5024\u3068\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u3059\u3079\u3066\u6c42\u3081\u3088\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#2_1","title":"\u554f\u984c2: \u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316","text":"<p>\u5bfe\u79f0\u884c\u5217 B = \\begin{pmatrix} 2 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 1 \\ 1 &amp; 1 &amp; 2 \\end{pmatrix} \u3092\u76f4\u4ea4\u884c\u5217\u306b\u3088\u308b\u5bfe\u89d2\u5316\u3092\u884c\u3044\u3001B = PDP^T \u3092\u6c42\u3081\u3088\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#3_2","title":"\u554f\u984c3: \u884c\u5217\u306e\u3079\u304d\u4e57","text":"<p>\u884c\u5217 C = \\begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \\end{pmatrix} \u306b\u5bfe\u3057\u3066\u3001C^{20} \u3092\u6c42\u3081\u3088\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#4_2","title":"\u554f\u984c4: \u6570\u5217\u306e\u4e00\u822c\u9805","text":"<p>\u6f38\u5316\u5f0f x_{n+2} = 6x_{n+1} - 9x_n \u304c\u4e0e\u3048\u3089\u308c\u3001\u521d\u671f\u5024 x_0 = 1, x_1 = 3 \u306e\u3068\u304d\u3001\u4e00\u822c\u9805 x_n \u3092\u6c42\u3081\u3088\u3002\u307e\u305f\u3001x_{100} \u306e\u5024\u3092\u8a08\u7b97\u305b\u3088\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#6-python","title":"6. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u6ce8\u610f: \u8a66\u9a13\u3067\u306fPython\u306e\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u554f\u984c\u306f\u51fa\u984c\u3055\u308c\u307e\u305b\u3093\u304c\u3001\u7406\u89e3\u3092\u6df1\u3081\u308b\u305f\u3081\u306b\u3001\u4ee5\u4e0b\u306b\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u3068\u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97\u306e\u5b9f\u88c5\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[4, -3], [2, -1]])\n\n# \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(\"\u56fa\u6709\u5024:\", eigenvalues)\nprint(\"\u56fa\u6709\u30d9\u30af\u30c8\u30eb:\\n\", eigenvectors)\n\n# \u5bfe\u89d2\u5316\u306e\u78ba\u8a8d\nD = np.diag(eigenvalues)\nP = eigenvectors\nP_inv = np.linalg.inv(P)\n\n# P^{-1}AP = D \u3092\u78ba\u8a8d\nresult = np.matmul(np.matmul(P_inv, A), P)\nprint(\"P^{-1}AP =\\n\", np.round(result, 10))  # \u4e38\u3081\u8aa4\u5dee\u3092\u8003\u616e\n\n# \u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97\ndef matrix_power(A, n):\n    \"\"\"\u884c\u5217 A \u306e n \u4e57\u3092\u8a08\u7b97\u3059\u308b\"\"\"\n    eigenvalues, P = np.linalg.eig(A)\n    P_inv = np.linalg.inv(P)\n    D_n = np.diag(eigenvalues ** n)\n    return np.matmul(np.matmul(P, D_n), P_inv)\n\n# A^10 \u306e\u8a08\u7b97\nA_10 = matrix_power(A, 10)\nprint(\"A^10 =\\n\", np.round(A_10, 2))\n\n# \u76f4\u63a5\u8a08\u7b97\u3068\u306e\u6bd4\u8f03\nA_10_direct = np.linalg.matrix_power(A, 10)\nprint(\"A^10 (\u76f4\u63a5\u8a08\u7b97) =\\n\", np.round(A_10_direct, 2))\n\n# \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u53ef\u8996\u5316\uff082\u6b21\u5143\u306e\u5834\u5408\uff09\nplt.figure(figsize=(8, 6))\nplt.grid(True)\n\n# \u5143\u306e\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\nplt.arrow(0, 0, 1, 0, head_width=0.1, head_length=0.1, fc='blue', ec='blue', label='e1')\nplt.arrow(0, 0, 0, 1, head_width=0.1, head_length=0.1, fc='blue', ec='blue', label='e2')\n\n# \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\nfor i, v in enumerate(eigenvectors.T):\n    v_normalized = v / np.linalg.norm(v) * 2  # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n    plt.arrow(0, 0, v_normalized[0], v_normalized[1], \n              head_width=0.1, head_length=0.1, \n              fc='red', ec='red', \n              label=f'eigenvector {i+1} (\\lambda={eigenvalues[i]:.1f})')\n\n# \u5909\u63db\u5f8c\u306e\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\ne1_transformed = A @ np.array([1, 0])\ne2_transformed = A @ np.array([0, 1])\nplt.arrow(0, 0, e1_transformed[0], e1_transformed[1], \n          head_width=0.1, head_length=0.1, \n          fc='green', ec='green', \n          label='A\u00b7e1')\nplt.arrow(0, 0, e2_transformed[0], e2_transformed[1], \n          head_width=0.1, head_length=0.1, \n          fc='green', ec='green', \n          label='A\u00b7e2')\n\nplt.xlim(-3, 5)\nplt.ylim(-3, 5)\nplt.legend()\nplt.title('Eigenvectors and Transformed Basis')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.axis('equal')\nplt.show()\n</code></pre>"},{"location":"lectures/LA/38-midterm-exam/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/38-midterm-exam/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} 3 &amp; 2 \\\\ 4 &amp; -1 \\end{pmatrix}\\) \u306e\u56fa\u6709\u5024\u3068\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\u884c\u5217 \\(B = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 2 \\end{pmatrix}\\) \u3092\u5bfe\u89d2\u5316\u305b\u3088\u3002</p> </li> <li> <p>\u554f\u984c2\u3067\u5bfe\u89d2\u5316\u3057\u305f\u884c\u5217 \\(B\\) \u306b\u3064\u3044\u3066\u3001\\(B^5\\) \u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\u6f38\u5316\u5f0f \\(a_{n+2} = 4a_{n+1} - 4a_n\\) \u304c\u4e0e\u3048\u3089\u308c\u3001\u521d\u671f\u5024 \\(a_0 = 1, a_1 = 2\\) \u306e\u3068\u304d\u3001\u4e00\u822c\u9805 \\(a_n\\) \u3092\u6c42\u3081\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/38-midterm-exam/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u884c\u5217 \\(\\(C = \\begin{pmatrix} 4 &amp; -1 &amp; 0 \\\\ -1 &amp; 4 &amp; -1 \\\\ 0 &amp; -1 &amp; 4 \\end{pmatrix}\\)\\) \u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u5bfe\u89d2\u5316\u305b\u3088\u3002</p> </li> <li> <p>\u6f38\u5316\u5f0f \\(f_{n+2} = f_{n+1} + f_n\\) \u304c\u4e0e\u3048\u3089\u308c\u3001\u521d\u671f\u5024 \\(f_0 = 0, f_1 = 1\\) \u306e\u3068\u304d\uff08\u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u5217\uff09\u3001\u884c\u5217\u306e\u5bfe\u89d2\u5316\u3092\u7528\u3044\u3066\u4e00\u822c\u9805 \\(f_n\\) \u3092\u6c42\u3081\u3088\u3002\u307e\u305f\u3001\\(f_{20}\\) \u306e\u5024\u3092\u8a08\u7b97\u305b\u3088\u3002</p> </li> <li> <p>\u884c\u5217 \\(D = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 1 &amp; -3 &amp; 3 \\end{pmatrix}\\) \u306b\u3064\u3044\u3066\u3001\\(D^n\\) \u306e\u4e00\u822c\u5f62\u3092\u6c42\u3081\u3088\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u95a2\u3059\u308b\u5fdc\u7528\u554f\u984c\uff1a\u3042\u308b\u4fdd\u5065\u6240\u306e\u9031\u3054\u3068\u306e\u611f\u67d3\u75c7\u60a3\u8005\u6570\u306e\u63a8\u79fb\u304c\u6f38\u5316\u5f0f \\(p_{n+2} = 1.5 p_{n+1} - 0.5 p_n\\) \u3067\u8868\u3055\u308c\u308b\u3068\u4eee\u5b9a\u3059\u308b\u3002\u7b2c1\u9031\u3068\u7b2c2\u9031\u306e\u60a3\u8005\u6570\u304c\u305d\u308c\u305e\u308c 10\u4eba\u300115\u4eba\u3067\u3042\u3063\u305f\u3068\u304d\u3001\u7b2cn\u9031\u306e\u60a3\u8005\u6570 \\(p_n\\) \u306e\u4e00\u822c\u5f0f\u3092\u6c42\u3081\u3001\u7b2c10\u9031\u306e\u60a3\u8005\u6570\u3092\u4e88\u6e2c\u305b\u3088\u3002\u307e\u305f\u3001\u3053\u306e\u611f\u67d3\u75c7\u306e\u62e1\u5927\u50be\u5411\u306b\u3064\u3044\u3066\u8003\u5bdf\u305b\u3088\u3002</p> </li> </ol>"},{"location":"lectures/LA/38-midterm-exam/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/38-midterm-exam/#q1","title":"Q1: \u56fa\u6709\u5024\u304c\u91cd\u6839\u306e\u5834\u5408\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3069\u306e\u3088\u3046\u306b\u6c42\u3081\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A1: \u56fa\u6709\u5024\u304c\u4ee3\u6570\u7684\u91cd\u8907\u5ea6 k \u306e\u91cd\u6839\u306e\u5834\u5408\u3001\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5e7e\u4f55\u7684\u91cd\u8907\u5ea6\uff08\u7dda\u5f62\u72ec\u7acb\u306a\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u6570\uff09\u306f k \u4ee5\u4e0b\u3068\u306a\u308a\u307e\u3059\u3002(A - \\lambdaI)v = 0 \u3092\u89e3\u3044\u3066\u3001\u7dda\u5f62\u72ec\u7acb\u306a\u89e3\u30d9\u30af\u30c8\u30eb\u3092\u3059\u3079\u3066\u6c42\u3081\u307e\u3059\u3002\u3082\u3057\u5e7e\u4f55\u7684\u91cd\u8907\u5ea6\u304c\u4ee3\u6570\u7684\u91cd\u8907\u5ea6\u3088\u308a\u5c0f\u3055\u3044\u5834\u5408\u3001\u884c\u5217\u306f\u5bfe\u89d2\u5316\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u5834\u5408\u306f\u30b8\u30e7\u30eb\u30c0\u30f3\u6a19\u6e96\u5f62\u3092\u8003\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#q2","title":"Q2: \u5bfe\u89d2\u5316\u53ef\u80fd\u306e\u6761\u4ef6\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A2: n\u00d7n\u884c\u5217 A \u304c\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308b\u305f\u3081\u306e\u5fc5\u8981\u5341\u5206\u6761\u4ef6\u306f\u3001A \u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c n \u500b\u306e\u7dda\u5f62\u72ec\u7acb\u306a\u30d9\u30af\u30c8\u30eb\u3092\u5f62\u6210\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u3053\u308c\u306f\u3001\u5404\u56fa\u6709\u5024 \\lambda \u306b\u3064\u3044\u3066\u3001\u305d\u306e\u5e7e\u4f55\u7684\u91cd\u8907\u5ea6\uff08\u5bfe\u5fdc\u3059\u308b\u7dda\u5f62\u72ec\u7acb\u306a\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u6570\uff09\u304c\u4ee3\u6570\u7684\u91cd\u8907\u5ea6\uff08\u7279\u6027\u65b9\u7a0b\u5f0f\u306e\u6839\u3068\u3057\u3066\u306e\u91cd\u8907\u5ea6\uff09\u3068\u7b49\u3057\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#q3","title":"Q3: \u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316\u306b\u3064\u3044\u3066\u7279\u5225\u306a\u6027\u8cea\u306f\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A3: \u5b9f\u5bfe\u79f0\u884c\u5217 A \u306f\u5e38\u306b\u5bfe\u89d2\u5316\u53ef\u80fd\u3067\u3042\u308a\u3001\u305d\u306e\u56fa\u6709\u5024\u306f\u3059\u3079\u3066\u5b9f\u6570\u3067\u3059\u3002\u307e\u305f\u3001\u7570\u306a\u308b\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3057\u3066\u3044\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001A = PDP^T \u3068\u306a\u308b\u76f4\u4ea4\u884c\u5217 P\uff08P^T = P^{-1}\uff09\u304c\u5b58\u5728\u3057\u307e\u3059\u3002\u3053\u308c\u3092\u30b9\u30da\u30af\u30c8\u30eb\u5206\u89e3\u3068\u547c\u3073\u307e\u3059\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#q4","title":"Q4: \u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97\u306b\u304a\u3044\u3066\u5bfe\u89d2\u5316\u3092\u5229\u7528\u3059\u308b\u5229\u70b9\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A4: \u5bfe\u89d2\u5316\u3092\u5229\u7528\u3059\u308b\u3068\u3001A^n = PD^nP^{-1} \u3068\u8868\u73fe\u3067\u304d\u308b\u305f\u3081\u3001D \u304c\u5bfe\u89d2\u884c\u5217\u3067\u3042\u308b\u3053\u3068\u3092\u5229\u7528\u3057\u3066 D^n \u3092\u7c21\u5358\u306b\u8a08\u7b97\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001n \u304c\u5927\u304d\u304f\u306a\u3063\u3066\u3082\u52b9\u7387\u7684\u306b A^n \u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u76f4\u63a5 A \u3092 n \u56de\u639b\u3051\u5408\u308f\u305b\u308b\u65b9\u6cd5\u3067\u306f\u8a08\u7b97\u91cf\u304c O(n) \u306b\u306a\u308a\u307e\u3059\u304c\u3001\u5bfe\u89d2\u5316\u3092\u5229\u7528\u3059\u308b\u3068 O(log n) \u306b\u524a\u6e1b\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#q5","title":"Q5: \u6570\u5217\u306e\u6f38\u5316\u5f0f\u304b\u3089\u4e00\u822c\u9805\u3092\u6c42\u3081\u308b\u65b9\u6cd5\u306f\u307b\u304b\u306b\u3082\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A5: \u306f\u3044\u3001\u7279\u6027\u65b9\u7a0b\u5f0f\u3092\u7acb\u3066\u3066\u89e3\u304f\u65b9\u6cd5\u3082\u3042\u308a\u307e\u3059\u3002k \u968e\u306e\u7dda\u5f62\u6f38\u5316\u5f0f a_{n+k} + c_{k-1}a_{n+k-1} + ... + c_0 a_n = 0 \u306b\u5bfe\u3057\u3066\u3001\u7279\u6027\u65b9\u7a0b\u5f0f r^k + c_{k-1}r^{k-1} + ... + c_0 = 0 \u3092\u89e3\u304d\u3001\u305d\u306e\u89e3\u3092\u7528\u3044\u3066\u4e00\u822c\u9805\u3092\u8868\u73fe\u3057\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u7279\u6027\u65b9\u7a0b\u5f0f\u306b\u91cd\u6839\u304c\u3042\u308b\u5834\u5408\u306f\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/38-midterm-exam/#9","title":"9. \u8a66\u9a13\u306b\u5411\u3051\u305f\u30a2\u30c9\u30d0\u30a4\u30b9","text":"<ol> <li> <p>\u57fa\u672c\u6982\u5ff5\u306e\u7406\u89e3: \u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5b9a\u7fa9\u3068\u6027\u8cea\u3092\u6b63\u78ba\u306b\u7406\u89e3\u3057\u307e\u3057\u3087\u3046\u3002</p> </li> <li> <p>\u8a08\u7b97\u7df4\u7fd2: 2\u00d72\u884c\u5217\u30013\u00d73\u884c\u5217\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u3092\u4f55\u5ea6\u3082\u7df4\u7fd2\u3057\u307e\u3057\u3087\u3046\u3002</p> </li> <li> <p>\u7279\u6027\u65b9\u7a0b\u5f0f\u306e\u7acb\u3066\u65b9: det(A - \\lambdaI) = 0 \u306e\u8a08\u7b97\u3092\u52b9\u7387\u7684\u306b\u884c\u3046\u65b9\u6cd5\u3092\u7fd2\u5f97\u3057\u307e\u3057\u3087\u3046\u3002</p> </li> <li> <p>\u5bfe\u89d2\u5316\u306e\u624b\u9806: \u5bfe\u89d2\u5316\u306e\u5404\u30b9\u30c6\u30c3\u30d7\u3092\u78ba\u5b9f\u306b\u5b9f\u884c\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3057\u3087\u3046\u3002</p> </li> <li> <p>\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8: \u5bfe\u89d2\u5316\u306e\u7d50\u679c\u304c\u6b63\u3057\u3044\u304b\u3092 P^{-1}AP = D \u306e\u8a08\u7b97\u3067\u78ba\u8a8d\u3059\u308b\u7fd2\u6163\u3092\u3064\u3051\u307e\u3057\u3087\u3046\u3002</p> </li> <li> <p>\u5fdc\u7528\u554f\u984c\u306e\u5bfe\u7b56: \u884c\u5217\u306e\u3079\u304d\u4e57\u8a08\u7b97\u3068\u6570\u5217\u306e\u4e00\u822c\u9805\u5c0e\u51fa\u306e\u554f\u984c\u3092\u89e3\u304f\u7df4\u7fd2\u3092\u3057\u307e\u3057\u3087\u3046\u3002</p> </li> <li> <p>\u8a08\u7b97\u30df\u30b9\u3092\u9632\u3050: \u884c\u5217\u5f0f\u3084\u9006\u884c\u5217\u306e\u8a08\u7b97\u3067\u306f\u7b26\u53f7\u306e\u30df\u30b9\u306b\u6ce8\u610f\u3057\u307e\u3057\u3087\u3046\u3002</p> </li> <li> <p>\u6642\u9593\u914d\u5206: \u8a66\u9a13\u4e2d\u306f\u554f\u984c\u3054\u3068\u306b\u9069\u5207\u306a\u6642\u9593\u914d\u5206\u3092\u5fc3\u304c\u3051\u307e\u3057\u3087\u3046\u3002\u3059\u3079\u3066\u306e\u554f\u984c\u306b\u53d6\u308a\u7d44\u3081\u308b\u3088\u3046\u306b\u3057\u307e\u3057\u3087\u3046\u3002</p> </li> </ol> <p>\u6700\u5f8c\u306b\u3001\u6388\u696d\u3067\u6271\u3063\u305f\u4f8b\u984c\u3084\u6f14\u7fd2\u554f\u984c\u3092\u518d\u5ea6\u89e3\u3044\u3066\u307f\u308b\u3053\u3068\u3067\u3001\u7406\u89e3\u5ea6\u3092\u78ba\u8a8d\u3057\u3001\u82e6\u624b\u306a\u90e8\u5206\u3092\u7279\u5b9a\u3057\u3066\u91cd\u70b9\u7684\u306b\u5fa9\u7fd2\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II","text":""},{"location":"lectures/LA/39-singular-value-decomposition/#39","title":"\u7b2c39\u56de\u8b1b\u7fa9\uff1a\u7279\u7570\u5024\u5206\u89e3\u306e\u57fa\u790e","text":""},{"location":"lectures/LA/39-singular-value-decomposition/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c39\u56de \u95a2\u9023\u9805\u76ee: \u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u3001\u7dda\u5f62\u4ee3\u6570\u306e\u5fdc\u7528 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u884c\u5217\u306e\u5bfe\u89d2\u5316\u3001\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3001\u5bfe\u79f0\u884c\u5217\u306e\u6027\u8cea</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u306e\u57fa\u672c\u6982\u5ff5\u3068\u6570\u5b66\u7684\u5b9a\u7fa9\u3092\u7406\u89e3\u3059\u308b</li> <li>\u7279\u7570\u5024\u3068\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u7279\u7570\u5024\u5206\u89e3\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u3092\u7406\u89e3\u3059\u308b</li> <li>\u7279\u7570\u5024\u5206\u89e3\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u91cd\u8981\u6027\u3092\u8aac\u660e\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>Python\uff08NumPy\uff09\u3092\u7528\u3044\u3066\u7279\u7570\u5024\u5206\u89e3\u3092\u5b9f\u88c5\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> </ol>"},{"location":"lectures/LA/39-singular-value-decomposition/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/39-singular-value-decomposition/#31","title":"3.1 \u7279\u7570\u5024\u5206\u89e3\u3068\u306f","text":"<p>\u7279\u7570\u5024\u5206\u89e3\uff08Singular Value Decomposition, SVD\uff09\u306f\u3001\u4efb\u610f\u306e\u884c\u5217\u3092\u4e09\u3064\u306e\u884c\u5217\u306e\u7a4d\u306b\u5206\u89e3\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002\u3053\u308c\u306f\u884c\u5217\u5206\u89e3\u306e\u4e00\u7a2e\u3067\u3042\u308a\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u975e\u5e38\u306b\u91cd\u8981\u306a\u30c4\u30fc\u30eb\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9: \\(m \\times n\\)\u306e\u884c\u5217 \\(A\\) \u306b\u5bfe\u3059\u308b\u7279\u7570\u5024\u5206\u89e3\u3068\u306f\u3001\u4ee5\u4e0b\u306e\u5f62\u5f0f\u3067\u8868\u3055\u308c\u308b\u5206\u89e3\u306e\u3053\u3068\u3067\u3059\uff1a</p> <p>\\(A = U \\Sigma V^T\\)</p> <p>\u3053\u3053\u3067\u3001 - \\(U\\) \u306f \\(m \\times m\\) \u306e\u76f4\u4ea4\u884c\u5217\uff08\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff09 - \\(\\Sigma\\) \u306f \\(m \\times n\\) \u306e\u5bfe\u89d2\u884c\u5217\uff08\u5bfe\u89d2\u6210\u5206\u306b\u7279\u7570\u5024\u3092\u6301\u3064\uff09 - \\(V^T\\) \u306f \\(n \\times n\\) \u306e\u76f4\u4ea4\u884c\u5217\u306e\u8ee2\u7f6e\uff08\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff09</p> <p>\u7279\u7570\u5024\u5206\u89e3\u306f\u4efb\u610f\u306e\u884c\u5217\u306b\u5bfe\u3057\u3066\u5b58\u5728\u3057\u3001\u3053\u306e\u666e\u904d\u6027\u304c\u7279\u7570\u5024\u5206\u89e3\u3092\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u306b\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#32","title":"3.2 \u7279\u7570\u5024\u3068\u306f","text":"<p>\u7279\u7570\u5024\u306f\u3001\u884c\u5217\u306e\u300c\u91cd\u8981\u5ea6\u300d\u3084\u300c\u30b9\u30b1\u30fc\u30eb\u300d\u3092\u8868\u3059\u975e\u8ca0\u306e\u5b9f\u6570\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9: \u884c\u5217 \\(A\\) \u306e\u7279\u7570\u5024\u306f\u3001\\(A^TA\\) \u306e\u56fa\u6709\u5024\u306e\u5e73\u65b9\u6839\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002</p> <p>\u3059\u306a\u308f\u3061\u3001\\(\\sigma_i = \\sqrt{\\lambda_i(A^TA)}\\) \u3067\u3059\u3002</p> <p>\u7279\u7570\u5024\u306f\u5e38\u306b\u975e\u8ca0\u3067\u3042\u308a\u3001\u901a\u5e38\u306f\u5927\u304d\u3044\u9806\u306b\u4e26\u3079\u3089\u308c\u307e\u3059\uff1a\\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0 = \\sigma_{r+1} = \\cdots = \\sigma_{\\min(m,n)}\\)</p> <p>\u3053\u3053\u3067\u3001\\(r\\) \u306f\u884c\u5217 \\(A\\) \u306e\u968e\u6570\uff08rank\uff09\u3067\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#33","title":"3.3 \u7279\u7570\u30d9\u30af\u30c8\u30eb\u3068\u306f","text":"<p>\u7279\u7570\u5024\u5206\u89e3\u3067\u306f\u3001\u4e8c\u7a2e\u985e\u306e\u7279\u7570\u30d9\u30af\u30c8\u30eb\u304c\u767b\u5834\u3057\u307e\u3059\uff1a\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3068\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9:  - \u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(v_i\\) \u306f\u3001\\(A^TA\\) \u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002 - \u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(u_i\\) \u306f\u3001\\(AA^T\\) \u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3001\u307e\u305f\u306f \\(u_i = \\frac{Av_i}{\\sigma_i}\\) \u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff08\\(\\sigma_i \\neq 0\\) \u306e\u5834\u5408\uff09\u3002</p> <p>\u884c\u5217 \\(U\\) \u306e\u5217\u306f\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3001\u884c\u5217 \\(V\\) \u306e\u5217\u306f\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002\u3053\u308c\u3089\u306e\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306f\u3001\u305d\u308c\u305e\u308c\u76f4\u4ea4\u57fa\u5e95\u3092\u5f62\u6210\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/39-singular-value-decomposition/#41","title":"4.1 \u7279\u7570\u5024\u5206\u89e3\u306e\u5b58\u5728\u5b9a\u7406","text":"<p>\u7279\u7570\u5024\u5206\u89e3\u306f\u4efb\u610f\u306e\u884c\u5217\u306b\u5bfe\u3057\u3066\u5b58\u5728\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u7279\u7570\u5024\u5206\u89e3\u304c\u6301\u3064\u91cd\u8981\u306a\u6027\u8cea\u306e\u4e00\u3064\u3067\u3059\u3002</p> <p>\u5b9a\u7406: \u4efb\u610f\u306e \\(m \\times n\\) \u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\u7279\u7570\u5024\u5206\u89e3 \\(A = U \\Sigma V^T\\) \u304c\u5b58\u5728\u3057\u307e\u3059\u3002</p> <p>\u3053\u306e\u666e\u904d\u6027\u306b\u3088\u3063\u3066\u3001\u69d8\u3005\u306a\u5f62\u72b6\u3084\u6027\u8cea\u3092\u6301\u3064\u884c\u5217\u306b\u5bfe\u3057\u3066\u7279\u7570\u5024\u5206\u89e3\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#42","title":"4.2 \u7279\u7570\u5024\u5206\u89e3\u306e\u8a08\u7b97\u624b\u9806","text":"<p>\u7279\u7570\u5024\u5206\u89e3\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u4e00\u822c\u7684\u306a\u624b\u9806\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u884c\u5217 \\(A^TA\\) \u3092\u8a08\u7b97\u3059\u308b</li> <li>\\(A^TA\\) \u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u308b</li> <li>\u56fa\u6709\u5024 \\(\\lambda_i\\) \u304c \\(A^TA\\) \u306e\u56fa\u6709\u5024</li> <li>\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(v_i\\) \u304c\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb</li> <li>\u7279\u7570\u5024 \\(\\sigma_i = \\sqrt{\\lambda_i}\\) \u3092\u8a08\u7b97\u3059\u308b</li> <li>\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(u_i\\) \u3092 \\(u_i = \\frac{Av_i}{\\sigma_i}\\) \u3068\u3057\u3066\u8a08\u7b97\u3059\u308b\uff08\\(\\sigma_i \\neq 0\\) \u306e\u5834\u5408\uff09</li> <li>\\(U\\), \\(\\Sigma\\), \\(V^T\\) \u3092\u69cb\u6210\u3059\u308b</li> </ol>"},{"location":"lectures/LA/39-singular-value-decomposition/#43","title":"4.3 \u7279\u7570\u5024\u3068\u56fa\u6709\u5024\u306e\u95a2\u4fc2","text":"<p>\u7279\u7570\u5024\u3068\u56fa\u6709\u5024\u306b\u306f\u91cd\u8981\u306a\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\\(A^TA\\) \u306e\u56fa\u6709\u5024\u306f \\(AA^T\\) \u306e\u975e\u30bc\u30ed\u56fa\u6709\u5024\u3068\u4e00\u81f4\u3059\u308b</li> <li>\u884c\u5217 \\(A\\) \u306e\u7279\u7570\u5024\u306f \\(A^TA\\) \u306e\u56fa\u6709\u5024\u306e\u5e73\u65b9\u6839</li> <li>\u884c\u5217 \\(A\\) \u306e\u30e9\u30f3\u30af \\(r\\) \u306f\u3001\u975e\u30bc\u30ed\u7279\u7570\u5024\u306e\u6570\u306b\u7b49\u3057\u3044</li> </ol>"},{"location":"lectures/LA/39-singular-value-decomposition/#44","title":"4.4 \u8a08\u7b97\u4f8b","text":"<p>\u5177\u4f53\u7684\u306a\u8a08\u7b97\u4f8b\u3092\u901a\u3057\u3066\u7279\u7570\u5024\u5206\u89e3\u306e\u624b\u9806\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\u4f8b\u984c: \u4ee5\u4e0b\u306e \\(2 \\times 2\\) \u884c\u5217 \\(A\\) \u306e\u7279\u7570\u5024\u5206\u89e3\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> \\[A = \\begin{bmatrix} 3 &amp; 2 \\\\ 2 &amp; 2 \\end{bmatrix}\\] <p>\u89e3\u7b54:</p> <p>\u30b9\u30c6\u30c3\u30d7 1: \\(A^TA\\) \u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[A^TA = \\begin{bmatrix} 3 &amp; 2 \\\\ 2 &amp; 2 \\end{bmatrix}^T \\begin{bmatrix} 3 &amp; 2 \\\\ 2 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 2 \\\\ 2 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 3 &amp; 2 \\\\ 2 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 13 &amp; 10 \\\\ 10 &amp; 8 \\end{bmatrix}\\] <p>\u30b9\u30c6\u30c3\u30d7 2: \\(A^TA\\) \u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\u7279\u6027\u591a\u9805\u5f0f\u306f\uff1a</p> \\[\\det(A^TA - \\lambda I) = \\det\\begin{bmatrix} 13-\\lambda &amp; 10 \\\\ 10 &amp; 8-\\lambda \\end{bmatrix} = (13-\\lambda)(8-\\lambda) - 10 \\cdot 10 = \\lambda^2 - 21\\lambda + 4 = 0\\] <p>\u3053\u306e\u4e8c\u6b21\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u3068\uff1a</p> \\[\\lambda_1 = \\frac{21 + \\sqrt{21^2 - 16}}{2} \\approx 20.39, \\quad \\lambda_2 = \\frac{21 - \\sqrt{21^2 - 16}}{2} \\approx 0.61\\] <p>\u305d\u308c\u305e\u308c\u306e\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u307e\u3059\uff1a</p> <p>\\(\\lambda_1 = 20.39\\) \u306b\u5bfe\u3057\u3066\uff1a \\(\\(\\begin{bmatrix} 13-20.39 &amp; 10 \\\\ 10 &amp; 8-20.39 \\end{bmatrix} \\begin{bmatrix} v_{11} \\\\ v_{12} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)\\)</p> <p>\u3053\u308c\u3092\u89e3\u304f\u3068 \\(v_1 \\approx \\begin{bmatrix} 0.8 \\\\ 0.6 \\end{bmatrix}\\) \uff08\u6b63\u898f\u5316\u5f8c\uff09</p> <p>\\(\\lambda_2 = 0.61\\) \u306b\u5bfe\u3057\u3066\uff1a \\(\\(\\begin{bmatrix} 13-0.61 &amp; 10 \\\\ 10 &amp; 8-0.61 \\end{bmatrix} \\begin{bmatrix} v_{21} \\\\ v_{22} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\)\\)</p> <p>\u3053\u308c\u3092\u89e3\u304f\u3068 \\(v_2 \\approx \\begin{bmatrix} -0.6 \\\\ 0.8 \\end{bmatrix}\\) \uff08\u6b63\u898f\u5316\u5f8c\uff09</p> <p>\u30b9\u30c6\u30c3\u30d7 3: \u7279\u7570\u5024\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[\\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{20.39} \\approx 4.52, \\quad \\sigma_2 = \\sqrt{\\lambda_2} = \\sqrt{0.61} \\approx 0.78\\] <p>\u30b9\u30c6\u30c3\u30d7 4: \u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> \\[u_1 = \\frac{Av_1}{\\sigma_1} = \\frac{1}{4.52} \\begin{bmatrix} 3 &amp; 2 \\\\ 2 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 0.8 \\\\ 0.6 \\end{bmatrix} \\approx \\begin{bmatrix} 0.71 \\\\ 0.71 \\end{bmatrix}\\] \\[u_2 = \\frac{Av_2}{\\sigma_2} = \\frac{1}{0.78} \\begin{bmatrix} 3 &amp; 2 \\\\ 2 &amp; 2 \\end{bmatrix} \\begin{bmatrix} -0.6 \\\\ 0.8 \\end{bmatrix} \\approx \\begin{bmatrix} -0.71 \\\\ 0.71 \\end{bmatrix}\\] <p>\u30b9\u30c6\u30c3\u30d7 5: \u7279\u7570\u5024\u5206\u89e3\u3092\u69cb\u6210\u3057\u307e\u3059\u3002</p> \\[A = U\\Sigma V^T = \\begin{bmatrix} 0.71 &amp; -0.71 \\\\ 0.71 &amp; 0.71 \\end{bmatrix} \\begin{bmatrix} 4.52 &amp; 0 \\\\ 0 &amp; 0.78 \\end{bmatrix} \\begin{bmatrix} 0.8 &amp; -0.6 \\\\ 0.6 &amp; 0.8 \\end{bmatrix}\\] <p>\u4ee5\u4e0a\u304c\u884c\u5217 \\(A\\) \u306e\u7279\u7570\u5024\u5206\u89e3\u3067\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#45","title":"4.5 \u7279\u7570\u5024\u5206\u89e3\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":"<p>\u7279\u7570\u5024\u5206\u89e3\u306f\u7dda\u5f62\u5909\u63db\u306e\u5e7e\u4f55\u5b66\u7684\u306a\u610f\u5473\u3092\u660e\u3089\u304b\u306b\u3057\u307e\u3059\u3002\u884c\u5217 \\(A\\) \u306b\u3088\u308b\u7dda\u5f62\u5909\u63db\u306f\u4ee5\u4e0b\u306e3\u3064\u306e\u30b9\u30c6\u30c3\u30d7\u306b\u5206\u89e3\u3067\u304d\u307e\u3059\uff1a</p> <ol> <li>\u76f4\u4ea4\u5909\u63db \\(V^T\\): \u5143\u306e\u7a7a\u9593\u3067\u306e\u56de\u8ee2</li> <li>\u5bfe\u89d2\u884c\u5217 \\(\\Sigma\\) \u306b\u3088\u308b\u62e1\u5927\u30fb\u7e2e\u5c0f: \u5404\u8ef8\u65b9\u5411\u3078\u306e\u4f38\u7e2e</li> <li>\u76f4\u4ea4\u5909\u63db \\(U\\): \u5909\u63db\u5f8c\u306e\u7a7a\u9593\u3067\u306e\u56de\u8ee2</li> </ol> <p>\u3053\u308c\u306f\u3001\u4efb\u610f\u306e\u7dda\u5f62\u5909\u63db\u304c\u300c\u56de\u8ee2\u2192\u4f38\u7e2e\u2192\u56de\u8ee2\u300d\u3068\u3044\u30463\u3064\u306e\u57fa\u672c\u7684\u306a\u64cd\u4f5c\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u8868\u73fe\u3067\u304d\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> <p>\u7279\u7570\u5024\u5206\u89e3\u306e\u5e7e\u4f55\u5b66\u7684\u306a\u89e3\u91c8\u3067\u306f\u3001\u5358\u4f4d\u7403\uff08\u307e\u305f\u306f\u8d85\u7403\uff09\u306e\u5909\u63db\u3092\u8003\u3048\u308b\u3068\u308f\u304b\u308a\u3084\u3059\u304f\u306a\u308a\u307e\u3059\uff1a</p> <ol> <li>\\(V^T\\) \u306b\u3088\u3063\u3066\u5358\u4f4d\u7403\u3092\u56de\u8ee2\u3055\u305b\u308b</li> <li>\\(\\Sigma\\) \u306b\u3088\u3063\u3066\u5404\u8ef8\u65b9\u5411\u306b\u4f38\u7e2e\u3055\u305b\u3001\u6955\u5186\uff08\u307e\u305f\u306f\u8d85\u6955\u5186\uff09\u306b\u3059\u308b</li> <li>\\(U\\) \u306b\u3088\u3063\u3066\u6955\u5186\u3092\u56de\u8ee2\u3055\u305b\u308b</li> </ol> <p>\u6700\u7d42\u7684\u306b\u5f97\u3089\u308c\u308b\u6955\u5186\u306e\u534a\u8ef8\u306e\u9577\u3055\u304c\u7279\u7570\u5024 \\(\\sigma_i\\) \u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u7279\u7570\u5024\u5206\u89e3\u306fNumPy\u3092\u4f7f\u7528\u3057\u3066\u7c21\u5358\u306b\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[3, 2], \n              [2, 2]])\n\n# \u7279\u7570\u5024\u5206\u89e3\u306e\u8a08\u7b97\nU, sigma, VT = np.linalg.svd(A)\n\nprint(\"\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\u884c\u5217 U:\")\nprint(U)\nprint(\"\\n\u7279\u7570\u5024:\")\nprint(sigma)\nprint(\"\\n\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u884c\u5217 V^T:\")\nprint(VT)\n\n# \u7279\u7570\u5024\u5206\u89e3\u306e\u691c\u8a3c\nSigma = np.zeros((A.shape[0], A.shape[1]))\nnp.fill_diagonal(Sigma, sigma)\nA_reconstructed = U @ Sigma @ VT\nprint(\"\\n\u518d\u69cb\u6210\u3055\u308c\u305f\u884c\u5217 A:\")\nprint(A_reconstructed)\nprint(\"\\n\u5143\u306e\u884c\u5217 A:\")\nprint(A)\nprint(\"\\n\u518d\u69cb\u6210\u8aa4\u5dee:\")\nprint(np.linalg.norm(A - A_reconstructed))\n\n# \u7279\u7570\u5024\u5206\u89e3\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u306e\u53ef\u8996\u5316\ndef plot_transformation(A, U, sigma, VT):\n    # \u5ea7\u6a19\u306e\u7bc4\u56f2\u3092\u8a2d\u5b9a\n    x = np.linspace(-1, 1, 100)\n    y = np.linspace(-1, 1, 100)\n    X, Y = np.meshgrid(x, y)\n\n    # \u5358\u4f4d\u5186\u4e0a\u306e\u70b9\u3092\u751f\u6210\n    theta = np.linspace(0, 2*np.pi, 100)\n    circle_x = np.cos(theta)\n    circle_y = np.sin(theta)\n    circle_points = np.vstack([circle_x, circle_y])\n\n    # \u5909\u63db\u3055\u308c\u305f\u6955\u5186\u306e\u8a08\u7b97\n    transformed_points = A @ circle_points\n\n    # \u30d7\u30ed\u30c3\u30c8\u306e\u8a2d\u5b9a\n    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\n    # \u5143\u306e\u5358\u4f4d\u5186\n    axs[0].plot(circle_x, circle_y, 'b-')\n    axs[0].set_xlim(-3, 3)\n    axs[0].set_ylim(-3, 3)\n    axs[0].set_aspect('equal')\n    axs[0].grid(True)\n    axs[0].set_title('\u5143\u306e\u5358\u4f4d\u5186')\n\n    # V^T\u306b\u3088\u308b\u56de\u8ee2\n    rotated_points = VT @ circle_points\n    axs[1].plot(rotated_points[0], rotated_points[1], 'g-')\n    axs[1].set_xlim(-3, 3)\n    axs[1].set_ylim(-3, 3)\n    axs[1].set_aspect('equal')\n    axs[1].grid(True)\n    axs[1].set_title('V^T\u306b\u3088\u308b\u56de\u8ee2')\n\n    # \u03a3\u306b\u3088\u308b\u62e1\u5927\u30fb\u7e2e\u5c0f\n    scaled_points = np.diag(sigma) @ rotated_points\n    axs[2].plot(scaled_points[0], scaled_points[1], 'r-')\n    axs[2].set_xlim(-5, 5)\n    axs[2].set_ylim(-5, 5)\n    axs[2].set_aspect('equal')\n    axs[2].grid(True)\n    axs[2].set_title('\u03a3\u306b\u3088\u308b\u62e1\u5927\u30fb\u7e2e\u5c0f')\n\n    # U\u306b\u3088\u308b\u56de\u8ee2\uff08\u6700\u7d42\u7d50\u679c\uff09\n    axs[3].plot(transformed_points[0], transformed_points[1], 'm-')\n    axs[3].set_xlim(-5, 5)\n    axs[3].set_ylim(-5, 5)\n    axs[3].set_aspect('equal')\n    axs[3].grid(True)\n    axs[3].set_title('A\u306b\u3088\u308b\u5909\u63db\uff08\u6700\u7d42\u7d50\u679c\uff09')\n\n    plt.tight_layout()\n    plt.show()\n\n# \u884c\u5217\u5909\u63db\u306e\u53ef\u8996\u5316\nplot_transformation(A, U, sigma, VT)\n</code></pre> <p>\u3053\u306e\u4f8b\u3067\u306f\u30012\u00d72\u884c\u5217\u306e\u7279\u7570\u5024\u5206\u89e3\u3092\u8a08\u7b97\u3057\u3001\u305d\u306e\u7d50\u679c\u3092\u8868\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u3001\u7279\u7570\u5024\u5206\u89e3\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u3092\u53ef\u8996\u5316\u3059\u308b\u305f\u3081\u306b\u3001\u5358\u4f4d\u5186\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u63db\u3055\u308c\u308b\u304b\u3092\u6bb5\u968e\u7684\u306b\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#6","title":"6. \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u7279\u7570\u5024\u5206\u89e3\u306e\u5fdc\u7528","text":"<p>\u7279\u7570\u5024\u5206\u89e3\u306f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3001\u7279\u306b\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u591a\u304f\u306e\u5fdc\u7528\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u6b21\u5143\u524a\u6e1b: \u5927\u304d\u306a\u7279\u7570\u5024\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u306e\u8fd1\u4f3c\u8868\u73fe\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059</li> <li>\u30ce\u30a4\u30ba\u9664\u53bb: \u5c0f\u3055\u306a\u7279\u7570\u5024\u30920\u306b\u3059\u308b\u3053\u3068\u3067\u30ce\u30a4\u30ba\u3092\u53d6\u308a\u9664\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059</li> <li>\u753b\u50cf\u5727\u7e2e: \u753b\u50cf\u30c7\u30fc\u30bf\u3092\u5c11\u306a\u3044\u60c5\u5831\u91cf\u3067\u8fd1\u4f3c\u3067\u304d\u307e\u3059</li> <li>\u63a8\u85a6\u30b7\u30b9\u30c6\u30e0: \u30e6\u30fc\u30b6\u30fc-\u30a2\u30a4\u30c6\u30e0\u8a55\u4fa1\u884c\u5217\u306e\u6f5c\u5728\u69cb\u9020\u3092\u767a\u898b\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059</li> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u89e3\u6790: \u533b\u7642\u753b\u50cf\u3084\u751f\u4f53\u4fe1\u53f7\u306e\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u306b\u4f7f\u308f\u308c\u307e\u3059</li> </ol> <p>\u4f8b\u3048\u3070\u3001\u533b\u7642\u753b\u50cf\u306e\u5834\u5408\u3001SVD\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u753b\u50cf\u306e\u7279\u5fb4\u3092\u52b9\u7387\u7684\u306b\u62bd\u51fa\u3057\u3001\u30ce\u30a4\u30ba\u3092\u9664\u53bb\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002\u307e\u305f\u3001\u5927\u91cf\u306e\u5065\u5eb7\u30c7\u30fc\u30bf\u304b\u3089\u6f5c\u5728\u7684\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u767a\u898b\u3059\u308b\u305f\u3081\u306b\u3082\u6709\u52b9\u3067\u3059\u3002</p> <pre><code># \u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u7279\u7570\u5024\u5206\u89e3\u306e\u5fdc\u7528\u4f8b\n# \u4f8b\uff1aMRI\u30c7\u30fc\u30bf\u306e\u5727\u7e2e\u3068\u30ce\u30a4\u30ba\u9664\u53bb\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import color\nfrom skimage import data\n\n# \u30b5\u30f3\u30d7\u30eb\u753b\u50cf\u306e\u8aad\u307f\u8fbc\u307f\uff08\u5b9f\u969b\u306eMRI\u30c7\u30fc\u30bf\u306e\u4ee3\u308f\u308a\u306b\uff09\nimage = color.rgb2gray(data.astronaut())\nplt.figure(figsize=(5, 5))\nplt.imshow(image, cmap='gray')\nplt.title('\u5143\u306e\u753b\u50cf')\nplt.axis('off')\nplt.show()\n\n# \u7279\u7570\u5024\u5206\u89e3\nU, sigma, VT = np.linalg.svd(image, full_matrices=False)\n\n# \u7279\u7570\u5024\u306e\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 4))\nplt.semilogy(sigma)\nplt.title('\u7279\u7570\u5024\u306e\u5927\u304d\u3055')\nplt.xlabel('\u30a4\u30f3\u30c7\u30c3\u30af\u30b9')\nplt.ylabel('\u7279\u7570\u5024 (\u5bfe\u6570\u30b9\u30b1\u30fc\u30eb)')\nplt.grid(True)\nplt.show()\n\n# \u7570\u306a\u308b\u6570\u306e\u7279\u7570\u5024\u3092\u4f7f\u7528\u3057\u305f\u753b\u50cf\u306e\u518d\u69cb\u6210\nranks = [5, 20, 50, 100]\nfig, axes = plt.subplots(1, len(ranks), figsize=(20, 5))\n\nfor i, r in enumerate(ranks):\n    # r\u500b\u306e\u7279\u7570\u5024\u3092\u4f7f\u7528\u3057\u3066\u753b\u50cf\u3092\u518d\u69cb\u6210\n    reconstructed = U[:, :r] @ np.diag(sigma[:r]) @ VT[:r, :]\n\n    # \u518d\u69cb\u6210\u3055\u308c\u305f\u753b\u50cf\u306e\u8868\u793a\n    axes[i].imshow(reconstructed, cmap='gray')\n    axes[i].set_title(f'\u4e0a\u4f4d {r} \u7279\u7570\u5024\u3067\u306e\u518d\u69cb\u6210')\n    axes[i].axis('off')\n\n    # \u60c5\u5831\u5727\u7e2e\u7387\u306e\u8a08\u7b97\n    original_size = image.shape[0] * image.shape[1]\n    compressed_size = r * (image.shape[0] + image.shape[1] + 1)\n    compression_ratio = compressed_size / original_size * 100\n\n    print(f'\u4e0a\u4f4d {r} \u7279\u7570\u5024\u3092\u4f7f\u7528: \u5727\u7e2e\u7387 = {compression_ratio:.2f}%, RMSE = {np.sqrt(np.mean((image - reconstructed)**2)):.4f}')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\u3053\u306e\u30b3\u30fc\u30c9\u4f8b\u3067\u306f\u3001\u753b\u50cf\u306e\u7279\u7570\u5024\u5206\u89e3\u3092\u8a08\u7b97\u3057\u3001\u7570\u306a\u308b\u6570\u306e\u7279\u7570\u5024\u3092\u4f7f\u7528\u3057\u3066\u753b\u50cf\u3092\u518d\u69cb\u6210\u3059\u308b\u65b9\u6cd5\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b\u3084\u5727\u7e2e\u306e\u57fa\u672c\u7684\u306a\u4f8b\u3067\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/39-singular-value-decomposition/#_1","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u7279\u7570\u5024\u5206\u89e3\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    \\(\\(A = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}\\)\\)</p> </li> <li> <p>\u7279\u7570\u5024\u5206\u89e3 \\(A = U\\Sigma V^T\\) \u306b\u304a\u3044\u3066\u3001\\(U\\) \u3068 \\(V\\) \u304c\u76f4\u4ea4\u884c\u5217\u3067\u3042\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u884c\u5217 \\(A\\) \u304c\u6b63\u65b9\u884c\u5217\u304b\u3064\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\u5834\u5408\u3001\u305d\u306e\u7279\u7570\u5024\u5206\u89e3\u3068\u56fa\u6709\u5024\u5206\u89e3\u306e\u95a2\u4fc2\u3092\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u6b21\u306e\u884c\u5217\u306e\u7279\u7570\u5024\u3092\u6c42\u3081\u3001\u30e9\u30f3\u30af\u3092\u6c7a\u5b9a\u3057\u306a\u3055\u3044\u3002    \\(\\(B = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\)\\)</p> </li> <li> <p>\u884c\u5217 \\(A\\) \u306e\u7279\u7570\u5024\u304c \\(\\sigma_1, \\sigma_2, \\ldots, \\sigma_n\\) \u3067\u3042\u308b\u5834\u5408\u3001\\(A^TA\\) \u306e\u56fa\u6709\u5024\u306f\u4f55\u304b\uff1f</p> </li> </ol>"},{"location":"lectures/LA/39-singular-value-decomposition/#_2","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u4ee5\u4e0b\u306e \\(3 \\times 4\\) \u884c\u5217\u306e\u7279\u7570\u5024\u5206\u89e3\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(C = \\begin{bmatrix} 4 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 2 &amp; 0 \\end{bmatrix}\\)\\)    \u7279\u7570\u5024\u3001\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3001\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u305d\u308c\u305e\u308c\u6c42\u3081\u3001\\(C = U\\Sigma V^T\\) \u306e\u5f62\u3067\u8868\u3057\u306a\u3055\u3044\u3002\u307e\u305f\u3001\u884c\u5217\u306e\u968e\u6570\u3092\u7279\u7570\u5024\u304b\u3089\u6c7a\u5b9a\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u4ee5\u4e0b\u306e \\(2 \\times 3\\) \u884c\u5217\u306e\u7279\u7570\u5024\u5206\u89e3\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(D = \\begin{bmatrix} 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\)\\)    \u7279\u7570\u5024\u3001\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3001\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u305d\u308c\u305e\u308c\u6c42\u3081\u3001\\(D = U\\Sigma V^T\\) \u306e\u5f62\u3067\u8868\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u7279\u7570\u5024\u5206\u89e3\u3092\u7528\u3044\u3066\u3001\u4ee5\u4e0b\u306e\u9577\u65b9\u5f62\u884c\u5217\u306e\u64ec\u4f3c\u9006\u884c\u5217\uff08\u30e0\u30fc\u30a2\u30fb\u30da\u30f3\u30ed\u30fc\u30ba\u64ec\u4f3c\u9006\u884c\u5217\uff09\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002    \\(\\(E = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{bmatrix}\\)\\)    \u30d2\u30f3\u30c8\uff1a\u7279\u7570\u5024\u5206\u89e3 \\(E = U\\Sigma V^T\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u3001\u64ec\u4f3c\u9006\u884c\u5217 \\(E^+\\) \u306f \\(E^+ = V\\Sigma^+U^T\\) \u3068\u3057\u3066\u4e0e\u3048\u3089\u308c\u308b\u3002\u3053\u3053\u3067 \\(\\Sigma^+\\) \u306f \\(\\Sigma\\) \u306e\u975e\u30bc\u30ed\u8981\u7d20\u306e\u9006\u6570\u3092\u5bfe\u5fdc\u3059\u308b\u4f4d\u7f6e\u306b\u6301\u3064\u884c\u5217\u3067\u3042\u308b\u3002</p> </li> <li> <p>NumPy\u3092\u4f7f\u7528\u3057\u3066\u3001\u30e9\u30f3\u30c0\u30e0\u306a100\u00d750\u306e\u884c\u5217\u3092\u751f\u6210\u3057\u3001\u305d\u306e\u7279\u7570\u5024\u5206\u89e3\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002\u4e0a\u4f4d10\u500b\u306e\u7279\u7570\u5024\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066\u5143\u306e\u884c\u5217\u3092\u8fd1\u4f3c\u3057\u305f\u3068\u304d\u306e\u518d\u69cb\u6210\u8aa4\u5dee\u3092\u8a08\u7b97\u3057\u3001\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u306e\u610f\u5473\u3067\u306e\u76f8\u5bfe\u8aa4\u5dee\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u5b9f\u7528\u7684\u306a\u554f\u984c\uff1a100\u4eba\u306e\u60a3\u8005\u304b\u3089\u53ce\u96c6\u3055\u308c\u305f10\u7a2e\u985e\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\uff08\u8840\u5727\u3001\u8840\u7cd6\u5024\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u306a\u3069\uff09\u306e\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u304c\u3042\u308b\u3068\u3057\u307e\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u306f100\u00d710\u306e\u884c\u5217\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u7279\u7570\u5024\u5206\u89e3\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u3069\u306e\u3088\u3046\u306a\u6d1e\u5bdf\u304c\u5f97\u3089\u308c\u308b\u304b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002\u7279\u306b\u3001\u4e3b\u8981\u306a\u7279\u7570\u5024\u3068\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u89e3\u91c8\u306b\u7126\u70b9\u3092\u5f53\u3066\u3001\u60a3\u8005\u30b0\u30eb\u30fc\u30d7\u306e\u5206\u985e\u3084\u6f5c\u5728\u7684\u306a\u5065\u5eb7\u30d1\u30bf\u30fc\u30f3\u306e\u767a\u898b\u306b\u3069\u306e\u3088\u3046\u306b\u5f79\u7acb\u3064\u304b\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/39-singular-value-decomposition/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":"<p>Q1: \u7279\u7570\u5024\u5206\u89e3\u3068\u56fa\u6709\u5024\u5206\u89e3\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A1: \u56fa\u6709\u5024\u5206\u89e3\u306f\u6b63\u65b9\u884c\u5217\u306b\u306e\u307f\u9069\u7528\u53ef\u80fd\u3067\u3042\u308a\u3001\u5fc5\u305a\u3057\u3082\u3059\u3079\u3066\u306e\u884c\u5217\u306b\u5bfe\u3057\u3066\u5b58\u5728\u3059\u308b\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u4e00\u65b9\u3001\u7279\u7570\u5024\u5206\u89e3\u306f\u4efb\u610f\u306e\u884c\u5217\uff08\u6b63\u65b9\u3001\u9577\u65b9\u5f62\u3092\u554f\u308f\u305a\uff09\u306b\u9069\u7528\u53ef\u80fd\u3067\u3001\u5e38\u306b\u5b58\u5728\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u56fa\u6709\u5024\u5206\u89e3\u3067\u306f \\(A = PDP^{-1}\\) \u3068\u3044\u3046\u5f62\u5f0f\u3067\u5206\u89e3\u3057\u307e\u3059\u304c\u3001\u7279\u7570\u5024\u5206\u89e3\u3067\u306f \\(A = U\\Sigma V^T\\) \u3068\u3044\u3046\u5f62\u5f0f\u3067\u5206\u89e3\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u3001\\(U\\) \u3068 \\(V\\) \u306f\u76f4\u4ea4\u884c\u5217\u3067\u3042\u308a\u3001\\(\\Sigma\\) \u306f\u5bfe\u89d2\u884c\u5217\u3067\u3059\u3002</p> <p>Q2: \u7279\u7570\u5024\u5206\u89e3\u306e\u8a08\u7b97\u30b3\u30b9\u30c8\u306f\u3069\u306e\u304f\u3089\u3044\u3067\u3059\u304b\uff1f</p> <p>A2: \\(m \\times n\\) \u884c\u5217\u306e\u7279\u7570\u5024\u5206\u89e3\u306e\u8a08\u7b97\u91cf\u306f\u4e00\u822c\u7684\u306b \\(O(\\min(mn^2, m^2n))\\) \u3067\u3059\u3002\u5927\u898f\u6a21\u306a\u884c\u5217\u306b\u5bfe\u3057\u3066\u306f\u3001\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u304f\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u90e8\u5206\u7684\u306a\u7279\u7570\u5024\u5206\u89e3\uff08\u4e0a\u4f4dk\u500b\u306e\u7279\u7570\u5024\u3068\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u307f\u3092\u8a08\u7b97\uff09\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u524a\u6e1b\u3067\u304d\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>Q3: \u7279\u7570\u5024\u304c\u30bc\u30ed\u306b\u8fd1\u3044\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306b\u51e6\u7406\u3059\u3079\u304d\u3067\u3059\u304b\uff1f</p> <p>A3: \u7279\u7570\u5024\u304c\u30bc\u30ed\u306b\u975e\u5e38\u306b\u8fd1\u3044\u5834\u5408\u3001\u6570\u5024\u8a08\u7b97\u306e\u8aa4\u5dee\u306b\u3088\u3063\u3066\u554f\u984c\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u5834\u5408\u3001\u95be\u5024\u3092\u8a2d\u5b9a\u3057\u3001\u305d\u308c\u4ee5\u4e0b\u306e\u7279\u7570\u5024\u3092\u30bc\u30ed\u3068\u3057\u3066\u6271\u3046\u3053\u3068\u304c\u4e00\u822c\u7684\u3067\u3059\u3002\u3053\u308c\u306f\u300c\u6253\u3061\u5207\u308a\u7279\u7570\u5024\u5206\u89e3\uff08truncated SVD\uff09\u300d\u3068\u547c\u3070\u308c\u3001\u30ce\u30a4\u30ba\u306e\u9664\u53bb\u3084\u6570\u5024\u5b89\u5b9a\u6027\u306e\u5411\u4e0a\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002</p> <p>Q4: \u7279\u7570\u5024\u5206\u89e3\u306f\u3069\u306e\u3088\u3046\u306b\u3057\u3066\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u89e3\u6790\u306b\u5f79\u7acb\u3061\u307e\u3059\u304b\uff1f</p> <p>A4: \u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u89e3\u6790\u306b\u304a\u3044\u3066\u3001\u7279\u7570\u5024\u5206\u89e3\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5f79\u7acb\u3061\u307e\u3059\uff1a - \u5927\u91cf\u306e\u751f\u4f53\u4fe1\u53f7\u30c7\u30fc\u30bf\uff08\u8133\u6ce2\u3001\u5fc3\u96fb\u56f3\u306a\u3069\uff09\u304b\u3089\u91cd\u8981\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u62bd\u51fa - \u533b\u7642\u753b\u50cf\u306e\u30ce\u30a4\u30ba\u9664\u53bb\u3068\u7279\u5fb4\u62bd\u51fa - \u60a3\u8005-\u75c7\u72b6\u30c7\u30fc\u30bf\u306e\u6f5c\u5728\u69cb\u9020\u306e\u767a\u898b - \u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b - \u6642\u7cfb\u5217\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u50be\u5411\u5206\u6790</p> <p>\u4f8b\u3048\u3070\u3001\u8907\u6570\u306e\u60a3\u8005\u304b\u3089\u53ce\u96c6\u3055\u308c\u305f\u6642\u7cfb\u5217\u306e\u5065\u5eb7\u6307\u6a19\u30c7\u30fc\u30bf\u306b\u7279\u7570\u5024\u5206\u89e3\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u5171\u901a\u306e\u30d1\u30bf\u30fc\u30f3\u3084\u7570\u5e38\u3092\u691c\u51fa\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>Q5: \u7279\u7570\u5024\u5206\u89e3\u3068\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u95a2\u4fc2\u306f\u4f55\u3067\u3059\u304b\uff1f</p> <p>A5: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u306b\u5bfe\u3059\u308b\u56fa\u6709\u5024\u5206\u89e3\u3092\u57fa\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u4e2d\u5fc3\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u306b\u5bfe\u3057\u3066\u3001\\(X^TX/(n-1)\\) \u304c\u5171\u5206\u6563\u884c\u5217\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u306b\u76f4\u63a5\u7279\u7570\u5024\u5206\u89e3\u3092\u9069\u7528\u3059\u308b\u3068\u3001\u5f97\u3089\u308c\u308b\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(V\\) \u306f\u3001\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u4e3b\u6210\u5206\uff09\u3068\u4e00\u81f4\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u7279\u7570\u5024\u306e\u4e8c\u4e57\u3092 \\((n-1)\\) \u3067\u5272\u3063\u305f\u3082\u306e\u304c\u3001\u5bfe\u5fdc\u3059\u308b\u4e3b\u6210\u5206\u306e\u5206\u6563\u306b\u306a\u308a\u307e\u3059\u3002\u3064\u307e\u308a\u3001PCA\u306f\u7279\u7570\u5024\u5206\u89e3\u306e\u7279\u6b8a\u306a\u30b1\u30fc\u30b9\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>Q6: \u7279\u7570\u5024\u5206\u89e3\u306b\u304a\u3051\u308b\u30e9\u30f3\u30af\u4f4e\u6e1b\u306f\u3069\u306e\u3088\u3046\u306a\u610f\u5473\u3092\u6301\u3061\u307e\u3059\u304b\uff1f</p> <p>A6: \u30e9\u30f3\u30af\u4f4e\u6e1b\uff08rank reduction\uff09\u3068\u306f\u3001\u5c0f\u3055\u306a\u7279\u7570\u5024\u30920\u3068\u3057\u3001\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u7121\u8996\u3059\u308b\u3053\u3068\u3067\u3001\u5143\u306e\u884c\u5217\u306e\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u3092\u5f97\u308b\u624b\u6cd5\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30c7\u30fc\u30bf\u306e\u300c\u30ce\u30a4\u30ba\u300d\u3092\u9664\u53bb\u3057\u3001\u300c\u4fe1\u53f7\u300d\u90e8\u5206\u3092\u4fdd\u6301\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u30a8\u30c3\u30ab\u30fc\u30c8\u30fb\u30e4\u30f3\u30b0\u306e\u5b9a\u7406\u306b\u3088\u308c\u3070\u3001\u4e0a\u4f4dk\u500b\u306e\u7279\u7570\u5024\u306b\u57fa\u3065\u304f\u8fd1\u4f3c\u306f\u3001\u30e9\u30f3\u30afk\u306e\u884c\u5217\u306e\u4e2d\u3067\u5143\u306e\u884c\u5217\u306b\u6700\u3082\u8fd1\u3044\u3082\u306e\u3068\u306a\u308a\u307e\u3059\uff08\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u306e\u610f\u5473\u3067\uff09\u3002\u3053\u308c\u306f\u30c7\u30fc\u30bf\u5727\u7e2e\u3084\u30ce\u30a4\u30ba\u9664\u53bb\u306b\u975e\u5e38\u306b\u6709\u7528\u3067\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#5","title":"\u5fdc\u7528\u554f\u984c5\u306e\u89e3\u7b54","text":"<p>\u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u306f\u3001100\u4eba\u306e\u60a3\u8005\u304b\u3089\u53ce\u96c6\u3055\u308c\u305f10\u7a2e\u985e\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\uff08100\u00d710\u884c\u5217\uff09\u306e\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u91cd\u8981\u306a\u6d1e\u5bdf\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\uff08\u5b9f\u4f8b</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#1_1","title":"1. \u30c7\u30fc\u30bf\u306e\u6f5c\u5728\u69cb\u9020\u306e\u62bd\u51fa","text":"<p>\u7279\u7570\u5024\u5206\u89e3\u3092\u9069\u7528\u3059\u308b\u3068\u3001\u30c7\u30fc\u30bf\u306e\u6f5c\u5728\u7684\u306a\u69cb\u9020\u3092\u7279\u7570\u5024\u3068\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306b\u3088\u3063\u3066\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002100\u00d710\u306e\u884c\u5217 \\(X\\) \u3092\u7279\u7570\u5024\u5206\u89e3\u3059\u308b\u3068\uff1a</p> \\[X = U\\Sigma V^T\\] <p>\u3053\u3053\u3067\uff1a - \\(U\\) \u306f100\u00d7100\u306e\u76f4\u4ea4\u884c\u5217\u3067\u3001\u5217\uff08\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff09\u306f\u60a3\u8005\u306e\u300c\u6f5c\u5728\u30d1\u30bf\u30fc\u30f3\u300d\u3092\u8868\u3059 - \\(\\Sigma\\) \u306f100\u00d710\u306e\u5bfe\u89d2\u884c\u5217\u3067\u3001\u7279\u7570\u5024\u3092\u5bfe\u89d2\u6210\u5206\u306b\u6301\u3064 - \\(V^T\\) \u306f10\u00d710\u306e\u76f4\u4ea4\u884c\u5217\u3067\u3001\u884c\uff08\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff09\u306f\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u300c\u7279\u5fb4\u30d1\u30bf\u30fc\u30f3\u300d\u3092\u8868\u3059</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#2_1","title":"2. \u4e3b\u8981\u306a\u5909\u52d5\u30d1\u30bf\u30fc\u30f3\u306e\u7279\u5b9a","text":"<p>\u7279\u7570\u5024\u306e\u5927\u304d\u3055\u306f\u3001\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u91cd\u8981\u5ea6\u3092\u793a\u3057\u307e\u3059\u3002\u6700\u5927\u306e\u7279\u7570\u5024 \\(\\sigma_1\\) \u306b\u5bfe\u5fdc\u3059\u308b\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(v_1\\) \u306f\u3001\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u9593\u306e\u6700\u3082\u652f\u914d\u7684\u306a\u76f8\u95a2\u30d1\u30bf\u30fc\u30f3\u3092\u8868\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\uff1a</p> <ul> <li>\\(v_1\\) \u306e\u6210\u5206\u304c\u8840\u5727\u3068\u5fc3\u62cd\u6570\u306b\u5927\u304d\u306a\u5024\u3092\u6301\u3064\u5834\u5408\u3001\u3053\u308c\u3089\u306e\u6307\u6a19\u304c\u5f37\u304f\u76f8\u95a2\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u5506\u3057\u307e\u3059</li> <li>\\(v_1\\) \u306e\u6210\u5206\u304c\u6b63\u8ca0\u306e\u5024\u3092\u6301\u3064\u5834\u5408\u3001\u305d\u308c\u3089\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u9593\u306b\u9006\u76f8\u95a2\u304c\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u307e\u3059</li> </ul> <p>\u540c\u69d8\u306b\u3001\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(u_1\\) \u306f\u3001\u3053\u306e\u30d1\u30bf\u30fc\u30f3\u306b\u6700\u3082\u5f37\u304f\u53cd\u5fdc\u3059\u308b\u60a3\u8005\u30b0\u30eb\u30fc\u30d7\u3092\u7279\u5b9a\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#3_1","title":"3. \u60a3\u8005\u306e\u5206\u985e\u3068\u30b5\u30d6\u30b0\u30eb\u30fc\u30d7\u306e\u767a\u898b","text":"<p>\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(U\\) \u306e\u5217\u306f\u3001\u60a3\u8005\u3092\u7570\u306a\u308b\u300c\u6f5c\u5728\u7684\u5065\u5eb7\u72b6\u614b\u300d\u306b\u5206\u985e\u3059\u308b\u305f\u3081\u306e\u57fa\u5e95\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\uff1a</p> <ul> <li>\u7279\u5b9a\u306e\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(u_i\\) \u306b\u304a\u3051\u308b\u5024\u306b\u57fa\u3065\u3044\u3066\u60a3\u8005\u3092\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u3001\u985e\u4f3c\u3057\u305f\u5065\u5eb7\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u3092\u6301\u3064\u30b5\u30d6\u30b0\u30eb\u30fc\u30d7\u3092\u7279\u5b9a\u3067\u304d\u307e\u3059</li> <li>\u3053\u308c\u3089\u306e\u30b5\u30d6\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u7570\u306a\u308b\u75c5\u614b\u751f\u7406\u5b66\u7684\u30d7\u30ed\u30bb\u30b9\u3084\u6cbb\u7642\u53cd\u5fdc\u30d1\u30bf\u30fc\u30f3\u3092\u8868\u3059\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059</li> </ul> <p>\u5b9f\u969b\u306b\u306f\u3001\u5404\u60a3\u8005\u3092\u4e0a\u4f4d2\u301c3\u500b\u306e\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u7a7a\u9593\u306b\u30d7\u30ed\u30c3\u30c8\u3059\u308b\u3053\u3068\u3067\u3001\u8996\u899a\u7684\u306b\u60a3\u8005\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u8b58\u5225\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#4_1","title":"4. \u6b21\u5143\u524a\u6e1b\u3068\u30ce\u30a4\u30ba\u9664\u53bb","text":"<p>\u7279\u7570\u5024\u306e\u5927\u304d\u3055\u304c\u6025\u901f\u306b\u6e1b\u5c11\u3059\u308b\u5834\u5408\u3001\u4e0a\u4f4dk\u500b\u306e\u7279\u7570\u5024\u3068\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u3092\u8fd1\u4f3c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> \\[X_k = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T\\] <p>\u3053\u306e\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u306b\u3088\u308a\uff1a - \u30c7\u30fc\u30bf\u306e\u672c\u8cea\u7684\u306a\u69cb\u9020\u3092\u4fdd\u6301\u3057\u306a\u304c\u3089\u6b21\u5143\u3092\u524a\u6e1b\u3067\u304d\u307e\u3059 - \u6e2c\u5b9a\u30ce\u30a4\u30ba\u3084\u5076\u7136\u306e\u5909\u52d5\u3092\u9664\u53bb\u3067\u304d\u307e\u3059 - \u5c0f\u3055\u3044\u7279\u7570\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u6210\u5206\u306f\u3001\u591a\u304f\u306e\u5834\u5408\u30ce\u30a4\u30ba\u3092\u8868\u3057\u3066\u3044\u307e\u3059</p> <p>\u4f8b\u3048\u3070\u3001\u4e0a\u4f4d3\u3064\u306e\u7279\u7570\u5024\u3067\u5168\u5909\u52d5\u306e80%\u3092\u8aac\u660e\u3067\u304d\u308b\u5834\u5408\u300110\u6b21\u5143\u306e\u30c7\u30fc\u30bf\u30923\u6b21\u5143\u306b\u52b9\u679c\u7684\u306b\u524a\u6e1b\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#5_1","title":"5. \u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u91cd\u8981\u5ea6\u8a55\u4fa1","text":"<p>\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(V\\) \u306e\u884c\u306f\u3001\u5404\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u304c\u30c7\u30fc\u30bf\u306e\u4e3b\u8981\u306a\u5909\u52d5\u30d1\u30bf\u30fc\u30f3\u306b\u3069\u306e\u3088\u3046\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u793a\u3057\u307e\u3059\uff1a</p> <ul> <li>\u8907\u6570\u306e\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3067\u5927\u304d\u306a\u6210\u5206\u3092\u6301\u3064\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306f\u3001\u60a3\u8005\u7fa4\u306e\u3055\u307e\u3056\u307e\u306a\u5065\u5eb7\u5074\u9762\u306b\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3066\u3044\u307e\u3059</li> <li>\u7279\u5b9a\u306e\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3067\u5927\u304d\u306a\u5024\u3092\u6301\u3064\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u5171\u5909\u52d5\u3059\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u304f\u3001\u5171\u901a\u306e\u751f\u7406\u5b66\u7684\u7d4c\u8def\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059</li> </ul> <p>\u3053\u308c\u306b\u3088\u308a\u3001\u8a3a\u65ad\u3084\u6cbb\u7642\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u306b\u6700\u3082\u6709\u7528\u306a\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u7279\u5b9a\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/39-singular-value-decomposition/#6_1","title":"6. \u6642\u9593\u7684\u5909\u5316\u306e\u691c\u51fa\u3068\u4e88\u6e2c","text":"<p>\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306bSVD\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u6642\u9593\u3068\u3068\u3082\u306b\u5909\u5316\u3059\u308b\u30d1\u30bf\u30fc\u30f3\u3092\u7279\u5b9a\u3067\u304d\u307e\u3059\uff1a</p> <ul> <li>\u4e3b\u8981\u306a\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u6642\u9593\u7684\u5909\u5316\u3092\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u3001\u75be\u60a3\u306e\u9032\u884c\u3084\u6cbb\u7642\u3078\u306e\u53cd\u5fdc\u3092\u8ffd\u8de1\u3067\u304d\u307e\u3059</li> <li>\u60a3\u8005\u306e\u6642\u9593\u7684\u8ecc\u8de1\u3092\u7279\u7570\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u5206\u6790\u3059\u308b\u3053\u3068\u3067\u3001\u4e88\u5f8c\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u958b\u767a\u3067\u304d\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059</li> </ul> <p>\u4f8b\u3048\u3070\u3001\u6cbb\u7642\u306b\u826f\u304f\u53cd\u5fdc\u3059\u308b\u60a3\u8005\u306f\u7279\u7570\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u985e\u4f3c\u3057\u305f\u8ecc\u8de1\u3092\u305f\u3069\u308b\u50be\u5411\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II","text":""},{"location":"lectures/LA/40-singular-value-decomposition/#40","title":"\u7b2c40\u56de\u8b1b\u7fa9\uff1a\u7279\u7570\u5024\u5206\u89e3\u306e\u5fdc\u7528","text":""},{"location":"lectures/LA/40-singular-value-decomposition/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c40\u56de \u95a2\u9023\u9805\u76ee: \u7279\u7570\u5024\u5206\u89e3\u3001\u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\u3001\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u3001\u30c7\u30fc\u30bf\u5727\u7e2e \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u7279\u7570\u5024\u5206\u89e3(SVD)\u306e\u57fa\u790e\u6982\u5ff5\u3001\u884c\u5217\u306e\u968e\u6570\u3001\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3057\u3001SVD\u3092\u7528\u3044\u305f\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u884c\u5217\u306e\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u3068\u30a8\u30c3\u30ab\u30fc\u30c8\u30fb\u30e4\u30f3\u30b0\u306e\u5b9a\u7406\u306e\u610f\u7fa9\u3092\u7406\u89e3\u3059\u308b</li> <li>SVD\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u5727\u7e2e\u3068\u30ce\u30a4\u30ba\u9664\u53bb\u306e\u539f\u7406\u3092\u7406\u89e3\u3059\u308b</li> <li>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308bSVD\u306e\u5fdc\u7528\u4f8b\u3092\u901a\u3058\u3066\u3001\u305d\u306e\u5b9f\u8df5\u7684\u4fa1\u5024\u3092\u8a8d\u8b58\u3059\u308b</li> </ol>"},{"location":"lectures/LA/40-singular-value-decomposition/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/40-singular-value-decomposition/#31","title":"3.1. \u7279\u7570\u5024\u5206\u89e3\u306e\u5fa9\u7fd2","text":"<p>\u7279\u7570\u5024\u5206\u89e3(SVD)\u306f\u4efb\u610f\u306e\u884c\u5217\u30923\u3064\u306e\u7279\u5225\u306a\u884c\u5217\u306e\u7a4d\u306b\u5206\u89e3\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002\\(m \\times n\\)\u306e\u884c\u5217\\(A\\)\u306b\u5bfe\u3057\u3066\uff1a</p> <p>\u5b9a\u7fa9: \u7279\u7570\u5024\u5206\u89e3 \u4efb\u610f\u306e\\(m \\times n\\)\u884c\u5217\\(A\\)\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5206\u89e3\u3067\u304d\u307e\u3059\uff1a \\(A = U\\Sigma V^T\\)</p> <p>\u3053\u3053\u3067\uff1a - \\(U\\)\u306f\\(m \\times m\\)\u306e\u76f4\u4ea4\u884c\u5217\uff08\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff09 - \\(\\Sigma\\)\u306f\\(m \\times n\\)\u306e\u5bfe\u89d2\u884c\u5217\uff08\u7279\u7570\u5024\u3092\u5bfe\u89d2\u306b\u914d\u7f6e\uff09 - \\(V\\)\u306f\\(n \\times n\\)\u306e\u76f4\u4ea4\u884c\u5217\uff08\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff09</p> <p>\u7279\u306b\u3001\\(\\Sigma\\)\u306e\u5bfe\u89d2\u6210\u5206\\(\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_r &gt; 0\\)\uff08\\(r\\)\u306f\\(A\\)\u306e\u968e\u6570\uff09\u306f\\(A\\)\u306e\u7279\u7570\u5024\u3068\u547c\u3070\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#32","title":"3.2. \u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217","text":"<p>\u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\uff08\u4e00\u822c\u9006\u884c\u5217\uff09\u306f\u3001\u901a\u5e38\u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u3067\u3082\u5b9a\u7fa9\u3055\u308c\u308b\u884c\u5217\u3067\u3059\u3002</p> <p>\u5b9a\u7fa9: \u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217 \u884c\u5217\\(A\\)\u306e\u64ec\u4f3c\u9006\u884c\u5217\\(A^+\\)\u306f\u3001\u4ee5\u4e0b\u306e4\u3064\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u552f\u4e00\u306e\u884c\u5217\u3067\u3059\uff1a</p> <ol> <li>\\(AA^+A = A\\)</li> <li>\\(A^+AA^+ = A^+\\)</li> <li>\\((AA^+)^T = AA^+\\)</li> <li>\\((A^+A)^T = A^+A\\)</li> </ol> <p>SVD\u3092\u7528\u3044\u305f\u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\u306e\u8a08\u7b97\u65b9\u6cd5\u306f\u975e\u5e38\u306b\u7f8e\u3057\u304f\u5b9f\u7528\u7684\u3067\u3059\u3002</p> <p>\u5b9a\u7406: SVD\u306b\u3088\u308b\u64ec\u4f3c\u9006\u884c\u5217\u306e\u8a08\u7b97 \u884c\u5217\\(A = U\\Sigma V^T\\)\u306e\u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\u306f\uff1a</p> <p>\\(A^+ = V\\Sigma^+ U^T\\)</p> <p>\u3053\u3053\u3067\\(\\Sigma^+\\)\u306f\\(\\Sigma\\)\u306e\u975e\u30bc\u30ed\u7279\u7570\u5024\u306e\u9006\u6570\u3092\u5bfe\u5fdc\u3059\u308b\u4f4d\u7f6e\u306b\u914d\u7f6e\u3057\u3001\u4ed6\u306e\u6210\u5206\u306f\u30bc\u30ed\u306e\u307e\u307e\u3068\u3057\u305f\u884c\u5217\u3067\u3059\u3002</p> <p>\u4f8b\u984c1: \u6b21\u306e\u884c\u5217\\(A\\)\u306e\u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\u3092\u6c42\u3081\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> \\[A = \\begin{pmatrix} 4 &amp; 0 \\\\ 0 &amp; 3 \\\\ 0 &amp; 0 \\end{pmatrix}\\] <p>\u89e3\u7b54: \u307e\u305a\\(A\\)\u306eSVD\u3092\u6c42\u3081\u307e\u3059\u3002\u3053\u306e\u5834\u5408\u3001\u7279\u7570\u5024\u306f\\(\\sigma_1 = 4\\)\u3068\\(\\sigma_2 = 3\\)\u3067\u3059\u3002</p> \\[U = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}, \\Sigma = \\begin{pmatrix} 4 &amp; 0 \\\\ 0 &amp; 3 \\\\ 0 &amp; 0 \\end{pmatrix}, V = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}\\] <p>\\(\\Sigma^+\\)\u3092\u6c42\u3081\u308b\u305f\u3081\u306b\u3001\u975e\u30bc\u30ed\u7279\u7570\u5024\u306e\u9006\u6570\u3092\u5bfe\u5fdc\u3059\u308b\u4f4d\u7f6e\u306b\u914d\u7f6e\u3057\u307e\u3059\uff1a</p> \\[\\Sigma^+ = \\begin{pmatrix} 1/4 &amp; 0 \\\\ 0 &amp; 1/3 \\\\ 0 &amp; 0 \\end{pmatrix}^T = \\begin{pmatrix} 1/4 &amp; 0 &amp; 0 \\\\ 0 &amp; 1/3 &amp; 0 \\end{pmatrix}\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\u64ec\u4f3c\u9006\u884c\u5217\u306f\uff1a</p> \\[A^+ = V\\Sigma^+ U^T = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1/4 &amp; 0 &amp; 0 \\\\ 0 &amp; 1/3 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}^T = \\begin{pmatrix} 1/4 &amp; 0 &amp; 0 \\\\ 0 &amp; 1/3 &amp; 0 \\end{pmatrix}\\]"},{"location":"lectures/LA/40-singular-value-decomposition/#33","title":"3.3. \u64ec\u4f3c\u9006\u884c\u5217\u306e\u5fdc\u7528","text":"<p>\u64ec\u4f3c\u9006\u884c\u5217\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5834\u9762\u3067\u7279\u306b\u6709\u7528\u3067\u3059\uff1a</p> <ol> <li> <p>\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306b\u3088\u308b\u7dda\u5f62\u56de\u5e30: \u904e\u5270\u6c7a\u5b9a\u7cfb\uff08\u65b9\u7a0b\u5f0f\u306e\u6570\u304c\u672a\u77e5\u6570\u3088\u308a\u591a\u3044\u5834\u5408\uff09\u3067\u6700\u5c0f\u4e8c\u4e57\u89e3\u3092\u6c42\u3081\u308b\u969b\u306b\u4f7f\u7528\u3002    \\(\\hat{x} = A^+ b\\)\u306f\\(\\|Ax - b\\|_2\\)\u3092\u6700\u5c0f\u5316\u3059\u308b\u89e3\u3067\u3059\u3002</p> </li> <li> <p>\u52a3\u6c7a\u5b9a\u7cfb\u306e\u6700\u5c0f\u30ce\u30eb\u30e0\u89e3: \u672a\u77e5\u6570\u304c\u65b9\u7a0b\u5f0f\u3088\u308a\u591a\u3044\u5834\u5408\u3001\\(A^+ b\\)\u306f\\(Ax = b\\)\u3092\u6e80\u305f\u3059\u89e3\u306e\u4e2d\u3067\u6700\u5c0f\u30ce\u30eb\u30e0\\(\\|x\\|_2\\)\u3092\u6301\u3064\u3082\u306e\u3067\u3059\u3002</p> </li> <li> <p>\u884c\u5217\u65b9\u7a0b\u5f0f\u306e\u89e3: \\(AXB = C\\)\u306e\u5f62\u306e\u884c\u5217\u65b9\u7a0b\u5f0f\u3092\u89e3\u304f\u969b\u306b\u5229\u7528\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u30e9\u30f3\u30af\u6b20\u640d\u30c7\u30fc\u30bf\u306e\u51e6\u7406: \u591a\u5909\u91cf\u89e3\u6790\u3067\u4e0d\u5b8c\u5168\u306a\u30c7\u30fc\u30bf\u3092\u6271\u3046\u5834\u5408\u306b\u6709\u7528\u3067\u3059\u3002</p> </li> </ol>"},{"location":"lectures/LA/40-singular-value-decomposition/#331","title":"3.3.1. \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\u3068\u3057\u3066\u306e\u64ec\u4f3c\u9006\u884c\u5217","text":"<p>\u9023\u7acb\u65b9\u7a0b\u5f0f\\(Ax = b\\)\u306b\u3064\u3044\u3066\u8003\u3048\u307e\u3057\u3087\u3046\u3002\u3053\u308c\u306b\u306f\u4ee5\u4e0b\u306e3\u3064\u306e\u5834\u5408\u304c\u3042\u308a\u307e\u3059\uff1a</p> <p>i) \u552f\u4e00\u89e3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408: \\(A\\)\u304c\u6b63\u65b9\u304b\u3064\u30d5\u30eb\u30e9\u30f3\u30af\u306a\u3089\u3070\u3001\\(x = A^{-1}b = A^+b\\)</p> <p>ii) \u904e\u5270\u6c7a\u5b9a\u7cfb\uff08\u65b9\u7a0b\u5f0f\u304c\u591a\u3059\u304e\u308b\u5834\u5408\uff09: \u53b3\u5bc6\u306a\u89e3\u306f\u4e00\u822c\u306b\u5b58\u5728\u305b\u305a\u3001\u6700\u5c0f\u4e8c\u4e57\u89e3\\(x = A^+b\\)\u304c\\(\\|Ax - b\\|_2\\)\u3092\u6700\u5c0f\u5316</p> <p>iii) \u52a3\u6c7a\u5b9a\u7cfb\uff08\u672a\u77e5\u6570\u304c\u591a\u3059\u304e\u308b\u5834\u5408\uff09: \u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u3001\\(x = A^+b\\)\u306f\u305d\u306e\u4e2d\u3067\u6700\u5c0f\u30ce\u30eb\u30e0\uff08\\(\\|x\\|_2\\)\u304c\u6700\u5c0f\uff09\u3092\u6301\u3064\u89e3</p> <p>\u4f8b\u984c4: \u4ee5\u4e0b\u306e\u904e\u5270\u6c7a\u5b9a\u7cfb\uff08\u65b9\u7a0b\u5f0f\u306e\u6570\u304c\u672a\u77e5\u6570\u3088\u308a\u591a\u3044\uff09\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u64ec\u4f3c\u9006\u884c\u5217\u3092\u7528\u3044\u3066\u89e3\u304d\u307e\u3057\u3087\u3046\u3002</p> \\[\\begin{align} x + y &amp;= 3 \\\\ 2x + y &amp;= 5 \\\\ x + 2y &amp;= 4 \\end{align}\\] <p>\u89e3\u7b54: \u307e\u305a\u3001\u3053\u306e\u65b9\u7a0b\u5f0f\u3092\u884c\u5217\u5f62\u5f0f\u3067\u8868\u3057\u307e\u3059\u3002</p> \\[A = \\begin{pmatrix} 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 3 \\\\ 5 \\\\ 4 \\end{pmatrix}\\] <p>\u65b9\u7a0b\u5f0f\u306e\u6570\uff083\uff09\u304c\u672a\u77e5\u6570\uff082\uff09\u3088\u308a\u591a\u3044\u305f\u3081\u3001\u4e00\u822c\u306b\u306f\u53b3\u5bc6\u306a\u89e3\u306f\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\u64ec\u4f3c\u9006\u884c\u5217\\(A^+\\)\u3092\u7528\u3044\u3066\u6700\u5c0f\u4e8c\u4e57\u89e3\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\\(A\\)\u306eSVD\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\uff08\u8a08\u7b97\u904e\u7a0b\u306f\u7701\u7565\u3057\u3001\u7d50\u679c\u306e\u307f\u793a\u3057\u307e\u3059\uff09</p> \\[U = \\begin{pmatrix} 0.47 &amp; -0.34 &amp; 0.81 \\\\ 0.74 &amp; 0.64 &amp; -0.21 \\\\ 0.47 &amp; -0.69 &amp; -0.55 \\end{pmatrix}, \\quad \\Sigma = \\begin{pmatrix} 3.74 &amp; 0 \\\\ 0 &amp; 1.00 \\\\ 0 &amp; 0 \\end{pmatrix}, \\quad V = \\begin{pmatrix} 0.83 &amp; -0.55 \\\\ 0.55 &amp; 0.83 \\end{pmatrix}\\] <p>\\(\\Sigma^+\\)\u3092\u8a08\u7b97\u3057\u307e\u3059:</p> \\[\\Sigma^+ = \\begin{pmatrix} 1/3.74 &amp; 0 &amp; 0 \\\\ 0 &amp; 1/1.00 &amp; 0 \\end{pmatrix} = \\begin{pmatrix} 0.27 &amp; 0 &amp; 0 \\\\ 0 &amp; 1.00 &amp; 0 \\end{pmatrix}\\] <p>\u64ec\u4f3c\u9006\u884c\u5217\\(A^+\\)\u306f:</p> \\[A^+ = V\\Sigma^+ U^T = \\begin{pmatrix} 0.83 &amp; -0.55 \\\\ 0.55 &amp; 0.83 \\end{pmatrix} \\begin{pmatrix} 0.27 &amp; 0 &amp; 0 \\\\ 0 &amp; 1.00 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 0.47 &amp; 0.74 &amp; 0.47 \\\\ -0.34 &amp; 0.64 &amp; -0.69 \\\\ 0.81 &amp; -0.21 &amp; -0.55 \\end{pmatrix}\\] <p>\u8a08\u7b97\u3059\u308b\u3068:</p> \\[A^+ \\approx \\begin{pmatrix} 0.37 &amp; 0.59 &amp; 0.04 \\\\ 0.07 &amp; 0.15 &amp; 0.59 \\end{pmatrix}\\] <p>\u6700\u5c0f\u4e8c\u4e57\u89e3\u306f:</p> \\[x = A^+b \\approx \\begin{pmatrix} 0.37 &amp; 0.59 &amp; 0.04 \\\\ 0.07 &amp; 0.15 &amp; 0.59 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 5 \\\\ 4 \\end{pmatrix} \\approx \\begin{pmatrix} 2.0 \\\\ 1.1 \\end{pmatrix}\\] <p>\u3053\u306e\u89e3\u304c\u5b9f\u969b\u306b\u6700\u5c0f\u4e8c\u4e57\u8aa4\u5dee\u3092\u6301\u3064\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059:</p> \\[\\|Ax - b\\|_2^2 = \\left\\| \\begin{pmatrix} 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix} \\begin{pmatrix} 2.0 \\\\ 1.1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 5 \\\\ 4 \\end{pmatrix} \\right\\|_2^2 = \\left\\| \\begin{pmatrix} 3.1 \\\\ 5.1 \\\\ 4.2 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 5 \\\\ 4 \\end{pmatrix} \\right\\|_2^2 = 0.06\\] <p>\u3057\u305f\u304c\u3063\u3066\u3001\\(x \\approx 2.0, y \\approx 1.1\\)\u304c\u6700\u5c0f\u4e8c\u4e57\u89e3\u3068\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u89e3\u306f\u5143\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u53b3\u5bc6\u306b\u6e80\u305f\u3059\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u4e8c\u4e57\u8aa4\u5dee\u306e\u7dcf\u548c\u3092\u6700\u5c0f\u306b\u3059\u308b\u89e3\u3067\u3059\u3002</p> <p>\u4f8b\u984c5: \u4ee5\u4e0b\u306e\u52a3\u6c7a\u5b9a\u7cfb\uff08\u672a\u77e5\u6570\u304c\u65b9\u7a0b\u5f0f\u3088\u308a\u591a\u3044\uff09\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u64ec\u4f3c\u9006\u884c\u5217\u3092\u7528\u3044\u3066\u89e3\u304d\u307e\u3057\u3087\u3046\u3002</p> \\[\\begin{align} x + y + z &amp;= 6 \\\\ 2x - y + z &amp;= 3 \\end{align}\\] <p>\u89e3\u7b54: \u884c\u5217\u5f62\u5f0f\u3067\u306f:</p> \\[A = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; -1 &amp; 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 6 \\\\ 3 \\end{pmatrix}\\] <p>\u65b9\u7a0b\u5f0f\u306e\u6570\uff082\uff09\u304c\u672a\u77e5\u6570\uff083\uff09\u3088\u308a\u5c11\u306a\u3044\u305f\u3081\u3001\u7121\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\u3002\u64ec\u4f3c\u9006\u884c\u5217\\(A^+\\)\u3092\u7528\u3044\u3066\u6700\u5c0f\u30ce\u30eb\u30e0\u89e3\u3092\u6c42\u3081\u307e\u3059\u3002</p> <p>\\(A\\)\u306eSVD\u3092\u8a08\u7b97\u3057\u307e\u3059:</p> \\[U \\approx \\begin{pmatrix} 0.41 &amp; 0.91 \\\\ 0.91 &amp; -0.41 \\end{pmatrix}, \\quad \\Sigma \\approx \\begin{pmatrix} 2.65 &amp; 0 &amp; 0 \\\\ 0 &amp; 1.50 &amp; 0 \\end{pmatrix}, \\quad V \\approx \\begin{pmatrix} 0.74 &amp; -0.04 &amp; 0.67 \\\\ -0.11 &amp; 0.76 &amp; 0.64 \\\\ 0.66 &amp; 0.65 &amp; -0.38 \\end{pmatrix}\\] <p>\\(\\Sigma^+\\)\u3092\u8a08\u7b97\u3057\u307e\u3059:</p> \\[\\Sigma^+ = \\begin{pmatrix} 1/2.65 &amp; 0 \\\\ 0 &amp; 1/1.50 \\\\ 0 &amp; 0 \\end{pmatrix} \\approx \\begin{pmatrix} 0.38 &amp; 0 \\\\ 0 &amp; 0.67 \\\\ 0 &amp; 0 \\end{pmatrix}\\] <p>\u64ec\u4f3c\u9006\u884c\u5217\\(A^+\\)\u306f:</p> \\[A^+ = V\\Sigma^+ U^T \\approx \\begin{pmatrix} 0.74 &amp; -0.04 &amp; 0.67 \\\\ -0.11 &amp; 0.76 &amp; 0.64 \\\\ 0.66 &amp; 0.65 &amp; -0.38 \\end{pmatrix} \\begin{pmatrix} 0.38 &amp; 0 \\\\ 0 &amp; 0.67 \\\\ 0 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 0.41 &amp; 0.91 \\\\ 0.91 &amp; -0.41 \\end{pmatrix}\\] <p>\u8a08\u7b97\u3059\u308b\u3068:</p> \\[A^+ \\approx \\begin{pmatrix} 0.31 &amp; 0.35 \\\\ 0.61 &amp; -0.31 \\\\ 0.25 &amp; 0.31 \\end{pmatrix}\\] <p>\u6700\u5c0f\u30ce\u30eb\u30e0\u89e3\u306f:</p> \\[x = A^+b \\approx \\begin{pmatrix} 0.31 &amp; 0.35 \\\\ 0.61 &amp; -0.31 \\\\ 0.25 &amp; 0.31 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 3 \\end{pmatrix} \\approx \\begin{pmatrix} 2.9 \\\\ 2.7 \\\\ 2.4 \\end{pmatrix}\\] <p>\u3053\u306e\u89e3\u304c\u5143\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u6e80\u305f\u3059\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059:</p> \\[Ax = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; -1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 2.9 \\\\ 2.7 \\\\ 2.4 \\end{pmatrix} \\approx \\begin{pmatrix} 8.0 \\\\ 3.1 \\end{pmatrix}\\] <p>\u8a08\u7b97\u8aa4\u5dee\u304c\u3042\u308a\u3001\u5143\u306e\u65b9\u7a0b\u5f0f\u3092\u53b3\u5bc6\u306b\u306f\u6e80\u305f\u3057\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\u3002\u3088\u308a\u7cbe\u5bc6\u306a\u8a08\u7b97\u3092\u884c\u3046\u3068:</p> \\[x = A^+b = \\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix}\\] <p>\u3053\u308c\u306f: \\(\\(Ax = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; -1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 3 \\end{pmatrix} = b\\)\\)</p> <p>\u307e\u305f\u3001\u3053\u306e\u89e3\u306e\u30ce\u30eb\u30e0\u306f\\(\\|x\\|_2 = \\sqrt{2^2 + 3^2 + 1^2} = \\sqrt{14} \\approx 3.74\\)\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u4ed6\u306e\u89e3\u3001\u4f8b\u3048\u3070\\(x' = (1, 4, 1)^T\\)\u3082\u65b9\u7a0b\u5f0f\u3092\u6e80\u305f\u3057\u307e\u3059\u304c\u3001\u305d\u306e\u30ce\u30eb\u30e0\u306f\\(\\|x'\\|_2 = \\sqrt{18} \\approx 4.24\\)\u3068\u306a\u308a\u3001\u6700\u5c0f\u30ce\u30eb\u30e0\u89e3\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u64ec\u4f3c\u9006\u884c\u5217\u306f\u904e\u5270\u6c7a\u5b9a\u7cfb\u3067\u306f\u6700\u5c0f\u4e8c\u4e57\u89e3\u3092\u3001\u52a3\u6c7a\u5b9a\u7cfb\u3067\u306f\u6700\u5c0f\u30ce\u30eb\u30e0\u89e3\u3092\u4e0e\u3048\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u5b9f\u4e16\u754c\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3084\u4fe1\u53f7\u51e6\u7406\u306a\u3069\u3001\u591a\u304f\u306e\u5fdc\u7528\u3067\u91cd\u8981\u306a\u6027\u8cea\u3067\u3059\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/40-singular-value-decomposition/#41","title":"4.1. \u884c\u5217\u306e\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c","text":"<p>SVD\u306e\u91cd\u8981\u306a\u5fdc\u7528\u306e\u4e00\u3064\u306f\u884c\u5217\u306e\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u3067\u3059\u3002\u3053\u308c\u306f\u30c7\u30fc\u30bf\u5727\u7e2e\u3084\u30ce\u30a4\u30ba\u9664\u53bb\u306b\u5e83\u304f\u5229\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>\u5b9a\u7406: \u30a8\u30c3\u30ab\u30fc\u30c8\u30fb\u30e4\u30f3\u30b0\u306e\u5b9a\u7406 \u884c\u5217\\(A\\)\u306e\u30e9\u30f3\u30af\\(k\\)\u306e\u6700\u826f\u8fd1\u4f3c\\(A_k\\)\uff08\\(\\|A - A_k\\|_F\\)\u3092\u6700\u5c0f\u5316\u3059\u308b\\(\\text{rank}(A_k) \\leq k\\)\u306e\u884c\u5217\uff09\u306f\u3001\\(A\\)\u306eSVD\u8868\u73fe\\(A = U\\Sigma V^T\\)\u306e\u4e0a\u4f4d\\(k\\)\u500b\u306e\u7279\u7570\u5024\u306e\u307f\u3092\u4fdd\u6301\u3057\u3066\u4ed6\u3092\u30bc\u30ed\u306b\u3057\u305f\u3082\u306e\u3067\u3059\uff1a</p> <p>\\(A_k = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T\\)</p> <p>\u3053\u3053\u3067\\(u_i\\)\u306f\\(U\\)\u306e\u5217\u3001\\(v_i\\)\u306f\\(V\\)\u306e\u5217\u3067\u3059\u3002\u3053\u306e\u8fd1\u4f3c\u306e\u8aa4\u5dee\u306f\uff1a</p> <p>\\(\\|A - A_k\\|_F^2 = \\sum_{i=k+1}^{r} \\sigma_i^2\\)</p> <p>\u305f\u3060\u3057\u3001\\(\\|\\cdot\\|_F\\)\u306f\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u3053\u306e\u5b9a\u7406\u304b\u3089\u3001SVD\u306f\u60c5\u5831\u306e\u91cd\u8981\u5ea6\u306b\u5fdc\u3058\u305f\u5206\u89e3\u3092\u63d0\u4f9b\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u6700\u3082\u91cd\u8981\u306a\u60c5\u5831\u306f\u5927\u304d\u306a\u7279\u7570\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306b\u96c6\u7d04\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>\u4f8b\u984c2: \u6b21\u306e\u884c\u5217A\u306e\u30e9\u30f3\u30af1\u8fd1\u4f3c\u3092\u6c42\u3081\u307e\u3057\u3087\u3046\u3002</p> \\[A = \\begin{pmatrix} 4 &amp; 1 \\\\ 2 &amp; 3 \\end{pmatrix}\\] <p>\u89e3\u7b54: \u307e\u305a\u3001SVD\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\uff08\u6570\u5024\u306f\u8fd1\u4f3c\u5024\u3067\u3059\uff09 \u7279\u7570\u5024: \\(\\sigma_1 \\approx 5.3\\), \\(\\sigma_2 \\approx 1.9\\) \u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb: \\(u_1 \\approx (0.78, 0.63)^T\\), \\(u_2 \\approx (-0.63, 0.78)^T\\) \u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb: \\(v_1 \\approx (0.78, 0.63)^T\\), \\(v_2 \\approx (-0.63, 0.78)^T\\)</p> <p>\u30e9\u30f3\u30af1\u8fd1\u4f3c\u306f\u6700\u5927\u7279\u7570\u5024\u3068\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u307f\u3092\u4f7f\u7528\u3057\u307e\u3059\uff1a \\(\\(A_1 = \\sigma_1 u_1 v_1^T \\approx 5.3 \\begin{pmatrix} 0.78 \\\\ 0.63 \\end{pmatrix} \\begin{pmatrix} 0.78 &amp; 0.63 \\end{pmatrix} \\approx \\begin{pmatrix} 3.2 &amp; 2.6 \\\\ 2.6 &amp; 2.1 \\end{pmatrix}\\)\\)</p> <p>\u8fd1\u4f3c\u8aa4\u5dee\u306f\\(\\|A - A_1\\|_F^2 = \\sigma_2^2 \\approx 3.61\\)\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#42-svd","title":"4.2. \u30c8\u30e9\u30f3\u30b1\u30fc\u30b7\u30e7\u30f3SVD\u3068\u6700\u9069\u8fd1\u4f3c","text":"<p>\u30c7\u30fc\u30bf\u304c\u591a\u304f\u306e\u7279\u7570\u5024\u3092\u6301\u3064\u5834\u5408\u3001\u4e0a\u4f4d\\(k\\)\u500b\u306e\u7279\u7570\u5024\u306e\u307f\u3092\u4fdd\u6301\u3057\u3001\u4ed6\u3092\u30bc\u30ed\u306b\u3059\u308b\u30c8\u30e9\u30f3\u30b1\u30fc\u30b7\u30e7\u30f3SVD\u306f\u52b9\u679c\u7684\u306a\u30c7\u30fc\u30bf\u5727\u7e2e\u6cd5\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u5b9a\u7fa9: \u30c8\u30e9\u30f3\u30b1\u30fc\u30b7\u30e7\u30f3SVD \u884c\u5217\\(A\\)\u306e\u30c8\u30e9\u30f3\u30b1\u30fc\u30b7\u30e7\u30f3SVD\uff08\u5207\u65adSVD\uff09\u306f\uff1a</p> <p>\\(A_k = U_k \\Sigma_k V_k^T\\)</p> <p>\u3053\u3053\u3067\\(U_k\\)\u306f\\(U\\)\u306e\u6700\u521d\u306e\\(k\\)\u5217\u3001\\(\\Sigma_k\\)\u306f\u4e0a\u4f4d\\(k\\)\u500b\u306e\u7279\u7570\u5024\u3092\u5bfe\u89d2\u306b\u6301\u3064\\(k \\times k\\)\u884c\u5217\u3001\\(V_k\\)\u306f\\(V\\)\u306e\u6700\u521d\u306e\\(k\\)\u5217\u3067\u3059\u3002</p> <p>\u30c8\u30e9\u30f3\u30b1\u30fc\u30b7\u30e7\u30f3SVD\u306e\u91cd\u8981\u306a\u7279\u6027\uff1a</p> <ol> <li> <p>\u683c\u7d0d\u306b\u5fc5\u8981\u306a\u30c7\u30fc\u30bf\u91cf\u306e\u524a\u6e1b\uff1a\u5143\u306e\u884c\u5217\u306f\\(m \\times n\\)\u500b\u306e\u8981\u7d20\u3092\u6301\u3061\u307e\u3059\u304c\u3001\u30c8\u30e9\u30f3\u30b1\u30fc\u30b7\u30e7\u30f3SVD\u306f\\((m+n+1) \\times k\\)\u500b\u306e\u8981\u7d20\u3067\u8868\u73fe\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u8fd1\u4f3c\u8aa4\u5dee\u306e\u7ba1\u7406\uff1a\u7d2f\u7a4d\u7279\u7570\u5024\u30a8\u30cd\u30eb\u30ae\u30fc\u6bd4\uff08\u4e0a\u4f4d\\(k\\)\u500b\u306e\u7279\u7570\u5024\u306e\u4e8c\u4e57\u548c\u3092\u5168\u7279\u7570\u5024\u306e\u4e8c\u4e57\u548c\u3067\u5272\u3063\u305f\u5024\uff09\u3092\u7528\u3044\u3066\u4fdd\u6301\u3059\u308b\u60c5\u5831\u91cf\u3092\u5236\u5fa1\u3067\u304d\u307e\u3059\u3002    \\(E_k = \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{r} \\sigma_i^2}\\)</p> </li> <li> <p>\u30b9\u30da\u30af\u30c8\u30e9\u30eb\u30ce\u30eb\u30e0\u3067\u306e\u6700\u9069\u6027\uff1a\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u3060\u3051\u3067\u306a\u304f\u3001\u30b9\u30da\u30af\u30c8\u30e9\u30eb\u30ce\u30eb\u30e0\\(\\|A - A_k\\|_2 = \\sigma_{k+1}\\)\u306b\u95a2\u3057\u3066\u3082\u6700\u9069\u306a\u8fd1\u4f3c\u3092\u4e0e\u3048\u307e\u3059\u3002</p> </li> </ol>"},{"location":"lectures/LA/40-singular-value-decomposition/#43","title":"4.3. \u30c7\u30fc\u30bf\u5727\u7e2e\u3068\u30ce\u30a4\u30ba\u9664\u53bb","text":"<p>SVD\u306b\u3088\u308b\u30c7\u30fc\u30bf\u5727\u7e2e\u3068\u30ce\u30a4\u30ba\u9664\u53bb\u306e\u30e1\u30ab\u30cb\u30ba\u30e0\u3092\u7406\u89e3\u3057\u307e\u3057\u3087\u3046\u3002</p> <p>\u30c7\u30fc\u30bf\u5727\u7e2e:  \u591a\u304f\u306e\u5b9f\u4e16\u754c\u306e\u30c7\u30fc\u30bf\u306f\u5b9f\u8cea\u7684\u306b\u4f4e\u30e9\u30f3\u30af\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u5c11\u6570\u306e\u6f5c\u5728\u7684\u30d1\u30bf\u30fc\u30f3\uff08\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff09\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u8868\u73fe\u3067\u304d\u307e\u3059\u3002SVD\u306f\u3053\u308c\u3089\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u81ea\u52d5\u7684\u306b\u62bd\u51fa\u3057\u3001\u30c7\u30fc\u30bf\u306e\u52b9\u7387\u7684\u306a\u8868\u73fe\u3092\u53ef\u80fd\u306b\u3057\u307e\u3059\u3002</p> <p>\u30ce\u30a4\u30ba\u9664\u53bb: \u30ce\u30a4\u30ba\u306f\u901a\u5e38\u3001\u5c0f\u3055\u306a\u7279\u7570\u5024\u306b\u95a2\u9023\u4ed8\u3051\u3089\u308c\u3066\u3044\u307e\u3059\u3002\u4e0a\u4f4d\u306e\u7279\u7570\u5024\u3068\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u307f\u3092\u4fdd\u6301\u3059\u308b\u3053\u3068\u3067\u3001\u30ce\u30a4\u30ba\u306e\u5f71\u97ff\u3092\u6e1b\u5c11\u3055\u305b\u305f\u300c\u30af\u30ea\u30fc\u30f3\u306a\u300d\u30c7\u30fc\u30bf\u8868\u73fe\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u4f8b\u984c3\uff1a\u753b\u50cf\u5727\u7e2e\u306b\u304a\u3051\u308bSVD\u306e\u5fdc\u7528 512\u00d7512\u30d4\u30af\u30bb\u30eb\u306e\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u753b\u50cf\u3092\u8003\u3048\u307e\u3059\u3002\u3053\u308c\u306f512\u00d7512\u306e\u884c\u5217\\(A\\)\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u307e\u3059\u3002\u3053\u306e\u753b\u50cf\u306eSVD\u3092\u8a08\u7b97\u3057\u3001\u4e0a\u4f4d50\u500b\u306e\u7279\u7570\u5024\u306e\u307f\u3092\u4fdd\u6301\u3059\u308b\u5834\u5408\uff1a</p> <ul> <li>\u5143\u306e\u30c7\u30fc\u30bf\u91cf: \\(512 \\times 512 = 262,144\\)\u500b\u306e\u8981\u7d20</li> <li>\u5727\u7e2e\u5f8c\u306e\u30c7\u30fc\u30bf\u91cf: \\((512 + 512 + 1) \\times 50 = 51,250\\)\u500b\u306e\u8981\u7d20</li> <li>\u5727\u7e2e\u7387: \u7d0480%\u306e\u524a\u6e1b</li> </ul> <p>\u4e0a\u4f4d50\u500b\u306e\u7279\u7570\u5024\u304c\u5168\u30a8\u30cd\u30eb\u30ae\u30fc\u306e95%\u3092\u5360\u3081\u308b\u5834\u5408\u3001\u753b\u8cea\u306e\u52a3\u5316\u306f\u308f\u305a\u304b\u3067\u3042\u308a\u306a\u304c\u3089\u5927\u5e45\u306a\u30c7\u30fc\u30bf\u524a\u6e1b\u304c\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/40-singular-value-decomposition/#51-svd","title":"5.1. SVD\u3068\u64ec\u4f3c\u9006\u884c\u5217\u306e\u8a08\u7b97","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import svd, pinv\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[4, 0], [0, 3], [0, 0]])\nprint(\"\u5143\u306e\u884c\u5217 A:\")\nprint(A)\n\n# SVD\u306e\u8a08\u7b97\nU, sigma, VT = svd(A, full_matrices=True)\nprint(\"\\nSVD\u7d50\u679c:\")\nprint(\"U:\", U)\nprint(\"sigma:\", sigma)\nprint(\"VT:\", VT)\n\n# \u5bfe\u89d2\u884c\u5217\u03a3\u306e\u69cb\u7bc9\nSigma = np.zeros((A.shape[0], A.shape[1]))\nfor i in range(min(A.shape)):\n    if sigma[i] &gt; 0:  # \u975e\u30bc\u30ed\u306e\u7279\u7570\u5024\u306e\u307f\u3092\u8003\u616e\n        Sigma[i, i] = sigma[i]\nprint(\"\\n\u5bfe\u89d2\u884c\u5217 Sigma:\")\nprint(Sigma)\n\n# \u64ec\u4f3c\u9006\u884c\u5217\u03a3+\u306e\u69cb\u7bc9\nSigma_plus = np.zeros((A.shape[1], A.shape[0]))\nfor i in range(min(A.shape)):\n    if sigma[i] &gt; 0:  # \u975e\u30bc\u30ed\u306e\u7279\u7570\u5024\u306e\u307f\u9006\u6570\u3092\u53d6\u308b\n        Sigma_plus[i, i] = 1.0 / sigma[i]\nprint(\"\\n\u64ec\u4f3c\u9006\u884c\u5217 Sigma+:\")\nprint(Sigma_plus)\n\n# SVD\u3092\u4f7f\u3063\u305f\u64ec\u4f3c\u9006\u884c\u5217\u306e\u8a08\u7b97\nA_plus_manual = VT.T @ Sigma_plus @ U.T\nprint(\"\\nSVD\u304b\u3089\u8a08\u7b97\u3057\u305f\u64ec\u4f3c\u9006\u884c\u5217:\")\nprint(A_plus_manual)\n\n# NumPy\u306e\u95a2\u6570\u3092\u4f7f\u3063\u305f\u64ec\u4f3c\u9006\u884c\u5217\u306e\u8a08\u7b97\uff08\u6bd4\u8f03\u7528\uff09\nA_plus_numpy = pinv(A)\nprint(\"\\nNumPy\u306epinv\u95a2\u6570\u306b\u3088\u308b\u64ec\u4f3c\u9006\u884c\u5217:\")\nprint(A_plus_numpy)\n\n# A*A+*A\u3092\u8a08\u7b97\u3057\u3066\u5143\u306e\u884c\u5217\u306b\u623b\u308b\u3053\u3068\u3092\u78ba\u8a8d\nAA_plus_A = A @ A_plus_manual @ A\nprint(\"\\nA*A+*A (\u5143\u306e\u884c\u5217\u306b\u623b\u308b\u3053\u3068\u3092\u78ba\u8a8d):\")\nprint(AA_plus_A)\nprint(\"\\nA\uff08\u5143\u306e\u884c\u5217\uff09\u3068\u306e\u5dee\u306e\u30ce\u30eb\u30e0:\", np.linalg.norm(A - AA_plus_A))\n</code></pre>"},{"location":"lectures/LA/40-singular-value-decomposition/#52","title":"5.2. \u884c\u5217\u306e\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u306e\u5b9f\u88c5","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import svd\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# \u884c\u5217\u306e\u5b9a\u7fa9\nA = np.array([[4, 1], [2, 3]])\nprint(\"\u5143\u306e\u884c\u5217 A:\")\nprint(A)\n\n# SVD\u306e\u8a08\u7b97\nU, sigma, VT = svd(A)\nprint(\"\\nSVD\u7d50\u679c:\")\nprint(\"U:\", U)\nprint(\"sigma:\", sigma)\nprint(\"VT:\", VT)\n\n# \u5404\u30e9\u30f3\u30af\u3067\u306e\u8fd1\u4f3c\u884c\u5217\u306e\u8a08\u7b97\nA_approx = {}\nfor r in range(1, min(A.shape) + 1):\n    # \u30e9\u30f3\u30afr\u306e\u8fd1\u4f3c\u884c\u5217\u3092\u8a08\u7b97\n    A_approx[r] = (U[:, :r] * sigma[:r]) @ VT[:r, :]\n    print(f\"\\n\u30e9\u30f3\u30af{r}\u306e\u8fd1\u4f3c\u884c\u5217:\")\n    print(A_approx[r])\n    print(f\"\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u8aa4\u5dee: {np.linalg.norm(A - A_approx[r], 'fro'):.6f}\")\n    print(f\"\u30b9\u30da\u30af\u30c8\u30e9\u30eb\u30ce\u30eb\u30e0\u8aa4\u5dee: {np.linalg.norm(A - A_approx[r], 2):.6f}\")\n\n# \u7d2f\u7a4d\u7279\u7570\u5024\u30a8\u30cd\u30eb\u30ae\u30fc\u6bd4\u306e\u8a08\u7b97\ns_squared = sigma**2\nenergy_ratio = np.cumsum(s_squared) / np.sum(s_squared)\nprint(\"\\n\u7d2f\u7a4d\u7279\u7570\u5024\u30a8\u30cd\u30eb\u30ae\u30fc\u6bd4:\")\nfor r in range(len(sigma)):\n    print(f\"\u4e0a\u4f4d{r+1}\u500b\u306e\u7279\u7570\u5024: {energy_ratio[r]*100:.2f}%\")\n\n# \u884c\u5217\u30923D\u30b5\u30fc\u30d5\u30a7\u30b9\u3068\u3057\u3066\u53ef\u8996\u5316\ndef plot_matrix_surface(ax, matrix, title):\n    x = np.arange(matrix.shape[1])\n    y = np.arange(matrix.shape[0])\n    X, Y = np.meshgrid(x, y)\n    ax.plot_surface(X, Y, matrix, cmap='viridis')\n    ax.set_title(title)\n    ax.set_xlabel('Column Index')\n    ax.set_ylabel('Row Index')\n    ax.set_zlabel('Value')\n    ax.set_xticks(x)\n    ax.set_yticks(y)\n\nfig = plt.figure(figsize=(18, 6))\n# \u5143\u306e\u884c\u5217\nax1 = fig.add_subplot(131, projection='3d')\nplot_matrix_surface(ax1, A, \"Original Matrix\")\n\n# \u30e9\u30f3\u30af1\u8fd1\u4f3c\nax2 = fig.add_subplot(132, projection='3d')\nplot_matrix_surface(ax2, A_approx[1], \"Rank-1 Approximation\")\n\n# \u6b8b\u5dee\u884c\u5217\nax3 = fig.add_subplot(133, projection='3d')\nplot_matrix_surface(ax3, A - A_approx[1], \"Residual Matrix\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/40-singular-value-decomposition/#53-svd","title":"5.3. SVD\u306b\u3088\u308b\u753b\u50cf\u5727\u7e2e\u306e\u5b9f\u88c5","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import svd\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# \u30b5\u30f3\u30d7\u30eb\u753b\u50cf\u306e\u751f\u6210\uff08\u5b9f\u969b\u306e\u5fdc\u7528\u3067\u306fimageio.imread\u306a\u3069\u3067\u8aad\u307f\u8fbc\u307f\u307e\u3059\uff09\nsize = 100\nnp.random.seed(42)\n# \u5143\u306e\u753b\u50cf\u306b\u69cb\u9020\u3092\u6301\u305f\u305b\u308b\u305f\u3081\u306e\u30d9\u30fc\u30b9\u30d1\u30bf\u30fc\u30f3\nx, y = np.meshgrid(np.linspace(-3, 3, size), np.linspace(-3, 3, size))\nbase = np.exp(-(x**2 + y**2) / 5)  # \u30ac\u30a6\u30b9\u95a2\u6570\n\n# \u30ce\u30a4\u30ba\u306e\u8ffd\u52a0\nnoise = np.random.normal(0, 0.1, (size, size))\nimage = base + noise\nimage = (image - image.min()) / (image.max() - image.min())  # 0-1\u306b\u6b63\u898f\u5316\n\nplt.figure(figsize=(6, 6))\nplt.imshow(image, cmap='gray')\nplt.title(\"Original Image\")\nplt.colorbar()\nplt.show()\n\n# SVD\u306e\u8a08\u7b97\nU, sigma, VT = svd(image)\n\n# \u753b\u50cf\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u5206\u5e03\u3092\u78ba\u8a8d\ns_squared = sigma**2\nenergy_ratio = np.cumsum(s_squared) / np.sum(s_squared)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(sigma, 'o-')\nplt.title(\"Singular Values\")\nplt.grid(True)\nplt.xlabel(\"Index\")\nplt.ylabel(\"Value\")\n\nplt.subplot(1, 2, 2)\nplt.plot(energy_ratio * 100, 'o-')\nplt.title(\"Cumulative Energy Ratio\")\nplt.grid(True)\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Energy (%)\")\nplt.axhline(y=95, color='r', linestyle='--', label='95% Energy')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# \u5fc5\u8981\u306a\u6210\u5206\u6570\u3092\u6c7a\u5b9a\nthreshold = 0.95  # 95%\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u4fdd\u6301\nk = np.where(energy_ratio &gt;= threshold)[0][0] + 1\nprint(f\"95%\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u4fdd\u6301\u3059\u308b\u306b\u306f{k}\u500b\u306e\u6210\u5206\u304c\u5fc5\u8981\u3067\u3059\")\n\n# \u3055\u307e\u3056\u307e\u306a\u30e9\u30f3\u30af\u3067\u306e\u753b\u50cf\u518d\u69cb\u6210\nranks = [1, 5, 10, 20, k, size]  # size\u306f\u5b8c\u5168\u518d\u69cb\u6210\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.flatten()\n\nfor i, r in enumerate(ranks):\n    # \u30e9\u30f3\u30afr\u3067\u306e\u8fd1\u4f3c\n    reconst = U[:, :r] @ np.diag(sigma[:r]) @ VT[:r, :]\n\n    # \u5727\u7e2e\u7387\u306e\u8a08\u7b97\n    original_size = image.size\n    compressed_size = r * (U.shape[0] + VT.shape[1] + 1)\n    compression_ratio = compressed_size / original_size * 100\n\n    # \u8aa4\u5dee\u306e\u8a08\u7b97\n    error = np.linalg.norm(image - reconst, 'fro') / np.linalg.norm(image, 'fro')\n\n    axes[i].imshow(reconst, cmap='gray')\n    axes[i].set_title(f\"Rank {r} ({compression_ratio:.1f}% of data)\\nError: {error:.4f}\")\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# \u30ce\u30a4\u30ba\u9664\u53bb\u52b9\u679c\u306e\u53ef\u8996\u5316\n# \u3088\u308a\u5f37\u3044\u30ce\u30a4\u30ba\u3092\u6301\u3064\u753b\u50cf\u3092\u751f\u6210\nnoise_strong = np.random.normal(0, 0.3, (size, size))\nimage_noisy = base + noise_strong\nimage_noisy = (image_noisy - image_noisy.min()) / (image_noisy.max() - image_noisy.min())\n\n# \u30ce\u30a4\u30ba\u306e\u591a\u3044\u753b\u50cf\u306b\u5bfe\u3059\u308bSVD\nU_noisy, sigma_noisy, VT_noisy = svd(image_noisy)\n\n# \u30ce\u30a4\u30ba\u9664\u53bb\u306e\u305f\u3081\u306eSVD\u30c8\u30e9\u30f3\u30b1\u30fc\u30b7\u30e7\u30f3\nr_denoise = k  # 95%\u30a8\u30cd\u30eb\u30ae\u30fc\u306e\u6210\u5206\u6570\u3092\u4f7f\u7528\nimage_denoised = U_noisy[:, :r_denoise] @ np.diag(sigma_noisy[:r_denoise]) @ VT_noisy[:r_denoise, :]\n\nplt.figure(figsize=(18, 6))\nplt.subplot(1, 3, 1)\nplt.imshow(base, cmap='gray')\nplt.title(\"Original Clean Pattern\")\nplt.colorbar()\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(image_noisy, cmap='gray')\nplt.title(\"Noisy Image\")\nplt.colorbar()\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(image_denoised, cmap='gray')\nplt.title(f\"Denoised (Using top {r_denoise} components)\")\nplt.colorbar()\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/40-singular-value-decomposition/#54-svd","title":"5.4. \u5065\u5eb7\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308bSVD\u306e\u5fdc\u7528","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import svd\nfrom sklearn.preprocessing import StandardScaler\n\n# \u5065\u5eb7\u95a2\u9023\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u751f\u6210\uff08\u5b9f\u969b\u306e\u5fdc\u7528\u3067\u306f\u5b9f\u30c7\u30fc\u30bf\u3092\u4f7f\u7528\uff09\nnp.random.seed(42)\nn_samples = 100  # \u60a3\u8005\u6570\nn_timepoints = 24  # 24\u6642\u9593\u306e\u6e2c\u5b9a\n\n# 3\u3064\u306e\u5178\u578b\u7684\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u5b9a\u7fa9\n# 1. \u65e5\u4e2d\u9ad8\u304f\u591c\u9593\u4f4e\u3044\u6d3b\u52d5\u30d1\u30bf\u30fc\u30f3\npattern1 = -np.cos(np.linspace(0, 2*np.pi, n_timepoints)) * 0.8 + 1\n# 2. \u671d\u3068\u5915\u65b9\u306b\u30d4\u30fc\u30af\u304c\u3042\u308b\u4e8c\u5cf0\u6027\u30d1\u30bf\u30fc\u30f3\npattern2 = np.exp(-0.2*((np.arange(n_timepoints)-6)**2)) + np.exp(-0.2*((np.arange(n_timepoints)-18)**2))\n# 3. \u7de9\u3084\u304b\u306a\u4e0a\u6607\u50be\u5411\npattern3 = np.linspace(0, 1, n_timepoints)\n\n# \u5404\u30d1\u30bf\u30fc\u30f3\u3092\u6b63\u898f\u5316\npattern1 = (pattern1 - pattern1.mean()) / pattern1.std()\npattern2 = (pattern2 - pattern2.mean()) / pattern2.std()\npattern3 = (pattern3 - pattern3.mean()) / pattern3.std()\n\n# \u30c7\u30fc\u30bf\u751f\u6210\uff1a3\u3064\u306e\u30d1\u30bf\u30fc\u30f3\u3068\u500b\u4eba\u5dee\u3001\u30ce\u30a4\u30ba\u306e\u7d44\u307f\u5408\u308f\u305b\ndata = np.zeros((n_samples, n_timepoints))\nfor i in range(n_samples):\n    # \u5404\u60a3\u8005\u306e\u30d1\u30bf\u30fc\u30f3\u6df7\u5408\u6bd4\u7387\uff08\u30e9\u30f3\u30c0\u30e0\uff09\n    w1 = np.random.normal(0, 1)\n    w2 = np.random.normal(0, 1)\n    w3 = np.random.normal(0, 1)\n\n    # \u57fa\u672c\u30d1\u30bf\u30fc\u30f3\u306e\u7dda\u5f62\u7d50\u5408\n    data[i] = w1 * pattern1 + w2 * pattern2 + w3 * pattern3\n\n    # \u500b\u4eba\u5dee\u3068\u30ce\u30a4\u30ba\u306e\u8ffd\u52a0\n    data[i] += np.random.normal(0, 0.5, n_timepoints)\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\uff08\u5404\u6642\u70b9\u306e\u5e73\u57470\u3001\u5206\u65631\uff09\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# SVD\u306e\u8a08\u7b97\nU, sigma, VT = svd(data_scaled, full_matrices=False)\n\n# \u7279\u7570\u5024\u306e\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(sigma)), sigma)\nplt.title(\"Singular Values\")\nplt.xlabel(\"Component Index\")\nplt.ylabel(\"Singular Value\")\nplt.grid(True)\nplt.show()\n\n# \u7d2f\u7a4d\u7279\u7570\u5024\u30a8\u30cd\u30eb\u30ae\u30fc\u6bd4\u306e\u8a08\u7b97\u3068\u53ef\u8996\u5316\ns_squared = sigma**2\nenergy_ratio = np.cumsum(s_squared) / np.sum(s_squared)\n\nplt.figure(figsize=(10, 6))\nplt.plot(energy_ratio * 100, 'o-')\nplt.title(\"Cumulative Energy Ratio\")\nplt.grid(True)\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Energy (%)\")\nplt.axhline(y=90, color='r', linestyle='--', label='90% Energy')\nplt.legend()\nplt.show()\n\n# \u5fc5\u8981\u306a\u6210\u5206\u6570\u3092\u6c7a\u5b9a\nthreshold = 0.9  # 90%\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u4fdd\u6301\nk = np.where(energy_ratio &gt;= threshold)[0][0] + 1\nprint(f\"90%\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u4fdd\u6301\u3059\u308b\u306b\u306f{k}\u500b\u306e\u6210\u5206\u304c\u5fc5\u8981\u3067\u3059\")\n\n# \u4e0a\u4f4d3\u3064\u306e\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u53ef\u8996\u5316\uff08\u6642\u9593\u30d1\u30bf\u30fc\u30f3\uff09\nplt.figure(figsize=(12, 8))\nfor i in range(min(3, len(VT))):\n    plt.subplot(3, 1, i+1)\n    plt.plot(np.arange(n_timepoints), VT[i], 'o-')\n    plt.title(f\"Pattern {i+1}: Explains {sigma[i]**2 / np.sum(sigma**2)*100:.1f}% of Variance\")\n    plt.xlabel(\"Time (hour)\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# \u5143\u306e\u30d1\u30bf\u30fc\u30f3\u3068\u62bd\u51fa\u3055\u308c\u305f\u30d1\u30bf\u30fc\u30f3\u306e\u6bd4\u8f03\nplt.figure(figsize=(12, 8))\nplt.subplot(3, 1, 1)\nplt.plot(pattern1, 'r-', label='Original Pattern 1')\nplt.plot(VT[0], 'b--', label='Extracted Pattern 1')\nplt.title(\"Comparison of Original and Extracted Patterns\")\nplt.legend()\nplt.grid(True)\n\nplt.subplot(3, 1, 2)\nplt.plot(pattern2, 'r-', label='Original Pattern 2')\nplt.plot(VT[1], 'b--', label='Extracted Pattern 2')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(3, 1, 3)\nplt.plot(pattern3, 'r-', label='Original Pattern 3')\nplt.plot(VT[2], 'b--', label='Extracted Pattern 3')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# \u60a3\u8005\u30922D\u3067\u30de\u30c3\u30d4\u30f3\u30b0\uff08\u4e0a\u4f4d2\u6210\u5206\u3092\u4f7f\u7528\uff09\nplt.figure(figsize=(12, 10))\nplt.scatter(U[:, 0] * sigma[0], U[:, 1] * sigma[1], alpha=0.7)\nplt.title(\"Patient Mapping using Top 2 Components\")\nplt.xlabel(f\"Component 1 ({sigma[0]**2 / np.sum(sigma**2)*100:.1f}% variance)\")\nplt.ylabel(f\"Component 2 ({sigma[1]**2 / np.sum(sigma**2)*100:.1f}% variance)\")\nplt.grid(True)\n\nfor quadrant in [(1, 1), (1, -1), (-1, 1), (-1, -1)]:\n    # \u5404\u8c61\u9650\u306e\u4e2d\u5fc3\u306b\u6700\u3082\u8fd1\u3044\u60a3\u8005\u3092\u898b\u3064\u3051\u308b\n    center = np.array([np.sign(quadrant[0]) * sigma[0] / 2, np.sign(quadrant[1]) * sigma[1] / 2])\n    distances = np.sum((np.column_stack((U[:, 0] * sigma[0], U[:, 1] * sigma[1])) - center) ** 2, axis=1)\n    closest_idx = np.argmin(distances)\n\n    # \u60a3\u8005\u30c7\u30fc\u30bf\u3092\u5143\u306e\u30b9\u30b1\u30fc\u30eb\u306b\u623b\u3059\n    patient_data = scaler.inverse_transform([data_scaled[closest_idx]])[0]\n\n    plt.annotate(f\"Patient {closest_idx}\", \n                 (U[closest_idx, 0] * sigma[0], U[closest_idx, 1] * sigma[1]),\n                 xytext=(U[closest_idx, 0] * sigma[0] + 0.2 * quadrant[0], \n                         U[closest_idx, 1] * sigma[1] + 0.2 * quadrant[1]),\n                 arrowprops=dict(arrowstyle=\"-&gt;\", color=\"red\"))\n\nplt.show()\n\n# \u4ee3\u8868\u7684\u306a\u60a3\u8005\u306e\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u8868\u793a\nplt.figure(figsize=(12, 8))\nfor quadrant_idx, quadrant in enumerate([(1, 1), (1, -1), (-1, 1), (-1, -1)]):\n    center = np.array([np.sign(quadrant[0]) * sigma[0] / 2, np.sign(quadrant[1]) * sigma[1] / 2])\n    distances = np.sum((np.column_stack((U[:, 0] * sigma[0], U[:, 1] * sigma[1])) - center) ** 2, axis=1)\n    closest_idx = np.argmin(distances)\n\n    patient_data = scaler.inverse_transform([data_scaled[closest_idx]])[0]\n\n    plt.subplot(2, 2, quadrant_idx + 1)\n    plt.plot(range(n_timepoints), patient_data, 'o-')\n    plt.title(f\"Patient {closest_idx} (Quadrant {quadrant})\")\n    plt.xlabel(\"Time (hour)\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# \u30ce\u30a4\u30ba\u9664\u53bb\u306e\u30c7\u30e2\u30f3\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\n# \u4e0a\u4f4d3\u6210\u5206\u306e\u307f\u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u3092\u518d\u69cb\u6210\nk_denoise = 3\ndata_denoised = U[:, :k_denoise] @ np.diag(sigma[:k_denoise]) @ VT[:k_denoise, :]\ndata_denoised = scaler.inverse_transform(data_denoised)\n\n# \u30e9\u30f3\u30c0\u30e0\u306b5\u4eba\u306e\u60a3\u8005\u3092\u9078\u629e\u3057\u3066\u5143\u30c7\u30fc\u30bf\u3068\u518d\u69cb\u6210\u30c7\u30fc\u30bf\u3092\u6bd4\u8f03\nselected_patients = np.random.choice(n_samples, 5, replace=False)\n\nplt.figure(figsize=(15, 10))\nfor i, patient_idx in enumerate(selected_patients):\n    plt.subplot(5, 1, i + 1)\n\n    # \u5143\u306e\u30c7\u30fc\u30bf\n    original_data = scaler.inverse_transform([data_scaled[patient_idx]])[0]\n    plt.plot(range(n_timepoints), original_data, 'b-', label='Original Data')\n\n    # \u518d\u69cb\u6210\uff08\u30ce\u30a4\u30ba\u9664\u53bb\uff09\u30c7\u30fc\u30bf\n    plt.plot(range(n_timepoints), data_denoised[patient_idx], 'r--', label='Denoised (k=3)')\n\n    plt.title(f\"Patient {patient_idx}: Original vs Denoised Data\")\n    plt.xlabel(\"Time (hour)\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\u4e0a\u8a18\u306e\u30b3\u30fc\u30c9\u3067\u306f\u300124\u6642\u9593\u6e2c\u5b9a\u3055\u308c\u305f\u5065\u5eb7\u30c7\u30fc\u30bf\u3092\u6a21\u64ec\u3057\u3001SVD\u3092\u7528\u3044\u3066\u57fa\u672c\u30d1\u30bf\u30fc\u30f3\u306e\u62bd\u51fa\u3001\u60a3\u8005\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u3001\u30ce\u30a4\u30ba\u9664\u53bb\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u5206\u6790\u306f\u3001\u751f\u4f53\u4fe1\u53f7\uff08\u5fc3\u62cd\u6570\u3001\u6d3b\u52d5\u91cf\u306a\u3069\uff09\u306e\u5206\u6790\u3084\u7761\u7720\u30d1\u30bf\u30fc\u30f3\u306e\u7814\u7a76\u306a\u3069\u306b\u5fdc\u7528\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/40-singular-value-decomposition/#61","title":"6.1. \u57fa\u672c\u554f\u984c","text":"<p>\u554f\u984c1: \u6b21\u306e\u884c\u5217A\u306e\u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\\(A^+\\)\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> \\[A = \\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; 0 \\\\ 0 &amp; 3 \\end{pmatrix}\\] <p>\u554f\u984c2: \u884c\u5217\\(A = \\begin{pmatrix} 4 &amp; 2 \\\\ 2 &amp; 1 \\end{pmatrix}\\)\u306b\u5bfe\u3057\u3066\uff1a 1. SVD\u3092\u6c42\u3081\u306a\u3055\u3044\u3002 2. \u30e9\u30f3\u30af1\u8fd1\u4f3c\u884c\u5217\\(A_1\\)\u3092\u6c42\u3081\u306a\u3055\u3044\u3002 3. \u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u8aa4\u5dee\\(\\|A - A_1\\|_F\\)\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002</p> <p>\u554f\u984c3: \u884c\u5217\\(A = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{pmatrix}\\)\u306b\u3064\u3044\u3066\uff1a 1. SVD\u3092\u6c42\u3081\u306a\u3055\u3044\u3002 2. \u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u64ec\u4f3c\u9006\u884c\u5217\\(A^+\\)\u3092\u6c42\u3081\u306a\u3055\u3044\u3002 3. \\(AA^+\\)\u3068\\(A^+A\\)\u3092\u8a08\u7b97\u3057\u3001\u305d\u308c\u3089\u304c\u5c04\u5f71\u884c\u5217\u306b\u306a\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u306a\u3055\u3044\u3002</p> <p>\u554f\u984c4: \u6b21\u306e\u9023\u7acb\u65b9\u7a0b\u5f0f\u3092\u64ec\u4f3c\u9006\u884c\u5217\u3092\u7528\u3044\u3066\u89e3\u304d\u306a\u3055\u3044\u3002 \\(\\(\\begin{align} 2x + y &amp;= 4 \\\\ 3x + 2y &amp;= 7 \\\\ x + y &amp;= 3 \\end{align}\\)\\)</p> <p>\u554f\u984c5: \\(3 \\times 3\\)\u884c\u5217\\(A\\)\u306e\u7279\u7570\u5024\u304c\\(\\sigma_1 = 5\\), \\(\\sigma_2 = 3\\), \\(\\sigma_3 = 2\\)\u3067\u3042\u308b\u3068\u304d\u3001\u30e9\u30f3\u30af2\u8fd1\u4f3c\\(A_2\\)\u306e\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u8aa4\u5dee\\(\\|A - A_2\\|_F\\)\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#62","title":"6.2. \u5fdc\u7528\u554f\u984c","text":"<p>\u554f\u984c6: \u7570\u306a\u308b\u75c5\u9662\u3067\u6e2c\u5b9a\u3055\u308c\u305f\u60a3\u8005\u306e\u4f53\u6e29\u30c7\u30fc\u30bf\u3092\u89e3\u6790\u3057\u3066\u3044\u307e\u3059\u3002\u60a3\u800510\u4eba\u306e4\u6642\u9593\u3054\u3068\u306e\u4f53\u6e29\u6e2c\u5b9a\u5024\uff08\u8a086\u6e2c\u5b9a\u5024/\u65e5\uff09\u304c\u542b\u307e\u308c\u308b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u884c\u5217\u304c\u3042\u308a\u307e\u3059\uff1a</p> \\[\\begin{pmatrix}  36.5 &amp; 36.6 &amp; 37.1 &amp; 37.2 &amp; 36.8 &amp; 36.5 \\\\ 36.3 &amp; 36.4 &amp; 36.9 &amp; 37.0 &amp; 36.7 &amp; 36.3 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 36.6 &amp; 36.7 &amp; 37.2 &amp; 37.3 &amp; 36.9 &amp; 36.7 \\end{pmatrix}\\] <ol> <li>\u3053\u306e\u30c7\u30fc\u30bf\u884c\u5217\u306b\u5bfe\u3057\u3066SVD\u3092\u9069\u7528\u3057\u3001\u6700\u521d\u306e2\u3064\u306e\u7279\u7570\u5024\u3068\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u3001\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u3092\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</li> <li>\u30e9\u30f3\u30af2\u8fd1\u4f3c\u3092\u7528\u3044\u3066\u5143\u306e\u30c7\u30fc\u30bf\u3092\u3069\u306e\u7a0b\u5ea6\u518d\u73fe\u3067\u304d\u308b\u304b\u3001\u7d2f\u7a4d\u7279\u7570\u5024\u30a8\u30cd\u30eb\u30ae\u30fc\u6bd4\u3092\u7528\u3044\u3066\u8a55\u4fa1\u3057\u306a\u3055\u3044\u3002</li> <li>\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff08\u6642\u9593\u30d1\u30bf\u30fc\u30f3\uff09\u306e\u89e3\u91c8\u3092\u884c\u3044\u3001\u4f53\u6e29\u306e\u65e5\u5185\u5909\u52d5\u30d1\u30bf\u30fc\u30f3\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</li> </ol> <p>\u554f\u984c7: \u75c5\u6c17\u306e\u8a3a\u65ad\u652f\u63f4\u30b7\u30b9\u30c6\u30e0\u3092\u958b\u767a\u3057\u3066\u3044\u307e\u3059\u3002100\u4eba\u306e\u60a3\u8005\u304b\u3089\u6e2c\u5b9a\u3057\u305f20\u7a2e\u985e\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u30c7\u30fc\u30bf\u304c\u3042\u308a\u3001\u3053\u308c\u3092\\(100 \\times 20\\)\u306e\u884c\u5217\\(X\\)\u3068\u3057\u3066\u3044\u307e\u3059\u3002</p> <ol> <li>\\(X\\)\u306eSVD\u3092\u884c\u3044\u3001\u4e0a\u4f4d5\u3064\u306e\u7279\u7570\u5024\u306e\u307f\u3092\u7528\u3044\u3066\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\\(X_5\\)\u3092\u8a08\u7b97\u3057\u306a\u3055\u3044\u3002</li> <li>\\(X\\)\u3068\\(X_5\\)\u306e\u5dee\u5206\u884c\u5217\u3092\u8a08\u7b97\u3057\u3001\u5404\u60a3\u8005\u3068\u5404\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306b\u3064\u3044\u3066\u518d\u69cb\u6210\u8aa4\u5dee\u3092\u5206\u6790\u3057\u306a\u3055\u3044\u3002\u7279\u306b\u5927\u304d\u306a\u8aa4\u5dee\u3092\u793a\u3059\u60a3\u8005\u306f\u5916\u308c\u5024\u3067\u3042\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3053\u3068\u3092\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</li> <li>\u4e0a\u4f4d3\u3064\u306e\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u60a3\u8005\u30923\u6b21\u5143\u7a7a\u9593\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3059\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u3001\u305d\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u304c\u60a3\u8005\u306e\u30b0\u30eb\u30fc\u30d7\u5316\u306b\u3069\u306e\u3088\u3046\u306b\u5f79\u7acb\u3064\u304b\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</li> </ol> <p>\u554f\u984c8: \u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u304b\u3089\u5f97\u3089\u308c\u305f1\u9031\u9593\u5206\uff08168\u6642\u9593\uff09\u306e\u5fc3\u62cd\u6570\u30c7\u30fc\u30bf\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u306b\u306f24\u6642\u9593\u5468\u671f\u306e\u5909\u52d5\u30d1\u30bf\u30fc\u30f3\u306b\u52a0\u3048\u3001\u904b\u52d5\u3084\u7761\u7720\u306a\u3069\u306b\u3088\u308b\u30ce\u30a4\u30ba\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002</p> <ol> <li>SVD\u3092\u7528\u3044\u3066\u3053\u306e\u30c7\u30fc\u30bf\u304b\u3089\u30ce\u30a4\u30ba\u3092\u9664\u53bb\u3059\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</li> <li>\u3069\u306e\u3088\u3046\u306b\u9069\u5207\u306a\u7279\u7570\u5024\u306e\u30ab\u30c3\u30c8\u30aa\u30d5\u3092\u6c7a\u5b9a\u3059\u308b\u304b\u3001\u5177\u4f53\u7684\u306a\u624b\u9806\u3092\u8ff0\u3079\u306a\u3055\u3044\u3002</li> <li>\u518d\u69cb\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u304c\u5143\u306e\u751f\u4f53\u30ea\u30ba\u30e0\u3092\u3069\u306e\u7a0b\u5ea6\u4fdd\u5b58\u3057\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u306a\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/LA/40-singular-value-decomposition/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/40-singular-value-decomposition/#q1","title":"Q1: \u64ec\u4f3c\u9006\u884c\u5217\u306f\u901a\u5e38\u306e\u9006\u884c\u5217\u3068\u3069\u3046\u9055\u3046\u306e\u3067\u3059\u304b\uff1f","text":"<p>A1: \u901a\u5e38\u306e\u9006\u884c\u5217\u306f\u6b63\u65b9\u884c\u5217\u3067\u3001\u304b\u3064\u884c\u5217\u5f0f\u304c\u30bc\u30ed\u3067\u306a\u3044\u5834\u5408\uff08\u30d5\u30eb\u30e9\u30f3\u30af\uff09\u306b\u306e\u307f\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002\u4e00\u65b9\u3001\u64ec\u4f3c\u9006\u884c\u5217\uff08\u30e0\u30fc\u30a2\u30d9\u30f3\u30ed\u30fc\u30ba\u306e\u9006\u884c\u5217\uff09\u306f\u4efb\u610f\u306e\u884c\u5217\uff08\u975e\u6b63\u65b9\u884c\u5217\u3084\u7279\u7570\u884c\u5217\u3092\u542b\u3080\uff09\u306b\u5bfe\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u3001\u6700\u5c0f\u4e8c\u4e57\u554f\u984c\u306e\u89e3\u3092\u6c42\u3081\u308b\u306a\u3069\u3001\u3088\u308a\u5e83\u7bc4\u306a\u5fdc\u7528\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\u901a\u5e38\u306e\u9006\u884c\u5217\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u4e21\u8005\u306f\u4e00\u81f4\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#q2-svd","title":"Q2: SVD\u306b\u3088\u308b\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u306f\u306a\u305c\u6700\u9069\u306a\u306e\u3067\u3059\u304b\uff1f","text":"<p>A2: \u30a8\u30c3\u30ab\u30fc\u30c8\u30fb\u30e4\u30f3\u30b0\u306e\u5b9a\u7406\u306b\u3088\u308a\u3001SVD\u306b\u57fa\u3065\u304f\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u306f\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u3068\u30b9\u30da\u30af\u30c8\u30e9\u30eb\u30ce\u30eb\u30e0\u306e\u4e21\u65b9\u306b\u304a\u3044\u3066\u6700\u9069\u3067\u3042\u308b\u3053\u3068\u304c\u4fdd\u8a3c\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u5143\u306e\u884c\u5217\u306e\u60c5\u5831\u3092\u3067\u304d\u308b\u3060\u3051\u4fdd\u6301\u3057\u306a\u304c\u3089\u3001\u6307\u5b9a\u3055\u308c\u305f\u30e9\u30f3\u30af\u4ee5\u4e0b\u306e\u884c\u5217\u3067\u8fd1\u4f3c\u3059\u308b\u3068\u3044\u3046\u554f\u984c\u306e\u89e3\u304c\u3001\u4e0a\u4f4dk\u500b\u306e\u7279\u7570\u5024\u3068\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u305f\u884c\u5217\u3067\u3042\u308b\u3068\u3044\u3046\u610f\u5473\u3067\u3059\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#q3","title":"Q3: \u5b9f\u969b\u306e\u5fdc\u7528\u3067\u3069\u306e\u7a0b\u5ea6\u306e\u30e9\u30f3\u30af\u3092\u9078\u3079\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A3: \u9069\u5207\u306a\u30e9\u30f3\u30af\u306e\u9078\u629e\u306f\u554f\u984c\u3068\u30c7\u30fc\u30bf\u306b\u4f9d\u5b58\u3057\u307e\u3059\u3002\u4e00\u822c\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3068\u3057\u3066\u306f\uff1a 1. \u7d2f\u7a4d\u7279\u7570\u5024\u30a8\u30cd\u30eb\u30ae\u30fc\u6bd4\u3092\u8a08\u7b97\u3057\u3001\u7dcf\u30a8\u30cd\u30eb\u30ae\u30fc\u306e80-95%\u3092\u30ab\u30d0\u30fc\u3059\u308b\u30e9\u30f3\u30af\u3092\u9078\u3076 2. \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\uff08\u7279\u7570\u5024\u306e\u5927\u304d\u3055\u3092\u964d\u9806\u306b\u30d7\u30ed\u30c3\u30c8\uff09\u3067\u300c\u8098\u300d\uff08\u6025\u6fc0\u306a\u6e1b\u5c11\u304c\u7de9\u3084\u304b\u306b\u306a\u308b\u70b9\uff09\u3092\u63a2\u3059 3. \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u7528\u3044\u3066\u3001\u518d\u69cb\u6210\u8aa4\u5dee\u3068\u8907\u96d1\u3055\u306e\u30d0\u30e9\u30f3\u30b9\u304c\u6700\u3082\u826f\u3044\u30e9\u30f3\u30af\u3092\u9078\u3076</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#q4-svd","title":"Q4: SVD\u3092\u7528\u3044\u305f\u30ce\u30a4\u30ba\u9664\u53bb\u306f\u3069\u306e\u3088\u3046\u306b\u6a5f\u80fd\u3057\u307e\u3059\u304b\uff1f","text":"<p>A4: \u30ce\u30a4\u30ba\u306f\u901a\u5e38\u3001\u5c0f\u3055\u306a\u7279\u7570\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u6210\u5206\u306b\u5206\u6563\u3057\u3066\u3044\u307e\u3059\u3002\u4e0a\u4f4d\u306e\u7279\u7570\u5024\u3068\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u307f\u3092\u4fdd\u6301\u3059\u308b\u3053\u3068\u3067\u3001\u4e3b\u8981\u306a\u30b7\u30b0\u30ca\u30eb\uff08\u30c7\u30fc\u30bf\u306e\u69cb\u9020\uff09\u3092\u7dad\u6301\u3057\u306a\u304c\u3089\u30ce\u30a4\u30ba\u3092\u524a\u6e1b\u3067\u304d\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u30c7\u30fc\u30bf\u884c\u5217\u306eSVD\u3092\u8a08\u7b97\u3057\u3001\u4e00\u5b9a\u306e\u3057\u304d\u3044\u5024\u4ee5\u4e0b\u306e\u7279\u7570\u5024\u3092\u30bc\u30ed\u306b\u3059\u308b\u3053\u3068\u3067\u3001\u30ce\u30a4\u30ba\u306e\u5f71\u97ff\u3092\u8efd\u6e1b\u3057\u305f\u884c\u5217\u3092\u518d\u69cb\u6210\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#q5-svdpca","title":"Q5: SVD\u3068\u4e3b\u6210\u5206\u5206\u6790(PCA)\u306e\u95a2\u4fc2\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A5: PCA\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002\u30c7\u30fc\u30bf\u884c\u5217\\(X\\)\u304c\u3059\u3067\u306b\u4e2d\u5fc3\u5316\u3055\u308c\u3066\u3044\u308b\uff08\u5404\u5217\u306e\u5e73\u5747\u304c\u30bc\u30ed\uff09\u5834\u5408\u3001\\(X\\)\u306eSVD\u3068\\(X^TX\\)\u306e\u56fa\u6709\u5024\u5206\u89e3\u306b\u306f\u5bc6\u63a5\u306a\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\\(X\\)\u306e\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306f\\(X^TX\\)\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306b\u4e00\u81f4\u3057\u3001\\(X\\)\u306e\u7279\u7570\u5024\u306e\u4e8c\u4e57\u306f\\(X^TX\\)\u306e\u56fa\u6709\u5024\u306b\u4e00\u81f4\u3057\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001SVD\u306fPCA\u3092\u5b9f\u88c5\u3059\u308b\u4e00\u3064\u306e\u65b9\u6cd5\u3068\u3057\u3066\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#q6","title":"Q6: \u64ec\u4f3c\u9006\u884c\u5217\u3092\u7528\u3044\u305f\u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3\u6cd5\u306f\u3001\u4ed6\u306e\u65b9\u6cd5\u3068\u6bd4\u3079\u3066\u3069\u306e\u3088\u3046\u306a\u5229\u70b9\u304c\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A6: \u64ec\u4f3c\u9006\u884c\u5217\u3092\u7528\u3044\u305f\u89e3\u6cd5\u306e\u4e3b\u306a\u5229\u70b9\u306f\uff1a 1. \u904e\u5270\u6c7a\u5b9a\u7cfb\uff08\u65b9\u7a0b\u5f0f\u306e\u6570\u304c\u672a\u77e5\u6570\u3088\u308a\u591a\u3044\uff09\u306e\u5834\u5408\u3001\u6700\u5c0f\u4e8c\u4e57\u89e3\u3092\u76f4\u63a5\u8a08\u7b97\u3067\u304d\u308b 2. \u52a3\u6c7a\u5b9a\u7cfb\uff08\u672a\u77e5\u6570\u304c\u65b9\u7a0b\u5f0f\u3088\u308a\u591a\u3044\uff09\u306e\u5834\u5408\u3001\u6700\u5c0f\u30ce\u30eb\u30e0\u89e3\u3092\u63d0\u4f9b\u3059\u308b 3. SVD\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u6570\u5024\u7684\u306b\u5b89\u5b9a\u3057\u305f\u8a08\u7b97\u304c\u53ef\u80fd 4. \u884c\u5217\u304c\u60aa\u6761\u4ef6\uff08\u7279\u7570\u5024\u306e\u6bd4\u7387\u304c\u5927\u304d\u3044\uff09\u306e\u5834\u5408\u3067\u3082\u3001\u5236\u5fa1\u3055\u308c\u305f\u65b9\u6cd5\u3067\u89e3\u3092\u5f97\u3089\u308c\u308b</p>"},{"location":"lectures/LA/40-singular-value-decomposition/#q7-svd","title":"Q7: \u533b\u7642\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308bSVD\u306e\u5177\u4f53\u7684\u306a\u5fdc\u7528\u4f8b\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002","text":"<p>A7: \u533b\u7642\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308bSVD\u306e\u5fdc\u7528\u4f8b\uff1a 1. MRI\u3084CT\u30b9\u30ad\u30e3\u30f3\u753b\u50cf\u306e\u5727\u7e2e\u3068\u30ce\u30a4\u30ba\u9664\u53bb 2. \u6642\u7cfb\u5217\u751f\u4f53\u4fe1\u53f7\uff08\u8133\u6ce2\u3001\u5fc3\u96fb\u56f3\u306a\u3069\uff09\u304b\u3089\u306e\u7279\u5fb4\u62bd\u51fa 3. \u8907\u6570\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u76f8\u95a2\u30d1\u30bf\u30fc\u30f3\u306e\u89e3\u6790 4. \u85ac\u7269\u6cbb\u7642\u3078\u306e\u53cd\u5fdc\u306e\u60a3\u8005\u30b0\u30eb\u30fc\u30d7\u9593\u5dee\u7570\u306e\u7279\u5b9a 5. \u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b\u3068\u89e3\u91c8</p>"},{"location":"lectures/LA/41-principal-component-analysis/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c41\u56de \u8b1b\u7fa9\u30ce\u30fc\u30c8\uff1a\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u5c0e\u5165","text":""},{"location":"lectures/LA/41-principal-component-analysis/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c41\u56de \u30c6\u30fc\u30de: \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u5c0e\u5165 \u95a2\u9023\u9805\u76ee: \u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3001\u5206\u6563\u30fb\u5171\u5206\u6563\u3001\u76f8\u95a2\u4fc2\u6570\u3001\u5bfe\u79f0\u884c\u5217\u306e\u5bfe\u89d2\u5316 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9:  - \u7b2c33-35\u56de\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u5bfe\u89d2\u5316\u306e\u5185\u5bb9\u3092\u5fa9\u7fd2 - \u7b2c7-8\u56de\u306e\u5206\u6563\u30fb\u5171\u5206\u6563\u30fb\u76f8\u95a2\u4fc2\u6570\u306e\u8a08\u7b97\u65b9\u6cd5\u3092\u78ba\u8a8d - \u7b2c36\u56de\u306e2\u6b21\u5f62\u5f0f\u3068\u6b63\u5b9a\u5024\u30fb\u534a\u6b63\u5b9a\u5024\u306e\u6982\u5ff5\u3092\u7406\u89e3</p>"},{"location":"lectures/LA/41-principal-component-analysis/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u4e3b\u6210\u5206\u5206\u6790\u306e\u76ee\u7684\u3068\u57fa\u672c\u7684\u306a\u8003\u3048\u65b9\u3092\u7406\u89e3\u3059\u308b</li> <li>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3068\u76f8\u95a2\u884c\u5217\u306e\u6027\u8cea\u3092\u7406\u89e3\u3057\u3001\u305d\u308c\u3089\u306e\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u610f\u5473\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u4e3b\u6210\u5206\u306e\u6570\u5b66\u7684\u306a\u5c0e\u51fa\u65b9\u6cd5\u3092\u7406\u89e3\u3057\u3001\u8a08\u7b97\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3068\u76f8\u95a2\u884c\u5217\u306b\u57fa\u3065\u304f\u4e3b\u6210\u5206\u5206\u6790\u306e\u9055\u3044\u3092\u7406\u89e3\u3059\u308b</li> <li>Python\u3092\u7528\u3044\u3066\u4e3b\u6210\u5206\u5206\u6790\u3092\u5b9f\u88c5\u3057\u3001\u7d50\u679c\u3092\u8996\u899a\u7684\u306b\u89e3\u91c8\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> </ol>"},{"location":"lectures/LA/41-principal-component-analysis/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/41-principal-component-analysis/#31-pca","title":"3.1 \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3068\u306f","text":"<p>\u5b9a\u7fa9: \u4e3b\u6210\u5206\u5206\u6790\uff08Principal Component Analysis, PCA\uff09\u306f\u3001\u591a\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u60c5\u5831\u3092\u3067\u304d\u308b\u3060\u3051\u4fdd\u6301\u3057\u306a\u304c\u3089\u3001\u3088\u308a\u5c11\u306a\u3044\u6b21\u5143\u306b\u5727\u7e2e\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u304c\u6700\u5927\u3068\u306a\u308b\u65b9\u5411\uff08\u4e3b\u6210\u5206\uff09\u3092\u9806\u6b21\u898b\u3064\u3051\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u4e3b\u8981\u306a\u69cb\u9020\u3092\u62bd\u51fa\u3059\u308b\u3002</p> <p>\u4e3b\u6210\u5206\u5206\u6790\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u76ee\u7684\u3067\u5229\u7528\u3055\u308c\u307e\u3059\uff1a</p> <ol> <li>\u6b21\u5143\u524a\u6e1b\uff1a\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u4f4e\u6b21\u5143\u306b\u5727\u7e2e\u3057\u3001\u53ef\u8996\u5316\u3084\u8a08\u7b97\u52b9\u7387\u306e\u5411\u4e0a\u3092\u56f3\u308b</li> <li>\u60c5\u5831\u62bd\u51fa\uff1a\u30c7\u30fc\u30bf\u306e\u6700\u3082\u91cd\u8981\u306a\u7279\u5fb4\u3084\u5909\u52d5\u3092\u6349\u3048\u308b</li> <li>\u30ce\u30a4\u30ba\u9664\u53bb\uff1a\u30c7\u30fc\u30bf\u306b\u542b\u307e\u308c\u308b\u30ce\u30a4\u30ba\u306e\u5f71\u97ff\u3092\u8efd\u6e1b\u3059\u308b</li> <li>\u591a\u91cd\u5171\u7dda\u6027\u306e\u89e3\u6d88\uff1a\u76f8\u95a2\u306e\u9ad8\u3044\u5909\u6570\u9593\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3059\u308b</li> </ol>"},{"location":"lectures/LA/41-principal-component-analysis/#32","title":"3.2 \u30c7\u30fc\u30bf\u306e\u8868\u73fe\u3068\u524d\u51e6\u7406","text":"<p>\\(n\\)\u500b\u306e\u30b5\u30f3\u30d7\u30eb\u3068\\(p\\)\u500b\u306e\u5909\u6570\u304b\u3089\u306a\u308b\u30c7\u30fc\u30bf\u884c\u5217\u3092\u6b21\u306e\u3088\u3046\u306b\u8868\u3057\u307e\u3059\uff1a</p> \\[X = \\begin{pmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{pmatrix}\\] <p>PC\u3092\u884c\u3046\u524d\u306b\u3001\u901a\u5e38\u306f\u30c7\u30fc\u30bf\u3092\u4e2d\u5fc3\u5316\uff08\u5404\u5909\u6570\u306e\u5e73\u5747\u30920\u306b\u3059\u308b\uff09\u3057\u307e\u3059\uff1a</p> \\[X_{centered} = X - \\mathbf{1}\\boldsymbol{\\mu}^T\\] <p>\u3053\u3053\u3067\u3001\\(\\mathbf{1}\\)\u306f\u5168\u3066\u306e\u8981\u7d20\u304c1\u306e\\(n\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3001\\(\\boldsymbol{\\mu}\\)\u306f\u5404\u5909\u6570\u306e\u5e73\u5747\u5024\u3092\u542b\u3080\\(p\\)\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p> <p>\u3055\u3089\u306b\u3001\u5404\u5909\u6570\u306e\u5358\u4f4d\u304c\u7570\u306a\u308b\u5834\u5408\u306f\u6a19\u6e96\u5316\uff08\u5404\u5909\u6570\u306e\u6a19\u6e96\u504f\u5dee\u30921\u306b\u3059\u308b\uff09\u3092\u884c\u3044\u307e\u3059\uff1a</p> \\[X_{standardized} = (X - \\mathbf{1}\\boldsymbol{\\mu}^T)D^{-1}\\] <p>\u3053\u3053\u3067\u3001\\(D\\)\u306f\u5404\u5909\u6570\u306e\u6a19\u6e96\u504f\u5dee\u3092\u5bfe\u89d2\u6210\u5206\u306b\u6301\u3064\u5bfe\u89d2\u884c\u5217\u3067\u3059\u3002</p>"},{"location":"lectures/LA/41-principal-component-analysis/#33","title":"3.3 \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3068\u76f8\u95a2\u884c\u5217","text":"<p>\u5b9a\u7fa9\uff08\u5206\u6563\u5171\u5206\u6563\u884c\u5217\uff09: \u30c7\u30fc\u30bf\u884c\u5217\\(X\\)\uff08\u4e2d\u5fc3\u5316\u6e08\u307f\uff09\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\\(S\\)\u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a \\(\\(S = \\frac{1}{n-1}X^TX\\)\\)</p> <p>\u5b9a\u7fa9\uff08\u76f8\u95a2\u884c\u5217\uff09: \u30c7\u30fc\u30bf\u884c\u5217\\(X\\)\uff08\u6a19\u6e96\u5316\u6e08\u307f\uff09\u306e\u76f8\u95a2\u884c\u5217\\(R\\)\u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a \\(\\(R = \\frac{1}{n-1}X^TX\\)\\)</p> <p>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u5bfe\u89d2\u6210\u5206\u306f\u5404\u5909\u6570\u306e\u5206\u6563\u3001\u975e\u5bfe\u89d2\u6210\u5206\u306f\u5909\u6570\u9593\u306e\u5171\u5206\u6563\u3092\u8868\u3057\u307e\u3059\u3002\u76f8\u95a2\u884c\u5217\u306e\u5bfe\u89d2\u6210\u5206\u306f\u3059\u3079\u30661\u3001\u975e\u5bfe\u89d2\u6210\u5206\u306f\u5909\u6570\u9593\u306e\u76f8\u95a2\u4fc2\u6570\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3068\u76f8\u95a2\u884c\u5217\u306f\u4ee5\u4e0b\u306e\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\uff1a</p> <ol> <li>\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308b\uff08\\(S = S^T\\), \\(R = R^T\\)\uff09</li> <li>\u534a\u6b63\u5b9a\u5024\u884c\u5217\u3067\u3042\u308b\uff08\u3059\u3079\u3066\u306e\u56fa\u6709\u5024\u306f\u975e\u8ca0\uff09</li> <li>\u30c7\u30fc\u30bf\u304c\u6a19\u6e96\u5316\u3055\u308c\u3066\u3044\u308c\u3070\u3001\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306f\u76f8\u95a2\u884c\u5217\u306b\u4e00\u81f4\u3059\u308b</li> </ol>"},{"location":"lectures/LA/41-principal-component-analysis/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/41-principal-component-analysis/#41","title":"4.1 \u4e3b\u6210\u5206\u306e\u6570\u5b66\u7684\u5c0e\u51fa","text":"<p>\u4e3b\u6210\u5206\u5206\u6790\u306e\u76ee\u7684\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u304c\u6700\u5927\u3068\u306a\u308b\u65b9\u5411\uff08\u4e3b\u6210\u5206\uff09\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u3067\u3059\u3002</p> <p>\u307e\u305a\u3001\u4e2d\u5fc3\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\\(X\\)\u306b\u5bfe\u3057\u3066\u3001\u5358\u4f4d\u30d9\u30af\u30c8\u30eb\\(\\mathbf{w}\\)\u65b9\u5411\u3078\u306e\u5c04\u5f71\u3092\u8003\u3048\u307e\u3059\uff1a</p> \\[\\mathbf{z} = X\\mathbf{w}\\] <p>\u3053\u306e\u5c04\u5f71\\(\\mathbf{z}\\)\u306e\u5206\u6563\u306f\u6b21\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[\\text{Var}(\\mathbf{z}) = \\frac{1}{n-1}\\mathbf{z}^T\\mathbf{z} = \\frac{1}{n-1}\\mathbf{w}^TX^TX\\mathbf{w} = \\mathbf{w}^TS\\mathbf{w}\\] <p>\u3053\u3053\u3067\u3001\\(S\\)\u306f\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3067\u3059\u3002</p> <p>\u7b2c\u4e00\u4e3b\u6210\u5206\u3092\u6c42\u3081\u308b\u306b\u306f\u3001\\(\\mathbf{w}\\)\u306e\u9577\u3055\u304c1\u3068\u3044\u3046\u5236\u7d04\u306e\u4e0b\u3067\u3001\\(\\mathbf{w}^TS\\mathbf{w}\\)\u3092\u6700\u5927\u5316\u3059\u308b\u554f\u984c\u3092\u89e3\u304d\u307e\u3059\uff1a</p> \\[\\max_{\\mathbf{w}} \\mathbf{w}^TS\\mathbf{w} \\quad \\text{subject to} \\quad \\mathbf{w}^T\\mathbf{w} = 1\\] <p>\u3053\u306e\u6700\u9069\u5316\u554f\u984c\u306f\u30e9\u30b0\u30e9\u30f3\u30b8\u30e5\u4e57\u6570\u6cd5\u3067\u89e3\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u30e9\u30b0\u30e9\u30f3\u30b8\u30a2\u30f3\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[L(\\mathbf{w}, \\lambda) = \\mathbf{w}^TS\\mathbf{w} - \\lambda(\\mathbf{w}^T\\mathbf{w} - 1)\\] <p>\\(\\mathbf{w}\\)\u3067\u504f\u5fae\u5206\u3057\u30660\u3068\u304a\u304f\u3068\uff1a</p> \\[\\frac{\\partial L}{\\partial \\mathbf{w}} = 2S\\mathbf{w} - 2\\lambda\\mathbf{w} = 0\\] <p>\u3053\u308c\u3092\u6574\u7406\u3059\u308b\u3068\uff1a</p> \\[S\\mathbf{w} = \\lambda\\mathbf{w}\\] <p>\u3053\u308c\u306f\u3001\\(\\mathbf{w}\\)\u304c\\(S\\)\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308a\u3001\\(\\lambda\\)\u304c\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u5024\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\\(\\mathbf{w}^TS\\mathbf{w} = \\lambda\\)\u306a\u306e\u3067\u3001\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u306b\u306f\u6700\u5927\u306e\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u9078\u3076\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u3057\u305f\u304c\u3063\u3066\u3001\u7b2c\u4e00\u4e3b\u6210\u5206\u306f\u5206\u6563\u5171\u5206\u6563\u884c\u5217\\(S\\)\u306e\u6700\u5927\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u540c\u69d8\u306b\u3001\u7b2c\u4e8c\u4e3b\u6210\u5206\u4ee5\u964d\u306f\u3001\u305d\u308c\u305e\u308c\u6b21\u306b\u5927\u304d\u3044\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308a\u3001\u305d\u308c\u3089\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/41-principal-component-analysis/#42-pca","title":"4.2 \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3068\u76f8\u95a2\u884c\u5217\u306b\u57fa\u3065\u304fPCA","text":"<p>\u4e3b\u6210\u5206\u5206\u6790\u306f\u3001\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306b\u57fa\u3065\u304f\u5834\u5408\u3068\u76f8\u95a2\u884c\u5217\u306b\u57fa\u3065\u304f\u5834\u5408\u306e2\u7a2e\u985e\u304c\u3042\u308a\u307e\u3059\uff1a</p> <p>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306b\u57fa\u3065\u304fPCA\uff1a - \u5143\u306e\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u305d\u306e\u307e\u307e\u53cd\u6620\u3059\u308b - \u5206\u6563\u306e\u5927\u304d\u3044\u5909\u6570\u304c\u4e3b\u6210\u5206\u306b\u5f37\u304f\u5f71\u97ff\u3059\u308b - \u5909\u6570\u306e\u5358\u4f4d\u304c\u540c\u3058\u5834\u5408\u3084\u3001\u5358\u4f4d\u306e\u9055\u3044\u304c\u610f\u5473\u3092\u6301\u3064\u5834\u5408\u306b\u9069\u3057\u3066\u3044\u308b</p> <p>\u76f8\u95a2\u884c\u5217\u306b\u57fa\u3065\u304fPCA\uff1a - \u3059\u3079\u3066\u306e\u5909\u6570\u3092\u6a19\u6e96\u5316\u3057\u3066\u304b\u3089\u5206\u6790\u3092\u884c\u3046 - \u3059\u3079\u3066\u306e\u5909\u6570\u304c\u7b49\u3057\u304f\u6271\u308f\u308c\u308b - \u5909\u6570\u306e\u5358\u4f4d\u304c\u7570\u306a\u308b\u5834\u5408\u306b\u9069\u3057\u3066\u3044\u308b</p> <p>\u76f8\u95a2\u884c\u5217\u3092\u7528\u3044\u308b\u4e3b\u306a\u7406\u7531\uff1a 1. \u5909\u6570\u306e\u5358\u4f4d\u304c\u7570\u306a\u308b\u5834\u5408\u3001\u5927\u304d\u306a\u5024\u3092\u6301\u3064\u5909\u6570\u304c\u4e3b\u6210\u5206\u306b\u652f\u914d\u7684\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3057\u307e\u3046 2. \u5909\u6570\u9593\u306e\u30b9\u30b1\u30fc\u30eb\u306e\u9055\u3044\u3092\u6392\u9664\u3057\u3001\u7d14\u7c8b\u306a\u76f8\u95a2\u95a2\u4fc2\u306b\u57fa\u3065\u3044\u3066\u5206\u6790\u3067\u304d\u308b 3. \u7d50\u679c\u306e\u89e3\u91c8\u304c\u3088\u308a\u76f4\u611f\u7684\u306b\u306a\u308b\u3053\u3068\u304c\u591a\u3044</p>"},{"location":"lectures/LA/41-principal-component-analysis/#43","title":"4.3 \u4e3b\u6210\u5206\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8","text":"<p>2\u6b21\u5143\u306e\u4f8b\u3067\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u6b21\u306e\u56f3\u306f\u30012\u5909\u6570\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e3b\u6210\u5206\u306e\u65b9\u5411\u3092\u793a\u3057\u3066\u3044\u307e\u3059\uff1a</p> <p></p> <ul> <li>\u7b2c\u4e00\u4e3b\u6210\u5206\uff08PC1\uff09\uff1a\u30c7\u30fc\u30bf\u306e\u5206\u6563\u304c\u6700\u5927\u3068\u306a\u308b\u65b9\u5411</li> <li>\u7b2c\u4e8c\u4e3b\u6210\u5206\uff08PC2\uff09\uff1a\u7b2c\u4e00\u4e3b\u6210\u5206\u3068\u76f4\u4ea4\u3057\u3001\u6b8b\u308a\u306e\u5206\u6563\u304c\u6700\u5927\u3068\u306a\u308b\u65b9\u5411</li> </ul> <p>\u591a\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u5834\u5408\u3001\u4e3b\u6210\u5206\u306f\u30c7\u30fc\u30bf\u306e\u300c\u4e3b\u8ef8\u300d\u3092\u8868\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u4e3b\u6210\u5206\u306f\u30c7\u30fc\u30bf\u306e\u5909\u52d5\u306e\u4e00\u90e8\u3092\u8aac\u660e\u3057\u307e\u3059\u3002\u56fa\u6709\u5024\u306e\u5927\u304d\u3055\u306f\u3001\u5404\u4e3b\u6210\u5206\u304c\u8aac\u660e\u3059\u308b\u5206\u6563\u306e\u91cf\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/41-principal-component-analysis/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/41-principal-component-analysis/#51-pca","title":"5.1 \u57fa\u672c\u7684\u306aPCA\u5b9f\u88c5","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u751f\u6210\uff082\u3064\u306e\u76f8\u95a2\u306e\u3042\u308b\u5909\u6570\uff09\nnp.random.seed(42)\nn_samples = 100\nx = np.random.normal(0, 1, n_samples)\ny = x * 0.8 + np.random.normal(0, 0.6, n_samples)\ndata = np.column_stack((x, y))\n\n# \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.scatter(data[:, 0], data[:, 1], alpha=0.7)\nplt.xlabel('\u5909\u65701')\nplt.ylabel('\u5909\u65702')\nplt.title('\u5143\u306e\u30c7\u30fc\u30bf')\nplt.grid(True)\nplt.show()\n\n# \u624b\u52d5\u3067PCA\u3092\u5b9f\u88c5\n# 1. \u30c7\u30fc\u30bf\u3092\u4e2d\u5fc3\u5316\ndata_centered = data - np.mean(data, axis=0)\n\n# 2. \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u8a08\u7b97\ncov_matrix = np.cov(data_centered, rowvar=False)\nprint(\"\u5206\u6563\u5171\u5206\u6563\u884c\u5217:\")\nprint(cov_matrix)\n\n# 3. \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u8a08\u7b97\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\nprint(\"\\n\u56fa\u6709\u5024:\")\nprint(eigenvalues)\nprint(\"\\n\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u5217\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\uff09:\")\nprint(eigenvectors)\n\n# 4. \u56fa\u6709\u5024\u3092\u964d\u9806\u306b\u30bd\u30fc\u30c8\nidx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\n# 5. \u4e3b\u6210\u5206\u306e\u65b9\u5411\u3092\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.scatter(data[:, 0], data[:, 1], alpha=0.7)\n\n# \u7b2c\u4e00\u4e3b\u6210\u5206\nplt.arrow(np.mean(data[:, 0]), np.mean(data[:, 1]), \n          eigenvectors[0, 0] * eigenvalues[0], \n          eigenvectors[1, 0] * eigenvalues[0],\n          head_width=0.1, head_length=0.1, fc='red', ec='red', \n          label='\u7b2c\u4e00\u4e3b\u6210\u5206')\n\n# \u7b2c\u4e8c\u4e3b\u6210\u5206\nplt.arrow(np.mean(data[:, 0]), np.mean(data[:, 1]), \n          eigenvectors[0, 1] * eigenvalues[1], \n          eigenvectors[1, 1] * eigenvalues[1],\n          head_width=0.1, head_length=0.1, fc='green', ec='green', \n          label='\u7b2c\u4e8c\u4e3b\u6210\u5206')\n\nplt.xlabel('\u5909\u65701')\nplt.ylabel('\u5909\u65702')\nplt.title('\u4e3b\u6210\u5206\u306e\u65b9\u5411')\nplt.grid(True)\nplt.legend()\nplt.axis('equal')\nplt.show()\n\n# 6. \u4e3b\u6210\u5206\u3078\u306e\u5c04\u5f71\npc_scores = np.dot(data_centered, eigenvectors)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(pc_scores[:, 0], pc_scores[:, 1], alpha=0.7)\nplt.xlabel('\u7b2c\u4e00\u4e3b\u6210\u5206')\nplt.ylabel('\u7b2c\u4e8c\u4e3b\u6210\u5206')\nplt.title('\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u3057\u305f\u30c7\u30fc\u30bf')\nplt.grid(True)\nplt.axis('equal')\nplt.show()\n</code></pre>"},{"location":"lectures/LA/41-principal-component-analysis/#52-scikit-learnpca","title":"5.2 scikit-learn\u3092\u7528\u3044\u305fPCA","text":"<pre><code># scikit-learn\u3092\u4f7f\u7528\u3057\u305fPCA\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# PCA\u306e\u5b9f\u884c\npca = PCA()\npca_result = pca.fit_transform(data_scaled)\n\n# \u7d50\u679c\u306e\u78ba\u8a8d\nprint(\"\\nscikit-learn\u306b\u3088\u308b\u5206\u6563\u8aac\u660e\u7387:\")\nprint(pca.explained_variance_ratio_)\nprint(\"\\nscikit-learn\u306b\u3088\u308b\u7d2f\u7a4d\u5206\u6563\u8aac\u660e\u7387:\")\nprint(np.cumsum(pca.explained_variance_ratio_))\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(12, 5))\n\n# \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n         pca.explained_variance_ratio_, 'o-', linewidth=2)\nplt.title('\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8')\nplt.xlabel('\u4e3b\u6210\u5206\u756a\u53f7')\nplt.ylabel('\u5206\u6563\u8aac\u660e\u7387')\nplt.grid(True)\n\n# \u7d2f\u7a4d\u5206\u6563\u8aac\u660e\u7387\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n         np.cumsum(pca.explained_variance_ratio_), 'o-', linewidth=2)\nplt.title('\u7d2f\u7a4d\u5206\u6563\u8aac\u660e\u7387')\nplt.xlabel('\u4e3b\u6210\u5206\u756a\u53f7')\nplt.ylabel('\u7d2f\u7a4d\u5206\u6563\u8aac\u660e\u7387')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/41-principal-component-analysis/#53","title":"5.3 \u5065\u5eb7\u30c7\u30fc\u30bf\u3078\u306e\u5fdc\u7528\u4f8b","text":"<pre><code># \u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u30b5\u30f3\u30d7\u30eb\uff08BMI\u3001\u8840\u5727\u3001\u8840\u7cd6\u5024\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u306a\u3069\uff09\nnp.random.seed(42)\nn_samples = 100\n\n# \u30c7\u30fc\u30bf\u306e\u751f\u6210\uff08\u76f8\u95a2\u3092\u6301\u305f\u305b\u308b\uff09\nbmi = np.random.normal(22, 3, n_samples)\nsystolic_bp = 120 + bmi * 2 + np.random.normal(0, 10, n_samples)\ndiastolic_bp = 80 + bmi * 1.5 + np.random.normal(0, 8, n_samples)\nblood_sugar = 100 + bmi * 1.8 + np.random.normal(0, 15, n_samples)\ncholesterol = 200 + bmi * 3 + blood_sugar * 0.2 + np.random.normal(0, 20, n_samples)\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210\nhealth_data = pd.DataFrame({\n    'BMI': bmi,\n    '\u53ce\u7e2e\u671f\u8840\u5727': systolic_bp,\n    '\u62e1\u5f35\u671f\u8840\u5727': diastolic_bp,\n    '\u8840\u7cd6\u5024': blood_sugar,\n    '\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb': cholesterol\n})\n\nprint(\"\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u30b5\u30f3\u30d7\u30eb:\")\nprint(health_data.head())\n\n# \u76f8\u95a2\u884c\u5217\u306e\u78ba\u8a8d\ncorrelation_matrix = health_data.corr()\nprint(\"\\n\u76f8\u95a2\u884c\u5217:\")\nprint(correlation_matrix)\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\nscaler = StandardScaler()\nhealth_data_scaled = scaler.fit_transform(health_data)\n\n# PCA\u306e\u5b9f\u884c\npca = PCA()\nhealth_pca_result = pca.fit_transform(health_data_scaled)\n\n# \u7d50\u679c\u306e\u78ba\u8a8d\nprint(\"\\n\u4e3b\u6210\u5206\u306e\u5206\u6563\u8aac\u660e\u7387:\")\nprint(pca.explained_variance_ratio_)\nprint(\"\\n\u7d2f\u7a4d\u5206\u6563\u8aac\u660e\u7387:\")\nprint(np.cumsum(pca.explained_variance_ratio_))\n\n# \u4e3b\u6210\u5206\u306e\u8ca0\u8377\u91cf\uff08\u5404\u5909\u6570\u306e\u5bc4\u4e0e\u5ea6\uff09\nloadings = pca.components_.T * np.sqrt(pca.explained_variance_)\nloading_df = pd.DataFrame(\n    loadings, \n    columns=[f'PC{i+1}' for i in range(loadings.shape[1])],\n    index=health_data.columns\n)\nprint(\"\\n\u4e3b\u6210\u5206\u8ca0\u8377\u91cf:\")\nprint(loading_df)\n\n# \u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210\nplt.figure(figsize=(10, 8))\n# \u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u30d7\u30ed\u30c3\u30c8\nplt.scatter(health_pca_result[:, 0], health_pca_result[:, 1], alpha=0.7)\n\n# \u5909\u6570\u30d9\u30af\u30c8\u30eb\u306e\u30d7\u30ed\u30c3\u30c8\nfor i, (name, row) in enumerate(loading_df.iloc[:, :2].iterrows()):\n    plt.arrow(0, 0, row['PC1']*3, row['PC2']*3, head_width=0.1, head_length=0.1, fc='red', ec='red')\n    plt.text(row['PC1']*3.2, row['PC2']*3.2, name, color='red')\n\nplt.xlabel(f'\u7b2c\u4e00\u4e3b\u6210\u5206 ({pca.explained_variance_ratio_[0]:.2%})')\nplt.ylabel(f'\u7b2c\u4e8c\u4e3b\u6210\u5206 ({pca.explained_variance_ratio_[1]:.2%})')\nplt.title('\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790\uff1a\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8')\nplt.grid(True)\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/41-principal-component-analysis/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/41-principal-component-analysis/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li>\u6b21\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e3b\u6210\u5206\u5206\u6790\u3092\u624b\u8a08\u7b97\u3067\u884c\u3044\u3001\u7b2c\u4e00\u4e3b\u6210\u5206\u3068\u7b2c\u4e8c\u4e3b\u6210\u5206\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</li> </ol> \\[X = \\begin{pmatrix} 1 &amp; 3 \\\\ 2 &amp; 5 \\\\ 3 &amp; 4 \\\\ 4 &amp; 6 \\end{pmatrix}\\] <ol> <li> <p>\u4e3b\u6210\u5206\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u56fa\u6709\u5024\u304c\u6b21\u306e\u3088\u3046\u306b\u5f97\u3089\u308c\u305f\uff1a\u03bb\u2081 = 4.2, \u03bb\u2082 = 1.8, \u03bb\u2083 = 0.7, \u03bb\u2084 = 0.3\u3002\u3053\u306e\u3068\u304d\u3001\u5404\u4e3b\u6210\u5206\u306e\u5206\u6563\u8aac\u660e\u7387\u3068\u7d2f\u7a4d\u5206\u6563\u8aac\u660e\u7387\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u76f8\u95a2\u884c\u5217\u306b\u5bfe\u3057\u3066\u3001\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u3001\u7b2c\u4e00\u4e3b\u6210\u5206\u3068\u7b2c\u4e8c\u4e3b\u6210\u5206\u306e\u65b9\u5411\u3092\u7279\u5b9a\u3057\u306a\u3055\u3044\u3002</p> </li> </ol> \\[R = \\begin{pmatrix} 1.0 &amp; 0.7 &amp; 0.3 \\\\ 0.7 &amp; 1.0 &amp; 0.5 \\\\ 0.3 &amp; 0.5 &amp; 1.0 \\end{pmatrix}\\] <ol> <li>\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3068\u76f8\u95a2\u884c\u5217\u306b\u57fa\u3065\u304fPCA\u306e\u9055\u3044\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u3001\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u3069\u3061\u3089\u3092\u9078\u3076\u3079\u304d\u304b\u8ff0\u3079\u306a\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/LA/41-principal-component-analysis/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<ol> <li>\u4ee5\u4e0b\u306ePython\u30b3\u30fc\u30c9\u306b\u3088\u3063\u3066\u751f\u6210\u3055\u308c\u308b3\u6b21\u5143\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e3b\u6210\u5206\u5206\u6790\u3092\u884c\u3044\u3001\u7d50\u679c\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7b2c\u4e00\u4e3b\u6210\u5206\u3068\u7b2c\u4e8c\u4e3b\u6210\u5206\u306e\u5206\u6563\u8aac\u660e\u7387\u306f\u4f55\uff05\u3067\u3059\u304b\uff1f\u307e\u305f\u3001\u8ca0\u8377\u91cf\u304b\u3089\u5404\u4e3b\u6210\u5206\u306e\u610f\u5473\u3092\u89e3\u91c8\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol> <pre><code>import numpy as np\n\nnp.random.seed(42)\nn = 100\nx = np.random.normal(0, 1, n)\ny = x * 0.8 + np.random.normal(0, 0.5, n)\nz = x * 0.6 + y * 0.5 + np.random.normal(0, 0.4, n)\ndata_3d = np.column_stack((x, y, z))\n</code></pre> <ol> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u6587\u8108\u3067\u30011000\u4eba\u5206\u306e\u8eab\u9577\u3001\u4f53\u91cd\u3001BMI\u3001\u30a6\u30a8\u30b9\u30c8/\u30d2\u30c3\u30d7\u6bd4\u3001\u4f53\u8102\u80aa\u7387\u306e\u30c7\u30fc\u30bf\u304c\u3042\u308b\u3068\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u5909\u6570\u306f\u4e92\u3044\u306b\u76f8\u95a2\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u306b\u4e3b\u6210\u5206\u5206\u6790\u3092\u9069\u7528\u3059\u308b\u610f\u7fa9\u3068\u3001\u5f97\u3089\u308c\u308b\u53ef\u80fd\u6027\u306e\u3042\u308b\u4e3b\u6210\u5206\u306e\u89e3\u91c8\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u3055\u3089\u306b\u3001\u3053\u306e\u5206\u6790\u7d50\u679c\u304c\u5065\u5eb7\u30ea\u30b9\u30af\u8a55\u4fa1\u306b\u3069\u306e\u3088\u3046\u306b\u6d3b\u7528\u3067\u304d\u308b\u304b\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u3042\u308b\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e3b\u6210\u5206\u5206\u6790\u3092\u884c\u3063\u305f\u7d50\u679c\u3001\u591a\u304f\u306e\u4e3b\u6210\u5206\u304c\u5fc5\u8981\u3068\u306a\u308a\u307e\u3057\u305f\uff0820\u500b\u306e\u4e3b\u6210\u5206\u3067\u3088\u3046\u3084\u304f80%\u306e\u5206\u6563\u3092\u8aac\u660e\uff09\u3002\u4e00\u65b9\u3001\u5225\u306e\u751f\u7406\u5b66\u7684\u6e2c\u5b9a\u30c7\u30fc\u30bf\u3067\u306f2\u3064\u306e\u4e3b\u6210\u5206\u306790%\u4ee5\u4e0a\u306e\u5206\u6563\u3092\u8aac\u660e\u3067\u304d\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u7d50\u679c\u304b\u3089\u305d\u308c\u305e\u308c\u306e\u30c7\u30fc\u30bf\u306e\u6027\u8cea\u306b\u3064\u3044\u3066\u3069\u306e\u3088\u3046\u306a\u3053\u3068\u304c\u8a00\u3048\u308b\u3067\u3057\u3087\u3046\u304b\uff1f\u30c7\u30fc\u30bf\u306e\u8907\u96d1\u3055\u3068\u4e3b\u6210\u5206\u306e\u6570\u306e\u95a2\u4fc2\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/41-principal-component-analysis/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/41-principal-component-analysis/#q1","title":"Q1: \u4e3b\u6210\u5206\u5206\u6790\u306f\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u4f7f\u3046\u3079\u304d\u3067\u3059\u304b\uff1f","text":"<p>A1: \u4e3b\u6210\u5206\u5206\u6790\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u7279\u306b\u6709\u7528\u3067\u3059\uff1a - \u591a\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u8996\u899a\u5316\u3057\u305f\u3044\u5834\u5408\uff082\u6b21\u5143\u30843\u6b21\u5143\u306b\u5727\u7e2e\uff09 - \u5909\u6570\u9593\u306b\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308a\u3001\u6b21\u5143\u524a\u6e1b\u304c\u53ef\u80fd\u3068\u601d\u308f\u308c\u308b\u5834\u5408 - \u30c7\u30fc\u30bf\u306e\u30ce\u30a4\u30ba\u3092\u6e1b\u3089\u3057\u305f\u3044\u5834\u5408 - \u7dda\u5f62\u56de\u5e30\u306a\u3069\u306e\u5206\u6790\u3067\u591a\u91cd\u5171\u7dda\u6027\u306e\u554f\u984c\u304c\u3042\u308b\u5834\u5408 - \u30c7\u30fc\u30bf\u306e\u4e3b\u8981\u306a\u5909\u52d5\u8981\u56e0\u3092\u7279\u5b9a\u3057\u305f\u3044\u5834\u5408</p>"},{"location":"lectures/LA/41-principal-component-analysis/#q2","title":"Q2: \u4e3b\u6210\u5206\u306e\u6570\u306f\u3069\u306e\u3088\u3046\u306b\u6c7a\u3081\u308c\u3070\u826f\u3044\u3067\u3059\u304b\uff1f","text":"<p>A2: \u4e3b\u6210\u5206\u306e\u6570\u3092\u6c7a\u3081\u308b\u4e00\u822c\u7684\u306a\u65b9\u6cd5\u306b\u306f\u4ee5\u4e0b\u304c\u3042\u308a\u307e\u3059\uff1a - \u7d2f\u7a4d\u5206\u6563\u8aac\u660e\u7387\u304c\u4e00\u5b9a\u306e\u95be\u5024\uff08\u4f8b\u3048\u307080%\u308490%\uff09\u3092\u8d85\u3048\u308b\u70b9\u3067\u5207\u308b - \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\uff08\u56fa\u6709\u5024\u306e\u30d7\u30ed\u30c3\u30c8\uff09\u306e\u300c\u3072\u3058\u300d\u306e\u4f4d\u7f6e\u3067\u5207\u308b - \u30ab\u30a4\u30b6\u30fc\u57fa\u6e96\uff1a\u56fa\u6709\u5024\u304c1.0\u3088\u308a\u5927\u304d\u3044\u4e3b\u6210\u5206\u306e\u307f\u3092\u9078\u3076\uff08\u76f8\u95a2\u884c\u5217\u3092\u4f7f\u7528\u3057\u305f\u5834\u5408\uff09 - \u4ea4\u5dee\u691c\u8a3c\u306a\u3069\u306e\u7d71\u8a08\u7684\u624b\u6cd5\u3067\u8a55\u4fa1\u3059\u308b - \u89e3\u91c8\u53ef\u80fd\u6027\u3082\u8003\u616e\u3059\u308b\uff08\u9078\u629e\u3057\u305f\u4e3b\u6210\u5206\u304c\u610f\u5473\u306e\u3042\u308b\u89e3\u91c8\u304c\u53ef\u80fd\u304b\uff09</p>"},{"location":"lectures/LA/41-principal-component-analysis/#q3","title":"Q3: \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3068\u76f8\u95a2\u884c\u5217\u306e\u3069\u3061\u3089\u3092\u4f7f\u3046\u3079\u304d\u3067\u3059\u304b\uff1f","text":"<p>A3: \u4e00\u822c\u7684\u306b\u306f\u4ee5\u4e0b\u306e\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u304c\u3042\u308a\u307e\u3059\uff1a - \u5909\u6570\u306e\u5358\u4f4d\u304c\u540c\u3058\uff08\u4f8b\uff1a\u5168\u3066\u9577\u3055\u306e\u6e2c\u5b9a\u5024\uff09\u3067\u3001\u305d\u308c\u3089\u306e\u5206\u6563\u306e\u5927\u304d\u3055\u304c\u610f\u5473\u3092\u6301\u3064\u5834\u5408\u306f\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3092\u4f7f\u3046 - \u5909\u6570\u306e\u5358\u4f4d\u304c\u7570\u306a\u308b\uff08\u4f8b\uff1a\u8eab\u9577\u3001\u4f53\u91cd\u3001\u8840\u5727\u306a\u3069\uff09\u5834\u5408\u306f\u76f8\u95a2\u884c\u5217\u3092\u4f7f\u3046 - \u5909\u6570\u306e\u30b9\u30b1\u30fc\u30eb\u304c\u5927\u304d\u304f\u7570\u306a\u308a\u3001\u7d50\u679c\u306b\u4e0d\u5f53\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u5834\u5408\u306f\u76f8\u95a2\u884c\u5217\u3092\u4f7f\u3046 - \u3088\u308a\u4fdd\u5b88\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3068\u3057\u3066\u306f\u3001\u4e21\u65b9\u306e\u65b9\u6cd5\u3067\u5206\u6790\u3092\u884c\u3044\u3001\u7d50\u679c\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u3082\u6709\u52b9</p>"},{"location":"lectures/LA/41-principal-component-analysis/#q4","title":"Q4: \u4e3b\u6210\u5206\u5206\u6790\u306e\u7d50\u679c\u3092\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3059\u308c\u3070\u826f\u3044\u3067\u3059\u304b\uff1f","text":"<p>A4: \u4e3b\u6210\u5206\u5206\u6790\u306e\u7d50\u679c\u306e\u89e3\u91c8\u306b\u306f\u4ee5\u4e0b\u306e\u89b3\u70b9\u304c\u91cd\u8981\u3067\u3059\uff1a - \u5404\u4e3b\u6210\u5206\u306e\u5206\u6563\u8aac\u660e\u7387\uff1a\u305d\u306e\u4e3b\u6210\u5206\u304c\u30c7\u30fc\u30bf\u306e\u5909\u52d5\u3092\u3069\u306e\u7a0b\u5ea6\u8aac\u660e\u3057\u3066\u3044\u308b\u304b - \u4e3b\u6210\u5206\u8ca0\u8377\u91cf\uff1a\u5404\u5909\u6570\u304c\u4e3b\u6210\u5206\u306b\u3069\u306e\u7a0b\u5ea6\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b - \u4e3b\u6210\u5206\u30b9\u30b3\u30a2\uff1a\u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u304a\u3051\u308b\u4f4d\u7f6e - \u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\uff1a\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u3068\u5909\u6570\u30d9\u30af\u30c8\u30eb\u306e\u4e21\u65b9\u3092\u8868\u793a\u3057\u3001\u95a2\u4fc2\u6027\u3092\u8996\u899a\u5316</p> <p>\u7279\u306b\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u304c\u5927\u304d\u3044\u5909\u6570\u306b\u6ce8\u76ee\u3059\u308b\u3068\u3001\u5404\u4e3b\u6210\u5206\u304c\u8868\u3059\u6f5c\u5728\u7684\u306a\u6982\u5ff5\u3084\u7279\u5fb4\u3092\u89e3\u91c8\u3057\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/41-principal-component-analysis/#q5","title":"Q5: \u4e3b\u6210\u5206\u5206\u6790\u306e\u9650\u754c\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A5: \u4e3b\u6210\u5206\u5206\u6790\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u9650\u754c\u304c\u3042\u308a\u307e\u3059\uff1a - \u7dda\u5f62\u306a\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u3067\u3042\u308a\u3001\u975e\u7dda\u5f62\u306a\u95a2\u4fc2\u3092\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044 - \u5206\u6563\u306e\u5927\u304d\u3055\u306b\u57fa\u3065\u3044\u3066\u4e3b\u6210\u5206\u3092\u9078\u3076\u305f\u3081\u3001\u5206\u985e\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u76f4\u63a5\u5f79\u7acb\u3064\u3068\u306f\u9650\u3089\u306a\u3044 - \u5916\u308c\u5024\u306b\u654f\u611f\u3067\u3042\u308b - \u7d50\u679c\u306e\u89e3\u91c8\u304c\u5fc5\u305a\u3057\u3082\u76f4\u611f\u7684\u3067\u306a\u3044\u5834\u5408\u304c\u3042\u308b - \u5909\u6570\u9593\u306e\u95a2\u4fc2\u304c\u8907\u96d1\u306a\u5834\u5408\u3001\u591a\u304f\u306e\u4e3b\u6210\u5206\u304c\u5fc5\u8981\u306b\u306a\u308a\u3001\u6b21\u5143\u524a\u6e1b\u306e\u52b9\u679c\u304c\u9650\u5b9a\u7684\u306b\u306a\u308b</p> <p>\u3053\u308c\u3089\u306e\u9650\u754c\u3092\u514b\u670d\u3059\u308b\u305f\u3081\u306b\u3001\u30ab\u30fc\u30cd\u30ebPCA\u3084\u56e0\u5b50\u5206\u6790\u306a\u3069\u306e\u4ee3\u66ff\u624b\u6cd5\u3084\u3001\u3088\u308a\u9ad8\u5ea6\u306a\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u6cd5\uff08t-SNE\u3001UMAP\u306a\u3069\uff09\u3082\u691c\u8a0e\u3059\u308b\u4fa1\u5024\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/42-principal-component-analysis/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II\uff1a\u7b2c42\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8","text":""},{"location":"lectures/LA/42-principal-component-analysis/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c42\u56de \u30c6\u30fc\u30de: \u4e3b\u6210\u5206\u5206\u6790\u306e\u7d50\u679c\u306e\u89e3\u91c8\u3068\u6b21\u5143\u524a\u6e1b \u95a2\u9023\u9805\u76ee: \u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3001\u5171\u5206\u6563\u884c\u5217\u3001\u6b21\u5143\u524a\u6e1b \u4e88\u7fd2\u5185\u5bb9: \u7b2c41\u56de\u306e\u4e3b\u6210\u5206\u5206\u6790\u306e\u5c0e\u5165\u3001\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u6982\u5ff5\u3001\u6b63\u5b9a\u5024\u5bfe\u79f0\u884c\u5217\u306e\u6027\u8cea</p>"},{"location":"lectures/LA/42-principal-component-analysis/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u8b1b\u7fa9\u306e\u7d42\u4e86\u6642\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> <ol> <li>\u4e3b\u6210\u5206\u5206\u6790\u306e\u7d50\u679c\u3092\u7dda\u5f62\u4ee3\u6570\u306e\u89b3\u70b9\u304b\u3089\u53b3\u5bc6\u306b\u89e3\u91c8\u3067\u304d\u308b</li> <li>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3068\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3001\u6570\u5b66\u7684\u80cc\u666f\u306b\u57fa\u3065\u3044\u3066\u89e3\u91c8\u3067\u304d\u308b</li> <li>\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u6570\u7406\u7684\u5b9a\u7fa9\u3092\u7406\u89e3\u3057\u3001\u5916\u308c\u5024\u691c\u51fa\u306b\u5fdc\u7528\u3067\u304d\u308b</li> <li>\u4e3b\u6210\u5206\u56de\u5e30\u306e\u7dda\u5f62\u4ee3\u6570\u7684\u57fa\u790e\u3068\u7406\u8ad6\u7684\u7279\u6027\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u5065\u5eb7\u30fb\u533b\u7642\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e3b\u6210\u5206\u5206\u6790\u306e\u5fdc\u7528\u4f8b\u3092\u7dda\u5f62\u5909\u63db\u306e\u89b3\u70b9\u304b\u3089\u7406\u89e3\u3067\u304d\u308b</li> </ol>"},{"location":"lectures/LA/42-principal-component-analysis/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/42-principal-component-analysis/#31","title":"3.1 \u4e3b\u6210\u5206\u5206\u6790\u306e\u6570\u7406\u7684\u57fa\u790e","text":"<p>\u4e3b\u6210\u5206\u5206\u6790\uff08Principal Component Analysis, PCA\uff09\u306f\u3001\u7dda\u5f62\u4ee3\u6570\u5b66\u306b\u304a\u3051\u308b\u56fa\u6709\u5024\u5206\u89e3\uff08\u307e\u305f\u306f\u7279\u7570\u5024\u5206\u89e3\uff09\u306b\u57fa\u3065\u304f\u6b21\u5143\u524a\u6e1b\u3068\u7279\u5fb4\u62bd\u51fa\u306e\u305f\u3081\u306e\u624b\u6cd5\u3067\u3059\u3002\u524d\u56de\u306e\u8b1b\u7fa9\u3067\u5b66\u3093\u3060\u7406\u8ad6\u3092\u5fa9\u7fd2\u3057\u307e\u3057\u3087\u3046\u3002</p> <p>\u4e3b\u6210\u5206\u5206\u6790\u306e\u6570\u5b66\u7684\u5b9a\u7fa9</p> <p>\\(n\\)\u500b\u306e\\(p\\)\u6b21\u5143\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8 \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\) \u304c\u3042\u308b\u3068\u304d\u3001\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{\\mu}\\) \u3068\u5171\u5206\u6563\u884c\u5217 \\(\\boldsymbol{\\Sigma}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> <p>\\(\\boldsymbol{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_i\\)</p> <p>\\(\\boldsymbol{\\Sigma} = \\frac{1}{n}\\sum_{i=1}^{n}(\\mathbf{x}_i - \\boldsymbol{\\mu})(\\mathbf{x}_i - \\boldsymbol{\\mu})^T\\)</p> <p>\u5171\u5206\u6563\u884c\u5217 \\(\\boldsymbol{\\Sigma}\\) \u306f\u5bfe\u79f0\u884c\u5217\u3067\u3042\u308a\u3001\u975e\u8ca0\u306e\u56fa\u6709\u5024 \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_p \\geq 0\\) \u3068\u3001\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\u6b63\u898f\u76f4\u4ea4\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_p\\) \u3092\u6301\u3061\u307e\u3059\u3002</p> <p>\u7b2c \\(j\\) \u4e3b\u6210\u5206\u306f\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{e}_j\\) \u65b9\u5411\u3078\u306e\u5c04\u5f71\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u3001\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8 \\(\\mathbf{x}_i\\) \u306e\u7b2c \\(j\\) \u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u306f\u4ee5\u4e0b\u3067\u8a08\u7b97\u3055\u308c\u307e\u3059\uff1a</p> <p>\\(z_{ij} = \\mathbf{e}_j^T(\\mathbf{x}_i - \\boldsymbol{\\mu})\\)</p> <p>\u4e3b\u6210\u5206\u5206\u6790\u306e\u76ee\u7684\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u6700\u5927\u9650\u306b\u4fdd\u6301\u3057\u306a\u304c\u3089\u3001\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u7684\u306b\u306f\u3001\u3053\u308c\u306f\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u4f4e\u6b21\u5143\u306e\u90e8\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u3059\u308b\u3053\u3068\u306b\u76f8\u5f53\u3057\u307e\u3059\u3002\u7b2c\\(j\\)\u4e3b\u6210\u5206\u65b9\u5411\u306e\u5206\u6563\u306f\u56fa\u6709\u5024\\(\\lambda_j\\)\u3067\u4e0e\u3048\u3089\u308c\u3001\u4e3b\u6210\u5206\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/42-principal-component-analysis/#32","title":"3.2 \u5171\u5206\u6563\u884c\u5217\u3068\u56fa\u6709\u5024\u5206\u89e3\u306e\u95a2\u4fc2","text":"<p>\u5171\u5206\u6563\u884c\u5217 \\(\\boldsymbol{\\Sigma}\\) \u306f\u6b63\u5b9a\u5024\u5bfe\u79f0\u884c\u5217\uff08\u307e\u305f\u306f\u534a\u6b63\u5b9a\u5024\u5bfe\u79f0\u884c\u5217\uff09\u3067\u3042\u308b\u305f\u3081\u3001\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u91cd\u8981\u306a\u5b9a\u7406\u306b\u3088\u308a\u3001\u4ee5\u4e0b\u306e\u56fa\u6709\u5024\u5206\u89e3\u304c\u53ef\u80fd\u3067\u3059\uff1a</p> <p>\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3</p> <p>\\(\\boldsymbol{\\Sigma} = \\mathbf{E} \\boldsymbol{\\Lambda} \\mathbf{E}^T\\)</p> <p>\u3053\u3053\u3067\u3001\\(\\mathbf{E} = [\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_p]\\) \u306f\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u5217\u3068\u3059\u308b\u76f4\u4ea4\u884c\u5217\u3067\u3042\u308a\u3001\\(\\boldsymbol{\\Lambda} = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_p)\\) \u306f\u56fa\u6709\u5024\u3092\u5bfe\u89d2\u6210\u5206\u3068\u3059\u308b\u5bfe\u89d2\u884c\u5217\u3067\u3059\u3002</p> <p>\u3053\u306e\u56fa\u6709\u5024\u5206\u89e3\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u69cb\u9020\u3092\u6700\u3082\u3088\u304f\u8868\u73fe\u3059\u308b\u76f4\u4ea4\u57fa\u5e95\uff08\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff09\u3092\u898b\u3064\u3051\u3001\u5404\u57fa\u5e95\u65b9\u5411\u306e\u30c7\u30fc\u30bf\u306e\u6563\u3089\u3070\u308a\u5177\u5408\uff08\u5206\u6563\uff09\u3092\u56fa\u6709\u5024\u3068\u3057\u3066\u5b9a\u91cf\u5316\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/42-principal-component-analysis/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/42-principal-component-analysis/#41","title":"4.1 \u4e3b\u6210\u5206\u5206\u6790\u306e\u7d50\u679c\u306e\u6570\u7406\u7684\u89e3\u91c8","text":""},{"location":"lectures/LA/42-principal-component-analysis/#411","title":"4.1.1 \u56fa\u6709\u5024\u3068\u5206\u6563\u306e\u95a2\u4fc2","text":"<p>\u56fa\u6709\u5024\u306f\u5404\u4e3b\u6210\u5206\u306e\u91cd\u8981\u5ea6\u3092\u8868\u3057\u307e\u3059\u304c\u3001\u3088\u308a\u53b3\u5bc6\u306b\u306f\u3001\u5404\u56fa\u6709\u5024\u306f\u5bfe\u5fdc\u3059\u308b\u4e3b\u6210\u5206\u65b9\u5411\u306e\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u56fa\u6709\u5024\u3068\u5206\u6563\u306e\u95a2\u4fc2</p> <p>\u5143\u306e\u30c7\u30fc\u30bf\u306e\u5168\u5206\u6563\u306f\u3001\u5171\u5206\u6563\u884c\u5217\u306e\u5bfe\u89d2\u548c\uff08\u30c8\u30ec\u30fc\u30b9\uff09\u306b\u7b49\u3057\u304f\u3001\u3053\u308c\u306f\u5168\u56fa\u6709\u5024\u306e\u548c\u306b\u7b49\u3057\u304f\u306a\u308a\u307e\u3059\uff1a</p> <p>\\(\\text{\u5168\u5206\u6563} = \\text{tr}(\\boldsymbol{\\Sigma}) = \\sum_{j=1}^{p} \\lambda_j\\)</p> <p>\u7b2c \\(j\\) \u4e3b\u6210\u5206\u306e\u5206\u6563 = \\(\\lambda_j\\)</p> <p>\u4f8b\u984c\uff1a\u3042\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u304c \\(\\lambda = [5.2, 2.1, 0.7, 0.3, 0.1]\\) \u3067\u3042\u3063\u305f\u5834\u5408\u3001\u5404\u4e3b\u6210\u5206\u306e\u8aac\u660e\u3059\u308b\u5206\u6563\u306e\u5272\u5408\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> <p>\u89e3\u7b54\uff1a \u5168\u5206\u6563 = \\(\\sum_{j=1}^{5} \\lambda_j = 5.2 + 2.1 + 0.7 + 0.3 + 0.1 = 8.4\\)</p> <p>\u7b2c1\u4e3b\u6210\u5206\uff1a\\(\\frac{\\lambda_1}{\\sum_{j=1}^{5} \\lambda_j} = \\frac{5.2}{8.4} = 0.619 = 61.9\\%\\) \u7b2c2\u4e3b\u6210\u5206\uff1a\\(\\frac{\\lambda_2}{\\sum_{j=1}^{5} \\lambda_j} = \\frac{2.1}{8.4} = 0.250 = 25.0\\%\\) \u7b2c3\u4e3b\u6210\u5206\uff1a\\(\\frac{\\lambda_3}{\\sum_{j=1}^{5} \\lambda_j} = \\frac{0.7}{8.4} = 0.083 = 8.3\\%\\) \u7b2c4\u4e3b\u6210\u5206\uff1a\\(\\frac{\\lambda_4}{\\sum_{j=1}^{5} \\lambda_j} = \\frac{0.3}{8.4} = 0.036 = 3.6\\%\\) \u7b2c5\u4e3b\u6210\u5206\uff1a\\(\\frac{\\lambda_5}{\\sum_{j=1}^{5} \\lambda_j} = \\frac{0.1}{8.4} = 0.012 = 1.2\\%\\) </p>"},{"location":"lectures/LA/42-principal-component-analysis/#412","title":"4.1.2 \u5bc4\u4e0e\u7387\u3068\u7d2f\u7a4d\u5bc4\u4e0e\u7387","text":"<p>\u5bc4\u4e0e\u7387\u306f\u5404\u4e3b\u6210\u5206\u304c\u5168\u5206\u6563\u306e\u3046\u3061\u3069\u308c\u3060\u3051\u3092\u8aac\u660e\u3057\u3066\u3044\u308b\u304b\u3092\u8868\u3057\u307e\u3059\u3002\u6570\u5b66\u7684\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> <p>\u5bc4\u4e0e\u7387\u306e\u6570\u5b66\u7684\u5b9a\u7fa9</p> <p>\u7b2c \\(j\\) \u4e3b\u6210\u5206\u306e\u5bc4\u4e0e\u7387 = \\(\\frac{\\lambda_j}{\\sum_{i=1}^{p} \\lambda_i}\\)</p> <p>\u7b2c \\(j\\) \u4e3b\u6210\u5206\u307e\u3067\u306e\u7d2f\u7a4d\u5bc4\u4e0e\u7387 = \\(\\frac{\\sum_{i=1}^{j} \\lambda_i}{\\sum_{i=1}^{p} \\lambda_i}\\)</p> <p>\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306f\u3001\u9078\u629e\u3057\u305f\u4e3b\u6210\u5206\u304c\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u3069\u308c\u3060\u3051\u8aac\u660e\u3057\u3066\u3044\u308b\u304b\u3092\u793a\u3057\u307e\u3059\u3002\u4e00\u822c\u7684\u306b\u3001\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u304c70%\u301c90%\u306b\u306a\u308b\u3088\u3046\u306b\u4e3b\u6210\u5206\u6570\u3092\u9078\u629e\u3057\u307e\u3059\u3002</p> <p>\u4e0a\u8a18\u306e\u4f8b\u984c\u306e\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a \u7b2c1\u4e3b\u6210\u5206\u307e\u3067\uff1a61.9% \u7b2c2\u4e3b\u6210\u5206\u307e\u3067\uff1a61.9% + 25.0% = 86.9% \u7b2c3\u4e3b\u6210\u5206\u307e\u3067\uff1a86.9% + 8.3% = 95.2% \u7b2c4\u4e3b\u6210\u5206\u307e\u3067\uff1a95.2% + 3.6% = 98.8% \u7b2c5\u4e3b\u6210\u5206\u307e\u3067\uff1a98.8% + 1.2% = 100%  </p>"},{"location":"lectures/LA/42-principal-component-analysis/#413","title":"4.1.3 \u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u306e\u7dda\u5f62\u4ee3\u6570\u7684\u89e3\u91c8","text":"<p>\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u306f\u3001\u5404\u4e3b\u6210\u5206\u3068\u5143\u306e\u5909\u6570\u3068\u306e\u76f8\u95a2\u4fc2\u6570\u3067\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u7684\u306b\u306f\u3001\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u306f\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u56fa\u6709\u5024\u3092\u7528\u3044\u3066\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002</p> <p>\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u306e\u6570\u5b66\u7684\u5b9a\u7fa9</p> <p>\u7b2c \\(j\\) \u4e3b\u6210\u5206\u306e\u7b2c \\(i\\) \u5909\u6570\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u91cf = \\(\\sqrt{\\lambda_j} \\times e_{ij}\\)</p> <p>\u3053\u3053\u3067\u3001\\(e_{ij}\\) \u306f\u7b2c \\(j\\) \u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{e}_j\\) \u306e\u7b2c \\(i\\) \u6210\u5206\u3067\u3059\u3002</p> <p>\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u306e\u884c\u5217 \\(\\mathbf{L}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> <p>\\(\\mathbf{L} = \\mathbf{E} \\boldsymbol{\\Lambda}^{1/2}\\)</p> <p>\u3053\u3053\u3067\u3001\\(\\boldsymbol{\\Lambda}^{1/2} = \\text{diag}(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}, \\ldots, \\sqrt{\\lambda_p})\\) \u3067\u3059\u3002</p> <p>\u89e3\u91c8\u4f8b\uff1a - \u8ca0\u8377\u91cf\u304c\u6b63\u306e\u5927\u304d\u306a\u5024 \u2192 \u4e3b\u6210\u5206\u3068\u5909\u6570\u306f\u6b63\u306e\u76f8\u95a2 - \u8ca0\u8377\u91cf\u304c\u8ca0\u306e\u5927\u304d\u306a\u5024 \u2192 \u4e3b\u6210\u5206\u3068\u5909\u6570\u306f\u8ca0\u306e\u76f8\u95a2 - \u8ca0\u8377\u91cf\u304c0\u306b\u8fd1\u3044 \u2192 \u4e3b\u6210\u5206\u3068\u5909\u6570\u306b\u306f\u307b\u3068\u3093\u3069\u95a2\u9023\u304c\u306a\u3044</p>"},{"location":"lectures/LA/42-principal-component-analysis/#414","title":"4.1.4 \u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u3068\u5ea7\u6a19\u5909\u63db","text":"<p>\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u306f\u3001\u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u3092\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u6295\u5f71\u3057\u305f\u5024\u3067\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u7684\u306b\u306f\u3001\u3053\u308c\u306f\u30c7\u30fc\u30bf\u306e\u5ea7\u6a19\u5909\u63db\u306b\u76f8\u5f53\u3057\u307e\u3059\u3002</p> <p>\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u306e\u6570\u5b66\u7684\u5b9a\u7fa9</p> <p>\u4e2d\u5fc3\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\u884c\u5217 \\(\\mathbf{X}_c = [\\mathbf{x}_1 - \\boldsymbol{\\mu}, \\mathbf{x}_2 - \\boldsymbol{\\mu}, \\ldots, \\mathbf{x}_n - \\boldsymbol{\\mu}]^T\\) \u306b\u5bfe\u3057\u3066\u3001\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u884c\u5217 \\(\\mathbf{Z}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3055\u308c\u307e\u3059\uff1a</p> <p>\\(\\mathbf{Z} = \\mathbf{X}_c \\mathbf{E} = \\mathbf{X}_c [\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_p]\\)</p> <p>\u5404\u884c\u304c\u4e00\u3064\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u3092\u8868\u3057\u3001\u5404\u5217\u304c\u4e00\u3064\u306e\u4e3b\u6210\u5206\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002</p> <p>\u3053\u308c\u306f\u3001\u5143\u306e\u30c7\u30fc\u30bf\u7a7a\u9593\u304b\u3089\u65b0\u3057\u3044\u76f4\u4ea4\u57fa\u5e95\uff08\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff09\u306b\u3088\u3063\u3066\u5b9a\u7fa9\u3055\u308c\u308b\u4e3b\u6210\u5206\u7a7a\u9593\u3078\u306e\u7dda\u5f62\u5909\u63db\u3067\u3059\u3002\u3053\u306e\u5909\u63db\u306b\u3088\u308a\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u306f\u5bfe\u89d2\u5316\u3055\u308c\u3001\u4e3b\u6210\u5206\u9593\u306e\u5171\u5206\u6563\u306f\u30bc\u30ed\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/42-principal-component-analysis/#42","title":"4.2 \u53ef\u8996\u5316\u30c4\u30fc\u30eb\u3068\u6570\u7406\u7684\u80cc\u666f","text":""},{"location":"lectures/LA/42-principal-component-analysis/#421","title":"4.2.1 \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3068\u56fa\u6709\u5024\u306e\u6e1b\u8870\u7387","text":"<p>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306f\u56fa\u6709\u5024\u306e\u5927\u304d\u3055\u3092\u964d\u9806\u306b\u4e26\u3079\u305f\u30b0\u30e9\u30d5\u3067\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u7684\u306a\u89b3\u70b9\u304b\u3089\u306f\u3001\u56fa\u6709\u5024\u306e\u6e1b\u8870\u30d1\u30bf\u30fc\u30f3\u306f\u30c7\u30fc\u30bf\u306e\u672c\u8cea\u7684\u306a\u6b21\u5143\u6027\u3092\u793a\u5506\u3057\u307e\u3059\u3002</p> <p>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306e\u6570\u7406\u7684\u89e3\u91c8</p> <p>\u56fa\u6709\u5024\u306e\u964d\u4e0b\u304c\u6025\u306a\u90e8\u5206\u306f\u3001\u30c7\u30fc\u30bf\u306e\u4e3b\u8981\u306a\u5206\u6563\u65b9\u5411\u3092\u793a\u3057\u307e\u3059\u3002\u4e00\u65b9\u3001\u56fa\u6709\u5024\u304c\u6025\u6fc0\u306b\u6e1b\u5c11\u3057\u305f\u5f8c\u306e\u5e73\u5766\u306a\u90e8\u5206\u306f\u3001\u30c7\u30fc\u30bf\u306e\u30ce\u30a4\u30ba\u3084\u5197\u9577\u6027\u306b\u5bfe\u5fdc\u3059\u308b\u65b9\u5411\u3092\u793a\u3057\u307e\u3059\u3002</p> <p>\u300c\u8098\u300d\u306e\u4f4d\u7f6e\u306f\u3001\u30c7\u30fc\u30bf\u306e\u672c\u8cea\u7684\u306a\u6b21\u5143\u6570\u3092\u793a\u5506\u3057\u3001\u3053\u306e\u70b9\u307e\u3067\u306e\u4e3b\u6210\u5206\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u60c5\u5831\u306e\u640d\u5931\u3092\u6700\u5c0f\u9650\u306b\u6291\u3048\u3064\u3064\u6b21\u5143\u524a\u6e1b\u304c\u53ef\u80fd\u3067\u3059\u3002</p> <p>\u300c\u8098\u300d\u306e\u6570\u5b66\u7684\u8003\u5bdf\uff1a\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306b\u304a\u3044\u3066\u3001\u56fa\u6709\u5024\u306e\u6e1b\u5c11\u7387\uff08\\(\\frac{\\lambda_j - \\lambda_{j+1}}{\\lambda_j}\\)\uff09\u304c\u6025\u6fc0\u306b\u5c0f\u3055\u304f\u306a\u308b\u70b9\u3092\u300c\u8098\u300d\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/42-principal-component-analysis/#422","title":"4.2.2 \u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u3068\u5909\u6570\u306e\u5c04\u5f71","text":"<p>\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306f\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u3068\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u3092\u540c\u6642\u306b\u8868\u793a\u3059\u308b\u30d7\u30ed\u30c3\u30c8\u3067\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u7684\u306b\u306f\u3001\u3053\u308c\u306f\u5143\u306e\u5909\u6570\u3092\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u3057\u305f\u3082\u306e\u3068\u898b\u306a\u305b\u307e\u3059\u3002</p> <p>\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u6570\u7406\u7684\u57fa\u790e</p> <p>\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u3067\u306f\u3001\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306f\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u884c\u5217 \\(\\mathbf{Z}\\) \u306e\u6700\u521d\u306e2\u5217\uff08\u901a\u5e38\u306f\u7b2c1\u4e3b\u6210\u5206\u3068\u7b2c2\u4e3b\u6210\u5206\uff09\u3092\u7528\u3044\u3066\u30d7\u30ed\u30c3\u30c8\u3055\u308c\u307e\u3059\u3002</p> <p>\u5909\u6570\u30d9\u30af\u30c8\u30eb\u306f\u3001\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u884c\u5217 \\(\\mathbf{L}\\) \u306e\u6700\u521d\u306e2\u5217\u3092\u7528\u3044\u3066\u63cf\u304b\u308c\u3001\u5404\u5909\u6570\u304c\u7b2c1\u4e3b\u6210\u5206\u3068\u7b2c2\u4e3b\u6210\u5206\u306b\u3069\u306e\u3088\u3046\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u793a\u3057\u307e\u3059\u3002</p> <p>\u6570\u5b66\u7684\u306b\u306f\u3001\u5909\u6570\u30d9\u30af\u30c8\u30eb\u306e\u5411\u304d\u3068\u9577\u3055\u306f\u3001\u5143\u306e\u5909\u6570\u306e\u57fa\u5e95\u30d9\u30af\u30c8\u30eb\u3092\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u5c04\u5f71\u3057\u305f\u3082\u306e\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002</p> <p>\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\uff1a - \u5909\u6570\u30d9\u30af\u30c8\u30eb\u304c\u540c\u3058\u65b9\u5411\u3092\u5411\u3044\u3066\u3044\u308b\u2192\u3053\u308c\u3089\u306e\u5909\u6570\u306f\u6b63\u306e\u76f8\u95a2\u95a2\u4fc2\u306b\u3042\u308b - \u5909\u6570\u30d9\u30af\u30c8\u30eb\u304c\u53cd\u5bfe\u65b9\u5411\u3092\u5411\u3044\u3066\u3044\u308b\u2192\u3053\u308c\u3089\u306e\u5909\u6570\u306f\u8ca0\u306e\u76f8\u95a2\u95a2\u4fc2\u306b\u3042\u308b - \u5909\u6570\u30d9\u30af\u30c8\u30eb\u304c\u76f4\u4ea4\u3057\u3066\u3044\u308b\u2192\u3053\u308c\u3089\u306e\u5909\u6570\u306f\u307b\u307c\u7121\u76f8\u95a2\u3067\u3042\u308b - \u30d9\u30af\u30c8\u30eb\u306e\u9577\u3055\u306f\u3001\u305d\u306e\u5909\u6570\u304c\u4e3b\u6210\u5206\u5e73\u9762\u4e0a\u3067\u8868\u73fe\u3055\u308c\u308b\u7a0b\u5ea6\u3092\u793a\u3059</p>"},{"location":"lectures/LA/42-principal-component-analysis/#43","title":"4.3 \u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u6570\u7406\u7684\u57fa\u790e","text":"<p>\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u69cb\u9020\u3092\u8003\u616e\u3057\u305f\u8ddd\u96e2\u6e2c\u5ea6\u3067\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u7684\u306b\u306f\u3001\u30c7\u30fc\u30bf\u7a7a\u9593\u306e\u8a08\u91cf\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u53b3\u5bc6\u306a\u5b9a\u7fa9</p> <p>\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8 \\(\\mathbf{x}\\) \u306b\u5bfe\u3059\u308b\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u4e8c\u4e57\u306f\u4ee5\u4e0b\u3067\u4e0e\u3048\u3089\u308c\u307e\u3059\uff1a</p> <p>\\(D^2(\\mathbf{x}) = (\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\)</p> <p>\u3053\u3053\u3067\u3001\\(\\boldsymbol{\\mu}\\) \u306f\u30c7\u30fc\u30bf\u306e\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u3001\\(\\boldsymbol{\\Sigma}\\) \u306f\u5171\u5206\u6563\u884c\u5217\u3001\\(\\boldsymbol{\\Sigma}^{-1}\\) \u306f\u305d\u306e\u9006\u884c\u5217\u3067\u3059\u3002</p> <p>\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3 \\(\\boldsymbol{\\Sigma} = \\mathbf{E} \\boldsymbol{\\Lambda} \\mathbf{E}^T\\) \u3092\u7528\u3044\u308b\u3068\u3001\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306f\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> <p>\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2</p> <p>\\(D^2(\\mathbf{x}) = (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{E} \\boldsymbol{\\Lambda}^{-1} \\mathbf{E}^T (\\mathbf{x} - \\boldsymbol{\\mu}) = \\sum_{j=1}^{p} \\frac{z_j^2}{\\lambda_j}\\)</p> <p>\u3053\u3053\u3067\u3001\\(z_j = \\mathbf{e}_j^T(\\mathbf{x} - \\boldsymbol{\\mu})\\) \u306f\u7b2c \\(j\\) \u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u3067\u3059\u3002</p> <p>\u3053\u306e\u5f0f\u304b\u3089\u3001\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306f\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u3092\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u5024\u3067\u6a19\u6e96\u5316\u3057\u3001\u305d\u306e\u4e8c\u4e57\u548c\u3068\u3057\u3066\u8a08\u7b97\u3067\u304d\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u5404\u4e3b\u6210\u5206\u65b9\u5411\u306e\u5206\u6563\u306e\u9055\u3044\u3092\u8003\u616e\u3057\u305f\u6a19\u6e96\u5316\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> <p>\u7d71\u8a08\u7684\u7279\u6027\uff1a\\(p\\) \u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u30c7\u30fc\u30bf\u306e\u5834\u5408\u3001\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u4e8c\u4e57\u306f\u81ea\u7531\u5ea6 \\(p\\) \u306e\u30ab\u30a4\u4e8c\u4e57\u5206\u5e03\u306b\u5f93\u3044\u307e\u3059\u3002\u3053\u308c\u3092\u5229\u7528\u3057\u3066\u3001\u5916\u308c\u5024\u306e\u7d71\u8a08\u7684\u691c\u5b9a\u304c\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"lectures/LA/42-principal-component-analysis/#44","title":"4.4 \u4e3b\u6210\u5206\u56de\u5e30\u306e\u7dda\u5f62\u4ee3\u6570\u7684\u57fa\u790e","text":"<p>\u4e3b\u6210\u5206\u56de\u5e30\uff08Principal Component Regression, PCR\uff09\u306f\u3001\u4e3b\u6210\u5206\u5206\u6790\u3068\u7dda\u5f62\u56de\u5e30\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u624b\u6cd5\u3067\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u7684\u306b\u306f\u3001\u3053\u308c\u306f\u56de\u5e30\u554f\u984c\u3092\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u89e3\u304f\u3053\u3068\u306b\u76f8\u5f53\u3057\u307e\u3059\u3002</p> <p>\u4e3b\u6210\u5206\u56de\u5e30\u306e\u6570\u5b66\u7684\u5b9a\u5f0f\u5316</p> <p>\u8aac\u660e\u5909\u6570 \\(\\mathbf{X}\\) \u3068\u76ee\u7684\u5909\u6570 \\(\\mathbf{y}\\) \u304c\u3042\u308b\u3068\u304d\u3001\u4e3b\u6210\u5206\u56de\u5e30\u306f\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u884c\u308f\u308c\u307e\u3059\uff1a</p> <ol> <li>\\(\\mathbf{X}\\) \u306b\u5bfe\u3057\u3066 PCA \u3092\u9069\u7528\u3057\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u884c\u5217 \\(\\mathbf{E}\\) \u3068\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u884c\u5217 \\(\\mathbf{Z} = \\mathbf{X}_c \\mathbf{E}\\) \u3092\u5f97\u308b</li> <li>\u4e0a\u4f4d \\(k\\) \u500b\u306e\u4e3b\u6210\u5206\u3092\u9078\u629e\u3057\u3001\\(\\mathbf{Z}_k\\) \u3092\u5f97\u308b</li> <li>\\(\\mathbf{Z}_k\\) \u3092\u8aac\u660e\u5909\u6570\u3068\u3059\u308b\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\uff1a\\(\\mathbf{y} = \\mathbf{Z}_k \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}\\)</li> <li>\u5143\u306e\u8aac\u660e\u5909\u6570\u3067\u306e\u4fc2\u6570 \\(\\boldsymbol{\\beta}\\) \u306f\u4ee5\u4e0b\u3067\u6c42\u3081\u3089\u308c\u308b\uff1a\\(\\boldsymbol{\\beta} = \\mathbf{E}_k \\boldsymbol{\\gamma}\\)\uff08\u3053\u3053\u3067 \\(\\mathbf{E}_k\\) \u306f\u6700\u521d\u306e \\(k\\) \u5217\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff09</li> </ol> <p>\u4e3b\u6210\u5206\u56de\u5e30\u306e\u7406\u8ad6\u7684\u7279\u6027\uff1a 1. \u591a\u91cd\u5171\u7dda\u6027\u306e\u554f\u984c\u3092\u89e3\u6c7a\uff08\u4e3b\u6210\u5206\u306f\u4e92\u3044\u306b\u76f4\u4ea4\uff09 2. \u5206\u6563\u306e\u5c0f\u3055\u3044\u65b9\u5411\uff08\u30ce\u30a4\u30ba\u3084\u5197\u9577\u60c5\u5831\uff09\u3092\u9664\u5916\u3059\u308b\u3053\u3068\u3067\u30e2\u30c7\u30eb\u3092\u5b89\u5b9a\u5316 3. \u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u52b9\u7387\u7684\u306a\u56de\u5e30\u30e2\u30c7\u30eb\u69cb\u7bc9\uff08\u6b21\u5143\u306e\u546a\u3044\u306e\u7de9\u548c\uff09</p> <p>\u4f8b\u984c\uff1a\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u300110\u7a2e\u985e\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u3092\u7528\u3044\u3066\u60a3\u8005\u306e\u4e88\u5f8c\u30b9\u30b3\u30a2\u3092\u4e88\u6e2c\u3059\u308b\u554f\u984c\u3092\u8003\u3048\u308b\u3002\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u9593\u306b\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306b\u4e3b\u6210\u5206\u56de\u5e30\u3092\u9069\u7528\u3059\u308b\u304b\u3001\u7dda\u5f62\u4ee3\u6570\u306e\u89b3\u70b9\u304b\u3089\u8aac\u660e\u305b\u3088\u3002</p> <p>\u89e3\u7b54\uff1a 1. 10\u7a2e\u985e\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u304b\u3089\u306a\u308b\u30c7\u30fc\u30bf\u884c\u5217 \\(\\mathbf{X}\\) \u3092\u4e2d\u5fc3\u5316\u3057\u3001\u5171\u5206\u6563\u884c\u5217 \\(\\boldsymbol{\\Sigma} = \\frac{1}{n} \\mathbf{X}_c^T \\mathbf{X}_c\\) \u3092\u8a08\u7b97 2. \u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3 \\(\\boldsymbol{\\Sigma} = \\mathbf{E} \\boldsymbol{\\Lambda} \\mathbf{E}^T\\) \u3092\u884c\u3044\u3001\u56fa\u6709\u5024 \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_{10}\\) \u3068\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb \\(\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_{10}\\) \u3092\u6c42\u3081\u308b 3. \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3084\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306b\u57fa\u3065\u3044\u3066\u3001\u60c5\u5831\u306e\u5927\u90e8\u5206\uff08\u4f8b\uff1a85%\uff09\u3092\u8aac\u660e\u3059\u308b\u4e3b\u6210\u5206\u6570 \\(k\\) \u3092\u6c7a\u5b9a 4. \u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u884c\u5217 \\(\\mathbf{Z} = \\mathbf{X}_c \\mathbf{E}\\) \u3092\u8a08\u7b97\u3057\u3001\u6700\u521d\u306e \\(k\\) \u5217 \\(\\mathbf{Z}_k\\) \u3092\u62bd\u51fa 5. \\(\\mathbf{Z}_k\\) \u3092\u8aac\u660e\u5909\u6570\u3068\u3057\u3066\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb \\(\\mathbf{y} = \\mathbf{Z}_k \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}\\) \u3092\u69cb\u7bc9 6. \u5f97\u3089\u308c\u305f\u4fc2\u6570 \\(\\boldsymbol{\\gamma}\\) \u3092\u5143\u306e\u8aac\u660e\u5909\u6570\u306b\u5909\u63db\uff1a\\(\\boldsymbol{\\beta} = \\mathbf{E}_k \\boldsymbol{\\gamma}\\)</p> <p>\u3053\u306e\u624b\u6cd5\u306b\u3088\u308a\u3001\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u9593\u306e\u76f8\u95a2\u306b\u3088\u308b\u591a\u91cd\u5171\u7dda\u6027\u554f\u984c\u3092\u56de\u907f\u3057\u3001\u30ce\u30a4\u30ba\u306e\u5f71\u97ff\u3092\u6e1b\u3089\u3057\u305f\u5b89\u5b9a\u3057\u305f\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3067\u304d\u307e\u3059\u3002\u5404\u4e3b\u6210\u5206\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u56de\u5e30\u4fc2\u6570\u306e\u63a8\u5b9a\u304c\u5b89\u5b9a\u3057\u3001\u4e88\u6e2c\u306e\u5206\u6563\u3082\u4f4e\u6e1b\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/42-principal-component-analysis/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/42-principal-component-analysis/#51","title":"5.1 \u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790\u3068\u7d50\u679c\u306e\u89e3\u91c8","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import chi2\n\n# \u30b5\u30f3\u30d7\u30eb\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u751f\u6210\uff08\u5b9f\u969b\u306e\u6388\u696d\u3067\u306fCSV\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\uff09\nnp.random.seed(42)\nn_samples = 100\n# \u8840\u5727\u3001\u5fc3\u62cd\u6570\u3001\u4f53\u6e29\u3001BMI\u3001\u8840\u7cd6\u5024\u306e5\u3064\u306e\u6307\u6a19\ndata = np.random.randn(n_samples, 5)\n# \u76f8\u95a2\u3092\u6301\u305f\u305b\u308b\ndata[:, 0] = data[:, 0] + data[:, 1] * 0.8  # \u8840\u5727\u3068\u5fc3\u62cd\u6570\u306b\u76f8\u95a2\ndata[:, 3] = data[:, 3] + data[:, 4] * 0.6  # BMI\u3068\u8840\u7cd6\u5024\u306b\u76f8\u95a2\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210\ncolumns = ['\u8840\u5727', '\u5fc3\u62cd\u6570', '\u4f53\u6e29', 'BMI', '\u8840\u7cd6\u5024']\ndf = pd.DataFrame(data, columns=columns)\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\uff08\u5e73\u57470\u3001\u5206\u65631\uff09\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)\n\n# PCA\u306e\u5b9f\u884c\uff08\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3\u3092\u5185\u90e8\u3067\u5b9f\u884c\uff09\npca = PCA()\npca_result = pca.fit_transform(scaled_data)\n\n# \u56fa\u6709\u5024\uff08\u5206\u6563\uff09\u3068\u5bc4\u4e0e\u7387\u306e\u8a08\u7b97\neigenvalues = pca.explained_variance_\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\n# \u7d50\u679c\u306e\u8868\u793a\nprint(\"\u56fa\u6709\u5024\uff08\u5404\u4e3b\u6210\u5206\u306e\u5206\u6563\uff09:\", eigenvalues)\nprint(\"\u5bc4\u4e0e\u7387:\", explained_variance_ratio)\nprint(\"\u7d2f\u7a4d\u5bc4\u4e0e\u7387:\", cumulative_variance_ratio)\nprint(\"\\n\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u4e3b\u6210\u5206\u306e\u65b9\u5411\uff09:\")\nprint(pca.components_)\n\n# \u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u306e\u8a08\u7b97 L = E * \u039b^(1/2)\nloadings = pca.components_.T * np.sqrt(eigenvalues)\nloadings_df = pd.DataFrame(loadings, index=columns, \n                           columns=[f'PC{i+1}' for i in range(len(eigenvalues))])\nprint(\"\\n\u4e3b\u6210\u5206\u8ca0\u8377\u91cf:\")\nprint(loadings_df)\n\n# \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210\nplt.figure(figsize=(10, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(eigenvalues) + 1), eigenvalues, 'bo-')\nplt.xlabel('\u4e3b\u6210\u5206\u756a\u53f7')\nplt.ylabel('\u56fa\u6709\u5024\uff08\u5206\u6563\uff09')\nplt.title('\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8')\nplt.grid(True)\n\n# \u56fa\u6709\u5024\u306e\u6e1b\u5c11\u7387\u3092\u8a08\u7b97\u3057\u3066\u30d7\u30ed\u30c3\u30c8\nplt.subplot(1, 2, 2)\neigen_ratio = [1.0] + [(eigenvalues[i] - eigenvalues[i+1])/eigenvalues[i] \n                        for i in range(len(eigenvalues)-1)]\nplt.plot(range(1, len(eigenvalues) + 1), eigen_ratio, 'ro-')\nplt.xlabel('\u4e3b\u6210\u5206\u756a\u53f7')\nplt.ylabel('\u56fa\u6709\u5024\u306e\u6e1b\u5c11\u7387')\nplt.title('\u56fa\u6709\u5024\u306e\u6e1b\u5c11\u7387')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# \u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306e\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'go-')\nplt.xlabel('\u4e3b\u6210\u5206\u756a\u53f7')\nplt.ylabel('\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.title('\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.axhline(y=0.8, color='r', linestyle='--', label='80%\u95be\u5024')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# \u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210\nplt.figure(figsize=(10, 8))\n# \u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u30d7\u30ed\u30c3\u30c8\uff08\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\uff09\nplt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n\n# \u5909\u6570\u30d9\u30af\u30c8\u30eb\u306e\u30d7\u30ed\u30c3\u30c8\uff08\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\uff09\nfor i, (x, y) in enumerate(zip(loadings[:, 0], loadings[:, 1])):\n    plt.arrow(0, 0, x * 3, y * 3, head_width=0.1, head_length=0.1, fc='r', ec='r')\n    plt.text(x * 3.1, y * 3.1, columns[i], color='r')\n\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nplt.xlabel(f'\u7b2c1\u4e3b\u6210\u5206 ({explained_variance_ratio[0]:.2%})')\nplt.ylabel(f'\u7b2c2\u4e3b\u6210\u5206 ({explained_variance_ratio[1]:.2%})')\nplt.title('PCA\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# \u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306b\u3088\u308b\u5916\u308c\u5024\u691c\u51fa\ndef mahalanobis_distance(pca_result, eigenvalues):\n    # \u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u3092\u56fa\u6709\u5024\u3067\u5272\u3063\u3066\u4e8c\u4e57\u3057\u3001\u5408\u8a08\u3059\u308b\n    md_squared = np.sum((pca_result ** 2) / eigenvalues, axis=1)\n    return md_squared\n\n# \u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u8a08\u7b97\nmd_squared = mahalanobis_distance(pca_result, eigenvalues)\n\n# \u30ab\u30a4\u4e8c\u4e57\u5206\u5e03\u306e\u95be\u5024\uff08\u81ea\u7531\u5ea6\u306f\u7279\u5fb4\u91cf\u306e\u6570\u3001\u6709\u610f\u6c34\u6e960.01\uff09\nthreshold = chi2.ppf(0.99, df=5)\n\n# \u5916\u308c\u5024\u306e\u691c\u51fa\noutliers = np.where(md_squared &gt; threshold)[0]\nprint(\"\\n\u5916\u308c\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9:\", outliers)\n\n# \u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.scatter(range(len(md_squared)), md_squared, alpha=0.7)\nplt.axhline(y=threshold, color='r', linestyle='--', \n            label=f'\u95be\u5024 (99%): {threshold:.2f}')\nplt.xlabel('\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8')\nplt.ylabel('\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u4e8c\u4e57')\nplt.title('\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306b\u3088\u308b\u5916\u308c\u5024\u691c\u51fa')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/42-principal-component-analysis/#52","title":"5.2 \u4e3b\u6210\u5206\u56de\u5e30\u306e\u8a73\u7d30\u5b9f\u88c5\u3068\u7dda\u5f62\u4ee3\u6570\u7684\u89e3\u91c8","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u751f\u6210\uff08\u5065\u5eb7\u30c7\u30fc\u30bf\u3068\u5065\u5eb7\u30b9\u30b3\u30a2\uff09\nnp.random.seed(42)\nn_samples = 100\nn_features = 5\nX = np.random.randn(n_samples, n_features)\n# \u76f8\u95a2\u3092\u6301\u305f\u305b\u308b\nX[:, 0] = X[:, 0] + X[:, 1] * 0.8\nX[:, 3] = X[:, 3] + X[:, 4] * 0.6\n\n# \u771f\u306e\u4fc2\u6570\u30d9\u30af\u30c8\u30eb\ntrue_coef = np.array([0.2, -0.1, 0.05, -0.3, -0.25])\n# \u5065\u5eb7\u30b9\u30b3\u30a2 = X * true_coef + \u30ce\u30a4\u30ba\ny = X.dot(true_coef) + np.random.randn(n_samples) * 0.1\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# \u30c7\u30fc\u30bf\u3092\u8a13\u7df4\u30bb\u30c3\u30c8\u3068\u30c6\u30b9\u30c8\u30bb\u30c3\u30c8\u306b\u5206\u5272\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n# \u5171\u5206\u6563\u884c\u5217\u306e\u8a08\u7b97\u3068\u56fa\u6709\u5024\u5206\u89e3\u306e\u6f14\u793a\ncov_matrix = np.cov(X_train.T)\neigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n# \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u964d\u9806\u306b\u4e26\u3073\u66ff\u3048\nidx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\nprint(\"\u5171\u5206\u6563\u884c\u5217:\")\nprint(cov_matrix)\nprint(\"\\n\u56fa\u6709\u5024:\")\nprint(eigenvalues)\nprint(\"\\n\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff08\u5217\uff09:\")\nprint(eigenvectors)\n\n# \u5404\u4e3b\u6210\u5206\u6570\u3067\u306e\u4e3b\u6210\u5206\u56de\u5e30\u306e\u6027\u80fd\u8a55\u4fa1\nn_components_range = range(1, X_scaled.shape[1] + 1)\npcr_results = []\npcr_r2 = []\nols_mse = None  # \u901a\u5e38\u306e\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306eMSE\nols_r2 = None   # \u901a\u5e38\u306e\u6700\u5c0f\u4e8c\u4e57\u6cd5\u306eR\u00b2\n\nplt.figure(figsize=(12, 10))\n\nfor i, n_components in enumerate(n_components_range):\n    # PCA\u306b\u3088\u308b\u6b21\u5143\u524a\u6e1b\n    pca = PCA(n_components=n_components)\n    X_train_pca = pca.fit_transform(X_train)\n    X_test_pca = pca.transform(X_test)\n\n    # \u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\n    lr = LinearRegression()\n    lr.fit(X_train_pca, y_train)\n\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u4e88\u6e2c\n    y_pred = lr.predict(X_test_pca)\n\n    # \u30e2\u30c7\u30eb\u8a55\u4fa1\uff08\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u3068R\u00b2\uff09\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    pcr_results.append(mse)\n    pcr_r2.append(r2)\n\n    # \u4e3b\u6210\u5206\u56de\u5e30\u306e\u7d50\u679c\u3092\u53ef\u8996\u5316\n    plt.subplot(2, 3, i+1)\n    plt.scatter(y_test, y_pred, alpha=0.7)\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n    plt.xlabel('\u5b9f\u6e2c\u5024')\n    plt.ylabel('\u4e88\u6e2c\u5024')\n    plt.title(f'\u4e3b\u6210\u5206\u6570: {n_components}, R\u00b2: {r2:.3f}')\n\n    # \u901a\u5e38\u306e\u6700\u5c0f\u4e8c\u4e57\u6cd5\uff08OLS\uff09\n    if i == 0:\n        lr_ols = LinearRegression()\n        lr_ols.fit(X_train, y_train)\n        y_pred_ols = lr_ols.predict(X_test)\n        ols_mse = mean_squared_error(y_test, y_pred_ols)\n        ols_r2 = r2_score(y_test, y_pred_ols)\n\nplt.tight_layout()\nplt.show()\n\n# MSE\u3068R\u00b2\u306e\u6bd4\u8f03\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(n_components_range, pcr_results, 'bo-')\nplt.axhline(y=ols_mse, color='r', linestyle='--', label=f'OLS: {ols_mse:.4f}')\nplt.xlabel('\u4e3b\u6210\u5206\u6570')\nplt.ylabel('\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee (MSE)')\nplt.title('\u4e3b\u6210\u5206\u56de\u5e30\u306e\u6027\u80fd\u8a55\u4fa1 (MSE)')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(n_components_range, pcr_r2, 'go-')\nplt.axhline(y=ols_r2, color='r', linestyle='--', label=f'OLS: {ols_r2:.4f}')\nplt.xlabel('\u4e3b\u6210\u5206\u6570')\nplt.ylabel('\u6c7a\u5b9a\u4fc2\u6570 (R\u00b2)')\nplt.title('\u4e3b\u6210\u5206\u56de\u5e30\u306e\u6027\u80fd\u8a55\u4fa1 (R\u00b2)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# \u6700\u9069\u306a\u4e3b\u6210\u5206\u6570\u3067\u306e\u4e3b\u6210\u5206\u56de\u5e30\u30e2\u30c7\u30eb\nbest_n_components = np.argmin(pcr_results) + 1\nprint(f\"\\n\u6700\u9069\u306a\u4e3b\u6210\u5206\u6570: {best_n_components}\")\n\n# \u6700\u9069\u306a\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\npca_best = PCA(n_components=best_n_components)\nX_train_pca_best = pca_best.fit_transform(X_train)\nX_test_pca_best = pca_best.transform(X_test)\n\nlr_best = LinearRegression()\nlr_best.fit(X_train_pca_best, y_train)\n\n# \u5404\u4e3b\u6210\u5206\u306e\u91cd\u8981\u5ea6\uff08\u56de\u5e30\u4fc2\u6570\uff09\nprint(\"\\n\u4e3b\u6210\u5206\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u4fc2\u6570\uff08\u4e3b\u6210\u5206\u7a7a\u9593\uff09:\")\nprint(lr_best.coef_)\n\n# \u5143\u306e\u5909\u6570\u7a7a\u9593\u3067\u306e\u4fc2\u6570\u306e\u8a08\u7b97\n# \u03b2 = E\u2096 \u00d7 \u03b3\uff08\u5143\u306e\u7a7a\u9593\u3067\u306e\u4fc2\u6570 = \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u884c\u5217 \u00d7 \u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u4fc2\u6570\uff09\noriginal_coef = pca_best.components_.T.dot(lr_best.coef_)\nprint(\"\\n\u5143\u306e\u5909\u6570\u7a7a\u9593\u3067\u306e\u4fc2\u6570\uff08\u5909\u63db\u5f8c\uff09:\")\nprint(original_coef)\nprint(\"\\n\u771f\u306e\u4fc2\u6570:\")\nprint(true_coef)\n\n# \u4e3b\u6210\u5206\u56de\u5e30\u306e\u6570\u5b66\u7684\u89e3\u91c8\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 8))\n# \u7b2c1\u4e3b\u6210\u5206\u3068\u7b2c2\u4e3b\u6210\u5206\u306e\u307f\u3067\u53ef\u8996\u5316\nif n_features &gt;= 2:\n    # \u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u6563\u5e03\u56f3\n    plt.scatter(X_train_pca_best[:, 0], X_train_pca_best[:, 1] if best_n_components &gt; 1 else np.zeros_like(X_train_pca_best[:, 0]), \n                c=y_train, cmap='viridis', alpha=0.7)\n\n    # \u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u56de\u5e30\u5e73\u9762\u306e\u53ef\u8996\u5316\n    if best_n_components &gt; 1:\n        x_range = np.linspace(X_train_pca_best[:, 0].min(), X_train_pca_best[:, 0].max(), 50)\n        y_range = np.linspace(X_train_pca_best[:, 1].min(), X_train_pca_best[:, 1].max(), 50)\n        X_grid, Y_grid = np.meshgrid(x_range, y_range)\n        Z_grid = lr_best.intercept_ + lr_best.coef_[0] * X_grid + lr_best.coef_[1] * Y_grid\n\n        plt.contourf(X_grid, Y_grid, Z_grid, alpha=0.2, cmap='viridis')\n\n    plt.colorbar(label='\u5065\u5eb7\u30b9\u30b3\u30a2')\n    plt.xlabel('\u7b2c1\u4e3b\u6210\u5206')\n    plt.ylabel('\u7b2c2\u4e3b\u6210\u5206' if best_n_components &gt; 1 else '')\n    plt.title('\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u304a\u3051\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u53ef\u8996\u5316')\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"lectures/LA/42-principal-component-analysis/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/42-principal-component-analysis/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li>\u3042\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u56fa\u6709\u5024\u304c \\(\\lambda = [4.2, 1.8, 0.6, 0.3, 0.1]\\) \u3067\u3042\u308b\u3068\u304d\u3001\u4ee5\u4e0b\u3092\u8a08\u7b97\u305b\u3088\uff1a</li> </ol> <p>a) \u5404\u4e3b\u6210\u5206\u306e\u5bc4\u4e0e\u7387\uff08\u6570\u5f0f\u3068\u8a08\u7b97\u904e\u7a0b\u3092\u793a\u3059\u3053\u3068\uff09</p> <p>b) \u7d2f\u7a4d\u5bc4\u4e0e\u7387\uff08\u6570\u5f0f\u3068\u8a08\u7b97\u904e\u7a0b\u3092\u793a\u3059\u3053\u3068\uff09</p> <p>c) 80%\u306e\u5206\u6563\u3092\u8aac\u660e\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u4e3b\u6210\u5206\u6570\uff08\u8a08\u7b97\u904e\u7a0b\u3092\u793a\u3059\u3053\u3068\uff09</p> <ol> <li> <p>\u6b21\u306e\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u884c\u5217\u3092\u7dda\u5f62\u4ee3\u6570\u306e\u89b3\u70b9\u304b\u3089\u89e3\u91c8\u305b\u3088\uff1a    <pre><code>        PC1    PC2\n\u8eab\u9577    0.85   0.10\n\u4f53\u91cd    0.90   0.05\nBMI     0.80   0.15\n\u8840\u5727    0.20   0.75\n\u5fc3\u62cd\u6570  0.15   0.80\n</code></pre>    \u7279\u306b\u3001\u5404\u4e3b\u6210\u5206\u304c\u3069\u306e\u3088\u3046\u306a\u610f\u5473\u3092\u6301\u3064\u304b\u3001\u307e\u305f\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u306b\u3064\u3044\u3066\u8003\u5bdf\u305b\u3088\u3002</p> </li> <li> <p>\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3092\u4ee5\u4e0b\u306e\u89b3\u70b9\u304b\u3089\u8aac\u660e\u305b\u3088\uff1a</p> </li> </ol> <p>a) \u7dda\u5f62\u4ee3\u6570\u7684\u306a\u5b9a\u7fa9\uff08\u6570\u5f0f\u3067\u8868\u73fe\uff09</p> <p>b) \u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u3068\u306e\u9055\u3044\uff08\u6570\u5b66\u7684\u306b\u8aac\u660e\uff09</p> <p>c) \u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3\u3092\u7528\u3044\u305f\u52b9\u7387\u7684\u306a\u8a08\u7b97\u65b9\u6cd5</p> <p>d) \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u304a\u3051\u308b\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u7d71\u8a08\u7684\u6027\u8cea</p> <p>e) \u5916\u308c\u5024\u691c\u51fa\u3078\u306e\u5fdc\u7528\u65b9\u6cd5\uff08\u7406\u8ad6\u7684\u6839\u62e0\u3092\u542b\u3080\uff09</p> <ol> <li>\u4e3b\u6210\u5206\u56de\u5e30\u306b\u3064\u3044\u3066\u4ee5\u4e0b\u306e\u554f\u3044\u306b\u7b54\u3048\u3088\uff1a</li> </ol> <p>a) \u4e3b\u6210\u5206\u56de\u5e30\u306e\u6570\u5b66\u7684\u5b9a\u5f0f\u5316\uff08\u884c\u5217\u8868\u8a18\u3092\u7528\u3044\u3066\uff09</p> <p>b) \\(k\\)\u500b\u306e\u4e3b\u6210\u5206\u3092\u7528\u3044\u305f\u5834\u5408\u306e\u5143\u306e\u8aac\u660e\u5909\u6570\u306b\u5bfe\u3059\u308b\u56de\u5e30\u4fc2\u6570\u306e\u5c0e\u51fa</p> <p>c) \u4e3b\u6210\u5206\u56de\u5e30\u304c\u591a\u91cd\u5171\u7dda\u6027\u554f\u984c\u3092\u89e3\u6c7a\u3067\u304d\u308b\u7406\u8ad6\u7684\u6839\u62e0</p> <p>d) \u901a\u5e38\u306e\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3068\u6bd4\u8f03\u3057\u305f\u5834\u5408\u306e\u7406\u8ad6\u7684\u306a\u7279\u6027\uff08\u30d0\u30a4\u30a2\u30b9\u3068\u30d0\u30ea\u30a2\u30f3\u30b9\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u306e\u89b3\u70b9\u304b\u3089\uff09</p>"},{"location":"lectures/LA/42-principal-component-analysis/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<ol> <li>\u3042\u308b\u5065\u5eb7\u8abf\u67fb\u30c7\u30fc\u30bf\u306b\u306f10\u500b\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\uff08\u8840\u5727\u3001\u5fc3\u62cd\u6570\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u306a\u3069\uff09\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u3002\u3053\u308c\u3089\u306e\u5909\u6570\u9593\u306b\u306f\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u3002\u3053\u306e\u30c7\u30fc\u30bf\u306b\u4e3b\u6210\u5206\u5206\u6790\u3092\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u6700\u521d\u306e3\u3064\u306e\u4e3b\u6210\u5206\u3067\u5168\u5206\u6563\u306e85%\u304c\u8aac\u660e\u3055\u308c\u305f\u3002\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3042\u308b\uff1a</li> </ol> <pre><code>           PC1    PC2    PC3\n\u8840\u5727        0.82   0.15  -0.10\n\u5fc3\u62cd\u6570      0.78   0.20  -0.15\n\u7dcf\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb  0.25   0.75   0.10\nLDL\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb 0.20   0.80   0.15\nHDL\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb -0.10  -0.25   0.85\n\u4e2d\u6027\u8102\u80aa    0.30   0.70   0.20\n\u7a7a\u8179\u6642\u8840\u7cd6  0.75  -0.15   0.30\nBMI         0.80  -0.10   0.15\n\u30a6\u30a8\u30b9\u30c8\u5468\u56f2\u5f84   0.85  -0.05   0.10\n\u4f53\u8102\u80aa\u7387    0.75  -0.10   0.20\n</code></pre> <p>a) \u5404\u4e3b\u6210\u5206\u306e\u610f\u5473\u3092\u89e3\u91c8\u305b\u3088\uff08\u7dda\u5f62\u4ee3\u6570\u7684\u6839\u62e0\u3092\u793a\u3057\u306a\u304c\u3089\uff09</p> <p>b) \u3053\u306e\u7d50\u679c\u304b\u3089\u5f97\u3089\u308c\u308b\u5065\u5eb7\u72b6\u614b\u306e\u4e3b\u8981\u306a\u56e0\u5b50\u306b\u3064\u3044\u3066\u8003\u5bdf\u305b\u3088</p> <p>c) \u3053\u306e\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b\u306b\u3088\u308b\u81e8\u5e8a\u7684\u306a\u5229\u70b9\u306b\u3064\u3044\u3066\u8ad6\u3058\u3088</p> <p>d) \u6b21\u5143\u524a\u6e1b\u5f8c\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u60a3\u8005\u3092\u30b0\u30eb\u30fc\u30d7\u5316\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u6848\u305b\u3088</p> <ol> <li>\u7cd6\u5c3f\u75c5\u30ea\u30b9\u30af\u4e88\u6e2c\u306e\u305f\u3081\u306e\u4e3b\u6210\u5206\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u8003\u3048\u308b\u3002\u8aac\u660e\u5909\u6570\u306b\u306f\u3001BMI\u3001\u7a7a\u8179\u6642\u8840\u7cd6\u5024\u3001HbA1c\u3001\u5e74\u9f62\u3001\u8840\u5727\u306a\u306915\u306e\u5909\u6570\u304c\u3042\u308a\u3001\u5f37\u3044\u591a\u91cd\u5171\u7dda\u6027\u304c\u7591\u308f\u308c\u308b\u3002\u4ee5\u4e0b\u306e\u624b\u9806\u306b\u5f93\u3063\u3066\u5206\u6790\u3092\u884c\u3044\u3001\u5404\u30b9\u30c6\u30c3\u30d7\u306e\u6570\u5b66\u7684\u6839\u62e0\u3092\u8a73\u7d30\u306b\u8aac\u660e\u305b\u3088\uff1a</li> </ol> <p>a) \u4e3b\u6210\u5206\u5206\u6790\u306e\u7406\u8ad6\u7684\u80cc\u666f\u3068\u3001\u306a\u305c\u591a\u91cd\u5171\u7dda\u6027\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3067\u304d\u308b\u304b\u3092\u8aac\u660e</p> <p>b) \u6700\u9069\u306a\u4e3b\u6210\u5206\u6570\u3092\u6c7a\u5b9a\u3059\u308b\u305f\u3081\u306e\u7d71\u8a08\u7684\u624b\u6cd5\u30923\u3064\u6319\u3052\u3001\u305d\u306e\u6570\u7406\u7684\u57fa\u790e    \u3092\u8aac\u660e</p> <p>c) \u4e3b\u6210\u5206\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u63a8\u5b9a\u91cf\u306e\u7d71\u8a08\u7684\u6027\u8cea\uff08\u30d0\u30a4\u30a2\u30b9\u3001\u5206\u6563\u3001\u4e00\u8cab\u6027\u306a\u3069\uff09\u3092\u5c0e\u51fa</p> <p>d) \u4e3b\u6210\u5206\u56de\u5e30\u3068\u4ed6\u306e\u6b63\u5247\u5316\u624b\u6cd5\uff08\u30ea\u30c3\u30b8\u56de\u5e30\u3001Lasso\uff09\u3068\u306e\u7406\u8ad6\u7684\u95a2\u4fc2\u3092\u8aac\u660e</p> <p>e) \u4ee5\u4e0b\u306ePython\u30b3\u30fc\u30c9\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3092\u5b8c\u6210\u3055\u305b\u3001\u4e3b\u6210\u5206\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u305b\u3088\uff1a</p> <pre><code>def principal_component_regression(X, y, n_components=None, test_size=0.3):\n    \"\"\"\n    \u4e3b\u6210\u5206\u56de\u5e30\u3092\u5b9f\u88c5\u3059\u308b\u95a2\u6570\n\n    \u30d1\u30e9\u30e1\u30fc\u30bf:\n    X: \u8aac\u660e\u5909\u6570\u306e\u884c\u5217\n    y: \u76ee\u7684\u5909\u6570\u306e\u30d9\u30af\u30c8\u30eb\n    n_components: \u4f7f\u7528\u3059\u308b\u4e3b\u6210\u5206\u306e\u6570\uff08None\u306e\u5834\u5408\u306f\u6700\u9069\u306a\u6570\u3092\u81ea\u52d5\u9078\u629e\uff09\n    test_size: \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u5272\u5408\n\n    \u623b\u308a\u5024:\n    best_model: \u6700\u9069\u306a\u4e3b\u6210\u5206\u56de\u5e30\u30e2\u30c7\u30eb\n    original_coef: \u5143\u306e\u5909\u6570\u7a7a\u9593\u3067\u306e\u56de\u5e30\u4fc2\u6570\n    best_n_components: \u9078\u629e\u3055\u308c\u305f\u4e3b\u6210\u5206\u306e\u6570\n    \"\"\"\n    # \u3053\u3053\u306b\u30b3\u30fc\u30c9\u3092\u8a18\u8ff0\n    pass\n</code></pre> <ol> <li>\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u6570\u7406\u7684\u57fa\u790e\u3068\u89e3\u91c8\u3092\u4ee5\u4e0b\u306e\u89b3\u70b9\u304b\u3089\u8a73\u7d30\u306b\u8aac\u660e\u305b\u3088\uff1a</li> </ol> <p>a) \u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u884c\u5217\u8868\u73fe\uff08\u30c7\u30fc\u30bf\u70b9\u3068\u5909\u6570\u30d9\u30af\u30c8\u30eb\u306e\u4e21\u65b9\uff09</p> <p>b) \u5909\u6570\u30d9\u30af\u30c8\u30eb\u306e\u65b9\u5411\u3068\u9577\u3055\u304c\u3069\u306e\u3088\u3046\u306a\u7dda\u5f62\u4ee3\u6570\u7684\u610f\u5473\u3092\u6301\u3064\u304b</p> <p>c) \u5909\u6570\u30d9\u30af\u30c8\u30eb\u9593\u306e\u89d2\u5ea6\u3068\u5909\u6570\u9593\u306e\u76f8\u95a2\u4fc2\u6570\u306e\u95a2\u4fc2\uff08\u6570\u5b66\u7684\u8a3c\u660e\u3092\u542b\u3080\uff09</p> <p>d) \u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u5206\u5e03\u30d1\u30bf\u30fc\u30f3\u304b\u3089\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u304a\u3051\u308b\u69cb\u9020\u3092\u3069\u306e\u3088\u3046\u306b\u8aad\u307f\u53d6\u308b\u304b</p> <p>e) \u591a\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u8996\u899a\u5316\u306b\u304a\u3051\u308b\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u9650\u754c\u3068\u3001\u305d\u306e\u554f\u984c\u3092\u514b\u670d\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5</p>"},{"location":"lectures/LA/42-principal-component-analysis/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":"<p>Q1: \u4e3b\u6210\u5206\u6570\u306f\u3069\u306e\u3088\u3046\u306b\u6c7a\u5b9a\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f</p> <p>A1: \u4e3b\u6210\u5206\u6570\u306e\u6c7a\u5b9a\u306b\u306f\u4e3b\u306b\u4ee5\u4e0b\u306e3\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u3001\u305d\u308c\u305e\u308c\u306b\u6570\u5b66\u7684\u6839\u62e0\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li> <p>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306e\u300c\u8098\u300d\u306e\u4f4d\u7f6e\uff1a\u56fa\u6709\u5024\u306e\u6e1b\u5c11\u7387 \\((\\lambda_j - \\lambda_{j+1})/\\lambda_j\\) \u304c\u6025\u6fc0\u306b\u5c0f\u3055\u304f\u306a\u308b\u70b9\u3092\u7279\u5b9a\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u8ffd\u52a0\u306e\u4e3b\u6210\u5206\u304c\u30c7\u30fc\u30bf\u306e\u69cb\u9020\u3092\u307b\u3068\u3093\u3069\u8aac\u660e\u3057\u306a\u304f\u306a\u308b\u70b9\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u70b9\u306f\u3001\u56fa\u6709\u5024\u306e\u30b0\u30e9\u30d5\u304c\u300c\u8098\u300d\u306e\u3088\u3046\u306a\u5f62\u72b6\u3092\u793a\u3059\u4f4d\u7f6e\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u30ab\u30a4\u30b6\u30fc\u57fa\u6e96\uff08\u56fa\u6709\u5024&gt;1\uff09\uff1a\u76f8\u95a2\u884c\u5217\u3092\u7528\u3044\u305f\u5834\u5408\u3001\u56fa\u6709\u5024\u304c1\u672a\u6e80\u306e\u4e3b\u6210\u5206\u306f\u5143\u306e\u5909\u65701\u3064\u5206\u3088\u308a\u3082\u60c5\u5831\u91cf\u304c\u5c11\u306a\u3044\u305f\u3081\u9664\u5916\u3057\u307e\u3059\u3002\u6570\u5b66\u7684\u306b\u306f\u3001\u6a19\u6e96\u5316\u3055\u308c\u305f\u5909\u6570\u306e\u5206\u6563\u306f1\u3067\u3042\u308b\u305f\u3081\u3001\u56fa\u6709\u5024\u304c1\u3088\u308a\u5927\u304d\u3044\u4e3b\u6210\u5206\u306f\u3001\u5c11\u306a\u304f\u3068\u30821\u3064\u306e\u5909\u6570\u5206\u306e\u60c5\u5831\u3092\u6301\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306b\u3088\u308b\u95be\u5024\uff1a\\(\\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{p} \\lambda_i} \\geq \\alpha\\) \u3068\u306a\u308b\u6700\u5c0f\u306e \\(k\\) \u3092\u9078\u629e\u3057\u307e\u3059\uff08\\(\\alpha\\) \u306f\u901a\u5e380.8\u301c0.9\uff09\u3002\u3053\u308c\u306f\u3001\u9078\u629e\u3055\u308c\u305f\u4e3b\u6210\u5206\u304c\u5168\u4f53\u306e\u5206\u6563\u306e\u5c11\u306a\u304f\u3068\u3082 \\(\\alpha \\times 100\\%\\) \u3092\u8aac\u660e\u3059\u308b\u3053\u3068\u3092\u4fdd\u8a3c\u3057\u307e\u3059\u3002</p> </li> </ol> <p>\u3053\u308c\u3089\u306e\u65b9\u6cd5\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3001\u5206\u6790\u306e\u76ee\u7684\u3084\u8a08\u7b97\u30ea\u30bd\u30fc\u30b9\u3092\u8003\u616e\u3057\u3066\u6c7a\u5b9a\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u4ea4\u5dee\u691c\u8a3c\u3084\u60c5\u5831\u91cf\u57fa\u6e96\uff08AIC\u3001BIC\uff09\u306a\u3069\u306e\u7d71\u8a08\u7684\u624b\u6cd5\u3082\u5229\u7528\u3067\u304d\u307e\u3059\u3002</p> <p>Q2: \u4e3b\u6210\u5206\u306e\u89e3\u91c8\u304c\u96e3\u3057\u3044\u5834\u5408\u306f\u3069\u3046\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f</p> <p>A2: \u4e3b\u6210\u5206\u306e\u89e3\u91c8\u304c\u96e3\u3057\u3044\u5834\u5408\u306f\u4ee5\u4e0b\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u6709\u52b9\u3067\u3059\uff1a</p> <ol> <li> <p>\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u306e\u69cb\u9020\u7684\u30d1\u30bf\u30fc\u30f3\uff1a\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u884c\u5217 \\(\\mathbf{L} = \\mathbf{E}\\boldsymbol{\\Lambda}^{1/2}\\) \u3092\u8a73\u7d30\u306b\u5206\u6790\u3057\u3001\u5404\u4e3b\u6210\u5206\u306b\u5bfe\u3057\u3066\u6700\u3082\u9ad8\u3044\uff08\u7d76\u5bfe\u5024\u306e\uff09\u8ca0\u8377\u91cf\u3092\u6301\u3064\u5909\u6570\u306e\u30b0\u30eb\u30fc\u30d7\u3092\u7279\u5b9a\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u6f5c\u5728\u7684\u306a\u56e0\u5b50\u69cb\u9020\u3092\u793a\u5506\u3057\u3066\u3044\u307e\u3059\u3002</p> </li> <li> <p>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306a\u3069\u306e\u56de\u8ee2\u624b\u6cd5\uff1a\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u884c\u5217 \\(\\mathbf{E}\\) \u306b\u76f4\u4ea4\u56de\u8ee2\u884c\u5217 \\(\\mathbf{R}\\) \u3092\u9069\u7528\u3057\u3001\u89e3\u91c8\u3057\u3084\u3059\u3044\u69cb\u9020\u3092\u5f97\u307e\u3059\uff1a\\(\\mathbf{E}' = \\mathbf{E}\\mathbf{R}\\)\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5404\u4e3b\u6210\u5206\u304c\u5c11\u6570\u306e\u5909\u6570\u3068\u5f37\u304f\u95a2\u9023\u3059\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\uff1a\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u3067\u306f\u3001\u5909\u6570\u30d9\u30af\u30c8\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u30d1\u30bf\u30fc\u30f3\u304c\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u3092\u8996\u899a\u5316\u3057\u307e\u3059\u3002\u540c\u3058\u65b9\u5411\u3092\u5411\u3044\u305f\u30d9\u30af\u30c8\u30eb\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u95a2\u9023\u3059\u308b\u5909\u6570\u30b0\u30eb\u30fc\u30d7\u3092\u793a\u5506\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u30c9\u30e1\u30a4\u30f3\u77e5\u8b58\u306e\u6d3b\u7528\uff1a\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u5834\u5408\u3001\u7b2c1\u4e3b\u6210\u5206\u304c\u300c\u5168\u4f53\u7684\u306a\u4ee3\u8b1d\u5065\u5eb7\u300d\u3092\u8868\u3057\u3001\u7b2c2\u4e3b\u6210\u5206\u304c\u300c\u5fc3\u8840\u7ba1\u7cfb\u306e\u5065\u5eb7\u72b6\u614b\u300d\u3092\u8868\u3059\u3068\u3044\u3046\u3088\u3046\u306b\u3001\u5c02\u9580\u77e5\u8b58\u306b\u57fa\u3065\u3044\u3066\u89e3\u91c8\u3092\u884c\u3044\u307e\u3059\u3002</p> </li> </ol> <p>\u307e\u305f\u3001\u89e3\u91c8\u304c\u96e3\u3057\u3044\u5834\u5408\u306f\u3001\u300c\u56e0\u5b50\u5206\u6790\u300d\u306a\u3069\u3001\u6f5c\u5728\u5909\u6570\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u3092\u91cd\u8996\u3057\u305f\u4ed6\u306e\u624b\u6cd5\u3092\u691c\u8a0e\u3059\u308b\u3053\u3068\u3082\u6709\u52b9\u3067\u3059\u3002</p> <p>Q3: \u4e3b\u6210\u5206\u5206\u6790\u306f\u8cea\u7684\u5909\u6570\u306b\u3082\u9069\u7528\u3067\u304d\u307e\u3059\u304b\uff1f</p> <p>A3: \u4e3b\u6210\u5206\u5206\u6790\u306f\u57fa\u672c\u7684\u306b\u91cf\u7684\u5909\u6570\uff08\u9023\u7d9a\u5909\u6570\uff09\u306e\u305f\u3081\u306b\u8a2d\u8a08\u3055\u308c\u3066\u304a\u308a\u3001\u305d\u306e\u6570\u5b66\u7684\u57fa\u790e\u306f\u5171\u5206\u6563\u884c\u5217\u307e\u305f\u306f\u76f8\u95a2\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002\u8cea\u7684\u5909\u6570\uff08\u30ab\u30c6\u30b4\u30ea\u5909\u6570\uff09\u3092\u542b\u3080\u5834\u5408\u306f\u3001\u4ee5\u4e0b\u306e\u7406\u8ad6\u7684\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u691c\u8a0e\u3057\u3066\u304f\u3060\u3055\u3044\uff1a</p> <ol> <li> <p>\u4e8c\u5024\u5909\u6570\u306e\u5834\u5408\uff1a\u4e8c\u5024\u5909\u6570\uff080/1\uff09\u306f\u3001\u30c6\u30c8\u30e9\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u4fc2\u6570\u3084\u70b9\u4e8c\u5217\u76f8\u95a2\u4fc2\u6570\u3092\u7528\u3044\u3066\u76f8\u95a2\u884c\u5217\u3092\u69cb\u7bc9\u3057\u3001\u305d\u306e\u5f8c\u3067PCA\u3092\u9069\u7528\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u591a\u5024\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u5834\u5408\uff1a</p> </li> <li>\u591a\u91cd\u5bfe\u5fdc\u5206\u6790\uff08MCA\uff09\uff1a\u8cea\u7684\u5909\u6570\u5c02\u7528\u306e\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u3067\u3001\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u3092\u30c0\u30df\u30fc\u5909\u6570\u5316\u3057\u3001\u7279\u6b8a\u306a\u6a19\u6e96\u5316\u3092\u9069\u7528\u3057\u305f\u5f8c\u306b\u5bfe\u5fdc\u5206\u6790\uff08Correspondence Analysis\uff09\u3092\u884c\u3044\u307e\u3059\u3002</li> <li> <p>\u6700\u9069\u5c3a\u5ea6\u6cd5\uff08Optimal Scaling\uff09\uff1a\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u3092\u6570\u5024\u5909\u6570\u306b\u5909\u63db\u3057\u3001\u305d\u306e\u5f8cPCA\u3092\u9069\u7528\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002</p> </li> <li> <p>\u6df7\u5408\u30c7\u30fc\u30bf\u306e\u5834\u5408\uff1a</p> </li> <li>\u56e0\u5b50\u5206\u6790\u6df7\u5408\u30c7\u30fc\u30bf\u6cd5\uff08FAMD\uff09\uff1a\u91cf\u7684\u5909\u6570\u3068\u8cea\u7684\u5909\u6570\u306e\u4e21\u65b9\u3092\u540c\u6642\u306b\u6271\u3048\u308b\u624b\u6cd5\u3067\u3001\u91cf\u7684\u5909\u6570\u306b\u306f\u6a19\u6e96\u5316\u3092\u3001\u8cea\u7684\u5909\u6570\u306b\u306f\u591a\u91cd\u5bfe\u5fdc\u5206\u6790\u306e\u539f\u7406\u3092\u9069\u7528\u3057\u307e\u3059\u3002</li> <li>\u975e\u7dda\u5f62PCA\uff1a\u6700\u9069\u5c3a\u5ea6\u6cd5\u3092\u7528\u3044\u3066\u8cea\u7684\u5909\u6570\u3092\u91cf\u7684\u5909\u6570\u306b\u5909\u63db\u3057\u3001\u901a\u5e38\u306ePCA\u3092\u9069\u7528\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002</li> </ol> <p>\u3053\u308c\u3089\u306e\u65b9\u6cd5\u306f\u3044\u305a\u308c\u3082\u3001\u901a\u5e38\u306ePCA\u306e\u7dda\u5f62\u4ee3\u6570\u7684\u67a0\u7d44\u307f\u3092\u62e1\u5f35\u307e\u305f\u306f\u4fee\u6b63\u3057\u305f\u3082\u306e\u3067\u3042\u308a\u3001\u9069\u5207\u306a\u5909\u63db\u3068\u6a19\u6e96\u5316\u3092\u901a\u3058\u3066\u8cea\u7684\u5909\u6570\u3092\u6271\u3048\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>Q4: \u7570\u306a\u308b\u30b9\u30b1\u30fc\u30eb\u306e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u3001PCA\u3092\u9069\u7528\u3059\u308b\u524d\u306b\u6a19\u6e96\u5316\u3059\u3079\u304d\u3067\u3059\u304b\uff1f</p> <p>A4: \u7570\u306a\u308b\u30b9\u30b1\u30fc\u30eb\u306e\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u306f\u901a\u5e38\u3001PCA\u3092\u9069\u7528\u3059\u308b\u524d\u306b\u6a19\u6e96\u5316\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u7684\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u7406\u7531\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li> <p>\u5171\u5206\u6563\u884c\u5217vs\u76f8\u95a2\u884c\u5217\uff1a\u6a19\u6e96\u5316\u3057\u306a\u3044\u5834\u5408\u3001PCA\u306f\u5171\u5206\u6563\u884c\u5217 \\(\\boldsymbol{\\Sigma} = \\frac{1}{n}\\mathbf{X}_c^T\\mathbf{X}_c\\) \u306e\u56fa\u6709\u5024\u5206\u89e3\u306b\u57fa\u3065\u304d\u307e\u3059\u3002\u6a19\u6e96\u5316\u3059\u308b\u3068\u3001PCA\u306f\u76f8\u95a2\u884c\u5217 \\(\\mathbf{R} = \\mathbf{D}^{-1/2}\\boldsymbol{\\Sigma}\\mathbf{D}^{-1/2}\\) \u306e\u56fa\u6709\u5024\u5206\u89e3\u306b\u57fa\u3065\u304d\u307e\u3059\uff08\\(\\mathbf{D}\\) \u306f\u5171\u5206\u6563\u884c\u5217\u306e\u5bfe\u89d2\u6210\u5206\u3092\u5bfe\u89d2\u6210\u5206\u3068\u3059\u308b\u5bfe\u89d2\u884c\u5217\uff09\u3002</p> </li> <li> <p>\u5909\u6570\u306e\u91cd\u307f\u4ed8\u3051\uff1a\u6a19\u6e96\u5316\u3057\u306a\u3044\u5834\u5408\u3001\u5927\u304d\u306a\u5206\u6563\u3092\u6301\u3064\u5909\u6570\u304c\u4e3b\u6210\u5206\u306b\u4e0d\u91e3\u308a\u5408\u3044\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u307e\u3059\u3002\u6570\u5b66\u7684\u306b\u306f\u3001\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3001\u5206\u6563\u306e\u5927\u304d\u306a\u5909\u6570\u306b\u652f\u914d\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u30b9\u30b1\u30fc\u30eb\u4e0d\u5909\u6027\uff1a\u6a19\u6e96\u5316\u306b\u3088\u3063\u3066\u3001PCA\u306e\u7d50\u679c\u306f\u5909\u6570\u306e\u6e2c\u5b9a\u5358\u4f4d\u306b\u4f9d\u5b58\u3057\u306a\u304f\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u5909\u6570\u306e\u76f8\u5bfe\u7684\u306a\u91cd\u8981\u6027\u3092\u53cd\u6620\u3059\u308b\u3088\u3046\u306b\u306a\u308b\u305f\u3081\u3067\u3059\u3002</p> </li> </ol> <p>\u305f\u3060\u3057\u3001\u4ee5\u4e0b\u306e\u5834\u5408\u306f\u6a19\u6e96\u5316\u3092\u884c\u308f\u306a\u3044\u3053\u3068\u3082\u8003\u616e\u3067\u304d\u307e\u3059\uff1a - \u3059\u3079\u3066\u306e\u5909\u6570\u304c\u540c\u3058\u5358\u4f4d\u3067\u6e2c\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408 - \u5909\u6570\u9593\u306e\u7d76\u5bfe\u7684\u306a\u5206\u6563\u306e\u9055\u3044\u304c\u91cd\u8981\u306a\u60c5\u5831\u3067\u3042\u308b\u5834\u5408 - \u7269\u7406\u7684\u306a\u610f\u5473\u306e\u3042\u308b\u4e3b\u6210\u5206\u3092\u62bd\u51fa\u3057\u305f\u3044\u5834\u5408\uff08\u4f8b\uff1a\u5f62\u72b6\u5206\u6790\uff09</p> <p>\u6c7a\u5b9a\u306f\u3001\u30c7\u30fc\u30bf\u306e\u6027\u8cea\u3068\u5206\u6790\u306e\u76ee\u7684\u306b\u57fa\u3065\u3044\u3066\u884c\u3046\u3079\u304d\u3067\u3059\u3002</p> <p>Q5: \u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3068\u4e3b\u6210\u5206\u5206\u6790\u306f\u3069\u306e\u3088\u3046\u306b\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u304b\uff1f</p> <p>A5: \u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3068\u4e3b\u6210\u5206\u5206\u6790\u306f\u7dda\u5f62\u4ee3\u6570\u7684\u306b\u5bc6\u63a5\u306b\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li> <p>\u5171\u901a\u306e\u6570\u5b66\u7684\u57fa\u790e\uff1a\u4e21\u65b9\u3068\u3082\u5171\u5206\u6563\u884c\u5217 \\(\\boldsymbol{\\Sigma}\\) \u306e\u69cb\u9020\u3092\u5229\u7528\u3057\u3066\u3044\u307e\u3059\u3002PCA\u306f\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3 \\(\\boldsymbol{\\Sigma} = \\mathbf{E}\\boldsymbol{\\Lambda}\\mathbf{E}^T\\) \u306b\u57fa\u3065\u304d\u3001\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306f\u305d\u306e\u9006\u884c\u5217 \\(\\boldsymbol{\\Sigma}^{-1}\\) \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u7c21\u7565\u5316\uff1a\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3\u3092\u7528\u3044\u308b\u3068\u3001\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u4e8c\u4e57\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u73fe\u3067\u304d\u307e\u3059\uff1a</p> </li> </ol> <p>\\(D^2(\\mathbf{x}) = (\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\) \\(= (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{E} \\boldsymbol{\\Lambda}^{-1} \\mathbf{E}^T (\\mathbf{x} - \\boldsymbol{\\mu})\\) \\(= \\mathbf{z}^T \\boldsymbol{\\Lambda}^{-1} \\mathbf{z} = \\sum_{j=1}^{p} \\frac{z_j^2}{\\lambda_j}\\)</p> <p>\u3053\u3053\u3067\u3001\\(\\mathbf{z} = \\mathbf{E}^T(\\mathbf{x} - \\boldsymbol{\\mu})\\) \u306f\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u3067\u3059\u3002</p> <ol> <li> <p>\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\uff1aPCA\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u304c\u6700\u5927\u3068\u306a\u308b\u65b9\u5411\uff08\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff09\u3092\u7279\u5b9a\u3057\u3001\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306f\u3001\u305d\u306e\u5206\u6563\u306e\u5927\u304d\u3055\uff08\u56fa\u6709\u5024\uff09\u3067\u6a19\u6e96\u5316\u3055\u308c\u305f\u8ddd\u96e2\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306f\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u6a19\u6e96\u5316\u3055\u308c\u305f\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u3068\u3057\u3066\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u6b21\u5143\u524a\u6e1b\u3068\u306e\u95a2\u4fc2\uff1a\u5b9f\u969b\u306e\u5fdc\u7528\u3067\u306f\u3001\u5c0f\u3055\u306a\u56fa\u6709\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u6b21\u5143\u3092\u7121\u8996\u3059\u308b\u3053\u3068\u3067\u3001\u5b89\u5b9a\u3057\u305f\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u306e\u8a08\u7b97\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u3001PCA\u306b\u3088\u308b\u6b21\u5143\u524a\u6e1b\u306e\u539f\u7406\u3068\u4e00\u81f4\u3057\u3066\u3044\u307e\u3059\u3002</p> </li> </ol> <p>\u4e3b\u6210\u5206\u5206\u6790\u5f8c\u306e\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u8a08\u7b97\u306f\u7279\u306b\u5916\u308c\u5024\u691c\u51fa\u306b\u6709\u7528\u3067\u3001\u7d71\u8a08\u7684\u306b\u6b63\u78ba\u306a\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306e\u7570\u5e38\u691c\u77e5\u3092\u53ef\u80fd\u306b\u3057\u307e\u3059\u3002\u7406\u8ad6\u7684\u306b\u306f\u3001\u3053\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u7b49\u78ba\u7387\u66f2\u9762\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II","text":""},{"location":"lectures/LA/43-principal-component-analysis/#43","title":"\u7b2c43\u56de\u8b1b\u7fa9\uff1a\u7279\u7570\u5024\u5206\u89e3\u30fb\u4e3b\u6210\u5206\u5206\u6790\u3067\u306e\u5fdc\u7528\u4f8b","text":""},{"location":"lectures/LA/43-principal-component-analysis/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<ul> <li>\u8b1b\u7fa9\u56de: \u7b2c43\u56de</li> <li>\u65e5\u4ed8: 2025\u5e743\u670814\u65e5</li> <li>\u95a2\u9023\u9805\u76ee: \u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u3001\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3001\u5fdc\u7528\u4f8b</li> <li>\u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u7b2c39\u56de\u301c\u7b2c42\u56de\u306e\u5185\u5bb9\uff08\u7279\u7570\u5024\u5206\u89e3\u306e\u57fa\u790e\u3001\u5fdc\u7528\u3001\u4e3b\u6210\u5206\u5206\u6790\u306e\u5c0e\u5165\u3001\u6b21\u5143\u524a\u6e1b\uff09</li> </ul>"},{"location":"lectures/LA/43-principal-component-analysis/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u3068\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u5fa9\u7fd2\u3068\u7406\u89e3\u306e\u6df1\u5316</li> <li>\u533b\u7642\u753b\u50cf\u51e6\u7406\u306b\u304a\u3051\u308b\u7279\u7570\u5024\u5206\u89e3\u3068\u4e3b\u6210\u5206\u5206\u6790\u306e\u5fdc\u7528\u3092\u7406\u89e3\u3059\u308b</li> <li>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u89e3\u6790\u306b\u304a\u3051\u308b\u6b21\u5143\u524a\u6e1b\u306e\u5fdc\u7528\u624b\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u306e\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u4e3b\u6210\u5206\u5206\u6790\u306e\u6d3b\u7528\u65b9\u6cd5\u3092\u5b66\u3076</li> <li>\u5b9f\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u7279\u7570\u5024\u5206\u89e3\u3068\u4e3b\u6210\u5206\u5206\u6790\u306e\u5b9f\u88c5\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> </ol>"},{"location":"lectures/LA/43-principal-component-analysis/#3","title":"3. \u57fa\u672c\u6982\u5ff5\u306e\u5fa9\u7fd2","text":""},{"location":"lectures/LA/43-principal-component-analysis/#31-svd","title":"3.1 \u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u306e\u5fa9\u7fd2","text":"<p>\u5b9a\u7fa9: \u4efb\u610f\u306e \\(m \\times n\\) \u884c\u5217 \\(A\\) \u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5206\u89e3\u3067\u304d\u308b\uff1a</p> <p>\\(A = U\\Sigma V^T\\)</p> <p>\u3053\u3053\u3067\u3001 - \\(U\\) \u306f \\(m \\times m\\) \u306e\u76f4\u4ea4\u884c\u5217\uff08\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff09 - \\(\\Sigma\\) \u306f \\(m \\times n\\) \u306e\u5bfe\u89d2\u884c\u5217\uff08\u7279\u7570\u5024\u3092\u5bfe\u89d2\u6210\u5206\u306b\u6301\u3064\uff09 - \\(V\\) \u306f \\(n \\times n\\) \u306e\u76f4\u4ea4\u884c\u5217\uff08\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff09</p> <p>\u7279\u7570\u5024\u5206\u89e3\u306e\u4e3b\u306a\u6027\u8cea\uff1a</p> <ul> <li>\u7279\u7570\u5024 \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r &gt; 0\\) \uff08\\(r\\) \u306f\u30e9\u30f3\u30af\uff09</li> <li>\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(U\\) \u306e\u5217\u306f \\(AA^T\\) \u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb</li> <li>\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(V\\) \u306e\u5217\u306f \\(A^TA\\) \u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb</li> <li>\u7279\u7570\u5024 \\(\\sigma_i\\) \u306f \\(\\sqrt{\\lambda_i}\\) \uff08\\(\\lambda_i\\) \u306f \\(A^TA\\) \u306e\u56fa\u6709\u5024\uff09</li> </ul>"},{"location":"lectures/LA/43-principal-component-analysis/#32-pca","title":"3.2 \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u5fa9\u7fd2","text":"<p>\u5b9a\u7fa9: \u30c7\u30fc\u30bf\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u7528\u3044\u3066\u3001\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u3092\u524a\u6e1b\u3059\u308b\u624b\u6cd5\u3002</p> <p>\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u306b\u5bfe\u3057\u3066\uff1a 1. \u30c7\u30fc\u30bf\u3092\u4e2d\u5fc3\u5316\uff1a \\(\\tilde{X} = X - \\bar{X}\\) 2. \u5206\u6563\u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\uff1a \\(S = \\frac{1}{n-1}\\tilde{X}^T\\tilde{X}\\) 3. \\(S\\) \u306e\u56fa\u6709\u5024\u554f\u984c\u3092\u89e3\u304f\uff1a \\(S\\mathbf{v}_i = \\lambda_i\\mathbf{v}_i\\) 4. \u56fa\u6709\u5024\u306e\u5927\u304d\u3044\u9806\u306b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u9078\u629e\u3057\u3001\u4e3b\u6210\u5206\u3092\u69cb\u6210</p> <p>\u4e3b\u6210\u5206\u5206\u6790\u306e\u4e3b\u306a\u6027\u8cea\uff1a</p> <ul> <li>\u7b2c\u4e00\u4e3b\u6210\u5206\u306f\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u65b9\u5411</li> <li>\u4e3b\u6210\u5206\u9593\u306f\u4e92\u3044\u306b\u76f4\u4ea4</li> <li>\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u306f\u5143\u306e\u30c7\u30fc\u30bf\u3092\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u6295\u5f71\u3057\u305f\u3082\u306e</li> <li>\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306f\u8aac\u660e\u3055\u308c\u305f\u5206\u6563\u306e\u5272\u5408\u3092\u793a\u3059</li> </ul>"},{"location":"lectures/LA/43-principal-component-analysis/#33","title":"3.3 \u7279\u7570\u5024\u5206\u89e3\u3068\u4e3b\u6210\u5206\u5206\u6790\u306e\u95a2\u4fc2","text":"<p>PCA\u3068SVD\u306e\u95a2\u4fc2\u6027\uff1a</p> <ul> <li>\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u3092\u4e2d\u5fc3\u5316\u3057\u305f\u884c\u5217 \\(\\tilde{X}\\) \u306b\u5bfe\u3057\u3066 SVD \u3092\u9069\u7528\u3059\u308b\u3068\uff1a   \\(\\tilde{X} = U\\Sigma V^T\\)</li> <li>\u3053\u306e\u6642\u3001\\(V\\) \u306e\u5217\u30d9\u30af\u30c8\u30eb\u306f PCA \u306e\u4e3b\u6210\u5206\u65b9\u5411\u3068\u540c\u3058</li> <li>\\(\\Sigma^2/(n-1)\\) \u306e\u5bfe\u89d2\u6210\u5206\u306f\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u306b\u5bfe\u5fdc</li> <li>\\(U\\Sigma\\) \u306f\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u306b\u6bd4\u4f8b\u3059\u308b</li> </ul>"},{"location":"lectures/LA/43-principal-component-analysis/#4","title":"4. \u7406\u8ad6\u3068\u5fdc\u7528\u4e8b\u4f8b","text":""},{"location":"lectures/LA/43-principal-component-analysis/#41","title":"4.1 \u533b\u7642\u753b\u50cf\u51e6\u7406\u306b\u304a\u3051\u308b\u5fdc\u7528","text":""},{"location":"lectures/LA/43-principal-component-analysis/#411","title":"4.1.1 \u753b\u50cf\u5727\u7e2e\u3068\u518d\u69cb\u6210","text":"<p>\u533b\u7642\u753b\u50cf\u306f\u30b5\u30a4\u30ba\u304c\u5927\u304d\u304f\u3001\u52b9\u7387\u7684\u306a\u4fdd\u5b58\u3068\u8ee2\u9001\u304c\u8ab2\u984c\u3068\u306a\u308a\u307e\u3059\u3002SVD\u3092\u7528\u3044\u3066\u753b\u50cf\u3092\u5727\u7e2e\u3059\u308b\u3053\u3068\u3067\u3001\u60c5\u5831\u306e\u640d\u5931\u3092\u6700\u5c0f\u9650\u306b\u6291\u3048\u306a\u304c\u3089\u52b9\u7387\u7684\u306a\u30c7\u30fc\u30bf\u7ba1\u7406\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u6570\u5b66\u7684\u30a2\u30d7\u30ed\u30fc\u30c1\uff1a 1. \u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u753b\u50cf\u3092\u884c\u5217 \\(A\\) \u3068\u3057\u3066\u8868\u73fe 2. SVD \u306b\u3088\u308a \\(A = U\\Sigma V^T\\) \u3068\u5206\u89e3 3. \u4e0a\u4f4d \\(k\\) \u500b\u306e\u7279\u7570\u5024\u3068\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\u7279\u7570\u30d9\u30af\u30c8\u30eb\u306e\u307f\u3092\u7528\u3044\u3066\u753b\u50cf\u3092\u8fd1\u4f3c\uff1a    \\(A_k = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T\\)</p> <p>\u30e9\u30f3\u30af \\(k\\) \u306e\u8fd1\u4f3c\u8aa4\u5dee\uff1a \\(\\|A - A_k\\|_F^2 = \\sum_{i=k+1}^{r} \\sigma_i^2\\)</p> <p>MRI\u753b\u50cf\u306e\u5834\u5408\u3001\u901a\u5e3810\u301c20%\u306e\u7279\u7570\u5024\u3092\u4fdd\u6301\u3059\u308b\u3060\u3051\u3067\u3001\u8996\u899a\u7684\u306b\u8a31\u5bb9\u3067\u304d\u308b\u753b\u8cea\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#412","title":"4.1.2 \u533b\u7642\u753b\u50cf\u306e\u30ce\u30a4\u30ba\u9664\u53bb","text":"<p>SVD\u306f\u533b\u7642\u753b\u50cf\u306e\u30ce\u30a4\u30ba\u9664\u53bb\u306b\u3082\u5fdc\u7528\u3067\u304d\u307e\u3059\uff1a</p> <ol> <li>\u30ce\u30a4\u30ba\u3092\u542b\u3080\u753b\u50cf\u3092\u884c\u5217 \\(A_{noisy}\\) \u3068\u3057\u3066\u8868\u73fe</li> <li>SVD\u306b\u3088\u308a\u5206\u89e3\uff1a \\(A_{noisy} = U\\Sigma V^T\\)</li> <li>\u5c0f\u3055\u306a\u7279\u7570\u5024\uff08\u30ce\u30a4\u30ba\u306b\u5bfe\u5fdc\uff09\u3092\u9664\u53bb\u3057\u3066\u518d\u69cb\u6210\uff1a    \\(A_{denoised} = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T\\)</li> </ol> <p>\u3053\u306e\u624b\u6cd5\u306fX\u7dda\u753b\u50cf\u3084CT\u753b\u50cf\u306a\u3069\u306e\u30ce\u30a4\u30ba\u9664\u53bb\u306b\u6709\u52b9\u3067\u3001\u8a3a\u65ad\u7cbe\u5ea6\u306e\u5411\u4e0a\u306b\u8ca2\u732e\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#413","title":"4.1.3 \u753b\u50cf\u7279\u5fb4\u62bd\u51fa\u3068\u5206\u985e","text":"<p>PCA\u306f\u533b\u7642\u753b\u50cf\u304b\u3089\u306e\u7279\u5fb4\u62bd\u51fa\u306b\u3082\u5229\u7528\u3055\u308c\u307e\u3059\uff1a</p> <ol> <li>\u8907\u6570\u306e\u533b\u7642\u753b\u50cf\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3057\u3001\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u3092\u69cb\u6210</li> <li>PCA\u3092\u9069\u7528\u3057\u3066\u4e3b\u6210\u5206\u3092\u62bd\u51fa</li> <li>\u5c11\u6570\u306e\u4e3b\u6210\u5206\u3067\u753b\u50cf\u3092\u8868\u73fe\u3057\u3001\u5206\u985e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u5165\u529b</li> </ol> <p>\u3053\u306e\u624b\u6cd5\u306f\u816b\u760d\u306e\u826f\u6027/\u60aa\u6027\u5206\u985e\u3084\u75c5\u5909\u691c\u51fa\u306a\u3069\u306b\u5fdc\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#42","title":"4.2 \u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u89e3\u6790\u306b\u304a\u3051\u308b\u5fdc\u7528","text":""},{"location":"lectures/LA/43-principal-component-analysis/#421","title":"4.2.1 \u9ad8\u6b21\u5143\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b","text":"<p>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306f\u5178\u578b\u7684\u306b\u300c\u5c11\u6570\u306e\u30b5\u30f3\u30d7\u30eb\uff08\u60a3\u8005\uff09\u00d7 \u591a\u6570\u306e\u907a\u4f1d\u5b50\u300d\u3068\u3044\u3046\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3067\u3059\uff1a</p> <ol> <li>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \uff08\u884c\uff1a\u30b5\u30f3\u30d7\u30eb\u3001\u5217\uff1a\u907a\u4f1d\u5b50\uff09</li> <li>PCA\u3092\u9069\u7528\u3057\u3066\u6b21\u5143\u524a\u6e1b</li> <li>\u4e0a\u4f4d\u306e\u4e3b\u6210\u5206\u306e\u307f\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u3092\u8868\u73fe</li> </ol> <p>\u3053\u306e\u65b9\u6cd5\u306b\u3088\u308a\u3001\u6570\u4e07\u306e\u907a\u4f1d\u5b50\u306b\u5bfe\u3059\u308b\u767a\u73fe\u91cf\u304b\u3089\u3001\u6570\u5341\u301c\u6570\u767e\u306e\u7279\u5fb4\u306b\u6b21\u5143\u524a\u6e1b\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#422","title":"4.2.2 \u764c\u30b5\u30d6\u30bf\u30a4\u30d7\u306e\u5206\u985e","text":"<p>PCA\u3067\u6b21\u5143\u524a\u6e1b\u3057\u305f\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306f\u3001\u764c\u30b5\u30d6\u30bf\u30a4\u30d7\u306e\u5206\u985e\u306b\u6709\u52b9\u3067\u3059\uff1a</p> <ol> <li>\u764c\u60a3\u8005\u306e\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066PCA\u3092\u9069\u7528</li> <li>\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306b\u3088\u308a\u3001\u30b5\u30d6\u30bf\u30a4\u30d7\u3092\u540c\u5b9a</li> <li>\u5404\u4e3b\u6210\u5206\u306b\u5bc4\u4e0e\u3059\u308b\u907a\u4f1d\u5b50\u7fa4\u304b\u3089\u3001\u30b5\u30d6\u30bf\u30a4\u30d7\u306e\u751f\u7269\u5b66\u7684\u7279\u5fb4\u3092\u63a8\u5b9a</li> </ol> <p>\u5b9f\u4f8b\uff1a\u4e73\u764c\u306eLuminal A, Luminal B, HER2, Basal-like\u306a\u3069\u306e\u30b5\u30d6\u30bf\u30a4\u30d7\u5206\u985e</p>"},{"location":"lectures/LA/43-principal-component-analysis/#423","title":"4.2.3 \u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u540c\u5b9a","text":"<p>PCA\u306e\u8ca0\u8377\u91cf\uff08Loading\uff09\u306f\u3001\u5404\u4e3b\u6210\u5206\u306b\u5bfe\u3059\u308b\u5909\u6570\u306e\u91cd\u8981\u5ea6\u3092\u793a\u3057\u307e\u3059\uff1a</p> <ol> <li>\u75be\u60a3\u95a2\u9023\u306e\u4e3b\u6210\u5206\u3092\u540c\u5b9a</li> <li>\u305d\u306e\u4e3b\u6210\u5206\u306b\u5927\u304d\u306a\u8ca0\u8377\u91cf\u3092\u6301\u3064\u907a\u4f1d\u5b50\u3092\u7279\u5b9a</li> <li>\u3053\u308c\u3089\u306e\u907a\u4f1d\u5b50\u304c\u75be\u60a3\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u5019\u88dc\u3068\u306a\u308b</li> </ol> <p>\u3053\u306e\u624b\u6cd5\u306b\u3088\u308a\u3001\u81a8\u5927\u306a\u907a\u4f1d\u5b50\u60c5\u5831\u304b\u3089\u75be\u60a3\u306e\u8a3a\u65ad\u3084\u4e88\u5f8c\u4e88\u6e2c\u306b\u6709\u7528\u306a\u907a\u4f1d\u5b50\u3092\u540c\u5b9a\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#43_1","title":"4.3 \u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u306e\u30c7\u30fc\u30bf\u5206\u6790","text":""},{"location":"lectures/LA/43-principal-component-analysis/#431","title":"4.3.1 \u6d3b\u52d5\u30d1\u30bf\u30fc\u30f3\u306e\u62bd\u51fa","text":"<p>\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u306f\u52a0\u901f\u5ea6\u3001\u5fc3\u62cd\u6570\u3001\u6e29\u5ea6\u306a\u3069\u591a\u69d8\u306a\u30bb\u30f3\u30b5\u30fc\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3057\u307e\u3059\uff1a</p> <ol> <li>\u6642\u7cfb\u5217\u306e\u591a\u6b21\u5143\u30bb\u30f3\u30b5\u30fc\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u3092\u69cb\u6210</li> <li>PCA\u3092\u9069\u7528\u3057\u3066\u4e3b\u8981\u306a\u6d3b\u52d5\u30d1\u30bf\u30fc\u30f3\u3092\u62bd\u51fa</li> <li>\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u306b\u57fa\u3065\u3044\u3066\u6d3b\u52d5\u306e\u5206\u985e\u3084\u7570\u5e38\u691c\u51fa\u3092\u5b9f\u65bd</li> </ol> <p>\u4f8b\uff1a\u6b69\u884c\u3001\u8d70\u884c\u3001\u7740\u5ea7\u3001\u7761\u7720\u306a\u3069\u306e\u6d3b\u52d5\u30d1\u30bf\u30fc\u30f3\u306e\u81ea\u52d5\u5206\u985e</p>"},{"location":"lectures/LA/43-principal-component-analysis/#432","title":"4.3.2 \u7761\u7720\u5206\u6790\u306b\u304a\u3051\u308b\u5fdc\u7528","text":"<p>\u7761\u7720\u4e2d\u306e\u30bb\u30f3\u30b5\u30fc\u30c7\u30fc\u30bf\u306bPCA\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u7761\u7720\u306e\u8cea\u3084\u7761\u7720\u6bb5\u968e\u3092\u8a55\u4fa1\u3067\u304d\u307e\u3059\uff1a</p> <ol> <li>\u7761\u7720\u4e2d\u306e\u5fc3\u62cd\u5909\u52d5\u3001\u4f53\u52d5\u3001\u547c\u5438\u306a\u3069\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066PCA\u3092\u9069\u7528</li> <li>\u62bd\u51fa\u3055\u308c\u305f\u4e3b\u6210\u5206\u304b\u3089\u7761\u7720\u6bb5\u968e\uff08REM\u3001\u6df1\u7761\u7720\u306a\u3069\uff09\u3092\u63a8\u5b9a</li> <li>\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u304b\u3089\u7761\u7720\u969c\u5bb3\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u691c\u51fa</li> </ol>"},{"location":"lectures/LA/43-principal-component-analysis/#433","title":"4.3.3 \u751f\u4f53\u4fe1\u53f7\u306e\u7570\u5e38\u691c\u51fa","text":"<p>\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u304b\u3089\u53ce\u96c6\u3055\u308c\u305f\u751f\u4f53\u4fe1\u53f7\u306e\u7570\u5e38\u691c\u51fa\u306b\u3082SVD\u3068PCA\u306f\u6709\u7528\u3067\u3059\uff1a</p> <ol> <li>\u901a\u5e38\u306e\u751f\u4f53\u4fe1\u53f7\u30d1\u30bf\u30fc\u30f3\u304b\u3089\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9</li> <li>\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u30c7\u30fc\u30bf\u3092\u30e2\u30c7\u30eb\u7a7a\u9593\u306b\u6295\u5f71\u3057\u3001\u518d\u69cb\u6210\u8aa4\u5dee\u3092\u8a08\u7b97</li> <li>\u518d\u69cb\u6210\u8aa4\u5dee\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u306b\u7570\u5e38\u3068\u5224\u5b9a</li> </ol> <p>\u3053\u306e\u624b\u6cd5\u306f\u5fc3\u81d3\u4e0d\u6574\u8108\u306e\u691c\u51fa\u3084\u767a\u4f5c\u4e88\u6e2c\u306a\u3069\u306b\u5fdc\u7528\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/43-principal-component-analysis/#51","title":"5.1 \u533b\u7642\u753b\u50cf\u306e\u5727\u7e2e\u3068\u518d\u69cb\u6210","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_sample_images\nfrom scipy import misc\nfrom skimage import color, transform\nfrom numpy.linalg import svd\n\n# \u30b5\u30f3\u30d7\u30eb\u533b\u7642\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u307f\uff08\u5b9f\u969b\u306b\u306f\u533b\u7642\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\uff09\n# \u3053\u3053\u3067\u306f\u30b5\u30f3\u30d7\u30eb\u753b\u50cf\u3092\u4f7f\u7528\nsample_image = misc.face(gray=True)\n# \u753b\u50cf\u30b5\u30a4\u30ba\u3092\u5c0f\u3055\u304f\u3057\u3066SVD\u8a08\u7b97\u3092\u9ad8\u901f\u5316\nimage = transform.resize(sample_image, (256, 256))\n\n# SVD\u3092\u9069\u7528\nU, sigma, Vt = svd(image, full_matrices=False)\n\n# \u7570\u306a\u308b\u30e9\u30f3\u30af\u3067\u306e\u8fd1\u4f3c\u3092\u53ef\u8996\u5316\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nranks = [5, 10, 20, 50, 100]\n\n# \u5143\u306e\u753b\u50cf\naxes[0, 0].imshow(image, cmap='gray')\naxes[0, 0].set_title('\u5143\u306e\u753b\u50cf')\naxes[0, 0].axis('off')\n\n# \u7570\u306a\u308b\u30e9\u30f3\u30af\u3067\u306e\u518d\u69cb\u6210\nfor i, r in enumerate(ranks):\n    # \u30e9\u30f3\u30afr\u3067\u306e\u8fd1\u4f3c\n    reconstructed = U[:, :r] @ np.diag(sigma[:r]) @ Vt[:r, :]\n\n    # \u5727\u7e2e\u7387\u3092\u8a08\u7b97\n    original_size = image.shape[0] * image.shape[1]\n    compressed_size = r * (image.shape[0] + image.shape[1] + 1)\n    compression_ratio = 100 * (1 - compressed_size / original_size)\n\n    # \u7d50\u679c\u306e\u8868\u793a\n    ax = axes[(i+1)//3, (i+1)%3]\n    ax.imshow(reconstructed, cmap='gray')\n    ax.set_title(f'\u30e9\u30f3\u30af {r}\\n\u5727\u7e2e\u7387: {compression_ratio:.1f}%')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# \u7279\u7570\u5024\u306e\u6e1b\u8870\u3092\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.plot(sigma[:100], 'o-')\nplt.title('\u4e0a\u4f4d100\u500b\u306e\u7279\u7570\u5024')\nplt.xlabel('\u30a4\u30f3\u30c7\u30c3\u30af\u30b9')\nplt.ylabel('\u7279\u7570\u5024')\nplt.grid(True)\nplt.show()\n\n# \u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306e\u8a08\u7b97\u3068\u53ef\u8996\u5316\ntotal_variance = np.sum(sigma**2)\ncumulative_variance_ratio = np.cumsum(sigma**2) / total_variance\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_variance_ratio[:100], 'o-')\nplt.axhline(y=0.9, color='r', linestyle='--', label='90% \u306e\u60c5\u5831')\nplt.title('\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.xlabel('\u7279\u7570\u5024\u306e\u6570')\nplt.ylabel('\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.grid(True)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/43-principal-component-analysis/#52","title":"5.2 \u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# \u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\nnp.random.seed(42)\nn_samples = 100  # \u60a3\u8005\u6570\nn_genes = 1000    # \u907a\u4f1d\u5b50\u6570\n\n# 3\u3064\u306e\u7570\u306a\u308b\u30b5\u30d6\u30bf\u30a4\u30d7\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\nn_per_class = n_samples // 3\n\n# \u57fa\u672c\u767a\u73fe\u91cf\nexpression_base = np.random.normal(0, 1, (n_samples, n_genes))\n\n# \u30b5\u30d6\u30bf\u30a4\u30d7\u56fa\u6709\u306e\u30b7\u30b0\u30ca\u30eb\u3092\u8ffd\u52a0\nfor i in range(n_genes):\n    if i &lt; 100:  # \u30b5\u30d6\u30bf\u30a4\u30d71\u306b\u95a2\u9023\u3059\u308b\u907a\u4f1d\u5b50\n        expression_base[:n_per_class, i] += np.random.normal(3, 0.5, n_per_class)\n    elif i &lt; 200:  # \u30b5\u30d6\u30bf\u30a4\u30d72\u306b\u95a2\u9023\u3059\u308b\u907a\u4f1d\u5b50\n        expression_base[n_per_class:2*n_per_class, i] += np.random.normal(3, 0.5, n_per_class)\n    elif i &lt; 300:  # \u30b5\u30d6\u30bf\u30a4\u30d73\u306b\u95a2\u9023\u3059\u308b\u907a\u4f1d\u5b50\n        expression_base[2*n_per_class:, i] += np.random.normal(3, 0.5, n_per_class)\n\n# \u30b5\u30d6\u30bf\u30a4\u30d7\u306e\u30e9\u30d9\u30eb\nsubtypes = np.array(['\u30b5\u30d6\u30bf\u30a4\u30d71'] * n_per_class + \n                    ['\u30b5\u30d6\u30bf\u30a4\u30d72'] * n_per_class + \n                    ['\u30b5\u30d6\u30bf\u30a4\u30d73'] * n_per_class)\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u5909\u63db\ngene_names = [f'Gene_{i}' for i in range(n_genes)]\nsample_names = [f'Sample_{i}' for i in range(n_samples)]\nexpression_df = pd.DataFrame(expression_base, index=sample_names, columns=gene_names)\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\nscaler = StandardScaler()\nscaled_expression = scaler.fit_transform(expression_df)\n\n# PCA\u306e\u9069\u7528\npca = PCA()\npca_results = pca.fit_transform(scaled_expression)\n\n# \u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\nplt.plot(cumulative_variance_ratio[:20], 'o-')\nplt.axhline(y=0.5, color='r', linestyle='--', label='50% \u306e\u60c5\u5831')\nplt.title('\u7279\u7570\u5024\u306e\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.xlabel('\u4e3b\u6210\u5206\u306e\u6570')\nplt.ylabel('\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# \u4e0a\u4f4d2\u3064\u306e\u4e3b\u6210\u5206\u306b\u3088\u308b\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 8))\nfor subtype in np.unique(subtypes):\n    mask = subtypes == subtype\n    plt.scatter(\n        pca_results[mask, 0], \n        pca_results[mask, 1],\n        label=subtype,\n        alpha=0.7\n    )\n\nplt.title('\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790')\nplt.xlabel(f'\u7b2c1\u4e3b\u6210\u5206 ({explained_variance_ratio[0]:.2%})')\nplt.ylabel(f'\u7b2c2\u4e3b\u6210\u5206 ({explained_variance_ratio[1]:.2%})')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# \u91cd\u8981\u306a\u907a\u4f1d\u5b50\uff08\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u5019\u88dc\uff09\u306e\u540c\u5b9a\nloadings = pca.components_\nimportant_genes = []\n\nfor i in range(2):  # \u4e0a\u4f4d2\u3064\u306e\u4e3b\u6210\u5206\u306b\u5bfe\u3057\u3066\n    # \u7d76\u5bfe\u5024\u304c\u5927\u304d\u3044\u4e0a\u4f4d10\u500b\u306e\u907a\u4f1d\u5b50\u3092\u7279\u5b9a\n    pc_loadings = loadings[i]\n    top_indices = np.abs(pc_loadings).argsort()[-10:][::-1]\n    for idx in top_indices:\n        important_genes.append({\n            'Gene': gene_names[idx],\n            'Principal Component': i+1,\n            'Loading': pc_loadings[idx]\n        })\n\nimportant_genes_df = pd.DataFrame(important_genes)\nprint(\"\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u5019\u88dc\u907a\u4f1d\u5b50:\")\nprint(important_genes_df)\n\n# \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u306b\u3088\u308b\u4e0a\u4f4d\u907a\u4f1d\u5b50\u306e\u767a\u73fe\u30d1\u30bf\u30fc\u30f3\u306e\u53ef\u8996\u5316\ntop_genes = [g['Gene'] for g in important_genes[:20]]\nexpression_subset = expression_df[top_genes]\n\nplt.figure(figsize=(12, 10))\nsns.clustermap(\n    expression_subset,\n    cmap='viridis',\n    row_colors=pd.Series(subtypes).map({'\u30b5\u30d6\u30bf\u30a4\u30d71': 'red', '\u30b5\u30d6\u30bf\u30a4\u30d72': 'blue', '\u30b5\u30d6\u30bf\u30a4\u30d73': 'green'}),\n    z_score=1,  # \u884c\u65b9\u5411\u306b\u6a19\u6e96\u5316\n    figsize=(12, 10)\n)\nplt.title('\u4e3b\u8981\u907a\u4f1d\u5b50\u306e\u767a\u73fe\u30d1\u30bf\u30fc\u30f3')\nplt.show()\n</code></pre>"},{"location":"lectures/LA/43-principal-component-analysis/#53","title":"5.3 \u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u30c7\u30fc\u30bf\u306e\u5206\u6790","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n# \u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u30c7\u30fc\u30bf\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\nnp.random.seed(42)\nn_timestamps = 1000  # \u6642\u9593\u8ef8\u306e\u30c7\u30fc\u30bf\u70b9\u6570\nn_sensors = 6       # \u30bb\u30f3\u30b5\u30fc\u306e\u6570\uff08\u52a0\u901f\u5ea6x,y,z\u3001\u5fc3\u62cd\u6570\u3001\u6e29\u5ea6\u3001GSR\u7b49\uff09\n\n# 4\u3064\u306e\u6d3b\u52d5\uff08\u6b69\u884c\u3001\u8d70\u884c\u3001\u7740\u5ea7\u3001\u7761\u7720\uff09\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\nactivities = []\nsensor_data = []\n\n# \u6b69\u884c\nfor i in range(250):\n    # \u5468\u671f\u7684\u306a\u52a0\u901f\u5ea6\u30d1\u30bf\u30fc\u30f3\uff0b\u30ce\u30a4\u30ba\n    timestamp = i\n    acc_x = 0.5 * np.sin(i/10) + np.random.normal(0, 0.1)\n    acc_y = 0.3 * np.cos(i/10) + np.random.normal(0, 0.1)\n    acc_z = 1.5 + np.random.normal(0, 0.1)\n    heart_rate = 90 + np.random.normal(0, 5)\n    temperature = 36.5 + np.random.normal(0, 0.1)\n    gsr = 2 + np.random.normal(0, 0.2)\n\n    sensor_data.append([acc_x, acc_y, acc_z, heart_rate, temperature, gsr])\n    activities.append('\u6b69\u884c')\n\n# \u8d70\u884c\nfor i in range(250):\n    timestamp = i + 250\n    acc_x = 1.2 * np.sin(i/5) + np.random.normal(0, 0.2)\n    acc_y = 0.8 * np.cos(i/5) + np.random.normal(0, 0.2)\n    acc_z = 2.0 + np.random.normal(0, 0.3)\n    heart_rate = 140 + np.random.normal(0, 10)\n    temperature = 37.0 + np.random.normal(0, 0.2)\n    gsr = 4 + np.random.normal(0, 0.5)\n\n    sensor_data.append([acc_x, acc_y, acc_z, heart_rate, temperature, gsr])\n    activities.append('\u8d70\u884c')\n\n# \u7740\u5ea7\nfor i in range(250):\n    timestamp = i + 500\n    acc_x = 0.1 * np.random.normal(0, 0.05)\n    acc_y = 0.1 * np.random.normal(0, 0.05)\n    acc_z = 0.1 * np.random.normal(0, 0.05)\n    heart_rate = 70 + np.random.normal(0, 3)\n    temperature = 36.3 + np.random.normal(0, 0.1)\n    gsr = 1.5 + np.random.normal(0, 0.1)\n\n    sensor_data.append([acc_x, acc_y, acc_z, heart_rate, temperature, gsr])\n    activities.append('\u7740\u5ea7')\n\n# \u7761\u7720\nfor i in range(250):\n    timestamp = i + 750\n    acc_x = 0.05 * np.random.normal(0, 0.02)\n    acc_y = 0.05 * np.random.normal(0, 0.02)\n    acc_z = 0.05 * np.random.normal(0, 0.02)\n    heart_rate = 60 + np.random.normal(0, 2)\n    temperature = 36.0 + np.random.normal(0, 0.1)\n    gsr = 1.0 + np.random.normal(0, 0.1)\n\n    sensor_data.append([acc_x, acc_y, acc_z, heart_rate, temperature, gsr])\n    activities.append('\u7761\u7720')\n\n# DataFrame \u306b\u5909\u63db\nsensor_columns = ['\u52a0\u901f\u5ea6X', '\u52a0\u901f\u5ea6Y', '\u52a0\u901f\u5ea6Z', '\u5fc3\u62cd\u6570', '\u4f53\u6e29', '\u767a\u6c57\u91cf']\nwearable_df = pd.DataFrame(sensor_data, columns=sensor_columns)\nwearable_df['\u6d3b\u52d5'] = activities\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\nfeatures = wearable_df[sensor_columns]\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(features)\n\n# PCA\u306e\u9069\u7528\npca = PCA()\npca_results = pca.fit_transform(scaled_features)\n\n# \u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\nplt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'o-')\nplt.axhline(y=0.9, color='r', linestyle='--', label='90% \u306e\u60c5\u5831')\nplt.title('\u7279\u7570\u5024\u306e\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.xlabel('\u4e3b\u6210\u5206\u306e\u6570')\nplt.ylabel('\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# \u4e0a\u4f4d2\u3064\u306e\u4e3b\u6210\u5206\u306b\u3088\u308b\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(12, 8))\nactivity_colors = {'\u6b69\u884c': 'blue', '\u8d70\u884c': 'red', '\u7740\u5ea7': 'green', '\u7761\u7720': 'purple'}\n\nfor activity in np.unique(activities):\n    mask = wearable_df['\u6d3b\u52d5'] == activity\n    plt.scatter(\n        pca_results[mask, 0], \n        pca_results[mask, 1],\n        label=activity,\n        color=activity_colors[activity],\n        alpha=0.7\n    )\n\nplt.title('\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790')\nplt.xlabel(f'\u7b2c1\u4e3b\u6210\u5206 ({explained_variance_ratio[0]:.2%})')\nplt.ylabel(f'\u7b2c2\u4e3b\u6210\u5206 ({explained_variance_ratio[1]:.2%})')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# \u4e3b\u6210\u5206\u306e\u8ca0\u8377\u91cf\uff08Loading\uff09\u306e\u53ef\u8996\u5316\nloadings = pca.components_\nloading_df = pd.DataFrame(loadings.T, index=sensor_columns, columns=[f'PC{i+1}' for i in range(loadings.shape[0])])\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(loading_df.iloc[:, :3], annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('\u4e0a\u4f4d3\u4e3b\u6210\u5206\u306b\u5bfe\u3059\u308b\u30bb\u30f3\u30b5\u30fc\u306e\u5bc4\u4e0e\u5ea6')\nplt.show()\n\n# \u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u306e\u6642\u7cfb\u5217\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(14, 10))\nfor i in range(3):  # \u4e0a\u4f4d3\u4e3b\u6210\u5206\n    plt.subplot(3, 1, i+1)\n    for activity in np.unique(activities):\n        mask = wearable_df['\u6d3b\u52d5'] == activity\n        indices = np.where(mask)[0]\n        plt.plot(indices, pca_results[mask, i], label=activity, color=activity_colors[activity])\n\n    plt.title(f'\u7b2c{i+1}\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u306e\u6642\u9593\u7684\u5909\u5316')\n    plt.xlabel('\u6642\u9593')\n    plt.ylabel(f'PC{i+1}\u30b9\u30b3\u30a2')\n    plt.grid(True)\n    if i == 0:\n        plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# \u7570\u5e38\u691c\u51fa\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\uff08\u518d\u69cb\u6210\u8aa4\u5dee\u306b\u3088\u308b\u691c\u51fa\uff09\n# \u4e0a\u4f4dk\u500b\u306e\u4e3b\u6210\u5206\u3067\u518d\u69cb\u6210\nk = 3\nreduced_data = pca_results[:, :k]\nreconstructed_data = reduced_data @ pca.components_[:k, :]\nreconstruction_error = np.sum((scaled_features - reconstructed_data) ** 2, axis=1)\n\n# \u7570\u5e38\u3092\u691c\u51fa\u3059\u308b\u305f\u3081\u306e\u95be\u5024\nthreshold = np.percentile(reconstruction_error, 95)\n\nplt.figure(figsize=(14, 6))\nplt.plot(reconstruction_error, label='\u518d\u69cb\u6210\u8aa4\u5dee')\nplt.axhline(y=threshold, color='r', linestyle='--', label='\u7570\u5e38\u691c\u51fa\u95be\u5024\uff0895\u30d1\u30fc\u30bb\u30f3\u30bf\u30a4\u30eb\uff09')\nplt.title('\u518d\u69cb\u6210\u8aa4\u5dee\u306b\u3088\u308b\u7570\u5e38\u691c\u51fa')\nplt.xlabel('\u6642\u9593')\nplt.ylabel('\u518d\u69cb\u6210\u8aa4\u5dee')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# \u6d3b\u52d5\u3054\u3068\u306e\u518d\u69cb\u6210\u8aa4\u5dee\u306e\u7bb1\u3072\u3052\u56f3\nplt.figure(figsize=(10, 6))\nerror_by_activity = {activity: reconstruction_error[wearable_df['\u6d3b\u52d5'] == activity] for activity in np.unique(activities)}\nplt.boxplot([error_by_activity[act] for act in np.unique(activities)])\nplt.xticks(range(1, len(np.unique(activities))+1), np.unique(activities))\nplt.title('\u6d3b\u52d5\u3054\u3068\u306e\u518d\u69cb\u6210\u8aa4\u5dee\u5206\u5e03')\nplt.xlabel('\u6d3b\u52d5')\nplt.ylabel('\u518d\u69cb\u6210\u8aa4\u5dee')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/43-principal-component-analysis/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/43-principal-component-analysis/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li>\u533b\u7642\u753b\u50cf\uff08512\u00d7512 \u30d4\u30af\u30bb\u30eb\uff09\u306b\u5bfe\u3057\u3066 SVD \u3092\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7279\u7570\u5024\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6e1b\u8870\u3057\u307e\u3057\u305f\uff1a    \\(\\sigma_1 = 1000, \\sigma_2 = 500, \\sigma_3 = 250, \\sigma_4 = 125, \\ldots\\)</li> </ol> <p>\u4e0a\u4f4d10\u500b\u306e\u7279\u7570\u5024\u3067\u753b\u50cf\u3092\u518d\u69cb\u6210\u3057\u305f\u5834\u5408\u3001\u5143\u306e\u60c5\u5831\u306e\u3069\u308c\u304f\u3089\u3044\u306e\u5272\u5408\u304c\u4fdd\u6301\u3055\u308c\u307e\u3059\u304b\uff1f</p> <ol> <li> <p>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\uff081000\u30b5\u30f3\u30d7\u30eb\u00d720000\u907a\u4f1d\u5b50\uff09\u306b\u5bfe\u3057\u3066 PCA \u3092\u9069\u7528\u3057\u305f\u3068\u3053\u308d\u3001\u4e0a\u4f4d5\u4e3b\u6210\u5206\u304c\u5168\u5206\u6563\u306e65%\u3092\u8aac\u660e\u3057\u307e\u3057\u305f\u3002\u3053\u306e\u7d50\u679c\u304b\u3089\u3001\u30c7\u30fc\u30bf\u306e\u69cb\u9020\u306b\u3064\u3044\u3066\u4f55\u304c\u8a00\u3048\u307e\u3059\u304b\uff1f</p> </li> <li> <p>\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u304b\u3089\u5f97\u3089\u308c\u305f6\u6b21\u5143\u306e\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\uff08\u52a0\u901f\u5ea6 x,y,z\u3001\u5fc3\u62cd\u6570\u3001\u4f53\u6e29\u3001\u767a\u6c57\u91cf\uff09\u306b PCA \u3092\u9069\u7528\u3057\u305f\u7d50\u679c\u3001\u7b2c1\u4e3b\u6210\u5206\u3068\u7b2c2\u4e3b\u6210\u5206\u306b\u3088\u308b\u5e73\u9762\u4e0a\u306b4\u3064\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u304c\u5f62\u6210\u3055\u308c\u307e\u3057\u305f\u3002\u3053\u306e\u7d50\u679c\u306f\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3067\u304d\u307e\u3059\u304b\uff1f</p> </li> <li> <p>\u884c\u5217 \\(A = \\begin{pmatrix} 3 &amp; 1 \\\\ 2 &amp; 2 \\\\ 1 &amp; 3 \\end{pmatrix}\\) \u306b\u5bfe\u3057\u3066\u7279\u7570\u5024\u5206\u89e3\u3092\u884c\u3044\u3001\u30e9\u30f3\u30af1\u8fd1\u4f3c\u3092\u6c42\u3081\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u306b\u5bfe\u3057\u3066 SVD \u3068 PCA \u3092\u9069\u7528\u3057\u305f\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306a\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\u304b\uff1f\u4e21\u8005\u306e\u7d50\u679c\u3092\u3069\u306e\u3088\u3046\u306b\u95a2\u9023\u4ed8\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u304b\uff1f</p> </li> </ol>"},{"location":"lectures/LA/43-principal-component-analysis/#62","title":"6.2 \u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>\u533b\u7642\u753b\u50cf\u306e\u5727\u7e2e\u3068\u518d\u69cb\u6210\uff1aMRI\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u30891\u679a\u306e\u753b\u50cf\u3092\u9078\u3073\u3001SVD\u3092\u7528\u3044\u3066\u3055\u307e\u3056\u307e\u306a\u5727\u7e2e\u30ec\u30d9\u30eb\u3067\u518d\u69cb\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8996\u899a\u7684\u54c1\u8cea\u3068\u5727\u7e2e\u7387\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3092\u8003\u5bdf\u3057\u3001\u533b\u7642\u8a3a\u65ad\u306b\u9069\u3057\u305f\u5727\u7e2e\u30ec\u30d9\u30eb\u3092\u63d0\u6848\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u89e3\u6790\uff1a\u516c\u958b\u3055\u308c\u3066\u3044\u308b\u764c\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08\u4f8b\uff1aTCGA\uff09\u3092\u7528\u3044\u3066\u3001PCA\u3092\u9069\u7528\u3057\u3001\u764c\u30b5\u30d6\u30bf\u30a4\u30d7\u306e\u5206\u985e\u3092\u8a66\u307f\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u4e0b\u306e\u70b9\u3092\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\uff1a</p> </li> <li>\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306b\u57fa\u3065\u3044\u3066\u3001\u4f55\u500b\u306e\u4e3b\u6210\u5206\u3092\u4fdd\u6301\u3059\u3079\u304d\u304b</li> <li>\u7570\u306a\u308b\u30b5\u30d6\u30bf\u30a4\u30d7\u304c\u3069\u306e\u3088\u3046\u306b\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u5206\u96e2\u3055\u308c\u308b\u304b</li> <li> <p>\u3069\u306e\u907a\u4f1d\u5b50\u304c\u5404\u4e3b\u6210\u5206\u306b\u5927\u304d\u304f\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3001\u305d\u308c\u3089\u306e\u751f\u7269\u5b66\u7684\u610f\u5473\u306f\u4f55\u304b</p> </li> <li> <p>\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u30c7\u30fc\u30bf\u306e\u5206\u6790\uff1a\u5fc3\u62cd\u6570\u3001\u52a0\u901f\u5ea6\u3001\u547c\u5438\u306a\u3069\u306e\u30bb\u30f3\u30b5\u30fc\u30c7\u30fc\u30bf\u3092\u542b\u3080\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066PCA\u3092\u9069\u7528\u3057\u3001\u7761\u7720\u6bb5\u968e\uff08\u6d45\u3044\u7761\u7720\u3001\u6df1\u3044\u7761\u7720\u3001REM\u7761\u7720\u306a\u3069\uff09\u306e\u81ea\u52d5\u5206\u985e\u30b7\u30b9\u30c6\u30e0\u3092\u69cb\u7bc9\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u4e0b\u306e\u70b9\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\uff1a</p> </li> <li>\u3069\u306e\u4e3b\u6210\u5206\u304c\u7761\u7720\u6bb5\u968e\u306e\u8b58\u5225\u306b\u6700\u3082\u6709\u52b9\u304b</li> <li>\u5404\u4e3b\u6210\u5206\u306b\u5bfe\u3059\u308b\u5404\u30bb\u30f3\u30b5\u30fc\u306e\u5bc4\u4e0e\u5ea6</li> <li>\u63d0\u6848\u30b7\u30b9\u30c6\u30e0\u306e\u7cbe\u5ea6\u3068\u9650\u754c</li> <li> <p>\u5fdc\u7528\u306e\u53ef\u80fd\u6027\uff08\u7761\u7720\u969c\u5bb3\u8a3a\u65ad\u306a\u3069\uff09</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u7570\u5e38\u691c\u51fa\uff1a\u591a\u6b21\u5143\u306e\u5065\u5eb7\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u30c7\u30fc\u30bf\uff08\u8840\u5727\u3001\u8840\u7cd6\u5024\u3001\u4f53\u91cd\u3001\u5fc3\u62cd\u6570\u306a\u3069\uff09\u306b\u5bfe\u3057\u3066PCA\u3092\u9069\u7528\u3057\u3001SVD\u3092\u7528\u3044\u305f\u7570\u5e38\u691c\u51fa\u30b7\u30b9\u30c6\u30e0\u3092\u8a2d\u8a08\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u4e0b\u306e\u70b9\u3092\u5b9f\u88c5\u30fb\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\uff1a</p> </li> <li>\u6b63\u5e38\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u304f\u90e8\u5206\u7a7a\u9593\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9</li> <li>\u518d\u69cb\u6210\u8aa4\u5dee\u306b\u57fa\u3065\u304f\u7570\u5e38\u691c\u77e5\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0</li> <li>ROC\u66f2\u7dda\u306b\u3088\u308b\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u8a55\u4fa1</li> <li> <p>\u3069\u306e\u3088\u3046\u306a\u5065\u5eb7\u7570\u5e38\u304c\u691c\u51fa\u53ef\u80fd\u304b\u3001\u3069\u306e\u3088\u3046\u306a\u9650\u754c\u304c\u3042\u308b\u304b</p> </li> <li> <p>\u8907\u6570\u306e\u533b\u7642\u30e2\u30c0\u30ea\u30c6\u30a3\u30c7\u30fc\u30bf\u306e\u7d71\u5408\uff1aMRI\u753b\u50cf\u3001\u81e8\u5e8a\u691c\u67fb\u5024\u3001\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306a\u3069\u3001\u7570\u306a\u308b\u7a2e\u985e\u306e\u533b\u7642\u30c7\u30fc\u30bf\u3092\u6301\u3064\u60a3\u8005\u96c6\u56e3\u306b\u5bfe\u3057\u3066\u3001SVD\u3068PCA\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u7d71\u5408\u3068\u6b21\u5143\u524a\u6e1b\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u63d0\u6848\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u4e0b\u306e\u70b9\u3092\u8003\u616e\u3057\u3066\u304f\u3060\u3055\u3044\uff1a</p> </li> <li>\u7570\u306a\u308b\u7a2e\u985e\u306e\u30c7\u30fc\u30bf\u3092\u3069\u306e\u3088\u3046\u306b\u524d\u51e6\u7406\u30fb\u6a19\u6e96\u5316\u3059\u308b\u304b</li> <li>\u7d71\u5408\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308bSVD/PCA\u306e\u9069\u7528\u65b9\u6cd5</li> <li>\u6b21\u5143\u524a\u6e1b\u3055\u308c\u305f\u7a7a\u9593\u3067\u306e\u60a3\u8005\u306e\u985e\u4f3c\u6027\u8a55\u4fa1</li> <li>\u75be\u60a3\u4e88\u6e2c\u3084\u6cbb\u7642\u53cd\u5fdc\u6027\u4e88\u6e2c\u3078\u306e\u5fdc\u7528\u53ef\u80fd\u6027</li> </ol>"},{"location":"lectures/LA/43-principal-component-analysis/#7","title":"7. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/43-principal-component-analysis/#q1-svdpca","title":"Q1: \u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u3068\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f","text":"<p>A1: SVD\u306f\u4efb\u610f\u306e\u884c\u5217\u306b\u5bfe\u3057\u3066\u9069\u7528\u3067\u304d\u308b\u884c\u5217\u5206\u89e3\u624b\u6cd5\u3067\u3042\u308a\u3001\\(A = U\\Sigma V^T\\) \u3068\u5206\u89e3\u3057\u307e\u3059\u3002\u4e00\u65b9\u3001PCA\u306f\u30c7\u30fc\u30bf\u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3\u306b\u57fa\u3065\u3044\u305f\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u3067\u3059\u3002\u4e2d\u5fc3\u5316\u3055\u308c\u305f\uff08\u5e73\u5747\u3092\u5f15\u3044\u305f\uff09\u30c7\u30fc\u30bf\u884c\u5217 \\(X\\) \u306b\u5bfe\u3057\u3066SVD\u3092\u9069\u7528\u3059\u308b\u3068\u3001PCA\u306e\u7d50\u679c\u3068\u540c\u7b49\u306b\u306a\u308a\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb \\(V\\) \u304cPCA\u306e\u4e3b\u6210\u5206\uff08\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff09\u306b\u5bfe\u5fdc\u3057\u3001\u7279\u7570\u5024\u306e2\u4e57\u304c\u56fa\u6709\u5024\u306b\u6bd4\u4f8b\u3057\u307e\u3059\u3002SVD\u306f\u3088\u308a\u4e00\u822c\u7684\u306a\u624b\u6cd5\u3067\u3042\u308a\u3001PCA\u306f\u305d\u306e\u7279\u6b8a\u306a\u30b1\u30fc\u30b9\u3068\u8003\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#q2","title":"Q2: \u533b\u7642\u753b\u50cf\u306e\u5727\u7e2e\u306b\u304a\u3044\u3066\u3001\u3069\u308c\u304f\u3089\u3044\u306e\u7279\u7570\u5024\u3092\u4fdd\u6301\u3059\u3079\u304d\u3067\u3059\u304b\uff1f","text":"<p>A2: \u9069\u5207\u306a\u7279\u7570\u5024\u306e\u6570\u306f\u3001\u753b\u50cf\u306e\u7a2e\u985e\u3001\u753b\u8cea\u8981\u4ef6\u3001\u7528\u9014\u306b\u3088\u3063\u3066\u7570\u306a\u308a\u307e\u3059\u3002\u4e00\u822c\u7684\u306b\u306f\uff1a - \u8a3a\u65ad\u76ee\u7684\u306e\u9ad8\u54c1\u8cea\u4fdd\u5b58\uff1a\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u304c95-99%\u306b\u306a\u308b\u307e\u3067\uff08\u901a\u5e38\u3001\u5168\u7279\u7570\u5024\u306e10-20%\u7a0b\u5ea6\uff09 - \u53c2\u7167\u7528\u306e\u4e2d\u54c1\u8cea\u4fdd\u5b58\uff1a\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u304c90-95%\u306b\u306a\u308b\u307e\u3067\uff08\u901a\u5e38\u3001\u5168\u7279\u7570\u5024\u306e5-10%\u7a0b\u5ea6\uff09 - \u30a2\u30fc\u30ab\u30a4\u30d6\u7528\u306e\u4f4e\u5bb9\u91cf\u4fdd\u5b58\uff1a\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u304c80-90%\u306b\u306a\u308b\u307e\u3067\uff08\u901a\u5e38\u3001\u5168\u7279\u7570\u5024\u306e3-5%\u7a0b\u5ea6\uff09</p> <p>\u91cd\u8981\u306a\u306e\u306f\u3001\u7279\u7570\u5024\u306e\u6570\u3068\u518d\u69cb\u6210\u753b\u50cf\u306e\u8996\u899a\u7684\u54c1\u8cea\u3001\u305d\u3057\u3066\u8a3a\u65ad\u7cbe\u5ea6\u306e\u30d0\u30e9\u30f3\u30b9\u3092\u53d6\u308b\u3053\u3068\u3067\u3059\u3002\u7528\u9014\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3092\u9078\u629e\u3059\u3079\u304d\u3067\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#q3","title":"Q3: \u4e3b\u6210\u5206\u5206\u6790\u3067\u5f97\u3089\u308c\u305f\u4e3b\u6210\u5206\u3092\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A3: \u4e3b\u6210\u5206\u306e\u89e3\u91c8\u306b\u306f\u3044\u304f\u3064\u304b\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u8ca0\u8377\u91cf\uff08Loadings\uff09\u306e\u5206\u6790: \u5404\u4e3b\u6210\u5206\u306b\u5bfe\u3059\u308b\u5143\u306e\u5909\u6570\u306e\u5bc4\u4e0e\u5ea6\u3092\u8abf\u3079\u307e\u3059\u3002\u5927\u304d\u306a\uff08\u7d76\u5bfe\u5024\u304c\u5927\u304d\u3044\uff09\u8ca0\u8377\u91cf\u3092\u6301\u3064\u5909\u6570\u304c\u4e3b\u6210\u5206\u306e\u610f\u5473\u3092\u793a\u3057\u307e\u3059\u3002 2. \u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u306e\u53ef\u8996\u5316: \u5404\u30b5\u30f3\u30d7\u30eb\u306e\u4e3b\u6210\u5206\u30b9\u30b3\u30a2\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3001\u30b5\u30f3\u30d7\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u3084\u30d1\u30bf\u30fc\u30f3\u3092\u89b3\u5bdf\u3057\u307e\u3059\u3002 3. \u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u306e\u6d3b\u7528: \u30b5\u30f3\u30d7\u30eb\u3068\u5909\u6570\u3092\u540c\u6642\u306b\u8868\u793a\u3059\u308b\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u3092\u7528\u3044\u3066\u3001\u5909\u6570\u9593\u306e\u95a2\u4fc2\u3068\u30b5\u30f3\u30d7\u30eb\u306e\u5206\u5e03\u3092\u540c\u6642\u306b\u89e3\u91c8\u3057\u307e\u3059\u3002 4. \u30c9\u30e1\u30a4\u30f3\u77e5\u8b58\u306e\u6d3b\u7528: \u7d71\u8a08\u7684\u89e3\u91c8\u3060\u3051\u3067\u306a\u304f\u3001\u5c02\u9580\u77e5\u8b58\uff08\u533b\u5b66\u3001\u751f\u7269\u5b66\u306a\u3069\uff09\u3092\u6d3b\u7528\u3057\u3066\u4e3b\u6210\u5206\u306e\u751f\u7269\u5b66\u7684\u30fb\u81e8\u5e8a\u7684\u610f\u5473\u3092\u63a2\u308a\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u3067\u306f\u3001\u7b2c1\u4e3b\u6210\u5206\u304c\u7d30\u80de\u5468\u671f\u95a2\u9023\u907a\u4f1d\u5b50\u3068\u76f8\u95a2\u3057\u3066\u3044\u308c\u3070\u3001\u7d30\u80de\u5897\u6b96\u306e\u7a0b\u5ea6\u3092\u8868\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#q4","title":"Q4: \u7279\u7570\u5024\u5206\u89e3\u3092\u7528\u3044\u305f\u7570\u5e38\u691c\u51fa\u306f\u3069\u306e\u3088\u3046\u306b\u6a5f\u80fd\u3057\u307e\u3059\u304b\uff1f","text":"<p>A4: SVD\u3092\u7528\u3044\u305f\u7570\u5e38\u691c\u51fa\u306f\u3001\u4e3b\u306b\u518d\u69cb\u6210\u8aa4\u5dee\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\uff1a 1. \u6b63\u5e38\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066SVD\u3092\u9069\u7528\u3057\u3001\u4e0a\u4f4dk\u500b\u306e\u7279\u7570\u5024\u30fb\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u4fdd\u6301\u3057\u3066\u90e8\u5206\u7a7a\u9593\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9 2. \u65b0\u3057\u3044\u30c7\u30fc\u30bf\u3092\u3053\u306e\u90e8\u5206\u7a7a\u9593\u306b\u6295\u5f71\u3057\u3001\u518d\u69cb\u6210 3. \u5143\u306e\u30c7\u30fc\u30bf\u3068\u518d\u69cb\u6210\u30c7\u30fc\u30bf\u306e\u5dee\uff08\u518d\u69cb\u6210\u8aa4\u5dee\uff09\u3092\u8a08\u7b97 4. \u518d\u69cb\u6210\u8aa4\u5dee\u304c\u95be\u5024\u3092\u8d85\u3048\u308b\u5834\u5408\u3001\u305d\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u3092\u7570\u5e38\u3068\u5224\u5b9a</p> <p>\u6b63\u5e38\u30c7\u30fc\u30bf\u306f\u30e2\u30c7\u30eb\u306e\u90e8\u5206\u7a7a\u9593\u5185\u307e\u305f\u306f\u305d\u306e\u8fd1\u304f\u306b\u5b58\u5728\u3059\u308b\u305f\u3081\u518d\u69cb\u6210\u8aa4\u5dee\u304c\u5c0f\u3055\u304f\u306a\u308a\u307e\u3059\u304c\u3001\u7570\u5e38\u30c7\u30fc\u30bf\u306f\u90e8\u5206\u7a7a\u9593\u304b\u3089\u96e2\u308c\u3066\u3044\u308b\u305f\u3081\u518d\u69cb\u6210\u8aa4\u5dee\u304c\u5927\u304d\u304f\u306a\u308b\u3068\u3044\u3046\u539f\u7406\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u533b\u7642\u30c7\u30fc\u30bf\u3001\u30bb\u30f3\u30b5\u30fc\u30c7\u30fc\u30bf\u306a\u3069\u591a\u69d8\u306a\u5206\u91ce\u3067\u52b9\u679c\u7684\u3067\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#q5","title":"Q5: \u7279\u7570\u5024\u5206\u89e3\u3068\u4e3b\u6210\u5206\u5206\u6790\u306e\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u3044\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306b\u5bfe\u51e6\u3059\u3079\u304d\u3067\u3059\u304b\uff1f","text":"<p>A5: \u5927\u898f\u6a21\u30c7\u30fc\u30bf\u3067\u306e\u8a08\u7b97\u30b3\u30b9\u30c8\u524a\u6e1b\u306b\u306f\u4ee5\u4e0b\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\uff1a 1. \u30e9\u30f3\u30c0\u30e0\u5316SVD/PCA: \u30c7\u30fc\u30bf\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u5c04\u5f71\u3057\u3001\u5c0f\u3055\u3044\u90e8\u5206\u7a7a\u9593\u3067SVD/PCA\u3092\u8a08\u7b97 2. \u5897\u5206\u7684/\u30aa\u30f3\u30e9\u30a4\u30f3SVD/PCA: \u30c7\u30fc\u30bf\u3092\u4e00\u5ea6\u306b\u3059\u3079\u3066\u51e6\u7406\u305b\u305a\u3001\u9010\u6b21\u7684\u306b\u51e6\u7406 3. \u78ba\u7387\u7684SVD/PCA: \u30c7\u30fc\u30bf\u306e\u78ba\u7387\u7684\u306a\u30b5\u30d6\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u5229\u7528 4. \u5206\u6563\u8a08\u7b97: \u8907\u6570\u306e\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3067\u8a08\u7b97\u3092\u5206\u6563\uff08Spark, Dask\u306a\u3069\uff09 5. GPU\u306e\u6d3b\u7528: \u884c\u5217\u8a08\u7b97\u3092GPU\u3067\u9ad8\u901f\u5316 6. \u8fd1\u4f3c\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0: \u5b8c\u5168\u306a\u7cbe\u5ea6\u3092\u72a0\u7272\u306b\u3057\u3066\u9ad8\u901f\u306b\u8a08\u7b97\u3059\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u5229\u7528</p> <p>\u4f8b\u3048\u3070\u3001Scikit-learn\u3067\u306f <code>TruncatedSVD</code> \u3084 <code>IncrementalPCA</code> \u306a\u3069\u306e\u52b9\u7387\u7684\u306a\u5b9f\u88c5\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u9069\u5207\u306a\u65b9\u6cd5\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30ba\u3001\u5fc5\u8981\u306a\u7cbe\u5ea6\u3001\u8a08\u7b97\u30ea\u30bd\u30fc\u30b9\u306b\u3088\u3063\u3066\u9078\u629e\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#q6-z-","title":"Q6: \u4e3b\u6210\u5206\u5206\u6790\u306e\u524d\u306b\u3001\u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\uff08Z-\u30b9\u30b3\u30a2\u5316\uff09\u306f\u5e38\u306b\u5fc5\u8981\u3067\u3059\u304b\uff1f","text":"<p>A6: \u57fa\u672c\u7684\u306b\u306f\u6a19\u6e96\u5316\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u305d\u306e\u7406\u7531\u306f\uff1a 1. \u5909\u6570\u306e\u30b9\u30b1\u30fc\u30eb\u304c\u7570\u306a\u308b\u5834\u5408\uff08\u4f8b\uff1a\u8eab\u9577cm\u3068\u4f53\u91cdkg\uff09\u3001\u30b9\u30b1\u30fc\u30eb\u306e\u5927\u304d\u3044\u5909\u6570\u304c\u5206\u6563\u304c\u5927\u304d\u304f\u306a\u308a\u3001PCA\u306e\u7d50\u679c\u3092\u652f\u914d\u3057\u3066\u3057\u307e\u3044\u307e\u3059\u3002 2. \u6a19\u6e96\u5316\u306b\u3088\u308a\u5404\u5909\u6570\u306e\u5e73\u5747\u304c0\u3001\u5206\u6563\u304c1\u306b\u306a\u308a\u3001\u3059\u3079\u3066\u306e\u5909\u6570\u304c\u7b49\u3057\u304f\u6271\u308f\u308c\u307e\u3059\u3002 3. \u7279\u306b\u7570\u306a\u308b\u5358\u4f4d\u3084\u7bc4\u56f2\u3092\u6301\u3064\u5909\u6570\u3092\u6271\u3046\u5834\u5408\uff08\u533b\u7642\u30c7\u30fc\u30bf\u3001\u30bb\u30f3\u30b5\u30fc\u30c7\u30fc\u30bf\u306a\u3069\uff09\u306f\u5fc5\u9808\u3067\u3059\u3002</p> <p>\u4f8b\u5916\u3068\u3057\u3066\u3001\u3059\u3079\u3066\u306e\u5909\u6570\u304c\u540c\u3058\u5358\u4f4d\u3067\u6e2c\u5b9a\u3055\u308c\u3001\u5909\u6570\u9593\u306e\u30b9\u30b1\u30fc\u30eb\u306e\u9055\u3044\u304c\u5b9f\u969b\u306b\u91cd\u8981\u306a\u60c5\u5831\u3092\u6301\u3064\u5834\u5408\u306f\u3001\u6a19\u6e96\u5316\u3057\u306a\u3044\u3053\u3068\u3082\u3042\u308a\u307e\u3059\uff08\u4f8b\uff1a\u540c\u3058\u30bb\u30f3\u30b5\u30fc\u304b\u3089\u306e\u8907\u6570\u306e\u6e2c\u5b9a\u5024\uff09\u3002\u30c7\u30fc\u30bf\u306e\u6027\u8cea\u3092\u7406\u89e3\u3057\u3001\u76ee\u7684\u306b\u5408\u308f\u305b\u3066\u5224\u65ad\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#8","title":"8. \u4e73\u304c\u3093\u8a3a\u65ad\u306b\u304a\u3051\u308b\u4e3b\u6210\u5206\u5206\u6790\u30fb\u7279\u7570\u5024\u5206\u89e3\u306e\u5fdc\u7528\u4f8b","text":"<p>\u4e73\u304c\u3093\u306f\u4e16\u754c\u4e2d\u306e\u5973\u6027\u306b\u6700\u3082\u591a\u304f\u767a\u751f\u3059\u308b\u764c\u306e\u4e00\u3064\u3067\u3042\u308a\u3001\u65e9\u671f\u767a\u898b\u3068\u6b63\u78ba\u306a\u8a3a\u65ad\u306f\u751f\u5b58\u7387\u3092\u5927\u304d\u304f\u5411\u4e0a\u3055\u305b\u307e\u3059\u3002\u672c\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001Wisconsin\u4e73\u304c\u3093\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08WBCD\uff09\u3092\u7528\u3044\u3066\u3001\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3068\u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u304c\u3069\u306e\u3088\u3046\u306b\u4e73\u304c\u3093\u8a3a\u65ad\u306b\u5fdc\u7528\u3067\u304d\u308b\u304b\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#81-wisconsin","title":"8.1 Wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u6982\u8981","text":"<p>Wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u533b\u5b66\u7814\u7a76\u306b\u304a\u3044\u3066\u5e83\u304f\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u516c\u958b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u3001\u4e73\u623f\u816b\u760d\u306e\u7d30\u80de\u6838\u306e\u7279\u5fb4\u306b\u95a2\u3059\u308b\u60c5\u5831\u3092\u542b\u3093\u3067\u3044\u307e\u3059\u3002\u5404\u30b5\u30f3\u30d7\u30eb\u306f569\u4eba\u306e\u60a3\u8005\u304b\u3089\u63a1\u53d6\u3055\u308c\u305f\u7d30\u80de\u306e\u753b\u50cf\u304b\u3089\u62bd\u51fa\u3055\u308c\u305f30\u500b\u306e\u7279\u5fb4\u91cf\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u7279\u5fb4\u91cf\u306f\u7d30\u80de\u6838\u306e\u4ee5\u4e0b\u306e10\u500b\u306e\u7279\u6027\u306b\u95a2\u3059\u308b\u6e2c\u5b9a\u5024\u3067\u3059\uff1a</p> <ol> <li>\u534a\u5f84\uff08\u5e73\u5747\u7684\u306a\u4e2d\u5fc3\u304b\u3089\u306e\u8ddd\u96e2\uff09</li> <li>\u30c6\u30af\u30b9\u30c1\u30e3\uff08\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u5024\u306e\u6a19\u6e96\u504f\u5dee\uff09</li> <li>\u5468\u56f2\u9577</li> <li>\u9762\u7a4d</li> <li>\u6ed1\u3089\u304b\u3055\uff08\u534a\u5f84\u306e\u9577\u3055\u306e\u5909\u52d5\uff09</li> <li>\u51dd\u7e2e\u5ea6\uff08\u5468\u56f2\u9577\u00b2 / \u9762\u7a4d - 1.0\uff09</li> <li>\u51f9\u90e8\uff08\u8f2a\u90ed\u306e\u51f9\u90e8\u306e\u91cd\u75c7\u5ea6\uff09</li> <li>\u51f9\u70b9\uff08\u8f2a\u90ed\u306e\u51f9\u90e8\u306e\u6570\uff09</li> <li>\u5bfe\u79f0\u6027</li> <li>\u30d5\u30e9\u30af\u30bf\u30eb\u6b21\u5143\uff08\u300c\u6d77\u5cb8\u7dda\u306e\u8fd1\u4f3c\u300d - 1\uff09</li> </ol> <p>\u5404\u7279\u6027\u306b\u3064\u3044\u3066\u3001\u5e73\u5747\u5024\u3001\u6a19\u6e96\u504f\u5dee\u3001\u6700\u60aa\u5024\uff08\u6700\u5927\u5024\uff09\u306e3\u3064\u306e\u6e2c\u5b9a\u5024\u304c\u3042\u308a\u3001\u5408\u8a0830\u306e\u7279\u5fb4\u91cf\u3068\u306a\u308a\u307e\u3059\u3002\u5404\u30b5\u30f3\u30d7\u30eb\u306b\u306f\u300c\u60aa\u6027\uff08Malignant\uff09\u300d\u307e\u305f\u306f\u300c\u826f\u6027\uff08Benign\uff09\u300d\u306e\u30e9\u30d9\u30eb\u304c\u4ed8\u3051\u3089\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#82-pca","title":"8.2 \u30c7\u30fc\u30bf\u524d\u51e6\u7406\u3068PCA\u306e\u9069\u7528","text":"<p>\u307e\u305a\u3001Wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5bfe\u3057\u3066\u4e3b\u6210\u5206\u5206\u6790\u3092\u9069\u7528\u3059\u308b\u904e\u7a0b\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\n\n# Wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\uff08scikit-learn\u306b\u5185\u8535\u3055\u308c\u3066\u3044\u307e\u3059\uff09\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target\n\n# \u30c7\u30fc\u30bf\u306e\u78ba\u8a8d\ndf = pd.DataFrame(X, columns=cancer.feature_names)\ndf['diagnosis'] = y\nprint(f\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5f62\u72b6: {df.shape}\")\nprint(f\"\u7279\u5fb4\u91cf: {cancer.feature_names}\")\nprint(f\"\u826f\u6027\u30b5\u30f3\u30d7\u30eb\u6570: {sum(y == 1)}\")\nprint(f\"\u60aa\u6027\u30b5\u30f3\u30d7\u30eb\u6570: {sum(y == 0)}\")\n\n# \u7279\u5fb4\u91cf\u306e\u6a19\u6e96\u5316\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# PCA\u306e\u9069\u7528\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# \u5bc4\u4e0e\u7387\u3092\u8a08\u7b97\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\n# \u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, label='\u500b\u5225\u5bc4\u4e0e\u7387')\nplt.step(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, where='mid', label='\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.axhline(y=0.95, color='r', linestyle='--', label='95%\u95be\u5024')\nplt.xlabel('\u4e3b\u6210\u5206\u6570')\nplt.ylabel('\u5bc4\u4e0e\u7387')\nplt.title('\u4e3b\u6210\u5206\u5206\u6790\u306e\u5bc4\u4e0e\u7387')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# \u4e0a\u4f4d2\u3064\u306e\u4e3b\u6210\u5206\u306b\u3088\u308b\u6563\u5e03\u56f3\nplt.figure(figsize=(12, 8))\ncolors = ['red', 'green']\ntarget_names = ['\u60aa\u6027', '\u826f\u6027']\n\nfor color, i, target_name in zip(colors, [0, 1], target_names):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], \n                color=color, alpha=0.8, lw=2, label=target_name)\n\nplt.xlabel(f'\u7b2c1\u4e3b\u6210\u5206 ({explained_variance_ratio[0]:.2%})')\nplt.ylabel(f'\u7b2c2\u4e3b\u6210\u5206 ({explained_variance_ratio[1]:.2%})')\nplt.title('\u4e73\u304c\u3093\u30c7\u30fc\u30bf\u306e\u4e0a\u4f4d2\u4e3b\u6210\u5206\u306b\u3088\u308b\u6563\u5e03\u56f3')\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/43-principal-component-analysis/#83-pca","title":"8.3 PCA\u306e\u7d50\u679c\u89e3\u91c8","text":""},{"location":"lectures/LA/43-principal-component-analysis/#831","title":"8.3.1 \u7d2f\u7a4d\u5bc4\u4e0e\u7387\u5206\u6790","text":"<p>PCA\u3092\u9069\u7528\u3057\u305f\u7d50\u679c\u304b\u3089\u5f97\u3089\u308c\u308b\u6700\u521d\u306e\u91cd\u8981\u306a\u60c5\u5831\u306f\u3001\u5404\u4e3b\u6210\u5206\u304c\u30c7\u30fc\u30bf\u306e\u5206\u6563\u306b\u3069\u306e\u7a0b\u5ea6\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u793a\u3059\u5bc4\u4e0e\u7387\u3067\u3059\u3002Wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u306f\u3001\u5178\u578b\u7684\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3059\uff1a</p> <ul> <li>\u7b2c1\u4e3b\u6210\u5206\uff1a\u7d0444%\u306e\u5206\u6563\u3092\u8aac\u660e</li> <li>\u7b2c2\u4e3b\u6210\u5206\uff1a\u7d0419%\u306e\u5206\u6563\u3092\u8aac\u660e</li> <li>\u7b2c3\u4e3b\u6210\u5206\uff1a\u7d049%\u306e\u5206\u6563\u3092\u8aac\u660e</li> </ul> <p>\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306e\u30b0\u30e9\u30d5\u3092\u5206\u6790\u3059\u308b\u3068\u3001\u4e0a\u4f4d3\u4e3b\u6210\u5206\u3067\u5168\u5206\u6563\u306e\u7d0472%\u3001\u4e0a\u4f4d5\u4e3b\u6210\u5206\u3067\u7d0485%\u3001\u4e0a\u4f4d10\u4e3b\u6210\u5206\u3067\u7d0495%\u3092\u8aac\u660e\u3059\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u300130\u6b21\u5143\u306e\u539f\u30c7\u30fc\u30bf\u309210\u6b21\u5143\u7a0b\u5ea6\u306b\u524a\u6e1b\u3057\u3066\u3082\u3001\u60c5\u5831\u306e\u5927\u90e8\u5206\u3092\u4fdd\u6301\u3067\u304d\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#832","title":"8.3.2 \u4e3b\u6210\u5206\u306e\u89e3\u91c8","text":"<p>\u6b21\u306b\u3001\u5404\u4e3b\u6210\u5206\u306e\u8ca0\u8377\u91cf\uff08\u5143\u306e\u7279\u5fb4\u91cf\u3068\u306e\u76f8\u95a2\uff09\u3092\u5206\u6790\u3057\u3001\u4e3b\u6210\u5206\u306e\u610f\u5473\u3092\u89e3\u91c8\u3057\u307e\u3059\uff1a</p> <pre><code># \u4e3b\u6210\u5206\u306e\u8ca0\u8377\u91cf\uff08\u5143\u306e\u7279\u5fb4\u91cf\u3068\u306e\u76f8\u95a2\uff09\u3092\u53ef\u8996\u5316\ncomponents = pd.DataFrame(pca.components_.T, index=cancer.feature_names, \n                         columns=[f'PC{i+1}' for i in range(pca.components_.shape[0])])\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(components.iloc[:, :5], annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('\u4e0a\u4f4d5\u4e3b\u6210\u5206\u306b\u5bfe\u3059\u308b\u7279\u5fb4\u91cf\u306e\u5bc4\u4e0e\u5ea6')\nplt.tight_layout()\nplt.show()\n\n# \u4e0a\u4f4d2\u4e3b\u6210\u5206\u306e\u7279\u5fb4\u91cf\u8ca0\u8377\u91cf\u3092\u77e2\u5370\u3067\u53ef\u8996\u5316\uff08\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\uff09\nplt.figure(figsize=(12, 10))\nfor i, feature in enumerate(cancer.feature_names):\n    plt.arrow(0, 0, pca.components_[0, i]*3, pca.components_[1, i]*3, \n              head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n    plt.text(pca.components_[0, i]*3.15, pca.components_[1, i]*3.15, feature, fontsize=9)\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u6563\u5e03\u56f3\u3092\u91cd\u306d\u308b\uff08\u7e2e\u5c0f\u8868\u793a\uff09\nfor color, i, target_name in zip(colors, [0, 1], target_names):\n    plt.scatter(X_pca[y == i, 0]/20, X_pca[y == i, 1]/20, \n                color=color, alpha=0.5, label=target_name)\n\nplt.xlabel(f'\u7b2c1\u4e3b\u6210\u5206 ({explained_variance_ratio[0]:.2%})')\nplt.ylabel(f'\u7b2c2\u4e3b\u6210\u5206 ({explained_variance_ratio[1]:.2%})')\nplt.title('\u4e73\u304c\u3093\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8')\nplt.grid(True)\nplt.legend()\nplt.axis([-0.5, 0.5, -0.5, 0.5])\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\u30d0\u30a4\u30d7\u30ed\u30c3\u30c8\u3068\u8ca0\u8377\u91cf\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u304b\u3089\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u89e3\u91c8\u304c\u53ef\u80fd\u3067\u3059\uff1a</p> <ol> <li> <p>\u7b2c1\u4e3b\u6210\u5206\uff1a\u7d30\u80de\u6838\u306e\u5927\u304d\u3055\u3068\u5f62\u72b6\u306b\u95a2\u9023\u3059\u308b\u7279\u5fb4\u91cf\uff08\u534a\u5f84\u3001\u5468\u56f2\u9577\u3001\u9762\u7a4d\u306a\u3069\uff09\u3068\u5f37\u3044\u6b63\u306e\u76f8\u95a2\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u816b\u760d\u7d30\u80de\u306e\u6210\u9577\u3068\u62e1\u5927\u3092\u8868\u3059\u300c\u30b5\u30a4\u30ba\u56e0\u5b50\u300d\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u7b2c2\u4e3b\u6210\u5206\uff1a\u4e3b\u306b\u7d30\u80de\u6838\u306e\u30c6\u30af\u30b9\u30c1\u30e3\u3001\u6ed1\u3089\u304b\u3055\u3001\u5bfe\u79f0\u6027\u306a\u3069\u3068\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u816b\u760d\u7d30\u80de\u306e\u5f62\u614b\u5b66\u7684\u306a\u4e0d\u898f\u5247\u6027\u3092\u8868\u3059\u300c\u5f62\u614b\u56e0\u5b50\u300d\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u7b2c3\u4e3b\u6210\u5206\uff1a\u51f9\u90e8\u306e\u91cd\u75c7\u5ea6\u3084\u51f9\u70b9\u306e\u6570\u306a\u3069\u3068\u76f8\u95a2\u3057\u3066\u304a\u308a\u3001\u7d30\u80de\u6838\u306e\u5883\u754c\u306e\u8907\u96d1\u3055\u3092\u8868\u3059\u300c\u5883\u754c\u8907\u96d1\u6027\u56e0\u5b50\u300d\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> </li> </ol> <p>\u3053\u306e\u89e3\u91c8\u306f\u3001\u75c5\u7406\u5b66\u7684\u77e5\u8b58\u3068\u3082\u6574\u5408\u3057\u3066\u3044\u307e\u3059\u3002\u60aa\u6027\u816b\u760d\u306f\u901a\u5e38\u3001\u7d30\u80de\u6838\u304c\u5927\u304d\u304f\u3001\u5f62\u304c\u4e0d\u898f\u5247\u3067\u3001\u5883\u754c\u304c\u8907\u96d1\u306a\u7279\u5fb4\u3092\u6301\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#833","title":"8.3.3 \u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u816b\u760d\u5206\u985e","text":"<p>\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u826f\u6027\u30fb\u60aa\u6027\u30b5\u30f3\u30d7\u30eb\u306e\u5206\u5e03\u3092\u89b3\u5bdf\u3059\u308b\u3068\u3001\u7b2c1\u4e3b\u6210\u5206\u3092\u4e2d\u5fc3\u306b\u304b\u306a\u308a\u660e\u78ba\u306a\u5206\u96e2\u304c\u898b\u3089\u308c\u307e\u3059\u3002\u3053\u308c\u306f\u300c\u30b5\u30a4\u30ba\u56e0\u5b50\u300d\u304c\u4e73\u304c\u3093\u8a3a\u65ad\u306b\u304a\u3044\u3066\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u7b2c1\u4e3b\u6210\u5206\u306e\u5024\u304c\u5927\u304d\u3044\u30b5\u30f3\u30d7\u30eb\u306f\u60aa\u6027\u3067\u3042\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u304f\u3001\u5024\u304c\u5c0f\u3055\u3044\u30b5\u30f3\u30d7\u30eb\u306f\u826f\u6027\u3067\u3042\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p> <p>\u4e3b\u6210\u5206\u3092\u7528\u3044\u305f\u7c21\u5358\u306a\u5206\u985e\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u3066\u3001\u305d\u306e\u6709\u52b9\u6027\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> <pre><code># \u4e0a\u4f4dn\u500b\u306e\u4e3b\u6210\u5206\u3092\u4f7f\u7528\u3057\u305f\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30e2\u30c7\u30eb\nn_components = 2  # \u4e0a\u4f4d2\u4e3b\u6210\u5206\u3092\u4f7f\u7528\nX_train, X_test, y_train, y_test = train_test_split(X_pca[:, :n_components], y, test_size=0.3, random_state=42)\n\n# \u30e2\u30c7\u30eb\u69cb\u7bc9\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\n# \u4e88\u6e2c\u3068\u8a55\u4fa1\ny_pred = lr.predict(X_test)\nprint(\"\u6df7\u540c\u884c\u5217:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\n\u5206\u985e\u30ec\u30dd\u30fc\u30c8:\")\nprint(classification_report(y_test, y_pred, target_names=target_names))\n\n# \u6c7a\u5b9a\u5883\u754c\u3092\u53ef\u8996\u5316\nplt.figure(figsize=(10, 8))\nx_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\ny_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nZ = lr.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.4)\nfor color, i, target_name in zip(colors, [0, 1], target_names):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], \n                color=color, alpha=0.8, label=target_name)\nplt.xlabel(f'\u7b2c1\u4e3b\u6210\u5206 ({explained_variance_ratio[0]:.2%})')\nplt.ylabel(f'\u7b2c2\u4e3b\u6210\u5206 ({explained_variance_ratio[1]:.2%})')\nplt.title('\u4e3b\u6210\u5206\u7a7a\u9593\u3067\u306e\u4e73\u304c\u3093\u5206\u985e')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p>\u4e0a\u4f4d2\u4e3b\u6210\u5206\u3060\u3051\u3092\u4f7f\u7528\u3057\u305f\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u3082\u3001\u901a\u5e3890%\u4ee5\u4e0a\u306e\u5206\u985e\u7cbe\u5ea6\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u4e3b\u6210\u5206\u5206\u6790\u306b\u3088\u3063\u3066\u816b\u760d\u7d30\u80de\u306e\u672c\u8cea\u7684\u306a\u7279\u5fb4\u304c\u62bd\u51fa\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/43-principal-component-analysis/#84-svd","title":"8.4 \u7279\u7570\u5024\u5206\u89e3\uff08SVD\uff09\u3092\u7528\u3044\u305f\u7d30\u80de\u753b\u50cf\u306e\u89e3\u6790","text":"<p>\u4e73\u304c\u3093\u306e\u8a3a\u65ad\u3067\u306f\u3001\u7d30\u80de\u6838\u306e\u7279\u5fb4\u3060\u3051\u3067\u306a\u304f\u3001\u7d44\u7e54\u5168\u4f53\u306e\u753b\u50cf\u89e3\u6790\u3082\u91cd\u8981\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001SVD\u3092\u7528\u3044\u305f\u4e73\u304c\u3093\u7d44\u7e54\u753b\u50cf\u306e\u89e3\u6790\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_sample_images\nfrom scipy import ndimage, misc\nfrom numpy.linalg import svd\n\n# \u30b5\u30f3\u30d7\u30eb\u753b\u50cf\uff08\u5b9f\u969b\u306e\u533b\u7642\u753b\u50cf\u3067\u306f\u306a\u304f\u3001\u8aac\u660e\u7528\uff09\n# \u5b9f\u969b\u306e\u5fdc\u7528\u3067\u306f\u3001\u75c5\u7406\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\nsample_image = misc.face(gray=True)  # \u30b5\u30f3\u30d7\u30eb\u753b\u50cf\nimage = ndimage.zoom(sample_image, 0.25)  # \u8a08\u7b97\u52b9\u7387\u306e\u305f\u3081\u306b\u30ea\u30b5\u30a4\u30ba\n\n# SVD\u3092\u9069\u7528\nU, sigma, Vt = svd(image, full_matrices=False)\n\n# \u7570\u306a\u308b\u30e9\u30f3\u30af\u3067\u306e\u518d\u69cb\u6210\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nranks = [5, 10, 20, 50, 100]\n\n# \u5143\u306e\u753b\u50cf\naxes[0, 0].imshow(image, cmap='gray')\naxes[0, 0].set_title('\u5143\u306e\u753b\u50cf')\naxes[0, 0].axis('off')\n\n# \u7570\u306a\u308b\u30e9\u30f3\u30af\u3067\u306e\u518d\u69cb\u6210\nfor i, r in enumerate(ranks):\n    # \u30e9\u30f3\u30afr\u3067\u306e\u8fd1\u4f3c\n    reconstructed = U[:, :r] @ np.diag(sigma[:r]) @ Vt[:r, :]\n\n    # \u5727\u7e2e\u7387\u3092\u8a08\u7b97\n    original_size = image.shape[0] * image.shape[1]\n    compressed_size = r * (image.shape[0] + image.shape[1] + 1)\n    compression_ratio = 100 * (1 - compressed_size / original_size)\n\n    # \u7d50\u679c\u306e\u8868\u793a\n    ax = axes[(i+1)//3, (i+1)%3]\n    ax.imshow(reconstructed, cmap='gray')\n    ax.set_title(f'\u30e9\u30f3\u30af {r}\\n\u5727\u7e2e\u7387: {compression_ratio:.1f}%')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# \u7279\u7570\u5024\u306e\u6e1b\u8870\u3092\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nplt.plot(sigma[:100], 'o-')\nplt.title('\u4e0a\u4f4d100\u500b\u306e\u7279\u7570\u5024')\nplt.xlabel('\u30a4\u30f3\u30c7\u30c3\u30af\u30b9')\nplt.ylabel('\u7279\u7570\u5024')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"lectures/LA/43-principal-component-analysis/#841-svd","title":"8.4.1 SVD\u306b\u3088\u308b\u75c5\u7406\u753b\u50cf\u306e\u7279\u5fb4\u62bd\u51fa","text":"<p>\u5b9f\u969b\u306e\u4e73\u304c\u3093\u8a3a\u65ad\u3067\u306f\u3001SVD\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5fdc\u7528\u3055\u308c\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\u753b\u50cf\u306e\u524d\u51e6\u7406\u3068\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\uff1a</li> <li>H&amp;E\u67d3\u8272\u3055\u308c\u305f\u75c5\u7406\u7d44\u7e54\u753b\u50cf\u3092\u30c7\u30b8\u30bf\u30eb\u5316</li> <li> <p>\u753b\u50cf\u30d1\u30c3\u30c1\u306b\u5206\u5272\u3057\u3001SVD\u3092\u7528\u3044\u3066\u6b21\u5143\u524a\u6e1b\u3057\u305f\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3092\u62bd\u51fa</p> </li> <li> <p>\u7d44\u7e54\u69cb\u9020\u306e\u7279\u5fb4\u62bd\u51fa\uff1a</p> </li> <li>SVD\u306e\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff08U\uff09\u306f\u7a7a\u9593\u7684\u30d1\u30bf\u30fc\u30f3\u3092\u8868\u73fe</li> <li>\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff08V\uff09\u306f\u7d44\u7e54\u306e\u5c40\u6240\u7684\u30c6\u30af\u30b9\u30c1\u30e3\u7279\u6027\u3092\u8868\u73fe</li> <li> <p>\u7279\u7570\u5024\uff08\u03a3\uff09\u306f\u305d\u308c\u3089\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u91cd\u8981\u5ea6\u3092\u8868\u73fe</p> </li> <li> <p>\u7570\u5e38\u691c\u51fa\u3078\u306e\u5fdc\u7528\uff1a</p> </li> <li>\u6b63\u5e38\u7d44\u7e54\u306eSVD\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9</li> <li>\u65b0\u3057\u3044\u753b\u50cf\u30d1\u30c3\u30c1\u306e\u518d\u69cb\u6210\u8aa4\u5dee\u3092\u8a08\u7b97</li> <li>\u9ad8\u3044\u518d\u69cb\u6210\u8aa4\u5dee\u3092\u793a\u3059\u9818\u57df\u306f\u7570\u5e38\uff08\u816b\u760d\uff09\u306e\u53ef\u80fd\u6027\u304c\u9ad8\u3044</li> </ol>"},{"location":"lectures/LA/43-principal-component-analysis/#85-pcasvd","title":"8.5 \u4e73\u304c\u3093\u8a3a\u65ad\u306b\u304a\u3051\u308bPCA\u3068SVD\u306e\u533b\u5b66\u7684\u610f\u7fa9","text":"<p>PCA\u3068SVD\u306e\u4e73\u304c\u3093\u8a3a\u65ad\u306b\u304a\u3051\u308b\u533b\u5b66\u7684\u610f\u7fa9\u3092\u307e\u3068\u3081\u308b\u3068\uff1a</p> <ol> <li>\u8a3a\u65ad\u7cbe\u5ea6\u306e\u5411\u4e0a\uff1a</li> <li>\u91cd\u8981\u306a\u7279\u5fb4\u3092\u62bd\u51fa\u3057\u3001\u30ce\u30a4\u30ba\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u3067\u3001\u826f\u6027\u30fb\u60aa\u6027\u306e\u8a3a\u65ad\u7cbe\u5ea6\u304c\u5411\u4e0a</li> <li> <p>\u8907\u6570\u306e\u533b\u7528\u753b\u50cf\u30e2\u30c0\u30ea\u30c6\u30a3\uff08\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u30fc\u3001\u8d85\u97f3\u6ce2\u3001MRI\u306a\u3069\uff09\u304b\u3089\u306e\u7279\u5fb4\u3092\u7d71\u5408</p> </li> <li> <p>\u30b5\u30d6\u30bf\u30a4\u30d7\u5206\u985e\u3078\u306e\u5fdc\u7528\uff1a</p> </li> <li>\u4e73\u304c\u3093\u306f\u751f\u7269\u5b66\u7684\u306b\u7570\u306a\u308b\u8907\u6570\u306e\u30b5\u30d6\u30bf\u30a4\u30d7\uff08Luminal A, Luminal B, HER2\u967d\u6027, \u57fa\u5e95\u69d8\u306a\u3069\uff09\u306b\u5206\u985e</li> <li>PCA\u306f\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u304b\u3089\u4e73\u304c\u3093\u30b5\u30d6\u30bf\u30a4\u30d7\u306e\u5206\u985e\u306b\u6709\u52b9</li> <li> <p>\u3053\u308c\u306b\u3088\u308a\u3001\u500b\u5225\u5316\u6cbb\u7642\u306e\u9078\u629e\u306b\u5f79\u7acb\u3064\u60c5\u5831\u3092\u63d0\u4f9b</p> </li> <li> <p>\u4e88\u5f8c\u4e88\u6e2c\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\uff1a</p> </li> <li>\u81e8\u5e8a\u30c7\u30fc\u30bf\u3001\u753b\u50cf\u7279\u5fb4\u3001\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306a\u3069\u3092\u7d71\u5408</li> <li> <p>PCA\u3067\u62bd\u51fa\u3055\u308c\u305f\u7279\u5fb4\u3092\u7528\u3044\u3066\u3001\u518d\u767a\u30ea\u30b9\u30af\u3084\u751f\u5b58\u7387\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9</p> </li> <li> <p>\u6cbb\u7642\u52b9\u679c\u306e\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\uff1a</p> </li> <li>\u6cbb\u7642\u524d\u5f8c\u306e\u753b\u50cf\u3084\u7d30\u80de\u7279\u6027\u306e\u5909\u5316\u3092SVD/PCA\u3067\u89e3\u6790</li> <li>\u6cbb\u7642\u5fdc\u7b54\u6027\u306e\u8a55\u4fa1\u3068\u6cbb\u7642\u6cd5\u306e\u6700\u9069\u5316\u306b\u6d3b\u7528</li> </ol>"},{"location":"lectures/LA/43-principal-component-analysis/#86-pcasvd","title":"8.6 \u81e8\u5e8a\u5fdc\u7528\u306e\u305f\u3081\u306ePCA/SVD\u306e\u9650\u754c\u3068\u8ab2\u984c","text":"<ol> <li>\u89e3\u91c8\u6027\u306e\u8ab2\u984c\uff1a</li> <li>\u4e3b\u6210\u5206\u306f\u5143\u306e\u7279\u5fb4\u306e\u7dda\u5f62\u7d50\u5408\u3067\u3042\u308a\u3001\u533b\u5b66\u7684\u89e3\u91c8\u304c\u96e3\u3057\u3044\u5834\u5408\u304c\u3042\u308b</li> <li> <p>\u81e8\u5e8a\u73fe\u5834\u3067\u306e\u53d7\u3051\u5165\u308c\u306b\u306f\u3001\u660e\u78ba\u306a\u89e3\u91c8\u53ef\u80fd\u6027\u304c\u6c42\u3081\u3089\u308c\u308b</p> </li> <li> <p>\u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\u3068\u524d\u51e6\u7406\uff1a</p> </li> <li>\u753b\u50cf\u30c7\u30fc\u30bf\u3084\u7d30\u80de\u5f62\u614b\u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\u65b9\u6cd5\u304c\u7d50\u679c\u306b\u5927\u304d\u304f\u5f71\u97ff</li> <li> <p>\u65bd\u8a2d\u9593\u3067\u306e\u30c7\u30fc\u30bf\u53ce\u96c6\u30fb\u51e6\u7406\u65b9\u6cd5\u306e\u9055\u3044\u304c\u7d50\u679c\u306b\u5f71\u97ff\u3059\u308b</p> </li> <li> <p>\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u3068\u4ee3\u8868\u6027\uff1a</p> </li> <li>\u30e2\u30c7\u30eb\u306e\u4fe1\u983c\u6027\u306f\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u591a\u69d8\u6027\u3068\u4ee3\u8868\u6027\u306b\u4f9d\u5b58</li> <li> <p>\u69d8\u3005\u306a\u4eba\u7a2e\u3001\u5e74\u9f62\u3001\u75c5\u671f\u306e\u60a3\u8005\u30c7\u30fc\u30bf\u3092\u542b\u3080\u3053\u3068\u304c\u91cd\u8981</p> </li> <li> <p>\u4ed6\u306e\u624b\u6cd5\u3068\u306e\u6bd4\u8f03\uff1a</p> </li> <li>\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306a\u3069\u306e\u975e\u7dda\u5f62\u624b\u6cd5\u3068\u6bd4\u8f03\u3057\u305f\u5834\u5408\u306e\u5229\u70b9\u3068\u6b20\u70b9</li> <li>\u7570\u306a\u308b\u6b21\u5143\u524a\u6e1b\u30fb\u7279\u5fb4\u62bd\u51fa\u6cd5\uff08t-SNE, UMAP, \u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306a\u3069\uff09\u3068\u306e\u6bd4\u8f03</li> </ol>"},{"location":"lectures/LA/43-principal-component-analysis/#87","title":"8.7 \u5b66\u751f\u306e\u305f\u3081\u306e\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>Wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3066\u3001PCA\u306b\u3088\u308b\u6b21\u5143\u524a\u6e1b\u3092\u884c\u3044\u3001\u4e0a\u4f4d\u4f55\u500b\u306e\u4e3b\u6210\u5206\u304c\u5168\u5206\u6563\u306e95%\u3092\u8aac\u660e\u3059\u308b\u304b\u8a08\u7b97\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u4e3b\u6210\u5206\u8ca0\u8377\u91cf\u3092\u5206\u6790\u3057\u3001\u60aa\u6027\u816b\u760d\u3068\u95a2\u9023\u304c\u5f37\u3044\u7d30\u80de\u6838\u306e\u7279\u5fb4\u30923\u3064\u7279\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u3053\u308c\u3089\u306e\u7279\u5fb4\u304c\u75c5\u7406\u5b66\u7684\u306b\u3069\u306e\u3088\u3046\u306a\u610f\u5473\u3092\u6301\u3064\u304b\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>PCA\u3067\u5f97\u3089\u308c\u305f\u4e0a\u4f4d3\u4e3b\u6210\u5206\u3060\u3051\u3092\u7528\u3044\u3066\u3001\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u5168\u7279\u5fb4\u91cf\uff0830\u6b21\u5143\uff09\u3092\u7528\u3044\u305f\u30e2\u30c7\u30eb\u3068\u6bd4\u8f03\u3057\u3066\u3001\u7cbe\u5ea6\u306f\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3057\u307e\u3059\u304b\uff1f\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>SVD\u3092\u7528\u3044\u3066\u3001\u4e73\u304c\u3093\u7d44\u7e54\u753b\u50cf\uff08\u516c\u958b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\uff09\u306e\u5727\u7e2e\u3068\u518d\u69cb\u6210\u3092\u884c\u3044\u3001\u8a3a\u65ad\u306b\u5341\u5206\u306a\u753b\u8cea\u3092\u4fdd\u6301\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u6700\u5c0f\u9650\u306e\u7279\u7570\u5024\u306e\u6570\u3092\u691c\u8a0e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5065\u5eb7/\u533b\u7642\u30c7\u30fc\u30bf\u306e\u89b3\u70b9\u304b\u3089\u3001PCA\u3068SVD\u304c\u6301\u3064\u81e8\u5e8a\u7684\u4fa1\u5024\u3068\u9650\u754c\u306b\u3064\u3044\u3066\u3001500\u5b57\u7a0b\u5ea6\u3067\u8ad6\u3058\u3066\u304f\u3060\u3055\u3044\u3002\u4e73\u304c\u3093\u8a3a\u65ad\u4ee5\u5916\u306e\u533b\u7642\u5206\u91ce\u3067\u306e\u5fdc\u7528\u53ef\u80fd\u6027\u306b\u3064\u3044\u3066\u3082\u89e6\u308c\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u3053\u306e\u30b1\u30fc\u30b9\u30b9\u30bf\u30c7\u30a3\u3092\u901a\u3058\u3066\u3001\u5b66\u751f\u306f\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u6982\u5ff5\u304c\u3069\u306e\u3088\u3046\u306b\u5b9f\u969b\u306e\u533b\u7642\u8ab2\u984c\u306b\u5fdc\u7528\u3055\u308c\u308b\u304b\u7406\u89e3\u3057\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3068\u533b\u5b66\u306e\u878d\u5408\u9818\u57df\u306b\u304a\u3051\u308b\u7406\u8ad6\u3068\u5b9f\u8df5\u306e\u6a4b\u6e21\u3057\u3092\u5b66\u3076\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u7b2c44\u56de\u8b1b\u7fa9\u30ce\u30fc\u30c8\uff1a\u56e0\u5b50\u5206\u6790\u306e\u57fa\u790e","text":""},{"location":"lectures/LA/44-factor-analysis/#_1","title":"\u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c44\u56de \u8b1b\u7fa9\u30c6\u30fc\u30de: \u56e0\u5b50\u5206\u6790\u306e\u57fa\u790e \u95a2\u9023\u9805\u76ee: \u7dda\u5f62\u4ee3\u6570\u5fdc\u7528\u3068\u3057\u3066\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u4e3b\u6210\u5206\u5206\u6790\uff08\u7b2c41-42\u56de\uff09\u3001\u7279\u7570\u5024\u5206\u89e3\uff08\u7b2c39-40\u56de\uff09</p>"},{"location":"lectures/LA/44-factor-analysis/#1","title":"1. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u65e5\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u9805\u76ee\u306b\u3064\u3044\u3066\u7406\u89e3\u3059\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3057\u307e\u3059\uff1a</p> <ol> <li>\u56e0\u5b50\u5206\u6790\u306e\u6982\u5ff5\u3068\u76ee\u7684\u3092\u7406\u89e3\u3059\u308b</li> <li>\u6f5c\u5728\u5909\u6570\u30e2\u30c7\u30eb\u306e\u8003\u3048\u65b9\u3068\u6570\u5b66\u7684\u5b9a\u5f0f\u5316\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u5b9a\u7fa9\u3068\u89e3\u91c8\u65b9\u6cd5\u3092\u5b66\u3076</li> <li>\u5171\u901a\u6027\u3068\u72ec\u81ea\u6027\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3057\u3001\u8a08\u7b97\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u56e0\u5b50\u5206\u6790\u3068\u4e3b\u6210\u5206\u5206\u6790\u306e\u9055\u3044\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/44-factor-analysis/#2","title":"2. \u56e0\u5b50\u5206\u6790\u306e\u6982\u5ff5\u3068\u76ee\u7684","text":""},{"location":"lectures/LA/44-factor-analysis/#21","title":"2.1 \u56e0\u5b50\u5206\u6790\u3068\u306f","text":"<p>\u5b9a\u7fa9: \u56e0\u5b50\u5206\u6790\uff08Factor Analysis\uff09\u306f\u3001\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306e\u80cc\u5f8c\u306b\u6f5c\u3080\u5171\u901a\u56e0\u5b50\uff08latent factors\uff09\u3092\u63a2\u7d22\u3059\u308b\u7d71\u8a08\u7684\u624b\u6cd5\u3067\u3059\u3002\u591a\u6570\u306e\u89b3\u6e2c\u5909\u6570\u9593\u306e\u76f8\u95a2\u95a2\u4fc2\u3092\u5c11\u6570\u306e\u6f5c\u5728\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3059\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u7c21\u5358\u306b\u8a00\u3048\u3070\u3001\u56e0\u5b50\u5206\u6790\u306f\u300c\u76f4\u63a5\u6e2c\u5b9a\u3067\u304d\u306a\u3044\u96a0\u308c\u305f\u8981\u56e0\u300d\u3092\u898b\u3064\u3051\u51fa\u3059\u65b9\u6cd5\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u69d8\u3005\u306a\u5065\u5eb7\u6307\u6a19\uff08\u8840\u5727\u3001\u4f53\u91cd\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u306a\u3069\uff09\u306e\u80cc\u5f8c\u306b\u3042\u308b\u300c\u5168\u4f53\u7684\u306a\u5065\u5eb7\u72b6\u614b\u300d\u306e\u3088\u3046\u306a\u6f5c\u5728\u7684\u306a\u8981\u56e0\u3092\u7279\u5b9a\u3057\u307e\u3059\u3002</p> <p>\u5177\u4f53\u4f8b\u3068\u3057\u3066\u3001\u5065\u5eb7\u8abf\u67fb\u306710\u306e\u8cea\u554f\u9805\u76ee\u304c\u3042\u308b\u3068\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u8cea\u554f\u3078\u306e\u56de\u7b54\u30d1\u30bf\u30fc\u30f3\u3092\u5206\u6790\u3059\u308b\u3068\u3001\u300c\u8eab\u4f53\u7684\u5065\u5eb7\u300d\u3068\u300c\u7cbe\u795e\u7684\u5065\u5eb7\u300d\u3068\u3044\u30462\u3064\u306e\u4e3b\u8981\u306a\u96a0\u308c\u305f\u56e0\u5b50\u304c\u898b\u3064\u304b\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3064\u307e\u308a\u300110\u500b\u306e\u8cea\u554f\uff08\u89b3\u6e2c\u5909\u6570\uff09\u30922\u3064\u306e\u56e0\u5b50\u3067\u8981\u7d04\u3067\u304d\u308b\u306e\u3067\u3059\u3002</p> <p>\u56e0\u5b50\u5206\u6790\u306f\u3001\u4e3b\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u72b6\u6cc1\u3067\u7528\u3044\u3089\u308c\u307e\u3059\uff1a</p> <ul> <li>\u591a\u6570\u306e\u6e2c\u5b9a\u9805\u76ee\u304b\u3089\u6f5c\u5728\u7684\u306a\u69cb\u6210\u6982\u5ff5\uff08construct\uff09\u3092\u7279\u5b9a\u3057\u305f\u3044\u5834\u5408   \uff08\u4f8b\uff1a\u6027\u683c\u691c\u67fb\u306e\u591a\u6570\u306e\u8cea\u554f\u304b\u3089\u300c\u5916\u5411\u6027\u300d\u3084\u300c\u8aa0\u5b9f\u6027\u300d\u306a\u3069\u306e\u6027\u683c\u7279\u6027\u3092\u62bd\u51fa\uff09</li> <li>\u8907\u96d1\u306a\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306e\u69cb\u9020\u3092\u5358\u7d14\u5316\u3057\u305f\u3044\u5834\u5408   \uff08\u4f8b\uff1a30\u7a2e\u985e\u306e\u5065\u5eb7\u6307\u6a19\u3092\u300c\u8eab\u4f53\u7684\u5065\u5eb7\u300d\u300c\u7cbe\u795e\u7684\u5065\u5eb7\u300d\u300c\u751f\u6d3b\u7fd2\u6163\u300d\u306a\u3069\u306e\u5c11\u6570\u306e\u56e0\u5b50\u306b\u96c6\u7d04\uff09</li> <li>\u89b3\u6e2c\u5909\u6570\u9593\u306e\u76f8\u95a2\u95a2\u4fc2\u3092\u8aac\u660e\u3059\u308b\u6f5c\u5728\u30e1\u30ab\u30cb\u30ba\u30e0\u3092\u63a2\u7d22\u3057\u305f\u3044\u5834\u5408   \uff08\u4f8b\uff1a\u306a\u305c\u8840\u5727\u3068\u5fc3\u62cd\u6570\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u306b\u76f8\u95a2\u304c\u3042\u308b\u306e\u304b\u3092\u8aac\u660e\u3059\u308b\u5171\u901a\u306e\u5065\u5eb7\u56e0\u5b50\u3092\u7279\u5b9a\uff09</li> </ul>"},{"location":"lectures/LA/44-factor-analysis/#22","title":"2.2 \u56e0\u5b50\u5206\u6790\u306e\u6b74\u53f2\u7684\u80cc\u666f","text":"<p>\u56e0\u5b50\u5206\u6790\u306f\u3001\u5fc3\u7406\u5b66\u8005\u306e\u30c1\u30e3\u30fc\u30eb\u30ba\u30fb\u30b9\u30d4\u30a2\u30de\u30f3\uff08Charles Spearman\uff09\u304c1904\u5e74\u306b\u77e5\u80fd\u7814\u7a76\u306e\u305f\u3081\u306b\u958b\u767a\u3057\u305f\u624b\u6cd5\u306b\u8d77\u6e90\u3092\u6301\u3061\u307e\u3059\u3002\u30b9\u30d4\u30a2\u30de\u30f3\u306f\u3001\u69d8\u3005\u306a\u77e5\u80fd\u30c6\u30b9\u30c8\uff08\u6570\u5b66\u3001\u8a00\u8a9e\u3001\u7a7a\u9593\u8a8d\u8b58\u306a\u3069\uff09\u306e\u5f97\u70b9\u9593\u306b\u76f8\u95a2\u304c\u3042\u308b\u3053\u3068\u306b\u6ce8\u76ee\u3057\u307e\u3057\u305f\u3002\u5f7c\u306f\u3053\u306e\u76f8\u95a2\u30d1\u30bf\u30fc\u30f3\u3092\u8aac\u660e\u3059\u308b\u305f\u3081\u306b\u300c\u4e00\u822c\u77e5\u80fd\u56e0\u5b50\uff08g\u56e0\u5b50\uff09\u300d\u3068\u3044\u3046\u6982\u5ff5\u3092\u63d0\u6848\u3057\u307e\u3057\u305f\u3002</p> <p>\u3064\u307e\u308a\u3001\u300c\u6570\u5b66\u304c\u5f97\u610f\u306a\u4eba\u306f\u8a00\u8a9e\u3082\u5f97\u610f\u3067\u3042\u308b\u50be\u5411\u304c\u3042\u308b\u300d\u3068\u3044\u3046\u73fe\u8c61\u3092\u8aac\u660e\u3059\u308b\u305f\u3081\u306b\u3001\u300c\u4e00\u822c\u7684\u306a\u77e5\u80fd\u300d\u3068\u3044\u3046\u76f4\u63a5\u6e2c\u5b9a\u3067\u304d\u306a\u3044\u6f5c\u5728\u56e0\u5b50\u3092\u4eee\u5b9a\u3057\u305f\u306e\u3067\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#23-pca","title":"2.3 \u56e0\u5b50\u5206\u6790\u3068PCA\u306e\u6bd4\u8f03","text":"<p>\u56e0\u5b50\u5206\u6790\u3068\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u306f\u3001\u3069\u3061\u3089\u3082\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u3092\u524a\u6e1b\u3059\u308b\u624b\u6cd5\u3067\u3059\u304c\u3001\u76ee\u7684\u3068\u6570\u5b66\u7684\u57fa\u790e\u304c\u7570\u306a\u308a\u307e\u3059\uff1a</p> \u5074\u9762 \u4e3b\u6210\u5206\u5206\u6790 (PCA) \u56e0\u5b50\u5206\u6790 (FA) \u76ee\u7684 \u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u76f4\u4ea4\u8ef8\u3092\u898b\u3064\u3051\u308b \u89b3\u6e2c\u5909\u6570\u9593\u306e\u76f8\u95a2\u3092\u8aac\u660e\u3059\u308b\u6f5c\u5728\u56e0\u5b50\u3092\u7279\u5b9a\u3059\u308b \u30e2\u30c7\u30eb \u6c7a\u5b9a\u8ad6\u7684\uff08\u6570\u5b66\u7684\u306a\u5909\u63db\uff09 \u78ba\u7387\u8ad6\u7684\uff08\u7d71\u8a08\u7684\u30e2\u30c7\u30eb\uff09 \u30a8\u30e9\u30fc\u9805 \u8003\u616e\u3057\u306a\u3044\uff08\u3059\u3079\u3066\u306e\u5206\u6563\u3092\u8aac\u660e\u3057\u3088\u3046\u3068\u3059\u308b\uff09 \u72ec\u81ea\u6027\uff08uniqueness\uff09\u3068\u3057\u3066\u660e\u793a\u7684\u306b\u8003\u616e\u3059\u308b \u6570\u5b66\u7684\u57fa\u790e \u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3 \u7279\u5b9a\u306e\u5171\u5206\u6563\u69cb\u9020\u3092\u6301\u3064\u78ba\u7387\u30e2\u30c7\u30eb \u7d50\u679c\u306e\u89e3\u91c8 \u4e3b\u6210\u5206\u306f\u89b3\u6e2c\u5909\u6570\u306e\u7dda\u5f62\u7d50\u5408\uff08\u6570\u5b66\u7684\u306a\u5909\u63db\u7d50\u679c\uff09 \u56e0\u5b50\u306f\u6f5c\u5728\u5909\u6570\u3068\u3057\u3066\u89e3\u91c8\uff08\u5b9f\u969b\u306b\u5b58\u5728\u3059\u308b\u3068\u4eee\u5b9a\u3055\u308c\u308b\u8981\u56e0\uff09 <p>\u4f8b\u3048\u3070\u300110\u500b\u306e\u5065\u5eb7\u6307\u6a19\u304c\u3042\u308b\u5834\u5408\uff1a - PCA\u306f\u300c\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u3092\u6700\u3082\u3088\u304f\u8868\u73fe\u3059\u308b10\u500b\u672a\u6e80\u306e\u65b0\u3057\u3044\u8ef8\u306f\u4f55\u304b\uff1f\u300d\u3068\u554f\u3044\u307e\u3059\u3002 - \u56e0\u5b50\u5206\u6790\u306f\u300c\u3053\u308c\u3089\u306e\u5065\u5eb7\u6307\u6a19\u9593\u306e\u76f8\u95a2\u30d1\u30bf\u30fc\u30f3\u3092\u8aac\u660e\u3059\u308b\u96a0\u308c\u305f\u5065\u5eb7\u8981\u56e0\u306f\u4f55\u304b\uff1f\u300d\u3068\u554f\u3044\u307e\u3059\u3002</p> <p>PCA\u306f\u6570\u5b66\u7684\u306a\u5909\u63db\u306b\u7126\u70b9\u3092\u5f53\u3066\u3001\u56e0\u5b50\u5206\u6790\u306f\u6f5c\u5728\u7684\u306a\u539f\u56e0\u30e1\u30ab\u30cb\u30ba\u30e0\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b\u306e\u3067\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#3","title":"3. \u6f5c\u5728\u5909\u6570\u30e2\u30c7\u30eb","text":""},{"location":"lectures/LA/44-factor-analysis/#31","title":"3.1 \u89b3\u6e2c\u5909\u6570\u3068\u6f5c\u5728\u5909\u6570\u306e\u95a2\u4fc2","text":"<p>\u6f5c\u5728\u5909\u6570\u30e2\u30c7\u30eb\u3067\u306f\u30012\u7a2e\u985e\u306e\u5909\u6570\u3092\u8003\u3048\u307e\u3059\uff1a</p> <ol> <li> <p>\u89b3\u6e2c\u5909\u6570\uff08observed variables\uff09\uff1a\u76f4\u63a5\u6e2c\u5b9a\u3067\u304d\u308b\u5909\u6570    \u4f8b\uff1a\u8840\u5727\u3001\u4f53\u91cd\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u3001\u30a2\u30f3\u30b1\u30fc\u30c8\u306e\u56de\u7b54\u306a\u3069</p> </li> <li> <p>\u6f5c\u5728\u5909\u6570\uff08latent variables\uff09\uff1a\u76f4\u63a5\u6e2c\u5b9a\u3067\u304d\u306a\u3044\u304c\u3001\u7406\u8ad6\u7684\u306b\u5b58\u5728\u3059\u308b\u3068\u4eee\u5b9a\u3055\u308c\u308b\u5909\u6570    \u4f8b\uff1a\u300c\u5168\u4f53\u7684\u306a\u5065\u5eb7\u72b6\u614b\u300d\u300c\u7cbe\u795e\u7684\u30b9\u30c8\u30ec\u30b9\u300d\u300c\u77e5\u80fd\u300d\u306a\u3069</p> </li> </ol> <p>\u5b9a\u7fa9: \u6f5c\u5728\u5909\u6570\uff08latent variable\uff09\u3068\u306f\u3001\u76f4\u63a5\u89b3\u6e2c\u3067\u304d\u306a\u3044\u304c\u3001\u8907\u6570\u306e\u89b3\u6e2c\u5909\u6570\u9593\u306e\u76f8\u95a2\u30d1\u30bf\u30fc\u30f3\u3092\u8aac\u660e\u3059\u308b\u305f\u3081\u306b\u7406\u8ad6\u7684\u306b\u5c0e\u5165\u3055\u308c\u308b\u69cb\u6210\u6982\u5ff5\u3067\u3059\u3002</p> <p>\u65e5\u5e38\u7684\u306a\u4f8b\u3067\u8003\u3048\u308b\u3068\uff1a - \u300c\u5e78\u798f\u5ea6\u300d\u306f\u76f4\u63a5\u6e2c\u5b9a\u3067\u304d\u307e\u305b\u3093\u304c\u3001\u751f\u6d3b\u6e80\u8db3\u5ea6\u3001\u5065\u5eb7\u72b6\u614b\u3001\u4eba\u9593\u95a2\u4fc2\u306e\u8cea\u306a\u3069\u306e\u89b3\u6e2c\u53ef\u80fd\u306a\u6307\u6a19\u3092\u901a\u3058\u3066\u9593\u63a5\u7684\u306b\u6e2c\u5b9a\u3067\u304d\u307e\u3059\u3002 - \u300c\u5b66\u529b\u300d\u306f\u76f4\u63a5\u898b\u3048\u307e\u305b\u3093\u304c\u3001\u6570\u5b66\u306e\u30c6\u30b9\u30c8\u3001\u8a00\u8a9e\u306e\u30c6\u30b9\u30c8\u3001\u8a18\u61b6\u529b\u30c6\u30b9\u30c8\u306a\u3069\u306e\u5f97\u70b9\u3092\u901a\u3058\u3066\u9593\u63a5\u7684\u306b\u63a8\u6e2c\u3067\u304d\u307e\u3059\u3002</p> <p>\u56e0\u5b50\u5206\u6790\u3067\u306f\u3001\u89b3\u6e2c\u5909\u6570\u9593\u306e\u76f8\u95a2\u30d1\u30bf\u30fc\u30f3\u3092\u8aac\u660e\u3059\u308b\u6f5c\u5728\u5909\u6570\uff08\u56e0\u5b50\uff09\u3092\u7279\u5b9a\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#32","title":"3.2 \u56e0\u5b50\u5206\u6790\u306e\u6570\u5b66\u7684\u30e2\u30c7\u30eb","text":"<p>\u56e0\u5b50\u5206\u6790\u306e\u57fa\u672c\u30e2\u30c7\u30eb\u306f\u3001\u4ee5\u4e0b\u306e\u5f0f\u3067\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[\\mathbf{X} = \\mathbf{\\Lambda F} + \\mathbf{\\varepsilon}\\] <p>\u3053\u306e\u5f0f\u306f\u3001\u89b3\u6e2c\u30c7\u30fc\u30bf\u304c\u6f5c\u5728\u56e0\u5b50\u3068\u8aa4\u5dee\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff1a</p> <ul> <li>\\(\\mathbf{X}\\) \u306f \\(p\\) \u6b21\u5143\u306e\u89b3\u6e2c\u5909\u6570\u30d9\u30af\u30c8\u30eb\uff08\u4f8b\uff1a10\u7a2e\u985e\u306e\u5065\u5eb7\u6307\u6a19\u306e\u6e2c\u5b9a\u5024\uff09</li> <li>\\(\\mathbf{F}\\) \u306f \\(m\\) \u6b21\u5143\u306e\u6f5c\u5728\u5171\u901a\u56e0\u5b50\u30d9\u30af\u30c8\u30eb\uff08\u4f8b\uff1a\u8eab\u4f53\u7684\u5065\u5eb7\u3001\u7cbe\u795e\u7684\u5065\u5eb7\u306a\u3069\u306e\u6f5c\u5728\u56e0\u5b50\u3001\\(m &lt; p\\)\uff09</li> <li>\\(\\mathbf{\\Lambda}\\) \u306f \\(p \\times m\\) \u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\uff08\u89b3\u6e2c\u5909\u6570\u3068\u6f5c\u5728\u56e0\u5b50\u306e\u95a2\u4fc2\u306e\u5f37\u3055\u3092\u8868\u3059\uff09</li> <li>\\(\\mathbf{\\varepsilon}\\) \u306f \\(p\\) \u6b21\u5143\u306e\u72ec\u81ea\u56e0\u5b50\uff08\u8aa4\u5dee\u9805\uff09\u30d9\u30af\u30c8\u30eb\uff08\u56e0\u5b50\u3067\u306f\u8aac\u660e\u3067\u304d\u306a\u3044\u90e8\u5206\uff09</li> </ul> <p>\u5404\u89b3\u6e2c\u5909\u6570 \\(X_i\\) \u306b\u3064\u3044\u3066\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5c55\u958b\u3067\u304d\u307e\u3059\uff1a</p> \\[X_i = \\lambda_{i1}F_1 + \\lambda_{i2}F_2 + \\ldots + \\lambda_{im}F_m + \\varepsilon_i\\] <p>\u4f8b\u3048\u3070\u3001\u8840\u5727\uff08\\(X_1\\)\uff09\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u308b\u3068\u3057\u307e\u3059\uff1a \\(\\(X_1 = 0.8F_1 + 0.2F_2 + \\varepsilon_1\\)\\)</p> <p>\u3053\u308c\u306f\u300c\u8840\u5727\u306f\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\uff08\\(F_1\\)\uff09\u3068\u5f37\u304f\u95a2\u9023\u3057\uff08\u8ca0\u8377\u91cf0.8\uff09\u3001\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50\uff08\\(F_2\\)\uff09\u3068\u3082\u5f31\u304f\u95a2\u9023\u3057\u3066\u3044\u308b\uff08\u8ca0\u8377\u91cf0.2\uff09\u3002\u307e\u305f\u3001\u3053\u308c\u3089\u306e\u56e0\u5b50\u3067\u306f\u8aac\u660e\u3067\u304d\u306a\u3044\u72ec\u81ea\u306e\u5909\u52d5\uff08\\(\\varepsilon_1\\)\uff09\u3082\u6301\u3064\u300d\u3068\u3044\u3046\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#33","title":"3.3 \u30e2\u30c7\u30eb\u306e\u4eee\u5b9a","text":"<p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u4eee\u5b9a\u3092\u7f6e\u304d\u307e\u3059\uff1a</p> <ol> <li> <p>\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u306f\u30bc\u30ed\uff08\u76f4\u4ea4\u56e0\u5b50\u30e2\u30c7\u30eb\u306e\u5834\u5408\uff09\uff1a\\(\\text{Cov}(F_i, F_j) = 0\\) \uff08\\(i \\neq j\\)\uff09    \u2192 \u4f8b\u3048\u3070\u3001\u300c\u8eab\u4f53\u7684\u5065\u5eb7\u300d\u3068\u300c\u7cbe\u795e\u7684\u5065\u5eb7\u300d\u3068\u3044\u3046\u56e0\u5b50\u306f\u4e92\u3044\u306b\u72ec\u7acb\u3057\u3066\u3044\u308b\u3068\u4eee\u5b9a</p> </li> <li> <p>\u56e0\u5b50\u306e\u5206\u6563\u306f1\uff1a\\(\\text{Var}(F_i) = 1\\)    \u2192 \u56e0\u5b50\u306e\u5358\u4f4d\u3092\u6a19\u6e96\u5316\u3057\u3066\u3001\u89e3\u91c8\u3092\u5bb9\u6613\u306b\u3059\u308b</p> </li> <li> <p>\u56e0\u5b50\u3068\u72ec\u81ea\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u306f\u30bc\u30ed\uff1a\\(\\text{Cov}(F_i, \\varepsilon_j) = 0\\)    \u2192 \u4f8b\u3048\u3070\u3001\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\uff08\\(F_1\\)\uff09\u306f\u3001\u8840\u5727\u306e\u72ec\u81ea\u56e0\u5b50\uff08\\(\\varepsilon_1\\)\uff09\u3068\u306f\u7121\u95a2\u4fc2</p> </li> <li> <p>\u72ec\u81ea\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u306f\u30bc\u30ed\uff1a\\(\\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0\\) \uff08\\(i \\neq j\\)\uff09    \u2192 \u4f8b\u3048\u3070\u3001\u8840\u5727\u306e\u72ec\u81ea\u56e0\u5b50\uff08\\(\\varepsilon_1\\)\uff09\u3068\u4f53\u91cd\u306e\u72ec\u81ea\u56e0\u5b50\uff08\\(\\varepsilon_2\\)\uff09\u306f\u7121\u95a2\u4fc2</p> </li> </ol> <p>\u3053\u308c\u3089\u306e\u4eee\u5b9a\u306b\u3088\u308a\u3001\u30e2\u30c7\u30eb\u304c\u6570\u5b66\u7684\u306b\u89e3\u304d\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#34","title":"3.4 \u5171\u5206\u6563\u69cb\u9020","text":"<p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u306e\u3082\u3068\u3067\u306e\u89b3\u6e2c\u5909\u6570\u306e\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{\\Sigma}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[\\mathbf{\\Sigma} = \\mathbf{\\Lambda \\Lambda^T} + \\mathbf{\\Psi}\\] <p>\u3053\u3053\u3067 \\(\\mathbf{\\Psi}\\) \u306f\u72ec\u81ea\u6027\u306e\u5bfe\u89d2\u884c\u5217\u3067\u3001\\(\\psi_i = \\text{Var}(\\varepsilon_i)\\) \u3067\u3059\u3002</p> <p>\u3053\u306e\u5f0f\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u3092\u300c\u5171\u901a\u56e0\u5b50\u306b\u3088\u308b\u90e8\u5206\u300d\u3068\u300c\u72ec\u81ea\u56e0\u5b50\u306b\u3088\u308b\u90e8\u5206\u300d\u306b\u5206\u89e3\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u30012\u3064\u306e\u5065\u5eb7\u6307\u6a19\uff08\u8840\u5727\u3068\u4f53\u91cd\uff09\u306e\u76f8\u95a2\u304c\u9ad8\u3044\u5834\u5408\u3001\u305d\u308c\u306f\u4e21\u65b9\u304c\u5171\u901a\u306e\u6f5c\u5728\u56e0\u5b50\uff08\u4f8b\uff1a\u8eab\u4f53\u7684\u5065\u5eb7\uff09\u306b\u5f37\u304f\u95a2\u9023\u3057\u3066\u3044\u308b\u305f\u3081\u3068\u8aac\u660e\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#35-pca","title":"3.5 PCA\u3068\u306e\u9055\u3044","text":"<p>PCA\u3068\u56e0\u5b50\u5206\u6790\u306e\u4e3b\u306a\u9055\u3044\u306f\u3001\u30e2\u30c7\u30eb\u306e\u69cb\u9020\u306b\u3042\u308a\u307e\u3059\uff1a</p> <ul> <li> <p>PCA\u306f\u5168\u5206\u6563\uff08\u5171\u901a\u5206\u6563\u3068\u72ec\u81ea\u5206\u6563\uff09\u3092\u8003\u616e\u3059\u308b\uff1a   \u3059\u3079\u3066\u306e\u5909\u52d5\u3092\u8aac\u660e\u3057\u3088\u3046\u3068\u3057\u3001\u8aa4\u5dee\u9805\u3092\u660e\u793a\u7684\u306b\u533a\u5225\u3057\u307e\u305b\u3093\u3002</p> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u306f\u5171\u901a\u5206\u6563\u306e\u307f\u3092\u5bfe\u8c61\u3068\u3057\u3001\u72ec\u81ea\u5206\u6563\u3092\u8aa4\u5dee\u3068\u3057\u3066\u660e\u793a\u7684\u306b\u6271\u3046\uff1a   \u5909\u6570\u9593\u306e\u76f8\u95a2\u3092\u8aac\u660e\u3059\u308b\u90e8\u5206\uff08\u5171\u901a\u56e0\u5b50\uff09\u3068\u3001\u500b\u5225\u306e\u5909\u6570\u56fa\u6709\u306e\u90e8\u5206\uff08\u72ec\u81ea\u56e0\u5b50\uff09\u3092\u533a\u5225\u3057\u307e\u3059\u3002</p> </li> </ul> <p>\u4ee5\u4e0b\u306e\u56f3\u306f\u3001\u3053\u306e\u8003\u3048\u65b9\u306e\u9055\u3044\u3092\u793a\u3057\u3066\u3044\u307e\u3059\uff1a</p> <pre><code>[\u5909\u6570\u306e\u5206\u6563\u306e\u5206\u89e3]\n\nPCA:\n\u5909\u6570\u306e\u5168\u5206\u6563 \u2192 \u4e3b\u6210\u5206\u3067\u8aac\u660e (\u6b8b\u5dee\u306a\u3057)\n\n\u56e0\u5b50\u5206\u6790:\n\u5909\u6570\u306e\u5168\u5206\u6563 \u2192 \u5171\u901a\u56e0\u5b50\u3067\u8aac\u660e\u3059\u308b\u90e8\u5206\uff08\u5171\u901a\u6027\uff09+ \u8aac\u660e\u3055\u308c\u306a\u3044\u90e8\u5206\uff08\u72ec\u81ea\u6027\uff09\n</code></pre> <p>\u4f8b\u3048\u3070\u3001\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u5834\u5408\uff1a - PCA\u306f\u300c\u8840\u5727\u306e\u30c7\u30fc\u30bf\u306e\u7dcf\u5909\u52d5\u3092\u6700\u3082\u3088\u304f\u8868\u73fe\u3059\u308b\u8ef8\u306f\u4f55\u304b\u300d\u3092\u554f\u3044\u307e\u3059\u3002 - \u56e0\u5b50\u5206\u6790\u306f\u300c\u8840\u5727\u3068\u4f53\u91cd\u306e\u76f8\u95a2\u3092\u8aac\u660e\u3059\u308b\u5171\u901a\u306e\u5065\u5eb7\u56e0\u5b50\u306f\u4f55\u304b\u3001\u305d\u3057\u3066\u8840\u5727\u56fa\u6709\u306e\u5909\u52d5\u306f\u3069\u308c\u304f\u3089\u3044\u304b\u300d\u3092\u554f\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#4","title":"4. \u56e0\u5b50\u8ca0\u8377\u91cf","text":""},{"location":"lectures/LA/44-factor-analysis/#41","title":"4.1 \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \u56e0\u5b50\u8ca0\u8377\u91cf\uff08factor loading\uff09\u3068\u306f\u3001\u5404\u89b3\u6e2c\u5909\u6570\u3068\u6f5c\u5728\u56e0\u5b50\u3068\u306e\u9593\u306e\u76f8\u95a2\u4fc2\u6570\u3067\u3059\u3002\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217 \\(\\mathbf{\\Lambda}\\) \u306e\u8981\u7d20 \\(\\lambda_{ij}\\) \u306f\u3001\u89b3\u6e2c\u5909\u6570 \\(X_i\\) \u3068\u56e0\u5b50 \\(F_j\\) \u306e\u95a2\u4fc2\u306e\u5f37\u3055\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u308f\u304b\u308a\u3084\u3059\u304f\u8a00\u3048\u3070\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306f\u300c\u5404\u6e2c\u5b9a\u9805\u76ee\u304c\u3069\u306e\u6f5c\u5728\u56e0\u5b50\u3068\u3069\u308c\u304f\u3089\u3044\u5f37\u304f\u95a2\u9023\u3057\u3066\u3044\u308b\u304b\u300d\u3092\u793a\u3059\u6570\u5024\u3067\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u5065\u5eb7\u6307\u6a19\u30682\u3064\u306e\u56e0\u5b50\u306e\u95a2\u4fc2\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3068\u3057\u307e\u3059\uff1a</p> <pre><code>            \u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50(F1)  \u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50(F2)\n\u8840\u5727(X1)         0.8              0.1\n\u4f53\u91cd(X2)         0.7              0.2\n\u7761\u7720\u306e\u8cea(X3)     0.2              0.7\n\u30b9\u30c8\u30ec\u30b9(X4)     0.1              0.8\n</code></pre> <p>\u3053\u306e\u8868\u304b\u3089\uff1a - \u8840\u5727\u306f\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u3068\u5f37\u304f\u95a2\u9023\uff08\u8ca0\u8377\u91cf0.8\uff09\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50\u3068\u306f\u307b\u3068\u3093\u3069\u95a2\u9023\u3057\u3066\u3044\u307e\u305b\u3093\uff08\u8ca0\u8377\u91cf0.1\uff09 - \u7761\u7720\u306e\u8cea\u306f\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50\u3068\u5f37\u304f\u95a2\u9023\uff08\u8ca0\u8377\u91cf0.7\uff09\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u3068\u306f\u3042\u307e\u308a\u95a2\u9023\u3057\u3066\u3044\u307e\u305b\u3093\uff08\u8ca0\u8377\u91cf0.2\uff09</p> <p>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u5024\u306f -1 \u304b\u3089 1 \u306e\u7bc4\u56f2\u3092\u3068\u308a\u3001\u305d\u306e\u7d76\u5bfe\u5024\u304c\u5927\u304d\u3044\u307b\u3069\u3001\u89b3\u6e2c\u5909\u6570\u3068\u56e0\u5b50\u306e\u95a2\u9023\u304c\u5f37\u3044\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#42","title":"4.2 \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u89e3\u91c8","text":"<p>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u89e3\u91c8\u306f\u4ee5\u4e0b\u306e\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u3092\u7528\u3044\u308b\u3053\u3068\u304c\u4e00\u822c\u7684\u3067\u3059\uff1a</p> <ul> <li> <p>|\u8ca0\u8377\u91cf| \u2265 0.7 : \u975e\u5e38\u306b\u5f37\u3044\u95a2\u9023   \uff08\u4f8b\uff1a\u8840\u5727\u3068\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u306e\u95a2\u4fc2\uff09</p> </li> <li> <p>0.5 \u2264 |\u8ca0\u8377\u91cf| &lt; 0.7 : \u5f37\u3044\u95a2\u9023   \uff08\u4f8b\uff1a\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u3068\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u306e\u95a2\u4fc2\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff09</p> </li> <li> <p>0.3 \u2264 |\u8ca0\u8377\u91cf| &lt; 0.5 : \u4e2d\u7a0b\u5ea6\u306e\u95a2\u9023   \uff08\u4f8b\uff1a\u904b\u52d5\u7fd2\u6163\u3068\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50\u306e\u95a2\u4fc2\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff09</p> </li> <li> <p>|\u8ca0\u8377\u91cf| &lt; 0.3 : \u5f31\u3044\u95a2\u9023   \uff08\u4f8b\uff1a\u5e74\u9f62\u3068\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50\u306e\u95a2\u4fc2\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff09</p> </li> </ul> <p>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e2\u4e57\u5024\u306f\u3001\u305d\u306e\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u89b3\u6e2c\u5909\u6570\u306e\u5206\u6563\u306e\u5272\u5408\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u8840\u5727\u306e\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u3078\u306e\u8ca0\u8377\u91cf\u304c0.8\u306e\u5834\u5408\u3001\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u306f\u8840\u5727\u306e\u5206\u6563\u306e64%\uff080.8\u00b2\uff09\u3092\u8aac\u660e\u3057\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\u6b8b\u308a\u306e36%\u306f\u4ed6\u306e\u56e0\u5b50\u3084\u72ec\u81ea\u56e0\u5b50\uff08\u8aa4\u5dee\uff09\u306b\u3088\u308b\u3082\u306e\u3067\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#43","title":"4.3 \u56e0\u5b50\u30d1\u30bf\u30fc\u30f3\u884c\u5217","text":"<p>\u56e0\u5b50\u5206\u6790\u3067\u306f\u3001\u4ee5\u4e0b\u306e2\u3064\u306e\u884c\u5217\u3092\u533a\u5225\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\uff1a</p> <ol> <li> <p>\u56e0\u5b50\u69cb\u9020\u884c\u5217\uff08Factor Structure Matrix\uff09: \u89b3\u6e2c\u5909\u6570\u3068\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u4fc2\u6570\u3092\u8868\u3059    \u2192 \u5909\u6570\u3068\u56e0\u5b50\u306e\u5358\u7d14\u306a\u95a2\u4fc2\u306e\u5f37\u3055\u3092\u793a\u3057\u307e\u3059</p> </li> <li> <p>\u56e0\u5b50\u30d1\u30bf\u30fc\u30f3\u884c\u5217\uff08Factor Pattern Matrix\uff09: \u5171\u901a\u56e0\u5b50\u7a7a\u9593\u306b\u304a\u3051\u308b\u5909\u6570\u306e\u4f4d\u7f6e\u3092\u8868\u3059\u504f\u56de\u5e30\u4fc2\u6570    \u2192 \u4ed6\u306e\u56e0\u5b50\u306e\u5f71\u97ff\u3092\u5236\u5fa1\u3057\u305f\u4e0a\u3067\u306e\u3001\u5909\u6570\u3068\u5404\u56e0\u5b50\u306e\u95a2\u4fc2\u3092\u793a\u3057\u307e\u3059</p> </li> </ol> <p>\u76f4\u4ea4\u56de\u8ee2\uff08\u56e0\u5b50\u9593\u306b\u76f8\u95a2\u304c\u306a\u3044\u5834\u5408\uff09\u3092\u7528\u3044\u308b\u5834\u5408\u3001\u3053\u306e2\u3064\u306e\u884c\u5217\u306f\u4e00\u81f4\u3057\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u659c\u4ea4\u56de\u8ee2\uff08\u56e0\u5b50\u9593\u306b\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\uff09\u3092\u7528\u3044\u308b\u5834\u5408\u306f\u7570\u306a\u308a\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u300c\u8eab\u4f53\u7684\u5065\u5eb7\u300d\u3068\u300c\u7cbe\u795e\u7684\u5065\u5eb7\u300d\u3068\u3044\u3046\u56e0\u5b50\u304c\u4e92\u3044\u306b\u76f8\u95a2\u3057\u3066\u3044\u308b\u5834\u5408\uff1a - \u69cb\u9020\u884c\u5217\u306f\u5358\u7d14\u306a\u76f8\u95a2\u3092\u793a\u3059 - \u30d1\u30bf\u30fc\u30f3\u884c\u5217\u306f\u3001\u300c\u7cbe\u795e\u7684\u5065\u5eb7\u306e\u5f71\u97ff\u3092\u5236\u5fa1\u3057\u305f\u4e0a\u3067\u306e\u8eab\u4f53\u7684\u5065\u5eb7\u3068\u8840\u5727\u306e\u95a2\u4fc2\u300d\u306e\u3088\u3046\u306a\u3001\u3088\u308a\u7d14\u7c8b\u306a\u95a2\u4fc2\u3092\u793a\u3059</p>"},{"location":"lectures/LA/44-factor-analysis/#5","title":"5. \u5171\u901a\u6027\u3068\u72ec\u81ea\u6027","text":""},{"location":"lectures/LA/44-factor-analysis/#51","title":"5.1 \u5171\u901a\u6027\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \u5171\u901a\u6027\uff08communality\uff09\u3068\u306f\u3001\u3042\u308b\u89b3\u6e2c\u5909\u6570\u306e\u5206\u6563\u306e\u3046\u3061\u3001\u3059\u3079\u3066\u306e\u5171\u901a\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u90e8\u5206\u306e\u5272\u5408\u3067\u3059\u3002</p> <p>\u308f\u304b\u308a\u3084\u3059\u304f\u8a00\u3048\u3070\u3001\u5171\u901a\u6027\u306f\u300c\u305d\u306e\u5909\u6570\u304c\u3069\u308c\u3060\u3051\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u3066\u3044\u308b\u304b\u300d\u3092\u793a\u3059\u6307\u6a19\u3067\u3059\u3002</p> <p>\u6570\u5b66\u7684\u306b\u306f\u3001\u89b3\u6e2c\u5909\u6570 \\(X_i\\) \u306e\u5171\u901a\u6027 \\(h_i^2\\) \u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[h_i^2 = \\sum_{j=1}^{m} \\lambda_{ij}^2\\] <p>\u3064\u307e\u308a\u3001\u3059\u3079\u3066\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u306e2\u4e57\u548c\u304c\u5171\u901a\u6027\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u8840\u5727\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u304c\u300c\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u306b\u5bfe\u3057\u30660.8\u3001\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50\u306b\u5bfe\u3057\u30660.1\u300d\u3067\u3042\u308b\u5834\u5408\uff1a \\(\\(h_1^2 = 0.8^2 + 0.1^2 = 0.64 + 0.01 = 0.65\\)\\)</p> <p>\u3053\u308c\u306f\u3001\u8840\u5727\u306e\u5206\u6563\u306e65%\u304c\u5171\u901a\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#52","title":"5.2 \u72ec\u81ea\u6027\u306e\u5b9a\u7fa9","text":"<p>\u5b9a\u7fa9: \u72ec\u81ea\u6027\uff08uniqueness\uff09\u3068\u306f\u3001\u3042\u308b\u89b3\u6e2c\u5909\u6570\u306e\u5206\u6563\u306e\u3046\u3061\u3001\u5171\u901a\u56e0\u5b50\u3067\u306f\u8aac\u660e\u3055\u308c\u306a\u3044\u90e8\u5206\u306e\u5272\u5408\u3067\u3059\u3002</p> <p>\u72ec\u81ea\u6027\u306f\u3001\u305d\u306e\u5909\u6570\u306b\u56fa\u6709\u306e\u5909\u52d5\u3084\u6e2c\u5b9a\u8aa4\u5dee\u306b\u3088\u308b\u90e8\u5206\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u6570\u5b66\u7684\u306b\u306f\u3001\u89b3\u6e2c\u5909\u6570 \\(X_i\\) \u306e\u72ec\u81ea\u6027 \\(\\psi_i\\) \u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u8868\u3055\u308c\u307e\u3059\uff1a</p> \\[\\psi_i = 1 - h_i^2\\] <p>\u6a19\u6e96\u5316\u3055\u308c\u305f\u5909\u6570\u3092\u7528\u3044\u308b\u5834\u5408\u3001\u5909\u6570\u306e\u5168\u5206\u6563\u306f1\u306a\u306e\u3067\u3001\u5171\u901a\u6027\u3068\u72ec\u81ea\u6027\u306e\u548c\u306f1\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u5148\u307b\u3069\u306e\u8840\u5727\u306e\u4f8b\u3067\u306f\uff1a \\(\\(\\psi_1 = 1 - 0.65 = 0.35\\)\\)</p> <p>\u3053\u308c\u306f\u3001\u8840\u5727\u306e\u5206\u6563\u306e35%\u304c\u5171\u901a\u56e0\u5b50\u3067\u306f\u8aac\u660e\u3055\u308c\u305a\u3001\u8840\u5727\u306b\u56fa\u6709\u306e\u8981\u56e0\uff08\u4f8b\uff1a\u6e2c\u5b9a\u8aa4\u5dee\u3001\u500b\u4eba\u7279\u6709\u306e\u5909\u52d5\u306a\u3069\uff09\u306b\u3088\u308b\u3082\u306e\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#53","title":"5.3 \u5171\u901a\u56e0\u5b50\u3068\u72ec\u81ea\u56e0\u5b50","text":"<p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u3067\u306f\u3001\u89b3\u6e2c\u5909\u6570\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u8981\u56e0\u30922\u7a2e\u985e\u306b\u5206\u3051\u3066\u8003\u3048\u307e\u3059\uff1a</p> <ol> <li> <p>\u5171\u901a\u56e0\u5b50\uff08Common Factors\uff09: \u8907\u6570\u306e\u89b3\u6e2c\u5909\u6570\u306b\u5171\u901a\u3057\u3066\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u56e0\u5b50    \u2192 \u4f8b\uff1a\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u306f\u8840\u5727\u3001\u4f53\u91cd\u3001\u5fc3\u62cd\u6570\u306a\u3069\u306e\u8907\u6570\u306e\u5909\u6570\u306b\u5f71\u97ff</p> </li> <li> <p>\u72ec\u81ea\u56e0\u5b50\uff08Unique Factors\uff09: \u7279\u5b9a\u306e\u89b3\u6e2c\u5909\u6570\u3060\u3051\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u56e0\u5b50    \u2192 \u4f8b\uff1a\u8840\u5727\u306e\u6e2c\u5b9a\u65b9\u6cd5\u306b\u95a2\u9023\u3059\u308b\u8aa4\u5dee\u3001\u8840\u5727\u306b\u7279\u6709\u306e\u5909\u52d5\u8981\u56e0</p> </li> </ol> <p>\u72ec\u81ea\u56e0\u5b50\u306f\u3055\u3089\u306b\u4ee5\u4e0b\u306e2\u3064\u306b\u5206\u3051\u3089\u308c\u307e\u3059\uff1a - \u7279\u6b8a\u56e0\u5b50\uff08Specific Factors\uff09: \u305d\u306e\u5909\u6570\u7279\u6709\u306e\u7cfb\u7d71\u7684\u306a\u5f71\u97ff   \u2192 \u4f8b\uff1a\u7279\u5b9a\u306e\u8840\u5727\u8a08\u306e\u7279\u6027\u306b\u3088\u308b\u7cfb\u7d71\u7684\u306a\u504f\u308a</p> <ul> <li>\u8aa4\u5dee\u56e0\u5b50\uff08Error Factors\uff09: \u6e2c\u5b9a\u8aa4\u5dee\u306a\u3069\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u5f71\u97ff   \u2192 \u4f8b\uff1a\u6e2c\u5b9a\u6642\u306e\u74b0\u5883\u6761\u4ef6\u306b\u3088\u308b\u30e9\u30f3\u30c0\u30e0\u306a\u5909\u52d5</li> </ul> <p>\u901a\u5e38\u3001\u56e0\u5b50\u5206\u6790\u3067\u306f\u3053\u308c\u3089\u3092\u533a\u5225\u305b\u305a\u3001\u307e\u3068\u3081\u3066\u72ec\u81ea\u56e0\u5b50\u3068\u3057\u3066\u6271\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#54","title":"5.4 \u5171\u901a\u6027\u306e\u8a08\u7b97\u4f8b","text":"<p>5\u3064\u306e\u5065\u5eb7\u6307\u6a19\u306b\u5bfe\u3059\u308b3\u3064\u306e\u56e0\u5b50\u306e\u8ca0\u8377\u91cf\u884c\u5217\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3068\u3057\u307e\u3059\uff1a</p> <pre><code>\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217 \u039b:\n       F1    F2    F3\nX1   0.80  0.10  0.20  (\u8840\u5727)\nX2   0.75  0.30  0.15  (\u5fc3\u62cd\u6570)\nX3   0.20  0.70  0.40  (\u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb)\nX4   0.10  0.65  0.50  (\u7761\u7720\u306e\u8cea)\nX5   0.30  0.25  0.85  (\u904b\u52d5\u91cf)\n</code></pre> <p>\u5404\u5909\u6570\u306e\u5171\u901a\u6027\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> <p>\u8840\u5727(X1)\u306e\u5171\u901a\u6027: \\(h_1^2 = 0.80^2 + 0.10^2 + 0.20^2 = 0.64 + 0.01 + 0.04 = 0.69\\)</p> <p>\u5fc3\u62cd\u6570(X2)\u306e\u5171\u901a\u6027: \\(h_2^2 = 0.75^2 + 0.30^2 + 0.15^2 = 0.56 + 0.09 + 0.02 = 0.67\\)</p> <p>\u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb(X3)\u306e\u5171\u901a\u6027: \\(h_3^2 = 0.20^2 + 0.70^2 + 0.40^2 = 0.04 + 0.49 + 0.16 = 0.69\\)</p> <p>\u7761\u7720\u306e\u8cea(X4)\u306e\u5171\u901a\u6027: \\(h_4^2 = 0.10^2 + 0.65^2 + 0.50^2 = 0.01 + 0.42 + 0.25 = 0.68\\)</p> <p>\u904b\u52d5\u91cf(X5)\u306e\u5171\u901a\u6027: \\(h_5^2 = 0.30^2 + 0.25^2 + 0.85^2 = 0.09 + 0.06 + 0.72 = 0.87\\)</p> <p>\u5404\u5909\u6570\u306e\u72ec\u81ea\u6027\u3082\u8a08\u7b97\u3067\u304d\u307e\u3059\uff1a</p> <p>\u8840\u5727(X1)\u306e\u72ec\u81ea\u6027: \\(\\psi_1 = 1 - 0.69 = 0.31\\) \u5fc3\u62cd\u6570(X2)\u306e\u72ec\u81ea\u6027: \\(\\psi_2 = 1 - 0.67 = 0.33\\) \u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb(X3)\u306e\u72ec\u81ea\u6027: \\(\\psi_3 = 1 - 0.69 = 0.31\\) \u7761\u7720\u306e\u8cea(X4)\u306e\u72ec\u81ea\u6027: \\(\\psi_4 = 1 - 0.68 = 0.32\\) \u904b\u52d5\u91cf(X5)\u306e\u72ec\u81ea\u6027: \\(\\psi_5 = 1 - 0.87 = 0.13\\)</p> <p>\u3053\u306e\u7d50\u679c\u304b\u3089\u3001\u904b\u52d5\u91cf(X5)\u306e\u5206\u6563\u306f\u5171\u901a\u56e0\u5b50\u306b\u3088\u3063\u306687%\u8aac\u660e\u3055\u308c\u3066\u304a\u308a\uff08\u5171\u901a\u6027\u304c0.87\uff09\u3001\u6700\u3082\u5171\u901a\u6027\u304c\u9ad8\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u4e00\u65b9\u3001\u5fc3\u62cd\u6570(X2)\u306f\u5171\u901a\u6027\u304c\u6700\u3082\u4f4e\u304f\uff080.67\uff09\u3001\u72ec\u81ea\u56e0\u5b50\u306b\u3088\u308b\u8aac\u660e\u304c\u6bd4\u8f03\u7684\u591a\u3044\uff0833%\uff09\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#6-python","title":"6. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/44-factor-analysis/#61-scikit-learn","title":"6.1 scikit-learn\u306b\u3088\u308b\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u88c5","text":"<p>\u4ee5\u4e0b\u306b\u3001scikit-learn\u3092\u7528\u3044\u305f\u56e0\u5b50\u5206\u6790\u306e\u57fa\u672c\u7684\u306a\u5b9f\u88c5\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002\u3053\u306e\u4f8b\u3067\u306f\u3001\u5065\u5eb7\u95a2\u9023\u306e5\u3064\u306e\u5909\u6570\u306b\u5bfe\u3057\u3066\u56e0\u5b50\u5206\u6790\u3092\u9069\u7528\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import FactorAnalysis\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\uff08\u5065\u5eb7\u95a2\u9023\u306e5\u3064\u306e\u5909\u6570\uff09\nnp.random.seed(42)  # \u7d50\u679c\u3092\u518d\u73fe\u53ef\u80fd\u306b\u3059\u308b\u305f\u3081\u306e\u30b7\u30fc\u30c9\u8a2d\u5b9a\nn_samples = 200  # \u30b5\u30f3\u30d7\u30eb\u6570\n\n# 3\u3064\u306e\u6f5c\u5728\u56e0\u5b50\u3092\u751f\u6210\uff08\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3067\u306f\u89b3\u6e2c\u3067\u304d\u306a\u3044\uff09\nF1 = np.random.normal(0, 1, n_samples)  # \u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\nF2 = np.random.normal(0, 1, n_samples)  # \u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50\nF3 = np.random.normal(0, 1, n_samples)  # \u751f\u6d3b\u7fd2\u6163\u56e0\u5b50\n\n# \u89b3\u6e2c\u5909\u6570\u306e\u751f\u6210\uff08\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3067\u89b3\u6e2c\u3067\u304d\u308b\u90e8\u5206\uff09\ne = np.random.normal(0, 0.3, (n_samples, 5))  # \u8aa4\u5dee\u9805\uff08\u72ec\u81ea\u56e0\u5b50\uff09\n\n# \u89b3\u6e2c\u5909\u6570 = \u56e0\u5b50\u8ca0\u8377\u91cf \u00d7 \u6f5c\u5728\u56e0\u5b50 + \u8aa4\u5dee\nX = np.zeros((n_samples, 5))\nX[:, 0] = 0.8 * F1 + 0.1 * F2 + 0.2 * F3 + e[:, 0]  # \u8840\u5727\nX[:, 1] = 0.75 * F1 + 0.3 * F2 + 0.15 * F3 + e[:, 1]  # \u5fc3\u62cd\u6570\nX[:, 2] = 0.2 * F1 + 0.7 * F2 + 0.4 * F3 + e[:, 2]  # \u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb\nX[:, 3] = 0.1 * F1 + 0.65 * F2 + 0.5 * F3 + e[:, 3]  # \u7761\u7720\u306e\u8cea\nX[:, 4] = 0.3 * F1 + 0.25 * F2 + 0.85 * F3 + e[:, 4]  # \u904b\u52d5\u91cf\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210\ncolumns = ['\u8840\u5727', '\u5fc3\u62cd\u6570', '\u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb', '\u7761\u7720\u306e\u8cea', '\u904b\u52d5\u91cf']\ndf = pd.DataFrame(X, columns=columns)\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\uff08\u56e0\u5b50\u5206\u6790\u306e\u524d\u51e6\u7406\u3068\u3057\u3066\u91cd\u8981\uff09\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# \u56e0\u5b50\u5206\u6790\u306e\u5b9f\u884c\nn_factors = 3  # \u56e0\u5b50\u6570\u306e\u6307\u5b9a\nfa = FactorAnalysis(n_components=n_factors, random_state=42)\nfa.fit(X_scaled)\n\n# \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u53d6\u5f97\nloadings = fa.components_.T  # \u8ee2\u7f6e\u3057\u3066\u89b3\u6e2c\u5909\u6570\u00d7\u56e0\u5b50\u306e\u5f62\u5f0f\u306b\n\n# \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u8868\u793a\nloadings_df = pd.DataFrame(loadings, index=columns, \n                        columns=[f'Factor {i+1}' for i in range(n_factors)])\nprint(\"\u56e0\u5b50\u8ca0\u8377\u91cf:\")\nprint(loadings_df)\n\n# \u5171\u901a\u6027\u306e\u8a08\u7b97\uff08\u56e0\u5b50\u8ca0\u8377\u91cf\u306e2\u4e57\u548c\uff09\ncommunalities = np.sum(loadings**2, axis=1)\nuniquenesses = 1 - communalities  # \u72ec\u81ea\u6027 = 1 - \u5171\u901a\u6027\n\n# \u5171\u901a\u6027\u3068\u72ec\u81ea\u6027\u306e\u8868\u793a\nvariance_df = pd.DataFrame({\n    '\u5171\u901a\u6027': communalities,\n    '\u72ec\u81ea\u6027': uniquenesses\n}, index=columns)\nprint(\"\\n\u5171\u901a\u6027\u3068\u72ec\u81ea\u6027:\")\nprint(variance_df)\n\n# \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u8868\u793a\uff08\u8996\u899a\u7684\u306a\u7406\u89e3\u3092\u52a9\u3051\u308b\uff09\nplt.figure(figsize=(10, 6))\nsns.heatmap(loadings_df, annot=True, cmap='coolwarm', center=0, fmt='.2f')\nplt.title('\u56e0\u5b50\u8ca0\u8377\u91cf\u30d2\u30fc\u30c8\u30de\u30c3\u30d7')\nplt.tight_layout()\nplt.show()\n\n# 2D\u56e0\u5b50\u7a7a\u9593\u306b\u304a\u3051\u308b\u5909\u6570\u306e\u30d7\u30ed\u30c3\u30c8\uff08\u6700\u521d\u306e2\u56e0\u5b50\u306e\u307f\uff09\nplt.figure(figsize=(8, 8))\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)  # x\u8ef8\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)  # y\u8ef8\n\n# \u5404\u5909\u6570\u3092\u56e0\u5b50\u7a7a\u9593\u306b\u77e2\u5370\u3067\u30d7\u30ed\u30c3\u30c8\nfor i, variable in enumerate(columns):\n    plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], head_width=0.05, head_length=0.05, \n              fc='blue', ec='blue', length_includes_head=True)\n    plt.text(loadings[i, 0]*1.15, loadings[i, 1]*1.15, variable, color='red', ha='center', va='center')\n\nplt.xlim(-1, 1)  # x\u8ef8\u306e\u7bc4\u56f2\nplt.ylim(-1, 1)  # y\u8ef8\u306e\u7bc4\u56f2\nplt.xlabel('Factor 1')  # x\u8ef8\u30e9\u30d9\u30eb\nplt.ylabel('Factor 2')  # y\u8ef8\u30e9\u30d9\u30eb\nplt.grid(True)  # \u30b0\u30ea\u30c3\u30c9\u8868\u793a\nplt.title('2\u6b21\u5143\u56e0\u5b50\u7a7a\u9593\u306b\u304a\u3051\u308b\u5909\u6570\u306e\u4f4d\u7f6e')  # \u30bf\u30a4\u30c8\u30eb\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\u3053\u306e\u30b3\u30fc\u30c9\u306e\u8aac\u660e\uff1a</p> <ol> <li> <p>\u307e\u305a\u30013\u3064\u306e\u6f5c\u5728\u56e0\u5b50\uff08\u8eab\u4f53\u7684\u5065\u5eb7\u3001\u7cbe\u795e\u7684\u5065\u5eb7\u3001\u751f\u6d3b\u7fd2\u6163\uff09\u30685\u3064\u306e\u89b3\u6e2c\u5909\u6570\uff08\u8840\u5727\u3001\u5fc3\u62cd\u6570\u306a\u3069\uff09\u304b\u3089\u306a\u308b\u4eba\u5de5\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3066\u3044\u307e\u3059\u3002</p> </li> <li> <p>\u5404\u89b3\u6e2c\u5909\u6570\u306f\u30013\u3064\u306e\u6f5c\u5728\u56e0\u5b50\u306e\u7dda\u5f62\u7d50\u5408\u306b\u8aa4\u5dee\u9805\u3092\u52a0\u3048\u305f\u3082\u306e\u3068\u3057\u3066\u751f\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u8840\u5727\u306f\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u3068\u306e\u95a2\u9023\u304c\u5f37\u304f\uff080.8\uff09\u3001\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50\u3068\u306e\u95a2\u9023\u304c\u5f31\u3044\uff080.1\uff09\u3068\u3044\u3046\u8a2d\u5b9a\u3067\u3059\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u3092\u6a19\u6e96\u5316\u3057\u305f\u5f8c\u3001scikit-learn\u306eFactorAnalysis\u30af\u30e9\u30b9\u3092\u4f7f\u7528\u3057\u3066\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u884c\u3057\u3066\u3044\u307e\u3059\u3002</p> </li> <li> <p>\u5206\u6790\u7d50\u679c\u304b\u3089\u56e0\u5b50\u8ca0\u8377\u91cf\u3092\u53d6\u5f97\u3057\u3001\u5171\u901a\u6027\u3068\u72ec\u81ea\u6027\u3092\u8a08\u7b97\u3057\u3066\u3044\u307e\u3059\u3002</p> </li> <li> <p>\u6700\u5f8c\u306b\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u30682\u6b21\u5143\u56e0\u5b50\u7a7a\u9593\u306b\u304a\u3051\u308b\u5909\u6570\u306e\u4f4d\u7f6e\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5404\u5909\u6570\u3068\u56e0\u5b50\u306e\u95a2\u4fc2\u3092\u8996\u899a\u7684\u306b\u628a\u63e1\u3067\u304d\u307e\u3059\u3002</p> </li> </ol>"},{"location":"lectures/LA/44-factor-analysis/#62","title":"6.2 \u51fa\u529b\u7d50\u679c\u3068\u89e3\u91c8","text":"<p>\u4e0a\u8a18\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u5f97\u3089\u308c\u307e\u3059\uff1a</p> <pre><code>\u56e0\u5b50\u8ca0\u8377\u91cf:\n             Factor 1  Factor 2  Factor 3\n\u8840\u5727            0.82      0.12      0.18\n\u5fc3\u62cd\u6570          0.73      0.32      0.14\n\u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb  0.18      0.71      0.38\n\u7761\u7720\u306e\u8cea        0.09      0.67      0.48\n\u904b\u52d5\u91cf          0.31      0.24      0.83\n\n\u5171\u901a\u6027\u3068\u72ec\u81ea\u6027:\n           \u5171\u901a\u6027    \u72ec\u81ea\u6027\n\u8840\u5727        0.72     0.28\n\u5fc3\u62cd\u6570      0.65     0.35\n\u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb 0.68     0.32\n\u7761\u7720\u306e\u8cea     0.68     0.32\n\u904b\u52d5\u91cf       0.83     0.17\n</code></pre> <p>\u3053\u306e\u7d50\u679c\u304b\u3089\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u8aad\u307f\u53d6\u308c\u307e\u3059\uff1a</p> <ol> <li> <p>Factor 1\uff08\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\uff09: \u8840\u5727\uff080.82\uff09\u3068\u5fc3\u62cd\u6570\uff080.73\uff09\u306b\u5f37\u304f\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u660e\u3089\u304b\u306b\u8eab\u4f53\u7684\u5065\u5eb7\u3092\u8868\u3059\u56e0\u5b50\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>Factor 2\uff08\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50\uff09: \u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb\uff080.71\uff09\u3068\u7761\u7720\u306e\u8cea\uff080.67\uff09\u306b\u5f37\u304f\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u7cbe\u795e\u7684\u5065\u5eb7\u3084\u5fc3\u7406\u7684\u8981\u56e0\u3092\u8868\u3059\u56e0\u5b50\u3068\u89e3\u91c8\u3067\u304d\u308b\u3067\u3057\u3087\u3046\u3002</p> </li> <li> <p>Factor 3\uff08\u751f\u6d3b\u7fd2\u6163\u56e0\u5b50\uff09: \u904b\u52d5\u91cf\uff080.83\uff09\u306b\u7279\u306b\u5f37\u304f\u95a2\u9023\u3057\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb\uff080.38\uff09\u3084\u7761\u7720\u306e\u8cea\uff080.48\uff09\u3068\u3082\u4e2d\u7a0b\u5ea6\u306e\u95a2\u9023\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u751f\u6d3b\u7fd2\u6163\u3084\u6d3b\u52d5\u306b\u95a2\u9023\u3059\u308b\u56e0\u5b50\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u5171\u901a\u6027\u3068\u72ec\u81ea\u6027\u3092\u898b\u308b\u3068\uff1a</p> </li> <li>\u904b\u52d5\u91cf\u306e\u5171\u901a\u6027\u304c\u6700\u3082\u9ad8\u304f\uff080.83\uff09\u30013\u3064\u306e\u56e0\u5b50\u306b\u3088\u3063\u3066\u5206\u6563\u306e83%\u304c\u8aac\u660e\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u6b8b\u308a\u306e17%\u306f\u904b\u52d5\u91cf\u306b\u56fa\u6709\u306e\u8981\u56e0\u3067\u3059\u3002</li> <li>\u5fc3\u62cd\u6570\u306e\u5171\u901a\u6027\u304c\u6700\u3082\u4f4e\u304f\uff080.65\uff09\u300135%\u306f\u5171\u901a\u56e0\u5b50\u3067\u306f\u8aac\u660e\u3067\u304d\u306a\u3044\u72ec\u81ea\u306e\u5909\u52d5\u3067\u3059\u3002</li> </ol> <p>\u56e0\u5b50\u5206\u6790\u306e\u7d50\u679c\u3092\u89e3\u91c8\u3059\u308b\u969b\u306f\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u898b\u3066\u56e0\u5b50\u306e\u300c\u540d\u524d\u4ed8\u3051\u300d\u3092\u884c\u3046\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u8840\u5727\u3068\u5fc3\u62cd\u6570\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064Factor 1\u306f\u300c\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u300d\u3068\u89e3\u91c8\u3059\u308b\u306e\u304c\u81ea\u7136\u3067\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#63","title":"6.3 \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u53ef\u8996\u5316","text":"<p>\u56e0\u5b50\u8ca0\u8377\u91cf\u306f\u3001\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u30682\u6b21\u5143\u306e\u56e0\u5b50\u7a7a\u9593\u30d7\u30ed\u30c3\u30c8\u3067\u53ef\u8996\u5316\u3059\u308b\u3068\u7406\u89e3\u3057\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002</p> <p>\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\uff1a\u5404\u30bb\u30eb\u306e\u8272\u304c\u6fc3\u3044\u307b\u3069\u3001\u305d\u306e\u5909\u6570\u3068\u56e0\u5b50\u306e\u95a2\u9023\u304c\u5f37\u3044\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u8840\u5727\u3068Factor 1\u306e\u4ea4\u70b9\u304c\u6fc3\u3044\u8d64\u3067\u3042\u308c\u3070\u3001\u5f37\u3044\u6b63\u306e\u95a2\u9023\u304c\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> <p>2\u6b21\u5143\u56e0\u5b50\u7a7a\u9593\u30d7\u30ed\u30c3\u30c8\uff1a\u6700\u521d\u306e2\u3064\u306e\u56e0\u5b50\uff08Factor 1\u3068Factor 2\uff09\u306e\u7a7a\u9593\u306b\u304a\u3051\u308b\u5404\u5909\u6570\u306e\u4f4d\u7f6e\u3092\u793a\u3057\u307e\u3059\u3002\u539f\u70b9\u304b\u3089\u306e\u8ddd\u96e2\u304c\u5927\u304d\u3044\u307b\u3069\u3001\u305d\u306e\u56e0\u5b50\u3068\u306e\u95a2\u9023\u304c\u5f37\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> <ul> <li>\u53f3\u5074\uff08Factor 1\u8ef8\u306e\u6b63\u306e\u65b9\u5411\uff09\u306b\u4f4d\u7f6e\u3059\u308b\u5909\u6570\u306f\u3001Factor 1\u3068\u6b63\u306e\u95a2\u9023\u304c\u3042\u308a\u307e\u3059\uff08\u4f8b\uff1a\u8840\u5727\u3001\u5fc3\u62cd\u6570\uff09</li> <li>\u4e0a\u5074\uff08Factor 2\u8ef8\u306e\u6b63\u306e\u65b9\u5411\uff09\u306b\u4f4d\u7f6e\u3059\u308b\u5909\u6570\u306f\u3001Factor 2\u3068\u6b63\u306e\u95a2\u9023\u304c\u3042\u308a\u307e\u3059\uff08\u4f8b\uff1a\u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb\u3001\u7761\u7720\u306e\u8cea\uff09</li> <li>\u5909\u6570\u9593\u306e\u89d2\u5ea6\u304c\u5c0f\u3055\u3044\u307b\u3069\u3001\u305d\u308c\u3089\u306e\u5909\u6570\u9593\u306e\u76f8\u95a2\u304c\u9ad8\u3044\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059</li> </ul> <p>\u3053\u306e\u3088\u3046\u306b\u53ef\u8996\u5316\u3092\u884c\u3046\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u6f5c\u5728\u69cb\u9020\u3092\u76f4\u611f\u7684\u306b\u7406\u89e3\u3057\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/44-factor-analysis/#_2","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u56e0\u5b50\u5206\u6790\u3068\u4e3b\u6210\u5206\u5206\u6790\u306e\u4e3b\u306a\u9055\u3044\u30923\u3064\u6319\u3052\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u4ee5\u4e0b\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\u306b\u3064\u3044\u3066\u3001\u5404\u5909\u6570\u306e\u5171\u901a\u6027\u3068\u72ec\u81ea\u6027\u3092\u8a08\u7b97\u3057\u3066\u304f\u3060\u3055\u3044\u3002    <pre><code>\u5909\u6570    \u56e0\u5b501   \u56e0\u5b502\nX1      0.70    0.30\nX2      0.60    0.50\nX3      0.20    0.80\n</code></pre></p> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u306b\u304a\u3051\u308b\u4ee5\u4e0b\u306e\u7528\u8a9e\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\uff1a</p> </li> <li>\u6f5c\u5728\u5909\u6570</li> <li>\u56e0\u5b50\u8ca0\u8377\u91cf</li> <li>\u5171\u901a\u6027</li> <li> <p>\u72ec\u81ea\u6027</p> </li> <li> <p>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u5024\u304c0.8\u30010.4\u30010.1\u306e\u5834\u5408\u3001\u305d\u308c\u305e\u308c\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3057\u307e\u3059\u304b\uff1f</p> </li> </ol>"},{"location":"lectures/LA/44-factor-analysis/#_3","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li> <p>6\u3064\u306e\u5909\u6570\u304b\u3089\u306a\u308b\u76f8\u95a2\u884c\u5217\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4e0e\u3048\u3089\u308c\u305f\u5834\u5408\u3001\u56e0\u5b50\u5206\u6790\u3092\u9069\u7528\u3059\u308b\u969b\u306e\u9069\u5207\u306a\u56e0\u5b50\u6570\u3092\u3069\u306e\u3088\u3046\u306b\u6c7a\u5b9a\u3057\u307e\u3059\u304b\uff1f\u307e\u305f\u3001\u305d\u306e\u7406\u7531\u3092\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002    <pre><code>\u76f8\u95a2\u884c\u5217:\n  X1   X2   X3   X4   X5   X6\nX1 1.0 0.7 0.6 0.2 0.1 0.3\nX2 0.7 1.0 0.5 0.3 0.2 0.2\nX3 0.6 0.5 1.0 0.3 0.2 0.3\nX4 0.2 0.3 0.3 1.0 0.8 0.7\nX5 0.1 0.2 0.2 0.8 1.0 0.6\nX6 0.3 0.2 0.3 0.7 0.6 1.0\n</code></pre></p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u6587\u8108\u3067\u3001\u56e0\u5b50\u5206\u6790\u304c\u3069\u306e\u3088\u3046\u306b\u5fdc\u7528\u3055\u308c\u308b\u304b\u5177\u4f53\u4f8b\u3092\u6319\u3052\u3066\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7279\u306b\u3001\u591a\u6570\u306e\u5065\u5eb7\u6307\u6a19\u304b\u3089\u6f5c\u5728\u7684\u306a\u5065\u5eb7\u72b6\u614b\u3092\u63a8\u5b9a\u3059\u308b\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/44-factor-analysis/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/44-factor-analysis/#q1","title":"Q1: \u56e0\u5b50\u5206\u6790\u3067\u306f\u56e0\u5b50\u6570\u3092\u3069\u306e\u3088\u3046\u306b\u6c7a\u5b9a\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A1: \u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3059\u308b\u65b9\u6cd5\u3068\u3057\u3066\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u30ab\u30a4\u30b6\u30fc\u57fa\u6e96\uff08Kaiser criterion\uff09: \u56fa\u6709\u5024\u304c1\u4ee5\u4e0a\u306e\u56e0\u5b50\u3092\u63a1\u7528\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u30011\u3064\u306e\u56e0\u5b50\u304c\u5c11\u306a\u304f\u3068\u30821\u3064\u306e\u5909\u6570\u5206\u306e\u60c5\u5831\u3092\u542b\u3093\u3067\u3044\u308b\u3079\u304d\u3068\u3044\u3046\u8003\u3048\u65b9\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a\u56fa\u6709\u5024\u304c [2.5, 1.8, 1.2, 0.8, 0.5, 0.2] \u3067\u3042\u308c\u3070\u3001\u6700\u521d\u306e3\u3064\u306e\u56e0\u5b50\u3092\u63a1\u7528\u3057\u307e\u3059\u3002</p> <ol> <li>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\uff08Scree plot\uff09: \u56fa\u6709\u5024\u3092\u30b0\u30e9\u30d5\u306b\u30d7\u30ed\u30c3\u30c8\u3057\u3001\u6025\u6fc0\u306a\u6e1b\u5c11\uff08\u300c\u8098\u300d\uff09\u304c\u898b\u3089\u308c\u308b\u70b9\u3092\u8b58\u5225\u3057\u307e\u3059\u3002\u305d\u306e\u70b9\u3088\u308a\u524d\u306e\u56e0\u5b50\u3092\u63a1\u7528\u3057\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a\u56fa\u6709\u5024\u304c [4.2, 2.1, 0.9, 0.6, 0.5, 0.4] \u3068\u3044\u3046\u5834\u5408\u3001\u7b2c2\u56e0\u5b50\u3068\u7b2c3\u56e0\u5b50\u306e\u9593\u306b\u300c\u8098\u300d\u304c\u3042\u308b\u3068\u5224\u65ad\u3057\u30012\u3064\u306e\u56e0\u5b50\u3092\u63a1\u7528\u3057\u307e\u3059\u3002</p> <ol> <li>\u5e73\u884c\u5206\u6790\uff08Parallel Analysis\uff09: \u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u304b\u3089\u5f97\u3089\u308c\u308b\u56fa\u6709\u5024\u3068\u3001\u540c\u3058\u30b5\u30a4\u30ba\u306e\u30e9\u30f3\u30c0\u30e0\u30c7\u30fc\u30bf\u304b\u3089\u5f97\u3089\u308c\u308b\u56fa\u6709\u5024\u3092\u6bd4\u8f03\u3057\u307e\u3059\u3002\u5b9f\u969b\u306e\u56fa\u6709\u5024\u304c\u30e9\u30f3\u30c0\u30e0\u30c7\u30fc\u30bf\u306e\u56fa\u6709\u5024\u3092\u4e0a\u56de\u308b\u56e0\u5b50\u306e\u307f\u3092\u63a1\u7528\u3057\u307e\u3059\u3002</li> </ol> <p>\u3053\u308c\u306f\u3001\u5358\u306a\u308b\u5076\u7136\u3092\u8d85\u3048\u3066\u610f\u5473\u306e\u3042\u308b\u69cb\u9020\u3092\u6301\u3064\u56e0\u5b50\u3092\u7279\u5b9a\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002</p> <ol> <li>\u7d2f\u7a4d\u5bc4\u4e0e\u7387\uff08Cumulative Proportion of Variance\uff09: \u56e0\u5b50\u304c\u5168\u4f53\u306e\u5206\u6563\u306e\u4e00\u5b9a\u5272\u5408\uff08\u4f8b\u3048\u307070%\uff09\u3092\u8aac\u660e\u3059\u308b\u307e\u3067\u63a1\u7528\u3057\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a3\u56e0\u5b50\u306765%\u30014\u56e0\u5b50\u306772%\u306e\u5206\u6563\u3092\u8aac\u660e\u3059\u308b\u5834\u5408\u300170%\u3092\u57fa\u6e96\u3068\u3059\u308c\u30704\u56e0\u5b50\u3092\u63a1\u7528\u3057\u307e\u3059\u3002</p> <ol> <li>\u89e3\u91c8\u53ef\u80fd\u6027\uff08Interpretability\uff09: \u6700\u7d42\u7684\u306b\u306f\u3001\u5404\u56e0\u5b50\u304c\u7406\u8ad6\u7684\u306b\u610f\u5473\u3092\u6301\u3061\u3001\u89e3\u91c8\u53ef\u80fd\u3067\u3042\u308b\u304b\u3092\u8003\u616e\u3057\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a5\u56e0\u5b50\u30e2\u30c7\u30eb\u306e\u65b9\u304c\u7d71\u8a08\u7684\u306b\u306f\u826f\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u30015\u3064\u76ee\u306e\u56e0\u5b50\u304c\u610f\u5473\u3092\u6301\u305f\u306a\u3044\u5834\u5408\u306f4\u56e0\u5b50\u30e2\u30c7\u30eb\u3092\u9078\u3076\u3053\u3068\u304c\u9069\u5207\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p> <p>\u5b9f\u969b\u306e\u5206\u6790\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u57fa\u6e96\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u7dcf\u5408\u7684\u306b\u5224\u65ad\u3059\u308b\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u307e\u3059\u3002\u3069\u306e\u65b9\u6cd5\u3082\u5b8c\u74a7\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u7814\u7a76\u306e\u76ee\u7684\u3084\u7406\u8ad6\u7684\u80cc\u666f\u3082\u8003\u616e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#q2","title":"Q2: \u56e0\u5b50\u5206\u6790\u3068\u4e3b\u6210\u5206\u5206\u6790\u306f\u3069\u3061\u3089\u3092\u9078\u3076\u3079\u304d\u3067\u3059\u304b\uff1f","text":"<p>A2: \u4ee5\u4e0b\u306e\u57fa\u6e96\u3067\u9078\u629e\u3092\u691c\u8a0e\u3059\u308b\u3068\u3088\u3044\u3067\u3057\u3087\u3046\uff1a</p> <ul> <li> <p>\u76ee\u7684\u304c\u6b21\u5143\u524a\u6e1b\u3084\u5727\u7e2e \u2192 \u4e3b\u6210\u5206\u5206\u6790   \u4f8b\uff1a100\u7a2e\u985e\u306e\u5065\u5eb7\u6307\u6a19\u309210\u500b\u7a0b\u5ea6\u306e\u4e3b\u6210\u5206\u306b\u8981\u7d04\u3057\u3066\u30c7\u30fc\u30bf\u91cf\u3092\u6e1b\u3089\u3057\u305f\u3044\u5834\u5408</p> </li> <li> <p>\u76ee\u7684\u304c\u6f5c\u5728\u69cb\u9020\u306e\u63a2\u7d22 \u2192 \u56e0\u5b50\u5206\u6790   \u4f8b\uff1a\u5065\u5eb7\u8abf\u67fb\u306e\u8cea\u554f\u9805\u76ee\u306e\u80cc\u5f8c\u306b\u3042\u308b\u5065\u5eb7\u6982\u5ff5\u3092\u7279\u5b9a\u3057\u305f\u3044\u5834\u5408</p> </li> <li> <p>\u3059\u3079\u3066\u306e\u5206\u6563\uff08\u5171\u901a\u5206\u6563+\u72ec\u81ea\u5206\u6563\uff09\u304c\u91cd\u8981 \u2192 \u4e3b\u6210\u5206\u5206\u6790   \u4f8b\uff1a\u30c7\u30fc\u30bf\u306e\u5168\u4f53\u7684\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u5931\u308f\u305a\u306b\u8868\u73fe\u3057\u305f\u3044\u5834\u5408</p> </li> <li> <p>\u5171\u901a\u5206\u6563\u306e\u307f\u304c\u91cd\u8981 \u2192 \u56e0\u5b50\u5206\u6790   \u4f8b\uff1a\u5909\u6570\u9593\u306e\u76f8\u95a2\u95a2\u4fc2\u3092\u8aac\u660e\u3059\u308b\u6f5c\u5728\u56e0\u5b50\u3092\u898b\u3064\u3051\u305f\u3044\u5834\u5408</p> </li> <li> <p>\u5909\u6570\u306b\u6e2c\u5b9a\u8aa4\u5dee\u304c\u3042\u308a\u3001\u305d\u308c\u3092\u660e\u793a\u7684\u306b\u30e2\u30c7\u30eb\u5316\u3057\u305f\u3044 \u2192 \u56e0\u5b50\u5206\u6790   \u4f8b\uff1a\u8cea\u554f\u7d19\u8abf\u67fb\u306a\u3069\u3001\u6e2c\u5b9a\u8aa4\u5dee\u304c\u5927\u304d\u3044\u30c7\u30fc\u30bf\u3092\u6271\u3046\u5834\u5408</p> </li> </ul> <p>\u5177\u4f53\u4f8b\u3068\u3057\u3066\u3001\u5065\u5eb7\u8abf\u67fb\u30c7\u30fc\u30bf\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> <ul> <li>\u4e3b\u6210\u5206\u5206\u6790\u306e\u5834\u5408\uff1a\u300c\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u6700\u3082\u3088\u304f\u8868\u73fe\u3059\u308b\u5408\u6210\u5909\u6570\u306f\u4f55\u304b\uff1f\u300d\u3068\u3044\u3046\u554f\u3044\u306b\u7b54\u3048\u307e\u3059\u3002</li> <li>\u56e0\u5b50\u5206\u6790\u306e\u5834\u5408\uff1a\u300c\u3053\u308c\u3089\u306e\u5065\u5eb7\u6307\u6a19\u306e\u9593\u306e\u76f8\u95a2\u30d1\u30bf\u30fc\u30f3\u3092\u8aac\u660e\u3059\u308b\u6f5c\u5728\u7684\u306a\u5065\u5eb7\u72b6\u614b\u306e\u6b21\u5143\u306f\u4f55\u304b\uff1f\u300d\u3068\u3044\u3046\u554f\u3044\u306b\u7b54\u3048\u307e\u3059\u3002</li> </ul> <p>\u3069\u3061\u3089\u306e\u65b9\u6cd5\u3082\u4fa1\u5024\u304c\u3042\u308a\u3001\u7814\u7a76\u306e\u76ee\u7684\u306b\u3088\u3063\u3066\u9069\u5207\u306a\u9078\u629e\u304c\u5909\u308f\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#q3","title":"Q3: \u56e0\u5b50\u8ca0\u8377\u91cf\u304c\u8907\u6570\u306e\u56e0\u5b50\u3067\u9ad8\u3044\u5024\u3092\u793a\u3059\u5834\u5408\u3001\u3069\u3046\u89e3\u91c8\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A3: \u3053\u308c\u306f\u300c\u4ea4\u5dee\u8ca0\u8377\uff08cross-loading\uff09\u300d\u3068\u547c\u3070\u308c\u308b\u72b6\u6cc1\u3067\u3001\u5909\u6570\u304c\u8907\u6570\u306e\u56e0\u5b50\u3068\u95a2\u9023\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u300c\u904b\u52d5\u983b\u5ea6\u300d\u3068\u3044\u3046\u5909\u6570\u304c\u300c\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u300d\u3068\u300c\u751f\u6d3b\u7fd2\u6163\u56e0\u5b50\u300d\u306e\u4e21\u65b9\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u793a\u3059\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u3053\u306e\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u5bfe\u5fdc\u304c\u8003\u3048\u3089\u308c\u307e\u3059\uff1a</p> <ol> <li>\u56e0\u5b50\u56de\u8ee2\u3092\u9069\u7528\u3059\u308b: \u7279\u306bVarimax\u56de\u8ee2\u306a\u3069\u306e\u76f4\u4ea4\u56de\u8ee2\u3092\u9069\u7528\u3059\u308b\u3068\u3001\u5404\u5909\u6570\u304c\u4e00\u3064\u306e\u56e0\u5b50\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u793a\u3057\u3001\u4ed6\u306e\u56e0\u5b50\u3078\u306e\u8ca0\u8377\u91cf\u304c\u4f4e\u304f\u306a\u308b\u3088\u3046\u306a\u300c\u5358\u7d14\u69cb\u9020\u300d\u304c\u5f97\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a\u56de\u8ee2\u524d\u306f\u300c\u904b\u52d5\u983b\u5ea6\u300d\u304c\u56e0\u5b501\u306b0.6\u3001\u56e0\u5b502\u306b0.5\u306e\u8ca0\u8377\u91cf\u3060\u3063\u305f\u306e\u304c\u3001\u56de\u8ee2\u5f8c\u306f\u56e0\u5b501\u306b0.8\u3001\u56e0\u5b502\u306b0.2\u306b\u306a\u308b\u5834\u5408</p> <ol> <li>\u8907\u6570\u306e\u56e0\u5b50\u306b\u307e\u305f\u304c\u308b\u6982\u5ff5\u3068\u3057\u3066\u89e3\u91c8\u3059\u308b: \u305d\u306e\u5909\u6570\u304c\u672c\u8cea\u7684\u306b\u8907\u6570\u306e\u6982\u5ff5\u306b\u307e\u305f\u304c\u3063\u3066\u3044\u308b\u3082\u306e\u3068\u8003\u3048\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a\u300c\u7761\u7720\u306e\u8cea\u300d\u306f\u8eab\u4f53\u7684\u5065\u5eb7\u3068\u7cbe\u795e\u7684\u5065\u5eb7\u306e\u4e21\u65b9\u306b\u95a2\u9023\u3059\u308b\u6982\u5ff5\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p> <ol> <li>\u5909\u6570\u306e\u518d\u691c\u8a0e: \u7279\u306b\u8cea\u554f\u7d19\u8abf\u67fb\u306e\u5834\u5408\u3001\u305d\u306e\u9805\u76ee\u304c\u66d6\u6627\u3067\u8907\u6570\u306e\u6982\u5ff5\u3092\u6e2c\u5b9a\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a\u300c\u3042\u306a\u305f\u306e\u5168\u4f53\u7684\u306a\u5065\u5eb7\u72b6\u614b\u306f\u3069\u3046\u3067\u3059\u304b\uff1f\u300d\u3068\u3044\u3046\u8cea\u554f\u306f\u3001\u8eab\u4f53\u7684\u5065\u5eb7\u3068\u7cbe\u795e\u7684\u5065\u5eb7\u306e\u4e21\u65b9\u3092\u542b\u307f\u307e\u3059\u3002</p> <ol> <li>\u5909\u6570\u306e\u9664\u5916: \u5834\u5408\u306b\u3088\u3063\u3066\u306f\u3001\u4ea4\u5dee\u8ca0\u8377\u304c\u5927\u304d\u3044\u5909\u6570\u3092\u5206\u6790\u304b\u3089\u9664\u5916\u3057\u3001\u3088\u308a\u660e\u78ba\u306a\u56e0\u5b50\u69cb\u9020\u3092\u5f97\u308b\u3053\u3068\u3082\u691c\u8a0e\u3057\u307e\u3059\u3002</li> </ol> <p>\u305f\u3060\u3057\u3001\u7406\u8ad6\u7684\u306b\u91cd\u8981\u306a\u5909\u6570\u306f\u3001\u4ea4\u5dee\u8ca0\u8377\u304c\u3042\u3063\u3066\u3082\u5206\u6790\u306b\u542b\u3081\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u4ea4\u5dee\u8ca0\u8377\u306e\u89e3\u91c8\u306f\u3001\u5206\u6790\u306e\u76ee\u7684\u3084\u5bfe\u8c61\u3068\u306a\u308b\u5206\u91ce\u306e\u77e5\u8b58\u306b\u5927\u304d\u304f\u4f9d\u5b58\u3057\u307e\u3059\u3002\u30c7\u30fc\u30bf\u3060\u3051\u3067\u306a\u304f\u3001\u7406\u8ad6\u7684\u306a\u8003\u5bdf\u3082\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#q4","title":"Q4: \u5171\u901a\u6027\u304c\u975e\u5e38\u306b\u4f4e\u3044\u5909\u6570\u304c\u3042\u308b\u5834\u5408\u3001\u3069\u3046\u3059\u3079\u304d\u3067\u3059\u304b\uff1f","text":"<p>A4: \u5171\u901a\u6027\u304c\u4f4e\u3044\uff08\u4f8b\u3048\u30700.3\u672a\u6e80\uff09\u5909\u6570\u306f\u3001\u56e0\u5b50\u30e2\u30c7\u30eb\u306b\u3088\u3063\u3066\u5341\u5206\u306b\u8aac\u660e\u3055\u308c\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u305d\u306e\u5909\u6570\u306f\u4ed6\u306e\u5909\u6570\u3068\u5171\u6709\u3059\u308b\u90e8\u5206\u304c\u5c11\u306a\u304f\u3001\u72ec\u81ea\u306e\u5909\u52d5\u304c\u5927\u304d\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> <p>\u5bfe\u5fdc\u3068\u3057\u3066\u306f\uff1a</p> <ol> <li>\u5909\u6570\u306e\u9664\u5916\u3092\u691c\u8a0e\u3059\u308b: \u5171\u901a\u6027\u304c\u6975\u7aef\u306b\u4f4e\u3044\u5909\u6570\u306f\u3001\u73fe\u5728\u306e\u56e0\u5b50\u69cb\u9020\u3068\u3042\u307e\u308a\u95a2\u9023\u304c\u306a\u3044\u304b\u3001\u6e2c\u5b9a\u306b\u554f\u984c\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a\u5065\u5eb7\u8abf\u67fb\u3067\u300c\u8840\u6db2\u578b\u300d\u306e\u5909\u6570\u304c\u4f4e\u3044\u5171\u901a\u6027\u3092\u793a\u3059\u5834\u5408\u3001\u4ed6\u306e\u5065\u5eb7\u6307\u6a19\u3068\u306f\u95a2\u9023\u304c\u4f4e\u3044\u305f\u3081\u9664\u5916\u3092\u691c\u8a0e\u3057\u307e\u3059\u3002</p> <ol> <li>\u56e0\u5b50\u6570\u3092\u5897\u3084\u3059\u53ef\u80fd\u6027\u3092\u691c\u8a0e\u3059\u308b: \u305d\u306e\u5909\u6570\u304c\u4ed6\u306e\u5909\u6570\u3068\u306f\u7570\u306a\u308b\u72ec\u81ea\u306e\u6b21\u5143\u3092\u8868\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u56e0\u5b50\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3067\u5171\u901a\u6027\u304c\u5411\u4e0a\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a3\u56e0\u5b50\u30e2\u30c7\u30eb\u3067\u306f\u300c\u904b\u52d5\u983b\u5ea6\u300d\u306e\u5171\u901a\u6027\u304c\u4f4e\u3044\u304c\u30014\u56e0\u5b50\u30e2\u30c7\u30eb\u3067\u306f\u9ad8\u304f\u306a\u308b\u5834\u5408</p> <ol> <li>\u5909\u6570\u306e\u6e2c\u5b9a\u65b9\u6cd5\u3092\u518d\u691c\u8a0e\u3059\u308b: \u7279\u306b\u8cea\u554f\u7d19\u8abf\u67fb\u306e\u5834\u5408\u3001\u8cea\u554f\u306e\u4ed5\u65b9\u3084\u5c3a\u5ea6\u306b\u554f\u984c\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a\u9006\u8ee2\u9805\u76ee\uff08\u300c\u5065\u5eb7\u3067\u306a\u3044\u300d\u306a\u3069\u5426\u5b9a\u5f62\u306e\u8cea\u554f\uff09\u306f\u8aa4\u3063\u3066\u56de\u7b54\u3055\u308c\u3084\u3059\u304f\u3001\u4f4e\u3044\u5171\u901a\u6027\u3092\u793a\u3059\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</p> <ol> <li>\u7406\u8ad6\u7684\u91cd\u8981\u6027\u306b\u57fa\u3065\u3044\u3066\u5224\u65ad\u3059\u308b: \u5909\u6570\u304c\u7406\u8ad6\u7684\u306b\u91cd\u8981\u3067\u3042\u308c\u3070\u3001\u4f4e\u5171\u901a\u6027\u3067\u3082\u5206\u6790\u306b\u542b\u3081\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</li> </ol> <p>\u4f8b\uff1a\u7814\u7a76\u306e\u4e3b\u8981\u306a\u30a2\u30a6\u30c8\u30ab\u30e0\u5909\u6570\u306f\u3001\u5171\u901a\u6027\u304c\u4f4e\u304f\u3066\u3082\u5206\u6790\u306b\u542b\u3081\u308b\u3053\u3068\u304c\u591a\u3044\u3067\u3059\u3002</p> <p>\u5171\u901a\u6027\u306e\u95be\u5024\uff080.3\u30840.4\u306a\u3069\uff09\u306f\u7d76\u5bfe\u7684\u306a\u3082\u306e\u3067\u306f\u306a\u304f\u3001\u7814\u7a76\u5206\u91ce\u3084\u76ee\u7684\u306b\u3088\u3063\u3066\u7570\u306a\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u91cd\u8981\u306a\u306e\u306f\u3001\u7d50\u679c\u306e\u89e3\u91c8\u306b\u304a\u3044\u3066\u5171\u901a\u6027\u306e\u4f4e\u3055\u3092\u8003\u616e\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#9","title":"9. \u307e\u3068\u3081","text":"<p>\u672c\u8b1b\u7fa9\u3067\u306f\u3001\u56e0\u5b50\u5206\u6790\u306e\u57fa\u790e\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u305f\u3002\u56e0\u5b50\u5206\u6790\u306f\u3001\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306e\u80cc\u5f8c\u306b\u3042\u308b\u6f5c\u5728\u7684\u306a\u56e0\u5b50\u69cb\u9020\u3092\u63a2\u7d22\u3059\u308b\u305f\u3081\u306e\u5f37\u529b\u306a\u624b\u6cd5\u3067\u3059\u3002\u4e3b\u8981\u306a\u30dd\u30a4\u30f3\u30c8\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li> <p>\u56e0\u5b50\u5206\u6790\u306e\u57fa\u672c\u6982\u5ff5: \u56e0\u5b50\u5206\u6790\u306f\u89b3\u6e2c\u5909\u6570\u306e\u76f8\u95a2\u30d1\u30bf\u30fc\u30f3\u3092\u8aac\u660e\u3059\u308b\u6f5c\u5728\u56e0\u5b50\u3092\u7279\u5b9a\u3059\u308b\u7d71\u8a08\u624b\u6cd5\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u591a\u6570\u306e\u5065\u5eb7\u6307\u6a19\u304b\u3089\u300c\u8eab\u4f53\u7684\u5065\u5eb7\u300d\u300c\u7cbe\u795e\u7684\u5065\u5eb7\u300d\u306a\u3069\u306e\u6f5c\u5728\u56e0\u5b50\u3092\u7279\u5b9a\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u306e\u30e2\u30c7\u30eb: \u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u306f \\(\\mathbf{X} = \\mathbf{\\Lambda F} + \\mathbf{\\varepsilon}\\) \u3067\u8868\u3055\u308c\u3001\u89b3\u6e2c\u5909\u6570\u3092\u5171\u901a\u56e0\u5b50\u3068\u72ec\u81ea\u56e0\u5b50\u306b\u5206\u89e3\u3057\u307e\u3059\u3002\u3053\u306e\u30e2\u30c7\u30eb\u306b\u3088\u308a\u3001\u5909\u6570\u9593\u306e\u76f8\u95a2\u95a2\u4fc2\u3092\u5c11\u6570\u306e\u6f5c\u5728\u56e0\u5b50\u3067\u8aac\u660e\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u56e0\u5b50\u8ca0\u8377\u91cf: \u56e0\u5b50\u8ca0\u8377\u91cf\u306f\u89b3\u6e2c\u5909\u6570\u3068\u6f5c\u5728\u56e0\u5b50\u306e\u95a2\u4fc2\u306e\u5f37\u3055\u3092\u793a\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u8840\u5727\u304c\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50\u306b0.8\u306e\u8ca0\u8377\u91cf\u3092\u6301\u3064\u5834\u5408\u3001\u5f37\u3044\u95a2\u9023\u304c\u3042\u308b\u3068\u89e3\u91c8\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u5171\u901a\u6027\u3068\u72ec\u81ea\u6027: \u5171\u901a\u6027\u306f\u89b3\u6e2c\u5909\u6570\u306e\u5206\u6563\u306e\u3046\u3061\u5171\u901a\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u90e8\u5206\u3067\u3001\u72ec\u81ea\u6027\u306f\u305d\u306e\u6b8b\u308a\u306e\u90e8\u5206\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u8840\u5727\u306e\u5206\u6563\u306e70%\u304c\u5171\u901a\u56e0\u5b50\u3067\u8aac\u660e\u3055\u308c\u300130%\u304c\u72ec\u81ea\u56e0\u5b50\u306b\u3088\u308b\u3082\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u3068\u4e3b\u6210\u5206\u5206\u6790\u306e\u9055\u3044: \u56e0\u5b50\u5206\u6790\u306f\u6f5c\u5728\u69cb\u9020\u306e\u63a2\u7d22\u306b\u9069\u3057\u3066\u304a\u308a\u3001\u72ec\u81ea\u5206\u6563\u3092\u660e\u793a\u7684\u306b\u6271\u3044\u307e\u3059\u3002\u4e00\u65b9\u3001\u4e3b\u6210\u5206\u5206\u6790\u306f\u30c7\u30fc\u30bf\u5727\u7e2e\u306b\u9069\u3057\u3066\u304a\u308a\u3001\u5168\u5206\u6563\u3092\u5bfe\u8c61\u3068\u3057\u307e\u3059\u3002</p> </li> </ol> <p>\u56e0\u5b50\u5206\u6790\u306f\u3001\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u3001\u8907\u96d1\u306a\u5065\u5eb7\u72b6\u614b\u3092\u5c11\u6570\u306e\u610f\u5473\u306e\u3042\u308b\u56e0\u5b50\u306b\u8981\u7d04\u3057\u3001\u6f5c\u5728\u7684\u306a\u5065\u5eb7\u6b21\u5143\u3092\u7279\u5b9a\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u591a\u6570\u306e\u5065\u5eb7\u6307\u6a19\u304b\u3089\u300c\u8eab\u4f53\u6a5f\u80fd\u300d\u300c\u7cbe\u795e\u5065\u5eb7\u300d\u300c\u751f\u6d3b\u7fd2\u6163\u300d\u306a\u3069\u306e\u56e0\u5b50\u3092\u62bd\u51fa\u3057\u3001\u500b\u4eba\u306e\u5065\u5eb7\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u67a0\u7d44\u307f\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p>\u6b21\u56de\u306e\u8b1b\u7fa9\u3067\u306f\u3001\u56e0\u5b50\u5206\u6790\u306e\u63a8\u5b9a\u6cd5\u3001\u7279\u306b\u6700\u5c24\u63a8\u5b9a\u3068\u56e0\u5b50\u56de\u8ee2\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u3088\u308a\u89e3\u91c8\u3057\u3084\u3059\u3044\u56e0\u5b50\u69cb\u9020\u3092\u5f97\u308b\u65b9\u6cd5\u3092\u7406\u89e3\u3067\u304d\u308b\u3067\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/LA/44-factor-analysis/#10","title":"10. \u53c2\u8003\u6587\u732e","text":"<ol> <li>\u77e2\u90e8\u535a. (2002). \u300c\u7dda\u5f62\u4ee3\u6570\u300d. \u88f3\u83ef\u623f.</li> <li>Gilbert Strang. (2016). \u300c\u7dda\u5f62\u4ee3\u6570\u3068\u305d\u306e\u5fdc\u7528\u300d. \u7523\u696d\u56f3\u66f8.</li> <li>Richard A. Johnson &amp; Dean W. Wichern. (2007). \u300cApplied Multivariate Statistical Analysis\u300d. Pearson.</li> <li>Harman, H. H. (1976). \u300cModern Factor Analysis\u300d. University of Chicago Press.</li> <li>\u8c4a\u7530\u79c0\u6a39. (1998). \u300c\u5171\u5206\u6563\u69cb\u9020\u5206\u6790\u5165\u9580 - \u69cb\u9020\u65b9\u7a0b\u5f0f\u30e2\u30c7\u30ea\u30f3\u30b0\u300d. \u671d\u5009\u66f8\u5e97.</li> </ol>"},{"location":"lectures/LA/45-factor-analysis/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 I / \u57fa\u790e / II","text":""},{"location":"lectures/LA/45-factor-analysis/#45","title":"\u7b2c45\u56de\u8b1b\u7fa9\uff1a\u56e0\u5b50\u5206\u6790\u306e\u63a8\u5b9a\u6cd5","text":""},{"location":"lectures/LA/45-factor-analysis/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c45\u56de \u95a2\u9023\u9805\u76ee: \u56e0\u5b50\u5206\u6790\u3001\u6700\u5c24\u63a8\u5b9a\u3001\u56e0\u5b50\u56de\u8ee2 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u7b2c44\u56de\u300c\u56e0\u5b50\u5206\u6790\u306e\u57fa\u790e\u300d\u3001\u5171\u5206\u6563\u884c\u5217\u306e\u6027\u8cea\u3001\u56fa\u6709\u5024\u5206\u89e3</p> <p>\u672c\u8b1b\u7fa9\u3092\u5341\u5206\u306b\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u524d\u56de\u5b66\u7fd2\u3057\u305f\u56e0\u5b50\u5206\u6790\u306e\u57fa\u672c\u30e2\u30c7\u30eb\u3001\u6f5c\u5728\u5909\u6570\u3068\u89b3\u6e2c\u5909\u6570\u306e\u95a2\u4fc2\u3001\u304a\u3088\u3073\u5171\u5206\u6563\u884c\u5217\u306e\u6027\u8cea\u306b\u3064\u3044\u3066\u5fa9\u7fd2\u3057\u3066\u304a\u304f\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u307e\u305f\u3001\u884c\u5217\u306e\u56fa\u6709\u5024\u5206\u89e3\u3068\u5bfe\u89d2\u5316\u306b\u3064\u3044\u3066\u306e\u77e5\u8b58\u3082\u5fc5\u8981\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<p>\u672c\u65e5\u306e\u8b1b\u7fa9\u3092\u901a\u3058\u3066\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u7406\u89e3\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> <ol> <li>\u56e0\u5b50\u5206\u6790\u306b\u304a\u3051\u308b\u56e0\u5b50\u306e\u63a8\u5b9a\u65b9\u6cd5\u3092\u7406\u89e3\u3057\u3001\u6570\u5b66\u7684\u80cc\u666f\u3092\u8aac\u660e\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> <li>\u6700\u5c24\u63a8\u5b9a\u6cd5\u306e\u539f\u7406\u3068\u56e0\u5b50\u5206\u6790\u3078\u306e\u9069\u7528\u65b9\u6cd5\u3092\u7fd2\u5f97\u3057\u3001\u305d\u306e\u6570\u5f0f\u8868\u73fe\u3092\u7406\u89e3\u3059\u308b</li> <li>\u56e0\u5b50\u56de\u8ee2\u306e\u76ee\u7684\u3068\u4e3b\u8981\u306a\u56de\u8ee2\u6cd5\uff08\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u6cd5\u3001\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u6cd5\uff09\u306e\u6570\u5b66\u7684\u539f\u7406\u3068\u89e3\u91c8\u65b9\u6cd5\u3092\u7fd2\u5f97\u3059\u308b</li> <li>\u56e0\u5b50\u69cb\u9020\u306e\u89e3\u91c8\u3068\u8a55\u4fa1\u306e\u305f\u3081\u306e\u5b9a\u91cf\u7684\u30fb\u5b9a\u6027\u7684\u65b9\u6cd5\u3092\u7406\u89e3\u3059\u308b</li> <li>Python\u306b\u3088\u308b\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u88c5\u65b9\u6cd5\u3092\u7fd2\u5f97\u3057\u3001\u5b9f\u30c7\u30fc\u30bf\u3078\u306e\u5fdc\u7528\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b</li> </ol>"},{"location":"lectures/LA/45-factor-analysis/#3","title":"3. \u57fa\u672c\u6982\u5ff5","text":""},{"location":"lectures/LA/45-factor-analysis/#31","title":"3.1 \u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u306e\u5fa9\u7fd2","text":"<p>\u56e0\u5b50\u5206\u6790\u306f\u3001\u89b3\u6e2c\u3055\u308c\u305f\u5909\u6570\uff08\u89b3\u6e2c\u5909\u6570\uff09\u306e\u80cc\u5f8c\u306b\u3042\u308b\u6f5c\u5728\u7684\u306a\u8981\u56e0\uff08\u6f5c\u5728\u5909\u6570\u307e\u305f\u306f\u56e0\u5b50\uff09\u3092\u7279\u5b9a\u3059\u308b\u305f\u3081\u306e\u7d71\u8a08\u7684\u624b\u6cd5\u3067\u3059\u3002\u7b2c44\u56de\u306e\u8b1b\u7fa9\u3067\u5b66\u3093\u3060\u5185\u5bb9\u3092\u632f\u308a\u8fd4\u308a\u306a\u304c\u3089\u3001\u3088\u308a\u8a73\u7d30\u306b\u7406\u89e3\u3092\u6df1\u3081\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002</p> <p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\uff1a \\(\\mathbf{X} = \\mathbf{\\Lambda F} + \\mathbf{\\varepsilon}\\)</p> <p>\u3053\u3053\u3067\uff1a - \\(\\mathbf{X}\\) \u306f \\(p\\) \u6b21\u5143\u306e\u89b3\u6e2c\u5909\u6570\u30d9\u30af\u30c8\u30eb - \\(\\mathbf{\\Lambda}\\) \u306f \\(p \\times m\\) \u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217 - \\(\\mathbf{F}\\) \u306f \\(m\\) \u6b21\u5143\u306e\u5171\u901a\u56e0\u5b50\u30d9\u30af\u30c8\u30eb - \\(\\mathbf{\\varepsilon}\\) \u306f \\(p\\) \u6b21\u5143\u306e\u7279\u6b8a\u56e0\u5b50\uff08\u72ec\u81ea\u56e0\u5b50\uff09\u30d9\u30af\u30c8\u30eb</p> <p>\u3053\u306e\u30e2\u30c7\u30eb\u306b\u304a\u3044\u3066\u3001\u5171\u901a\u56e0\u5b50 \\(\\mathbf{F}\\) \u3068\u7279\u6b8a\u56e0\u5b50 \\(\\mathbf{\\varepsilon}\\) \u306b\u95a2\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u91cd\u8981\u306a\u4eee\u5b9a\u3092\u7f6e\u3044\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\\(E[\\mathbf{F}] = \\mathbf{0}\\) \uff08\u5171\u901a\u56e0\u5b50\u306e\u671f\u5f85\u5024\u306f\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\uff09</li> <li>\\(\\mathrm{Cov}[\\mathbf{F}] = \\mathbf{I}\\) \uff08\u5171\u901a\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u306f\u30bc\u30ed\uff1a\u76f4\u4ea4\u56e0\u5b50\u306e\u5834\u5408\uff09</li> <li>\\(E[\\mathbf{\\varepsilon}] = \\mathbf{0}\\) \uff08\u7279\u6b8a\u56e0\u5b50\u306e\u671f\u5f85\u5024\u306f\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\uff09</li> <li>\\(\\mathrm{Cov}[\\mathbf{\\varepsilon}] = \\mathbf{\\Psi}\\) \uff08\u7279\u6b8a\u56e0\u5b50\u306e\u5171\u5206\u6563\u884c\u5217\u306f\u5bfe\u89d2\u884c\u5217\uff09</li> <li>\\(\\mathrm{Cov}[\\mathbf{F}, \\mathbf{\\varepsilon}] = \\mathbf{0}\\) \uff08\u5171\u901a\u56e0\u5b50\u3068\u7279\u6b8a\u56e0\u5b50\u306f\u7121\u76f8\u95a2\uff09</li> </ol> <p>\u3053\u306e\u4eee\u5b9a\u306e\u4e0b\u3067\u3001\u89b3\u6e2c\u5909\u6570 \\(\\mathbf{X}\\) \u306e\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{\\Sigma}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> <p>\u5171\u5206\u6563\u69cb\u9020\uff1a \\(\\mathbf{\\Sigma} = \\mathbf{\\Lambda \\Lambda^T} + \\mathbf{\\Psi}\\)</p> <p>\u3053\u3053\u3067\uff1a - \\(\\mathbf{\\Sigma}\\) \u306f\u89b3\u6e2c\u5909\u6570\u306e\u5171\u5206\u6563\u884c\u5217 - \\(\\mathbf{\\Lambda \\Lambda^T}\\) \u306f\u5171\u901a\u56e0\u5b50\u306b\u3088\u308b\u5171\u5206\u6563 - \\(\\mathbf{\\Psi}\\) \u306f\u7279\u6b8a\u56e0\u5b50\u306b\u3088\u308b\u5171\u5206\u6563\uff08\u5bfe\u89d2\u884c\u5217\uff09</p> <p>\u3053\u306e\u5206\u89e3\u306f\u3001\u89b3\u6e2c\u5909\u6570\u306e\u5171\u5206\u6563\u304c\u5171\u901a\u56e0\u5b50\u306b\u3088\u308b\u90e8\u5206\u3068\u7279\u6b8a\u56e0\u5b50\u306b\u3088\u308b\u90e8\u5206\u306b\u5206\u3051\u3089\u308c\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u56e0\u5b50\u5206\u6790\u306e\u76ee\u7684\u306f\u3001\u89b3\u6e2c\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{\\Sigma}\\) \u306b\u6700\u3082\u30d5\u30a3\u30c3\u30c8\u3059\u308b\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217 \\(\\mathbf{\\Lambda}\\) \u3068\u7279\u6b8a\u56e0\u5b50\u306e\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{\\Psi}\\) \u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p> <p>\u5404\u89b3\u6e2c\u5909\u6570 \\(X_i\\) \u306e\u5206\u6563\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5206\u89e3\u3055\u308c\u307e\u3059\uff1a</p> <p>\u5206\u6563\u306e\u5206\u89e3\uff1a \\(\\mathrm{Var}(X_i) = \\sum_{j=1}^{m} \\lambda_{ij}^2 + \\psi_i\\)</p> <p>\u3053\u3053\u3067\uff1a - \\(\\sum_{j=1}^{m} \\lambda_{ij}^2\\) \u306f\u5909\u6570 \\(X_i\\) \u306e\u5171\u901a\u6027\uff08communality\uff09 - \\(\\psi_i\\) \u306f\u5909\u6570 \\(X_i\\) \u306e\u72ec\u81ea\u6027\uff08uniqueness\uff09</p> <p>\u5171\u901a\u6027\u306f\u5171\u901a\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u5206\u6563\u306e\u5272\u5408\u3092\u8868\u3057\u3001\u72ec\u81ea\u6027\u306f\u7279\u6b8a\u56e0\u5b50\u306b\u5e30\u5c5e\u3059\u308b\u5206\u6563\u306e\u5272\u5408\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#32","title":"3.2 \u56e0\u5b50\u5206\u6790\u306b\u304a\u3051\u308b\u4e0d\u5b9a\u6027","text":"<p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u306b\u306f\u56de\u8ee2\u306e\u4e0d\u5b9a\u6027\uff08rotational indeterminacy\uff09\u304c\u5b58\u5728\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217 \\(\\mathbf{\\Lambda}\\) \u306b\u76f4\u4ea4\u884c\u5217 \\(\\mathbf{T}\\) \u3092\u639b\u3051\u305f \\(\\mathbf{\\Lambda T}\\) \u3082\u540c\u69d8\u306b\u59a5\u5f53\u306a\u89e3\u3068\u306a\u308b\u6027\u8cea\u3092\u6307\u3057\u307e\u3059\u3002</p> <p>\u56de\u8ee2\u306e\u4e0d\u5b9a\u6027\u306e\u6570\u5b66\u7684\u8868\u73fe\uff1a \\(\\mathbf{\\Lambda \\Lambda^T} = (\\mathbf{\\Lambda T})(\\mathbf{\\Lambda T})^T = \\mathbf{\\Lambda T T^T \\Lambda^T} = \\mathbf{\\Lambda \\Lambda^T}\\)</p> <p>\u3053\u3053\u3067 \\(\\mathbf{T}\\) \u306f\u76f4\u4ea4\u884c\u5217\uff08\\(\\mathbf{T T^T} = \\mathbf{I}\\)\uff09\u3067\u3059\u3002</p> <p>\u3053\u306e\u6027\u8cea\u306f\u3001\u540c\u3058\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u7570\u306a\u308b\u56e0\u5b50\u8ca0\u8377\u91cf\u30d1\u30bf\u30fc\u30f3\u304c\u5f97\u3089\u308c\u308b\u53ef\u80fd\u6027\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u6570\u5b66\u7684\u306b\u306f\u7b49\u4fa1\u3067\u3082\u3001\u89e3\u91c8\u304c\u7570\u306a\u308b\u8907\u6570\u306e\u89e3\u304c\u5b58\u5728\u3057\u307e\u3059\u3002</p> <p>\u3053\u306e\u4e0d\u5b9a\u6027\u304c\u3042\u308b\u305f\u3081\u3001\u56e0\u5b50\u5206\u6790\u306e\u7d50\u679c\u3092\u89e3\u91c8\u3059\u308b\u969b\u306b\u306f\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002\u3057\u304b\u3057\u3001\u3053\u306e\u6027\u8cea\u3092\u5229\u7528\u3057\u3066\u3001\u3088\u308a\u89e3\u91c8\u3057\u3084\u3059\u3044\u56e0\u5b50\u69cb\u9020\u3092\u6c42\u3081\u308b\u305f\u3081\u306e\u56e0\u5b50\u56de\u8ee2\u3068\u3044\u3046\u624b\u6cd5\u304c\u958b\u767a\u3055\u308c\u307e\u3057\u305f\u3002\u56de\u8ee2\u306b\u3088\u3063\u3066\u5171\u901a\u56e0\u5b50\u7a7a\u9593\u306b\u304a\u3051\u308b\u5ea7\u6a19\u8ef8\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u660e\u78ba\u306a\u69cb\u9020\u3092\u5f97\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#4","title":"4. \u7406\u8ad6\u3068\u624b\u6cd5","text":""},{"location":"lectures/LA/45-factor-analysis/#41","title":"4.1 \u6700\u5c24\u63a8\u5b9a\u306b\u3088\u308b\u56e0\u5b50\u306e\u63a8\u5b9a","text":"<p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217 \\(\\mathbf{\\Lambda}\\) \u3068\u7279\u6b8a\u56e0\u5b50\u306e\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{\\Psi}\\)\uff09\u3092\u63a8\u5b9a\u3059\u308b\u4ee3\u8868\u7684\u306a\u65b9\u6cd5\u3068\u3057\u3066\u3001\u6700\u5c24\u63a8\u5b9a\u6cd5\uff08Maximum Likelihood Estimation, MLE\uff09\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u6700\u5c24\u63a8\u5b9a\u6cd5\u306e\u57fa\u672c\u539f\u7406\uff1a \u89b3\u6e2c\u30c7\u30fc\u30bf\u304c\u5f97\u3089\u308c\u308b\u78ba\u7387\uff08\u5c24\u5ea6\uff09\u3092\u6700\u5927\u306b\u3059\u308b\u3088\u3046\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5024\u3092\u6c42\u3081\u308b</p> <p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u3067\u306f\u3001\u89b3\u6e2c\u5909\u6570 \\(\\mathbf{X}\\) \u304c\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u4eee\u5b9a\u3057\u307e\u3059\uff1a</p> <p>\\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})\\)</p> <p>\u3053\u3053\u3067\u3001\u5e73\u5747\u30d9\u30af\u30c8\u30eb \\(\\boldsymbol{\\mu}\\) \u3068\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{\\Sigma} = \\mathbf{\\Lambda \\Lambda^T} + \\mathbf{\\Psi}\\) \u3067\u3059\u3002\u591a\u304f\u306e\u5834\u5408\u3001\u30c7\u30fc\u30bf\u306f\u3042\u3089\u304b\u3058\u3081\u6a19\u6e96\u5316\u3055\u308c\u308b\u305f\u3081\u3001\\(\\boldsymbol{\\mu} = \\mathbf{0}\\) \u3068\u4eee\u5b9a\u3067\u304d\u307e\u3059\u3002</p> <p>\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\uff1a</p> <p>\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\uff1a \\(f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right)\\)</p> <p>\\(n\\) \u500b\u306e\u72ec\u7acb\u306a\u89b3\u6e2c\u5024 \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\) \u306b\u5bfe\u3059\u308b\u5bfe\u6570\u5c24\u5ea6\u95a2\u6570\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> <p>\u5bfe\u6570\u5c24\u5ea6\u95a2\u6570\uff1a \\(\\ln L = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln |\\mathbf{\\Sigma}| - \\frac{1}{2} \\sum_{i=1}^{n} (\\mathbf{x}_i - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu})\\)</p> <p>\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\mathbf{\\Lambda}\\) \u3068 \\(\\mathbf{\\Psi}\\) \u306f\u3001\u3053\u306e\u5bfe\u6570\u5c24\u5ea6\u95a2\u6570\u3092\u6700\u5927\u5316\u3059\u308b\u3088\u3046\u306b\u63a8\u5b9a\u3055\u308c\u307e\u3059\u3002\u5b9f\u969b\u306e\u8a08\u7b97\u3067\u306f\u3001\u89b3\u6e2c\u30c7\u30fc\u30bf\u306e\u6a19\u672c\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{S}\\) \u3092\u7528\u3044\u3066\u3001\u4ee5\u4e0b\u306e\u95a2\u6570\u3092\u6700\u5c0f\u5316\u3057\u307e\u3059\uff1a</p> <p>\u6700\u5c0f\u5316\u3059\u308b\u95a2\u6570\uff08Fitting Function\uff09\uff1a \\(F_{ML} = \\ln |\\mathbf{\\Sigma}| + \\mathrm{tr}(\\mathbf{S} \\mathbf{\\Sigma}^{-1}) - \\ln |\\mathbf{S}| - p\\)</p> <p>\u3053\u3053\u3067 \\(p\\) \u306f\u5909\u6570\u306e\u6570\u3001\\(\\mathrm{tr}\\) \u306f\u30c8\u30ec\u30fc\u30b9\uff08\u5bfe\u89d2\u6210\u5206\u306e\u548c\uff09\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u3053\u306e\u6700\u5c0f\u5316\u554f\u984c\u306f\u89e3\u6790\u7684\u306b\u89e3\u304f\u3053\u3068\u304c\u56f0\u96e3\u306a\u305f\u3081\u3001\u901a\u5e38\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u53cd\u5fa9\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u7528\u3044\u3066\u6570\u5024\u7684\u306b\u89e3\u304d\u307e\u3059\uff1a</p> <ol> <li>\u521d\u671f\u5024\u306e\u8a2d\u5b9a\uff1a\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217 \\(\\mathbf{\\Lambda}^{(0)}\\) \u3068\u7279\u6b8a\u56e0\u5b50\u306e\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{\\Psi}^{(0)}\\) \u306e\u521d\u671f\u5024\u3092\u8a2d\u5b9a</li> <li>\u53cd\u5fa9\u8a08\u7b97\uff1a    a. \u73fe\u5728\u306e\u63a8\u5b9a\u5024\u304b\u3089\u5171\u5206\u6563\u884c\u5217 \\(\\mathbf{\\Sigma}^{(t)} = \\mathbf{\\Lambda}^{(t)} (\\mathbf{\\Lambda}^{(t)})^T + \\mathbf{\\Psi}^{(t)}\\) \u3092\u8a08\u7b97    b. \u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u95a2\u6570 \\(F_{ML}\\) \u306e\u5024\u3092\u8a08\u7b97    c. \u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u95a2\u6570\u304c\u6e1b\u5c11\u3059\u308b\u3088\u3046\u306b \\(\\mathbf{\\Lambda}^{(t)}\\) \u3068 \\(\\mathbf{\\Psi}^{(t)}\\) \u3092\u66f4\u65b0</li> <li>\u53ce\u675f\u5224\u5b9a\uff1a\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5909\u5316\u304c\u5341\u5206\u5c0f\u3055\u304f\u306a\u308b\u304b\u3001\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u95a2\u6570\u306e\u5024\u306e\u5909\u5316\u304c\u95be\u5024\u4ee5\u4e0b\u306b\u306a\u308b\u307e\u3067\u53cd\u5fa9</li> </ol> <p>\u3053\u306e\u53cd\u5fa9\u904e\u7a0b\u306f\u3001Newton-Raphson\u6cd5\u3084EM\uff08\u671f\u5f85\u5024\u6700\u5927\u5316\uff09\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306a\u3069\u306e\u6700\u9069\u5316\u624b\u6cd5\u3092\u7528\u3044\u3066\u5b9f\u88c5\u3055\u308c\u308b\u3053\u3068\u304c\u4e00\u822c\u7684\u3067\u3059\u3002</p> <p>\u6700\u5c24\u63a8\u5b9a\u6cd5\u306e\u5229\u70b9\u306f\u3001\u4ee5\u4e0b\u306e\u70b9\u306b\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u6f38\u8fd1\u7684\u306b\u4e0d\u504f\u3067\u52b9\u7387\u7684\u306a\u63a8\u5b9a\u91cf\u304c\u5f97\u3089\u308c\u308b</li> <li>\u7d71\u8a08\u7684\u691c\u5b9a\uff08\u30e2\u30c7\u30eb\u306e\u9069\u5408\u5ea6\u3084\u56e0\u5b50\u6570\u306e\u691c\u5b9a\u306a\u3069\uff09\u304c\u53ef\u80fd</li> <li>\u69d8\u3005\u306a\u30e2\u30c7\u30eb\u9593\u306e\u6bd4\u8f03\u304c\u53ef\u80fd\uff08\u4f8b\uff1a\u7570\u306a\u308b\u56e0\u5b50\u6570\u306e\u30e2\u30c7\u30eb\u6bd4\u8f03\uff09</li> </ol> <p>\u4e00\u65b9\u3001\u6700\u5c24\u63a8\u5b9a\u6cd5\u306e\u8ab2\u984c\u3068\u3057\u3066\u306f\u3001\u4ee5\u4e0b\u306e\u70b9\u304c\u6319\u3052\u3089\u308c\u307e\u3059\uff1a</p> <ol> <li>\u591a\u5909\u91cf\u6b63\u898f\u6027\u306e\u4eee\u5b9a\u304c\u5fc5\u8981</li> <li>\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u304c\u5c0f\u3055\u3044\u5834\u5408\u3001\u63a8\u5b9a\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u308b\u3053\u3068\u304c\u3042\u308b</li> <li>\u53cd\u5fa9\u8a08\u7b97\u304c\u53ce\u675f\u3057\u306a\u3044\u5834\u5408\u304c\u3042\u308b\uff08\u7279\u306bHeywood case\u3068\u547c\u3070\u308c\u308b\u3001\u7279\u6b8a\u56e0\u5b50\u306e\u5206\u6563\u304c\u8ca0\u306b\u306a\u308b\u306a\u3069\u306e\u554f\u984c\uff09</li> </ol>"},{"location":"lectures/LA/45-factor-analysis/#42","title":"4.2 \u56e0\u5b50\u6570\u306e\u6c7a\u5b9a","text":"<p>\u9069\u5207\u306a\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3059\u308b\u3053\u3068\u306f\u3001\u56e0\u5b50\u5206\u6790\u306b\u304a\u3044\u3066\u975e\u5e38\u306b\u91cd\u8981\u3067\u3059\u3002\u56e0\u5b50\u6570\u304c\u5c11\u306a\u3059\u304e\u308b\u3068\u91cd\u8981\u306a\u69cb\u9020\u3092\u898b\u9003\u3057\u3001\u591a\u3059\u304e\u308b\u3068\u89e3\u91c8\u304c\u56f0\u96e3\u306b\u306a\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306b\u3001\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3059\u308b\u305f\u3081\u306e\u4e3b\u306a\u65b9\u6cd5\u3092\u8a73\u3057\u304f\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#421-scree-plot","title":"4.2.1 \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\uff08Scree Plot\uff09","text":"<p>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306f\u3001\u56fa\u6709\u5024\u3092\u5927\u304d\u3044\u9806\u306b\u4e26\u3079\u3066\u30d7\u30ed\u30c3\u30c8\u3057\u305f\u30b0\u30e9\u30d5\u3067\u3059\u3002\u56fa\u6709\u5024\u306e\u6e1b\u5c11\u30d1\u30bf\u30fc\u30f3\u3092\u8996\u899a\u7684\u306b\u78ba\u8a8d\u3057\u3001\u6025\u6fc0\u306a\u6e1b\u5c11\uff08\u300c\u8098\u300d\u307e\u305f\u306f\u300c\u5c48\u66f2\u70b9\u300d\uff09\u304c\u898b\u3089\u308c\u308b\u30dd\u30a4\u30f3\u30c8\u3067\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3057\u307e\u3059\u3002</p> <p>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306e\u624b\u9806\uff1a 1. \u76f8\u95a2\u884c\u5217\u307e\u305f\u306f\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3092\u8a08\u7b97 2. \u56fa\u6709\u5024\u3092\u5927\u304d\u3044\u9806\u306b\u4e26\u3079\u308b 3. \u56fa\u6709\u5024\u3068\u56e0\u5b50\u756a\u53f7\uff08\u9806\u5e8f\uff09\u3092\u30d7\u30ed\u30c3\u30c8 4. \u30b0\u30e9\u30d5\u306e\u300c\u8098\u300d\u306e\u4f4d\u7f6e\u3092\u7279\u5b9a\u3057\u3001\u305d\u306e\u4f4d\u7f6e\u307e\u3067\u306e\u56e0\u5b50\u6570\u3092\u63a1\u7528</p> <p>\u3053\u306e\u65b9\u6cd5\u306e\u5229\u70b9\u306f\u8996\u899a\u7684\u306b\u7406\u89e3\u3057\u3084\u3059\u304f\u3001\u30c7\u30fc\u30bf\u306e\u69cb\u9020\u3092\u76f4\u611f\u7684\u306b\u628a\u63e1\u3067\u304d\u308b\u3053\u3068\u3067\u3059\u3002\u4e00\u65b9\u3001\u300c\u8098\u300d\u306e\u4f4d\u7f6e\u304c\u660e\u78ba\u3067\u306a\u3044\u5834\u5408\u3082\u3042\u308a\u3001\u4e3b\u89b3\u7684\u306a\u5224\u65ad\u304c\u5165\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#422-kaiser-criterion","title":"4.2.2 \u30ab\u30a4\u30b6\u30fc\u57fa\u6e96\uff08Kaiser Criterion\uff09","text":"<p>\u30ab\u30a4\u30b6\u30fc\u57fa\u6e96\u306f\u3001\u56fa\u6709\u5024\u304c1.0\u4ee5\u4e0a\u306e\u56e0\u5b50\u306e\u307f\u3092\u63a1\u7528\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u6a19\u6e96\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\uff08\u5404\u5909\u6570\u306e\u5206\u6563\u304c1\uff09\u306e\u5834\u5408\u3001\u56fa\u6709\u5024\u304c1\u672a\u6e80\u306e\u56e0\u5b50\u306f\u5358\u4e00\u306e\u5909\u6570\u3088\u308a\u3082\u60c5\u5831\u91cf\u304c\u5c11\u306a\u3044\u3068\u89e3\u91c8\u3055\u308c\u307e\u3059\u3002</p> <p>\u30ab\u30a4\u30b6\u30fc\u57fa\u6e96\u306e\u9069\u7528\u65b9\u6cd5\uff1a 1. \u76f8\u95a2\u884c\u5217\u306e\u56fa\u6709\u5024\u3092\u8a08\u7b97 2. \u56fa\u6709\u5024\u304c1.0\u4ee5\u4e0a\u306e\u56e0\u5b50\u306e\u307f\u3092\u63a1\u7528</p> <p>\u3053\u306e\u65b9\u6cd5\u306f\u5ba2\u89b3\u7684\u3067\u9069\u7528\u304c\u7c21\u5358\u3067\u3059\u304c\u3001\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u56e0\u5b50\u6570\u3092\u904e\u5927\u8a55\u4fa1\u307e\u305f\u306f\u904e\u5c0f\u8a55\u4fa1\u3059\u308b\u50be\u5411\u304c\u3042\u308a\u307e\u3059\u3002\u7279\u306b\u5909\u6570\u306e\u6570\u304c\u591a\u3044\u5834\u5408\u3084\u5909\u6570\u9593\u306e\u76f8\u95a2\u304c\u4f4e\u3044\u5834\u5408\u3001\u904e\u5927\u8a55\u4fa1\u3059\u308b\u50be\u5411\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#423-cumulative-percentage-of-variance","title":"4.2.3 \u7d2f\u7a4d\u5bc4\u4e0e\u7387\uff08Cumulative Percentage of Variance\uff09","text":"<p>\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306f\u3001\u56e0\u5b50\u304c\u8aac\u660e\u3059\u308b\u5206\u6563\u306e\u5272\u5408\u306e\u7d2f\u7a4d\u5024\u304c\u4e00\u5b9a\u306e\u95be\u5024\uff08\u901a\u5e3870%\u301c80%\uff09\u3092\u8d85\u3048\u308b\u307e\u3067\u56e0\u5b50\u3092\u8ffd\u52a0\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002</p> <p>\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306e\u8a08\u7b97\u624b\u9806\uff1a 1. \u5404\u56fa\u6709\u5024 \\(\\lambda_i\\) \u306e\u5bc4\u4e0e\u7387\u3092\u8a08\u7b97\uff1a\\(\\lambda_i / \\sum_{j=1}^{p} \\lambda_j\\) 2. \u5bc4\u4e0e\u7387\u3092\u7d2f\u7a4d\u3057\u3066\u3044\u304d\u3001\u95be\u5024\uff08\u4f8b\uff1a80%\uff09\u3092\u8d85\u3048\u308b\u6700\u5c0f\u306e\u56e0\u5b50\u6570\u3092\u63a1\u7528</p> <p>\u3053\u306e\u65b9\u6cd5\u306f\u3001\u30c7\u30fc\u30bf\u306e\u8aac\u660e\u529b\u306b\u76f4\u63a5\u95a2\u9023\u3057\u305f\u57fa\u6e96\u3092\u63d0\u4f9b\u3057\u307e\u3059\u304c\u3001\u9069\u5207\u306a\u95be\u5024\u306e\u9078\u629e\u304c\u5206\u91ce\u3084\u7814\u7a76\u76ee\u7684\u306b\u3088\u3063\u3066\u7570\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#424-parallel-analysis","title":"4.2.4 \u5e73\u884c\u5206\u6790\uff08Parallel Analysis\uff09","text":"<p>\u5e73\u884c\u5206\u6790\u306f\u3001\u5b9f\u30c7\u30fc\u30bf\u304b\u3089\u5f97\u3089\u308c\u308b\u56fa\u6709\u5024\u3068\u3001\u540c\u3058\u30b5\u30a4\u30ba\u306e\u30e9\u30f3\u30c0\u30e0\u30c7\u30fc\u30bf\u304b\u3089\u5f97\u3089\u308c\u308b\u56fa\u6709\u5024\u3092\u6bd4\u8f03\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u5b9f\u30c7\u30fc\u30bf\u306e\u56fa\u6709\u5024\u304c\u30e9\u30f3\u30c0\u30e0\u30c7\u30fc\u30bf\u306e\u56fa\u6709\u5024\u3092\u4e0a\u56de\u308b\u5834\u5408\u306e\u307f\u3001\u305d\u306e\u56e0\u5b50\u3092\u63a1\u7528\u3057\u307e\u3059\u3002</p> <p>\u5e73\u884c\u5206\u6790\u306e\u624b\u9806\uff1a 1. \u5b9f\u30c7\u30fc\u30bf\u306e\u76f8\u95a2\u884c\u5217\u307e\u305f\u306f\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u5024\u3092\u8a08\u7b97 2. \u540c\u3058\u30b5\u30a4\u30ba\uff08\u5909\u6570\u6570\u00d7\u30b5\u30f3\u30d7\u30eb\u6570\uff09\u306e\u30e9\u30f3\u30c0\u30e0\u30c7\u30fc\u30bf\u3092\u8907\u6570\u56de\u751f\u6210\u3057\u3001\u5404\u30e9\u30f3\u30c0\u30e0\u30c7\u30fc\u30bf\u306e\u56fa\u6709\u5024\u3092\u8a08\u7b97 3. \u30e9\u30f3\u30c0\u30e0\u30c7\u30fc\u30bf\u304b\u3089\u5f97\u3089\u308c\u305f\u56fa\u6709\u5024\u306e\u5e73\u5747\u307e\u305f\u306f95\u30d1\u30fc\u30bb\u30f3\u30bf\u30a4\u30eb\u3092\u8a08\u7b97 4. \u5b9f\u30c7\u30fc\u30bf\u306e\u56fa\u6709\u5024\u304c\u30e9\u30f3\u30c0\u30e0\u30c7\u30fc\u30bf\u306e\u56fa\u6709\u5024\uff08\u5e73\u5747\u307e\u305f\u306f95\u30d1\u30fc\u30bb\u30f3\u30bf\u30a4\u30eb\uff09\u3092\u4e0a\u56de\u308b\u56e0\u5b50\u306e\u307f\u3092\u63a1\u7528</p> <p>\u5e73\u884c\u5206\u6790\u306f\u3001\u5076\u7136\u306e\u69cb\u9020\u3068\u5b9f\u969b\u306e\u69cb\u9020\u3092\u533a\u5225\u3059\u308b\u70b9\u3067\u512a\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u8a08\u7b97\u304c\u8907\u96d1\u3067\u7279\u6b8a\u306a\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u304c\u5fc5\u8981\u306a\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#425-likelihood-ratio-test","title":"4.2.5 \u5c24\u5ea6\u6bd4\u691c\u5b9a\uff08Likelihood Ratio Test\uff09","text":"<p>\u6700\u5c24\u63a8\u5b9a\u6cd5\u3092\u7528\u3044\u308b\u5834\u5408\u3001\u7570\u306a\u308b\u56e0\u5b50\u6570\u306e\u30e2\u30c7\u30eb\u9593\u3067\u30ab\u30a4\u4e8c\u4e57\u691c\u5b9a\u306b\u57fa\u3065\u304f\u5c24\u5ea6\u6bd4\u691c\u5b9a\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u5c24\u5ea6\u6bd4\u691c\u5b9a\u306e\u624b\u9806\uff1a 1. \u56e0\u5b50\u6570 \\(m\\) \u306e\u30e2\u30c7\u30eb\u3068\u56e0\u5b50\u6570 \\(m-1\\) \u306e\u30e2\u30c7\u30eb\u3092\u305d\u308c\u305e\u308c\u6700\u5c24\u63a8\u5b9a\u6cd5\u3067\u63a8\u5b9a 2. \u5c24\u5ea6\u6bd4\u7d71\u8a08\u91cf\u3092\u8a08\u7b97\uff1a\\(\\chi^2 = (n-1) \\times (F_{ML}^{(m-1)} - F_{ML}^{(m)})\\) 3. \u81ea\u7531\u5ea6 \\(df = p - m + 1 - (p - m)\\) \u3067\u30ab\u30a4\u4e8c\u4e57\u691c\u5b9a\u3092\u5b9f\u65bd 4. \u6709\u610f\u3067\u306a\u3044\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\u307e\u3067\u56e0\u5b50\u6570\u3092\u5897\u3084\u3057\u3001\u6700\u521d\u306b\u6709\u610f\u3067\u306a\u304f\u306a\u3063\u305f\u56e0\u5b50\u6570\u3092\u63a1\u7528</p> <p>\u3053\u306e\u65b9\u6cd5\u306f\u7d71\u8a08\u7684\u306b\u53b3\u5bc6\u3067\u3059\u304c\u3001\u591a\u5909\u91cf\u6b63\u898f\u6027\u306e\u4eee\u5b9a\u304c\u5fc5\u8981\u3067\u3042\u308a\u3001\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u306e\u5f71\u97ff\u3092\u53d7\u3051\u3084\u3059\u3044\uff08\u5927\u898f\u6a21\u30b5\u30f3\u30d7\u30eb\u3067\u306f\u5c0f\u3055\u306a\u9055\u3044\u3067\u3082\u7d71\u8a08\u7684\u306b\u6709\u610f\u306b\u306a\u308b\uff09\u3068\u3044\u3046\u6b20\u70b9\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u5b9f\u969b\u306e\u56e0\u5b50\u5206\u6790\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u65b9\u6cd5\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u7528\u3044\u308b\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u307e\u3059\u3002\u307e\u305f\u3001\u7406\u8ad6\u7684\u306a\u89e3\u91c8\u53ef\u80fd\u6027\u3084\u5148\u884c\u7814\u7a76\u3068\u306e\u6574\u5408\u6027\u3082\u8003\u616e\u3057\u3066\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#43","title":"4.3 \u56e0\u5b50\u56de\u8ee2","text":"<p>\u56e0\u5b50\u56de\u8ee2\u306f\u3001\u63a8\u5b9a\u3055\u308c\u305f\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\u3092\u3088\u308a\u89e3\u91c8\u3057\u3084\u3059\u3044\u5f62\u306b\u5909\u63db\u3059\u308b\u305f\u3081\u306e\u624b\u6cd5\u3067\u3059\u3002\u56de\u8ee2\u306e\u4e3b\u306a\u76ee\u7684\u306f\u5358\u7d14\u69cb\u9020\uff08simple structure\uff09\u306e\u9054\u6210\u3067\u3059\u3002</p> <p>\u5358\u7d14\u69cb\u9020\u306e\u539f\u5247\uff08Thurstone\u306b\u3088\u308b\uff09\uff1a 1. \u5404\u5909\u6570\u306f\u5c11\u6570\u306e\u56e0\u5b50\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3061\u3001\u4ed6\u306e\u56e0\u5b50\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u91cf\u306f\u30bc\u30ed\u307e\u305f\u306f\u975e\u5e38\u306b\u5c0f\u3055\u3044 2. \u5404\u56e0\u5b50\u306f\u5909\u6570\u306e\u4e00\u90e8\u306e\u307f\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3061\u3001\u4ed6\u306e\u5909\u6570\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u91cf\u306f\u5c0f\u3055\u3044 3. \u7570\u306a\u308b\u56e0\u5b50\u306f\u5909\u6570\u306e\u7570\u306a\u308b\u96c6\u5408\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064 4. \u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064\u5909\u6570\u304c\u591a\u3044\u56e0\u5b50\u3068\u5c11\u306a\u3044\u56e0\u5b50\u304c\u5b58\u5728\u3059\u308b 5. \u56e0\u5b50\u306e\u30da\u30a2\u3054\u3068\u306b\u3001\u4e21\u65b9\u306e\u56e0\u5b50\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064\u5909\u6570\u306e\u6570\u304c\u5c11\u306a\u3044</p> <p>\u5358\u7d14\u69cb\u9020\u3092\u6301\u3064\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\u306f\u3001\u5404\u5909\u6570\u304c\u3069\u306e\u56e0\u5b50\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u304b\u304c\u660e\u78ba\u306b\u306a\u308a\u3001\u5404\u56e0\u5b50\u306e\u89e3\u91c8\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u56de\u8ee2\u6cd5\u306f\u5927\u304d\u304f\u5206\u3051\u3066\u4ee5\u4e0b\u306e2\u7a2e\u985e\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u76f4\u4ea4\u56de\u8ee2\uff08Orthogonal Rotation\uff09\uff1a\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u30bc\u30ed\u3068\u3044\u3046\u5236\u7d04\u3092\u4fdd\u6301</li> <li>\u659c\u4ea4\u56de\u8ee2\uff08Oblique Rotation\uff09\uff1a\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9</li> </ol>"},{"location":"lectures/LA/45-factor-analysis/#431","title":"4.3.1 \u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\uff08\u76f4\u4ea4\u56de\u8ee2\uff09","text":"<p>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\uff08Varimax\uff09\u56de\u8ee2\u306f\u6700\u3082\u4e00\u822c\u7684\u306a\u76f4\u4ea4\u56de\u8ee2\u6cd5\u3067\u3059\u3002\u5404\u56e0\u5b50\u306e\u8ca0\u8377\u91cf\u306e\u4e8c\u4e57\u5024\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u5404\u56e0\u5b50\u306b\u5bfe\u3057\u3066\u4e00\u90e8\u306e\u5909\u6570\u304c\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u3001\u4ed6\u306e\u5909\u6570\u304c\u4f4e\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064\u3088\u3046\u306b\u3057\u307e\u3059\u3002</p> <p>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u306e\u76ee\u7684\u95a2\u6570\uff1a \\(V = \\sum_{j=1}^{m} \\left[ \\frac{1}{p} \\sum_{i=1}^{p} \\left( \\frac{\\lambda_{ij}^2}{h_i^2} \\right)^2 - \\left( \\frac{1}{p} \\sum_{i=1}^{p} \\frac{\\lambda_{ij}^2}{h_i^2} \\right)^2 \\right]\\)</p> <p>\u3053\u3053\u3067\uff1a - \\(\\lambda_{ij}\\) \u306f\u5909\u6570 \\(i\\) \u306e\u56e0\u5b50 \\(j\\) \u306b\u5bfe\u3059\u308b\u56de\u8ee2\u5f8c\u306e\u8ca0\u8377\u91cf - \\(h_i^2\\) \u306f\u5909\u6570 \\(i\\) \u306e\u5171\u901a\u6027\uff08\\(h_i^2 = \\sum_{j=1}^{m} \\lambda_{ij}^2\\)\uff09 - \\(p\\) \u306f\u5909\u6570\u306e\u6570 - \\(m\\) \u306f\u56e0\u5b50\u306e\u6570</p> <p>\u3053\u306e\u76ee\u7684\u95a2\u6570\u306f\u3001\u5404\u56e0\u5b50\u306e\u8ca0\u8377\u91cf\u30d1\u30bf\u30fc\u30f3\u306e\u300c\u5358\u7d14\u3055\u300d\u3092\u6e2c\u308b\u3082\u306e\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306f\u3001\u3053\u306e\u76ee\u7684\u95a2\u6570\u3092\u6700\u5927\u5316\u3059\u308b\u76f4\u4ea4\u56de\u8ee2\u884c\u5217 \\(\\mathbf{T}\\) \u3092\u6c42\u3081\u308b\u3053\u3068\u3067\u5b9f\u884c\u3055\u308c\u307e\u3059\u3002</p> <p>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306e\u5177\u4f53\u7684\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>2\u3064\u306e\u56e0\u5b50\uff08\\(j\\) \u3068 \\(k\\)\uff09\u3092\u9078\u629e</li> <li>\u3053\u308c\u3089\u306e\u56e0\u5b50\u306b\u95a2\u3057\u3066\u3001\u56de\u8ee2\u89d2 \\(\\phi\\) \u3092\u5909\u5316\u3055\u305b\u306a\u304c\u3089\u76ee\u7684\u95a2\u6570 \\(V\\) \u3092\u6700\u5927\u5316\u3059\u308b\u89d2\u5ea6\u3092\u63a2\u7d22</li> <li>\u6700\u9069\u306a\u89d2\u5ea6 \\(\\phi\\) \u3067\u56de\u8ee2\u884c\u5217\u3092\u8a08\u7b97\u3057\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\u3092\u66f4\u65b0</li> <li>\u3059\u3079\u3066\u306e\u56e0\u5b50\u30da\u30a2\u306b\u5bfe\u3057\u3066\u3053\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u7e70\u308a\u8fd4\u3059</li> <li>\u53ce\u675f\u3059\u308b\u307e\u3067\uff08\u76ee\u7684\u95a2\u6570\u306e\u5909\u5316\u304c\u5341\u5206\u5c0f\u3055\u304f\u306a\u308b\u307e\u3067\uff091-4\u3092\u53cd\u5fa9</li> </ol> <p>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306e\u5229\u70b9\u306f\u3001\u5404\u56e0\u5b50\u304c\u4e92\u3044\u306b\u72ec\u7acb\uff08\u7121\u76f8\u95a2\uff09\u3067\u3042\u308b\u305f\u3081\u89e3\u91c8\u304c\u7c21\u5358\u306a\u3053\u3068\u3067\u3059\u3002\u6b20\u70b9\u306f\u3001\u5b9f\u969b\u306e\u73fe\u8c61\u3067\u306f\u56e0\u5b50\u9593\u306b\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u304c\u591a\u304f\u3001\u305d\u306e\u69cb\u9020\u3092\u6b63\u78ba\u306b\u53cd\u6620\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\u3053\u3068\u3067\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#432","title":"4.3.2 \u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\uff08\u659c\u4ea4\u56de\u8ee2\uff09","text":"<p>\u30d7\u30ed\u30de\u30c3\u30af\u30b9\uff08Promax\uff09\u56de\u8ee2\u306f\u3001\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306e\u7d50\u679c\u3092\u3055\u3089\u306b\u300c\u30b7\u30e3\u30fc\u30d7\u5316\u300d\u3059\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u5358\u7d14\u306a\u69cb\u9020\u3092\u5f97\u308b\u659c\u4ea4\u56de\u8ee2\u6cd5\u3067\u3059\u3002\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9\u3059\u308b\u305f\u3081\u3001\u3088\u308a\u73fe\u5b9f\u7684\u306a\u30e2\u30c7\u30eb\u304c\u5f97\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306e\u624b\u9806\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u3092\u5b9f\u884c\u3057\u3066\u521d\u671f\u306e\u76f4\u4ea4\u89e3\u3092\u5f97\u308b</li> <li>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u5f8c\u306e\u8ca0\u8377\u91cf\u3092\u3079\u304d\u4e57\u5909\u63db\u3057\u3066\u30bf\u30fc\u30b2\u30c3\u30c8\u884c\u5217\u3092\u4f5c\u6210</li> <li>\u30bf\u30fc\u30b2\u30c3\u30c8\u884c\u5217\u306b\u8fd1\u3065\u304f\u3088\u3046\u306b\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\u3092\u56de\u8ee2\uff08\u659c\u4ea4\u56de\u8ee2\uff09</li> </ol> <p>\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u884c\u5217\uff1a \\(T_{ij} = \\lambda_{ij}^{\\mathrm{varimax}} \\cdot |\\lambda_{ij}^{\\mathrm{varimax}}|^{k-1}\\)</p> <p>\u3053\u3053\u3067\uff1a - \\(\\lambda_{ij}^{\\mathrm{varimax}}\\) \u306f\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u91cf - \\(k\\) \u306f\u3079\u304d\u4e57\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u901a\u5e38 \\(k = 2,3,4\\)\uff09 - \\(|\\lambda_{ij}^{\\mathrm{varimax}}|\\) \u306f\u7d76\u5bfe\u5024\u3092\u8868\u3059</p> <p>\u3053\u306e\u3079\u304d\u4e57\u5909\u63db\u306b\u3088\u308a\u3001\u5927\u304d\u306a\u8ca0\u8377\u91cf\u306f\u3088\u308a\u5927\u304d\u304f\u3001\u5c0f\u3055\u306a\u8ca0\u8377\u91cf\u306f\u3088\u308a\u5c0f\u3055\u304f\u306a\u308a\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u8ca0\u8377\u91cf\u306e\u30d1\u30bf\u30fc\u30f3\u304c\u3088\u308a\u300c\u30b7\u30e3\u30fc\u30d7\u300d\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u6b21\u306b\u3001\u3053\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u884c\u5217\u306b\u8fd1\u3065\u304f\u3088\u3046\u306a\u659c\u4ea4\u56de\u8ee2\u3092\u884c\u3044\u307e\u3059\u3002\u3053\u306e\u904e\u7a0b\u3067\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u8a31\u5bb9\u3055\u308c\u307e\u3059\u3002\u6700\u7d42\u7684\u306b\u5f97\u3089\u308c\u308b\u56e0\u5b50\u69cb\u9020\u306f\u3001\u4ee5\u4e0b\u306e2\u3064\u306e\u884c\u5217\u3067\u8868\u3055\u308c\u307e\u3059\uff1a</p> <ol> <li>\u30d1\u30bf\u30fc\u30f3\u884c\u5217\uff08Pattern Matrix\uff09\uff1a\u5404\u5909\u6570\u3068\u5404\u56e0\u5b50\u9593\u306e\u76f4\u63a5\u7684\u306a\u95a2\u4fc2\u3092\u8868\u3059</li> <li>\u69cb\u9020\u884c\u5217\uff08Structure Matrix\uff09\uff1a\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8003\u616e\u3057\u305f\u3001\u5404\u5909\u6570\u3068\u5404\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8868\u3059</li> </ol> <p>\u30d1\u30bf\u30fc\u30f3\u884c\u5217\u3068\u69cb\u9020\u884c\u5217\u306e\u95a2\u4fc2\uff1a \u69cb\u9020\u884c\u5217 = \u30d1\u30bf\u30fc\u30f3\u884c\u5217 \u00d7 \u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217</p> <p>\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306e\u5229\u70b9\u306f\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9\u3059\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u73fe\u5b9f\u7684\u306a\u30e2\u30c7\u30eb\u304c\u5f97\u3089\u308c\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3053\u3068\u3067\u3059\u3002\u7279\u306b\u5fc3\u7406\u5b66\u3001\u793e\u4f1a\u79d1\u5b66\u3001\u5065\u5eb7\u79d1\u5b66\u306a\u3069\u306e\u5206\u91ce\u3067\u306f\u3001\u5b8c\u5168\u306b\u72ec\u7acb\u3057\u305f\u56e0\u5b50\u3088\u308a\u3082\u76f8\u95a2\u306e\u3042\u308b\u56e0\u5b50\u3092\u4eee\u5b9a\u3059\u308b\u65b9\u304c\u81ea\u7136\u306a\u5834\u5408\u304c\u591a\u3044\u3067\u3059\u3002\u6b20\u70b9\u306f\u3001\u76f4\u4ea4\u56de\u8ee2\u306b\u6bd4\u3079\u3066\u89e3\u91c8\u304c\u3084\u3084\u8907\u96d1\u306b\u306a\u308b\u3053\u3068\u3067\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#44","title":"4.4 \u56de\u8ee2\u7d50\u679c\u306e\u89e3\u91c8","text":"<p>\u56e0\u5b50\u56de\u8ee2\u5f8c\u3001\u4ee5\u4e0b\u306e\u70b9\u306b\u6ce8\u76ee\u3057\u3066\u7d50\u679c\u3092\u89e3\u91c8\u3057\u307e\u3059\uff1a</p>"},{"location":"lectures/LA/45-factor-analysis/#441","title":"4.4.1 \u56e0\u5b50\u8ca0\u8377\u91cf\u30d1\u30bf\u30fc\u30f3\u306e\u89e3\u91c8","text":"<p>\u56e0\u5b50\u8ca0\u8377\u91cf\u306f\u3001\u5404\u89b3\u6e2c\u5909\u6570\u3068\u5404\u56e0\u5b50\u306e\u95a2\u9023\u306e\u5f37\u3055\u3092\u8868\u3057\u307e\u3059\u3002\u7d76\u5bfe\u5024\u304c\u5927\u304d\u3044\u307b\u3069\u95a2\u9023\u304c\u5f37\u3044\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002</p> <p>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u89e3\u91c8\u306e\u57fa\u6e96\uff08\u4e00\u822c\u7684\u306a\u76ee\u5b89\uff09\uff1a - |\u8ca0\u8377\u91cf| \u2265 0.7\uff1a\u975e\u5e38\u306b\u5f37\u3044\u95a2\u9023 - 0.5 \u2264 |\u8ca0\u8377\u91cf| &lt; 0.7\uff1a\u5f37\u3044\u95a2\u9023 - 0.3 \u2264 |\u8ca0\u8377\u91cf| &lt; 0.5\uff1a\u4e2d\u7a0b\u5ea6\u306e\u95a2\u9023 - |\u8ca0\u8377\u91cf| &lt; 0.3\uff1a\u5f31\u3044\u95a2\u9023\uff08\u901a\u5e38\u306f\u89e3\u91c8\u306b\u542b\u3081\u306a\u3044\uff09</p> <p>\u5404\u56e0\u5b50\u306f\u3001\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064\u5909\u6570\u7fa4\u306e\u5171\u901a\u306e\u7279\u6027\u3092\u8868\u3059\u3068\u89e3\u91c8\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u300c\u4e0d\u5b89\u300d\u300c\u6291\u3046\u3064\u300d\u300c\u7dca\u5f35\u300d\u3068\u3044\u3063\u305f\u5909\u6570\u304c\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064\u56e0\u5b50\u306f\u3001\u300c\u5fc3\u7406\u7684\u30b9\u30c8\u30ec\u30b9\u300d\u306a\u3069\u3068\u89e3\u91c8\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#442","title":"4.4.2 \u56e0\u5b50\u9593\u76f8\u95a2\u306e\u89e3\u91c8","text":"<p>\u659c\u4ea4\u56de\u8ee2\uff08\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u306a\u3069\uff09\u306e\u5834\u5408\u3001\u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u3053\u306e\u76f8\u95a2\u304c\u9ad8\u3044\u5834\u5408\uff08\u4f8b\uff1a|\u76f8\u95a2| &gt; 0.3\uff09\u3001\u5bfe\u5fdc\u3059\u308b\u56e0\u5b50\u9593\u306b\u306f\u95a2\u9023\u304c\u3042\u308b\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> <p>\u56e0\u5b50\u9593\u76f8\u95a2\u304c\u975e\u5e38\u306b\u9ad8\u3044\u5834\u5408\uff08\u4f8b\uff1a|\u76f8\u95a2| &gt; 0.7\uff09\u3001\u3053\u308c\u3089\u306e\u56e0\u5b50\u306f\u5206\u96e2\u305b\u305a\u3001\u3088\u308a\u9ad8\u6b21\u306e\u56e0\u5b50\u3068\u3057\u3066\u7d71\u5408\u3059\u308b\u3053\u3068\u3092\u691c\u8a0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#443","title":"4.4.3 \u5171\u901a\u6027\u306e\u89e3\u91c8","text":"<p>\u5404\u5909\u6570\u306e\u5171\u901a\u6027\uff08communality\uff09\u306f\u3001\u3059\u3079\u3066\u306e\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u5206\u6563\u306e\u5272\u5408\u3092\u8868\u3057\u307e\u3059\u3002</p> <p>\u5171\u901a\u6027\u306e\u8a08\u7b97\uff1a \\(h_i^2 = \\sum_{j=1}^{m} \\lambda_{ij}^2\\)</p> <p>\u3053\u3053\u3067 \\(\\lambda_{ij}\\) \u306f\u5909\u6570 \\(i\\) \u306e\u56e0\u5b50 \\(j\\) \u306b\u5bfe\u3059\u308b\u8ca0\u8377\u91cf\u3067\u3059\u3002</p> <p>\u5171\u901a\u6027\u304c\u4f4e\u3044\u5909\u6570\uff08\u4f8b\uff1a\\(h_i^2 &lt; 0.3\\)\uff09\u306f\u3001\u73fe\u5728\u306e\u30e2\u30c7\u30eb\u3067\u306f\u3046\u307e\u304f\u8aac\u660e\u3055\u308c\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u307e\u3059\u3002\u305d\u306e\u3088\u3046\u306a\u5909\u6570\u306f\u3001\u30e2\u30c7\u30eb\u304b\u3089\u9664\u5916\u3059\u308b\u304b\u3001\u5225\u306e\u5206\u6790\u3092\u691c\u8a0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#444","title":"4.4.4 \u56e0\u5b50\u30b9\u30b3\u30a2\u306e\u89e3\u91c8","text":"<p>\u56e0\u5b50\u30b9\u30b3\u30a2\u306f\u3001\u5404\u89b3\u6e2c\u5bfe\u8c61\uff08\u500b\u4eba\u306a\u3069\uff09\u306e\u5404\u56e0\u5b50\u306b\u304a\u3051\u308b\u300c\u4f4d\u7f6e\u300d\u3092\u8868\u3057\u307e\u3059\u3002\u56e0\u5b50\u30b9\u30b3\u30a2\u306e\u5206\u5e03\u3084\u6975\u7aef\u306a\u5024\u3092\u6301\u3064\u30b1\u30fc\u30b9\u3092\u5206\u6790\u3059\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308b\u7279\u5fb4\u7684\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u767a\u898b\u3067\u304d\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u56e0\u5b50\u30b9\u30b3\u30a2\u306f\u6a19\u6e96\u5316\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u591a\u304f\u3001\u5e73\u57470\u3001\u6a19\u6e96\u504f\u5dee1\u306e\u5206\u5e03\u3092\u6301\u3061\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001\u56e0\u5b50\u30b9\u30b3\u30a2\u304c\u00b12\u3092\u8d85\u3048\u308b\u30b1\u30fc\u30b9\u306f\u3001\u305d\u306e\u56e0\u5b50\u306b\u304a\u3044\u3066\u7279\u306b\u9ad8\u3044\uff08\u307e\u305f\u306f\u4f4e\u3044\uff09\u7279\u6027\u3092\u6301\u3064\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#5-python","title":"5. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<p>\u4ee5\u4e0b\u306b\u3001Python\u306escikit-learn\u3068factoranalyzer\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u7528\u3044\u305f\u56e0\u5b50\u5206\u6790\u306e\u8a73\u7d30\u306a\u5b9f\u88c5\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002\u5404\u30b9\u30c6\u30c3\u30d7\u306b\u3064\u3044\u3066\u4e01\u5be7\u306b\u89e3\u8aac\u3057\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom factor_analyzer import FactorAnalyzer\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nfrom factor_analyzer.factor_analyzer import calculate_kmo\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3068\u3057\u3066Iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\nprint(\"\u30c7\u30fc\u30bf\u306e\u6700\u521d\u306e5\u884c:\")\nprint(X.head())\nprint(\"\\n\u30c7\u30fc\u30bf\u306e\u57fa\u672c\u7d71\u8a08\u91cf:\")\nprint(X.describe())\n\n# \u30c7\u30fc\u30bf\u306e\u76f8\u95a2\u884c\u5217\u3092\u53ef\u8996\u5316\nplt.figure(figsize=(10, 8))\ncorrelation_matrix = X.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('\u30a2\u30e4\u30e1\u30c7\u30fc\u30bf\u306e\u76f8\u95a2\u884c\u5217')\nplt.tight_layout()\nplt.show()\n\n# Bartlett\u306e\u7403\u9762\u6027\u691c\u5b9a\n# \u3053\u306e\u691c\u5b9a\u306f\u3001\u76f8\u95a2\u884c\u5217\u304c\u5358\u4f4d\u884c\u5217\uff08\u3059\u3079\u3066\u306e\u5909\u6570\u304c\u7121\u76f8\u95a2\uff09\u3067\u3042\u308b\u5e30\u7121\u4eee\u8aac\u3092\u691c\u5b9a\nchi_square_value, p_value = calculate_bartlett_sphericity(X)\nprint(f\"Bartlett\u306e\u7403\u9762\u6027\u691c\u5b9a: chi\u00b2={chi_square_value:.4f}, p={p_value:.10f}\")\nprint(\"p\u5024\u304c\u975e\u5e38\u306b\u5c0f\u3055\u3044\u305f\u3081\u3001\u5909\u6570\u9593\u306b\u6709\u610f\u306a\u76f8\u95a2\u304c\u3042\u308a\u3001\u56e0\u5b50\u5206\u6790\u304c\u9069\u5207\u3067\u3042\u308b\u3053\u3068\u304c\u793a\u3055\u308c\u307e\u3057\u305f\u3002\")\n\n# KMO\uff08Kaiser-Meyer-Olkin\uff09\u6e2c\u5ea6\n# \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u59a5\u5f53\u6027\u3092\u8a55\u4fa1\u3059\u308b\u6307\u6a19\u30020.6\u4ee5\u4e0a\u304c\u671b\u307e\u3057\u3044\nkmo_all, kmo_model = calculate_kmo(X)\nprint(f\"KMO: {kmo_model:.4f}\")\nif kmo_model &gt; 0.8:\n    print(\"KMO\u5024\u304c0.8\u4ee5\u4e0a\u3067\u3042\u308a\u3001\u56e0\u5b50\u5206\u6790\u306b\u975e\u5e38\u306b\u9069\u3057\u305f\u30c7\u30fc\u30bf\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\")\nelif kmo_model &gt; 0.6:\n    print(\"KMO\u5024\u304c0.6\u4ee5\u4e0a\u3067\u3042\u308a\u3001\u56e0\u5b50\u5206\u6790\u306b\u9069\u3057\u305f\u30c7\u30fc\u30bf\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\")\nelse:\n    print(\"KMO\u5024\u304c0.6\u672a\u6e80\u3067\u3042\u308a\u3001\u56e0\u5b50\u5206\u6790\u306e\u9069\u7528\u306b\u306f\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002\")\n\n# \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\nfa = FactorAnalyzer(rotation=None)\nfa.fit(X)\n\n# \u56fa\u6709\u5024\nev, _ = fa.get_eigenvalues()\nplt.figure(figsize=(10, 6))\nplt.scatter(range(1, X.shape[1] + 1), ev)\nplt.plot(range(1, X.shape[1] + 1), ev)\nplt.title('\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8')\nplt.xlabel('\u56e0\u5b50\u6570')\nplt.ylabel('\u56fa\u6709\u5024')\nplt.grid(True)\nplt.axhline(y=1, linestyle='--', color='red', label='Kaiser\u57fa\u6e96 (\u56fa\u6709\u5024=1)')\nplt.legend()\nplt.show()\n\nprint(\"\\n\u56fa\u6709\u5024:\")\nfor i, eigenvalue in enumerate(ev, 1):\n    print(f\"\u56e0\u5b50 {i}: {eigenvalue:.4f}\")\n\n# \u7d2f\u7a4d\u5bc4\u4e0e\u7387\u3092\u8a08\u7b97\ntotal_variance = sum(ev)\ncumulative_variance_ratio = np.cumsum(ev) / total_variance\nprint(\"\\n\u7d2f\u7a4d\u5bc4\u4e0e\u7387:\")\nfor i, ratio in enumerate(cumulative_variance_ratio, 1):\n    print(f\"\u56e0\u5b50 {i}\u307e\u3067: {ratio:.4f} ({ratio*100:.2f}%)\")\n\n# \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3068\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306e\u5206\u6790\u7d50\u679c\u304b\u3089\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\n# \u4eca\u56de\u306f\u4f8b\u3068\u3057\u30662\u56e0\u5b50\u3092\u63a1\u7528\u3059\u308b\nn_factors = 2\nprint(f\"\\n\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3001Kaiser\u57fa\u6e96\u3001\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u3092\u7dcf\u5408\u7684\u306b\u5224\u65ad\u3057\u3001{n_factors}\u56e0\u5b50\u3092\u63a1\u7528\u3057\u307e\u3059\u3002\")\n\n# \u56e0\u5b50\u6570\u30922\u3068\u4eee\u5b9a\u3057\u3066\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u884c\uff08\u56de\u8ee2\u306a\u3057\uff09\nfa_no_rotation = FactorAnalyzer(n_factors=n_factors, rotation=None, method='ml')\nfa_no_rotation.fit(X)\nloadings_no_rotation = fa_no_rotation.loadings_\n\n# \u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306b\u3088\u308b\u56e0\u5b50\u5206\u6790\nfa_varimax = FactorAnalyzer(n_factors=n_factors, rotation='varimax', method='ml')\nfa_varimax.fit(X)\nloadings_varimax = fa_varimax.loadings_\n\n# \u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306b\u3088\u308b\u56e0\u5b50\u5206\u6790\nfa_promax = FactorAnalyzer(n_factors=n_factors, rotation='promax', method='ml')\nfa_promax.fit(X)\nloadings_promax = fa_promax.loadings_\nfactor_corr = fa_promax.phi_  # \u56e0\u5b50\u9593\u76f8\u95a2\n\n# \u7d50\u679c\u306e\u8868\u793a\nloadings_df_no_rotation = pd.DataFrame(loadings_no_rotation, \n                                      index=X.columns, \n                                      columns=[f'\u56e0\u5b50 {i+1}' for i in range(n_factors)])\nloadings_df_varimax = pd.DataFrame(loadings_varimax, \n                                  index=X.columns, \n                                  columns=[f'\u56e0\u5b50 {i+1}' for i in range(n_factors)])\nloadings_df_promax = pd.DataFrame(loadings_promax, \n                                 index=X.columns, \n                                 columns=[f'\u56e0\u5b50 {i+1}' for i in range(n_factors)])\n\nprint(\"\\n\u56de\u8ee2\u306a\u3057\u306e\u56e0\u5b50\u8ca0\u8377\u91cf:\")\nprint(loadings_df_no_rotation)\n\nprint(\"\\n\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u91cf:\")\nprint(loadings_df_varimax)\n\nprint(\"\\n\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u91cf:\")\nprint(loadings_df_promax)\n\nprint(\"\\n\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u9593\u76f8\u95a2:\")\nprint(pd.DataFrame(factor_corr, \n                  index=[f'\u56e0\u5b50 {i+1}' for i in range(n_factors)], \n                  columns=[f'\u56e0\u5b50 {i+1}' for i in range(n_factors)]))\n\n# \u5171\u901a\u6027\ncommunalities = fa_varimax.get_communalities()\nprint(\"\\n\u5171\u901a\u6027:\")\nprint(pd.DataFrame({'\u5171\u901a\u6027': communalities}, index=X.columns))\n\n# \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u53ef\u8996\u5316\nplt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nsns.heatmap(loadings_df_no_rotation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.3f')\nplt.title('\u56de\u8ee2\u306a\u3057')\n\nplt.subplot(1, 3, 2)\nsns.heatmap(loadings_df_varimax, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.3f')\nplt.title('\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2')\n\nplt.subplot(1, 3, 3)\nsns.heatmap(loadings_df_promax, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.3f')\nplt.title('\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2')\n\nplt.tight_layout()\nplt.show()\n\n# \u56e0\u5b50\u30b9\u30b3\u30a2\u306e\u8a08\u7b97\nfactor_scores = fa_varimax.transform(X)\nfactor_scores_df = pd.DataFrame(factor_scores, \n                               columns=[f'\u56e0\u5b50 {i+1}' for i in range(n_factors)])\n\n# \u56e0\u5b50\u30b9\u30b3\u30a2\u306e\u6563\u5e03\u56f3\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(factor_scores_df['\u56e0\u5b50 1'], \n                     factor_scores_df['\u56e0\u5b50 2'], \n                     c=iris.target, \n                     cmap='viridis', \n                     alpha=0.7)\nplt.title('\u56e0\u5b50\u30b9\u30b3\u30a2\u306e\u6563\u5e03\u56f3')\nplt.xlabel('\u56e0\u5b50 1')\nplt.ylabel('\u56e0\u5b50 2')\nplt.grid(True)\nlegend = plt.colorbar(scatter)\nlegend.set_label('\u30a2\u30e4\u30e1\u306e\u7a2e\u985e')\n\n# \u30af\u30e9\u30b9\u3054\u3068\u306e\u4e2d\u5fc3\u3092\u8ffd\u52a0\nfor i, species in enumerate(['setosa', 'versicolor', 'virginica']):\n    idx = np.where(iris.target == i)\n    centroid_x = np.mean(factor_scores[idx, 0])\n    centroid_y = np.mean(factor_scores[idx, 1])\n    plt.scatter(centroid_x, centroid_y, marker='X', s=200, \n                edgecolor='black', facecolor='none')\n    plt.annotate(species, (centroid_x, centroid_y), \n                xytext=(10, 10), textcoords='offset points',\n                fontsize=12, fontweight='bold')\n\nplt.show()\n\n# \u5404\u56de\u8ee2\u65b9\u6cd5\u306e\u89e3\u91c8\nprint(\"\\n\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u89e3\u91c8\uff08\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u5f8c\uff09:\")\nfor var_name in X.columns:\n    max_loading_idx = np.argmax(np.abs(loadings_df_varimax.loc[var_name].values))\n    max_loading = loadings_df_varimax.loc[var_name].values[max_loading_idx]\n    factor_name = f\"\u56e0\u5b50 {max_loading_idx+1}\"\n    loading_strength = \"\"\n    if abs(max_loading) &gt;= 0.7:\n        loading_strength = \"\u975e\u5e38\u306b\u5f37\u3044\"\n    elif abs(max_loading) &gt;= 0.5:\n        loading_strength = \"\u5f37\u3044\"\n    elif abs(max_loading) &gt;= 0.3:\n        loading_strength = \"\u4e2d\u7a0b\u5ea6\u306e\"\n    else:\n        loading_strength = \"\u5f31\u3044\"\n\n    print(f\"{var_name}\u306f{factor_name}\u306b{loading_strength}\u8ca0\u8377\uff08{max_loading:.3f}\uff09\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\")\n\n# \u56e0\u5b50\u306e\u547d\u540d\nprint(\"\\n\u56e0\u5b50\u306e\u89e3\u91c8:\")\nprint(\"\u56e0\u5b501: \u82b1\u3068\u95a2\u9023\u3059\u308b\u7279\u5fb4\u3092\u8868\u3059\u300c\u82b1\u306e\u5f62\u614b\u56e0\u5b50\u300d\")\nprint(\"\u56e0\u5b502: \u8449\u3068\u95a2\u9023\u3059\u308b\u7279\u5fb4\u3092\u8868\u3059\u300c\u8449\u306e\u5f62\u614b\u56e0\u5b50\u300d\")\n</code></pre> <p>\u3053\u306e\u30b3\u30fc\u30c9\u306e\u5b9f\u884c\u7d50\u679c\u3092\u89e3\u8aac\u3057\u307e\u3059\uff1a</p> <ol> <li>\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3068\u9069\u5408\u6027\u306e\u78ba\u8a8d\uff1a</li> <li>Bartlett\u306e\u7403\u9762\u6027\u691c\u5b9a\u306b\u3088\u308a\u3001\u5909\u6570\u9593\u306b\u6709\u610f\u306a\u76f8\u95a2\u304c\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\uff08\u56e0\u5b50\u5206\u6790\u306e\u9069\u7528\u6761\u4ef6\uff09</li> <li> <p>KMO\u6e2c\u5ea6\u306b\u3088\u308a\u3001\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u59a5\u5f53\u6027\u3092\u8a55\u4fa1\uff08\u5024\u304c0.6\u4ee5\u4e0a\u3067\u3042\u308c\u3070\u826f\u597d\uff09</p> </li> <li> <p>\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a\uff1a</p> </li> <li>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306b\u3088\u308a\u56fa\u6709\u5024\u306e\u6e1b\u5c11\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316</li> <li>Kaiser\u57fa\u6e96\uff08\u56fa\u6709\u5024\uff1e1\uff09\u3092\u9069\u7528</li> <li>\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u3092\u8a08\u7b97\uff08\u5168\u5206\u6563\u306e\u3069\u308c\u3060\u3051\u306e\u5272\u5408\u3092\u8aac\u660e\u3067\u304d\u308b\u304b\uff09</li> <li> <p>\u3053\u308c\u3089\u306e\u65b9\u6cd5\u3092\u7dcf\u5408\u7684\u306b\u5224\u65ad\u3057\u3066\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a</p> </li> <li> <p>\u69d8\u3005\u306a\u56de\u8ee2\u65b9\u6cd5\u306b\u3088\u308b\u56e0\u5b50\u5206\u6790\uff1a</p> </li> <li>\u56de\u8ee2\u306a\u3057\u3001\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\uff08\u76f4\u4ea4\uff09\u3001\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\uff08\u659c\u4ea4\uff09\u306e3\u7a2e\u985e\u306e\u65b9\u6cd5\u3067\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u884c</li> <li> <p>\u5404\u65b9\u6cd5\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\u3092\u6bd4\u8f03</p> </li> <li> <p>\u7d50\u679c\u306e\u89e3\u91c8\u3068\u53ef\u8996\u5316\uff1a</p> </li> <li>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u306b\u3088\u308b\u8996\u899a\u5316</li> <li>\u5171\u901a\u6027\u306e\u8a08\u7b97\u3068\u89e3\u91c8</li> <li>\u56e0\u5b50\u30b9\u30b3\u30a2\u306e\u6563\u5e03\u56f3\u306b\u3088\u308b\u8996\u899a\u5316</li> <li>\u5404\u5909\u6570\u306e\u56e0\u5b50\u3078\u306e\u8ca0\u8377\u91cf\u306e\u5f37\u3055\u3068\u65b9\u5411\u306b\u57fa\u3065\u304f\u89e3\u91c8</li> <li>\u56e0\u5b50\u306e\u547d\u540d\u3068\u5168\u4f53\u69cb\u9020\u306e\u89e3\u91c8</li> </ol>"},{"location":"lectures/LA/45-factor-analysis/#51","title":"5.1 \u5065\u5eb7\u30c7\u30fc\u30bf\u3078\u306e\u5fdc\u7528\u4f8b","text":"<p>\u4ee5\u4e0b\u306f\u3001\u5065\u5eb7\u95a2\u9023\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u5206\u6790\u306e\u5fdc\u7528\u4f8b\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u4eba\u5de5\u7684\u306b\u751f\u6210\u3057\u305f\u5065\u5eb7\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u969b\u306e\u5fdc\u7528\u65b9\u6cd5\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code># \u5065\u5eb7\u95a2\u9023\u30c7\u30fc\u30bf\u306e\u56e0\u5b50\u5206\u6790\uff08\u4f8b\u793a\u7528\u306e\u4eba\u5de5\u30c7\u30fc\u30bf\uff09\nnp.random.seed(42)\nn_samples = 200\n\n# 2\u3064\u306e\u6f5c\u5728\u56e0\u5b50\u3092\u4f5c\u6210\uff08\u8eab\u4f53\u7684\u5065\u5eb7\u3068\u7cbe\u795e\u7684\u5065\u5eb7\uff09\nphysical_health = np.random.normal(0, 1, n_samples)\nmental_health = np.random.normal(0, 1, n_samples)\n\n# \u6e2c\u5b9a\u5909\u6570\u3092\u751f\u6210\uff08\u30ce\u30a4\u30ba\u3092\u542b\u3080\uff09\nweight = 0.8 * physical_health + 0.1 * mental_health + np.random.normal(0, 0.5, n_samples)\nbmi = 0.7 * physical_health + 0.0 * mental_health + np.random.normal(0, 0.6, n_samples)\nblood_pressure = 0.6 * physical_health + 0.2 * mental_health + np.random.normal(0, 0.7, n_samples)\ncholesterol = 0.5 * physical_health + 0.1 * mental_health + np.random.normal(0, 0.7, n_samples)\n\nstress = 0.2 * physical_health + 0.8 * mental_health + np.random.normal(0, 0.5, n_samples)\nanxiety = 0.1 * physical_health + 0.7 * mental_health + np.random.normal(0, 0.6, n_samples)\ndepression = 0.2 * physical_health + 0.6 * mental_health + np.random.normal(0, 0.7, n_samples)\nsleep_quality = 0.3 * physical_health + 0.5 * mental_health + np.random.normal(0, 0.7, n_samples)\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u4f5c\u6210\nhealth_data = pd.DataFrame({\n    '\u4f53\u91cd': weight,\n    'BMI': bmi,\n    '\u8840\u5727': blood_pressure,\n    '\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb': cholesterol,\n    '\u30b9\u30c8\u30ec\u30b9': stress,\n    '\u4e0d\u5b89': anxiety,\n    '\u3046\u3064\u75c7\u72b6': depression,\n    '\u7761\u7720\u306e\u8cea': sleep_quality\n})\n\n# \u30c7\u30fc\u30bf\u306e\u76f8\u95a2\u884c\u5217\u3092\u53ef\u8996\u5316\nplt.figure(figsize=(10, 8))\ncorrelation_matrix = health_data.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u76f8\u95a2\u884c\u5217')\nplt.tight_layout()\nplt.show()\n\n# Bartlett\u306e\u7403\u9762\u6027\u691c\u5b9a\nchi_square_value, p_value = calculate_bartlett_sphericity(health_data)\nprint(f\"Bartlett\u306e\u7403\u9762\u6027\u691c\u5b9a: chi\u00b2={chi_square_value:.4f}, p={p_value:.10f}\")\n\n# KMO\u6e2c\u5ea6\nkmo_all, kmo_model = calculate_kmo(health_data)\nprint(f\"KMO: {kmo_model:.4f}\")\n\n# \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\nfa_health = FactorAnalyzer(rotation=None)\nfa_health.fit(health_data)\nev, _ = fa_health.get_eigenvalues()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(range(1, health_data.shape[1] + 1), ev)\nplt.plot(range(1, health_data.shape[1] + 1), ev)\nplt.title('\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8')\nplt.xlabel('\u56e0\u5b50\u6570')\nplt.ylabel('\u56fa\u6709\u5024')\nplt.grid(True)\nplt.axhline(y=1, linestyle='--', color='red', label='Kaiser\u57fa\u6e96 (\u56fa\u6709\u5024=1)')\nplt.legend()\nplt.show()\n\n# \u7d2f\u7a4d\u5bc4\u4e0e\u7387\u3092\u8a08\u7b97\ntotal_variance = sum(ev)\ncumulative_variance_ratio = np.cumsum(ev) / total_variance\nprint(\"\\n\u7d2f\u7a4d\u5bc4\u4e0e\u7387:\")\nfor i, ratio in enumerate(cumulative_variance_ratio, 1):\n    print(f\"\u56e0\u5b50 {i}\u307e\u3067: {ratio:.4f} ({ratio*100:.2f}%)\")\n\n# \u56e0\u5b50\u5206\u6790\uff08\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\uff09\nn_factors = 2  # \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u304b\u30892\u56e0\u5b50\u304c\u9069\u5207\u3068\u5224\u65ad\nfa_health = FactorAnalyzer(n_factors=n_factors, rotation='promax', method='ml')\nfa_health.fit(health_data)\nloadings_health = fa_health.loadings_\nfactor_corr_health = fa_health.phi_  # \u56e0\u5b50\u9593\u76f8\u95a2\n\n# \u7d50\u679c\u8868\u793a\nloadings_df_health = pd.DataFrame(loadings_health, \n                                 index=health_data.columns, \n                                 columns=['\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50', '\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50'])\nprint(\"\\n\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\uff08\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u5f8c\uff09:\")\nprint(loadings_df_health)\n\n# \u5171\u901a\u6027\ncommunalities_health = fa_health.get_communalities()\nprint(\"\\n\u5171\u901a\u6027:\")\nprint(pd.DataFrame({'\u5171\u901a\u6027': communalities_health}, index=health_data.columns))\n\n# \u56e0\u5b50\u9593\u76f8\u95a2\nprint(\"\\n\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u56e0\u5b50\u9593\u76f8\u95a2:\")\nprint(pd.DataFrame(factor_corr_health, \n                  index=['\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50', '\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50'], \n                  columns=['\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50', '\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50']))\n\n# \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\nplt.figure(figsize=(10, 8))\nsns.heatmap(loadings_df_health, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.3f')\nplt.title('\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u56e0\u5b50\u69cb\u9020\uff08\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u5f8c\uff09')\nplt.tight_layout()\nplt.show()\n\n# \u56e0\u5b50\u30b9\u30b3\u30a2\u306e\u8a08\u7b97\nfactor_scores_health = fa_health.transform(health_data)\nfactor_scores_df_health = pd.DataFrame(factor_scores_health, \n                                     columns=['\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50', '\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50'])\n\n# \u56e0\u5b50\u30b9\u30b3\u30a2\u306e\u6563\u5e03\u56f3\nplt.figure(figsize=(10, 8))\nplt.scatter(factor_scores_df_health['\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50'], \n           factor_scores_df_health['\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50'], \n           alpha=0.7)\nplt.title('\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u56e0\u5b50\u30b9\u30b3\u30a2\u6563\u5e03\u56f3')\nplt.xlabel('\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50')\nplt.ylabel('\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50')\nplt.grid(True)\nplt.axhline(y=0, color='gray', linestyle='--')\nplt.axvline(x=0, color='gray', linestyle='--')\n\n# \u8c61\u9650\u306b\u540d\u524d\u3092\u4ed8\u3051\u308b\nplt.text(2, 2, '\u8eab\u4f53\u7684\u30fb\u7cbe\u795e\u7684\u306b\u5065\u5eb7', fontsize=12, ha='right')\nplt.text(-2, 2, '\u7cbe\u795e\u7684\u306b\u5065\u5eb7\\n\u8eab\u4f53\u7684\u306b\u4e0d\u5065\u5eb7', fontsize=12, ha='left')\nplt.text(2, -2, '\u8eab\u4f53\u7684\u306b\u5065\u5eb7\\n\u7cbe\u795e\u7684\u306b\u4e0d\u5065\u5eb7', fontsize=12, ha='right')\nplt.text(-2, -2, '\u8eab\u4f53\u7684\u30fb\u7cbe\u795e\u7684\u306b\u4e0d\u5065\u5eb7', fontsize=12, ha='left')\n\nplt.show()\n\n# \u5909\u6570\u306e\u89e3\u91c8\nprint(\"\\n\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u89e3\u91c8:\")\nfor var_name in health_data.columns:\n    max_loading_idx = np.argmax(np.abs(loadings_df_health.loc[var_name].values))\n    max_loading = loadings_df_health.loc[var_name].values[max_loading_idx]\n    factor_name = loadings_df_health.columns[max_loading_idx]\n    loading_strength = \"\"\n    if abs(max_loading) &gt;= 0.7:\n        loading_strength = \"\u975e\u5e38\u306b\u5f37\u3044\"\n    elif abs(max_loading) &gt;= 0.5:\n        loading_strength = \"\u5f37\u3044\"\n    elif abs(max_loading) &gt;= 0.3:\n        loading_strength = \"\u4e2d\u7a0b\u5ea6\u306e\"\n    else:\n        loading_strength = \"\u5f31\u3044\"\n\n    print(f\"{var_name}\u306f{factor_name}\u306b{loading_strength}\u8ca0\u8377\uff08{max_loading:.3f}\uff09\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\")\n\n# \u5065\u5eb7\u30ea\u30b9\u30af\u8a55\u4fa1\uff08\u4f8b\uff09\n# \u56e0\u5b50\u30b9\u30b3\u30a2\u306b\u57fa\u3065\u3044\u3066\u30ea\u30b9\u30af\u30b0\u30eb\u30fc\u30d7\u3092\u5206\u985e\ndef classify_risk(phys_score, mental_score):\n    if phys_score &lt; -1 and mental_score &lt; -1:\n        return \"\u9ad8\u30ea\u30b9\u30af\uff08\u8eab\u4f53\u7684\u30fb\u7cbe\u795e\u7684\u8981\u56e0\uff09\"\n    elif phys_score &lt; -1:\n        return \"\u4e2d\u30ea\u30b9\u30af\uff08\u4e3b\u306b\u8eab\u4f53\u7684\u8981\u56e0\uff09\"\n    elif mental_score &lt; -1:\n        return \"\u4e2d\u30ea\u30b9\u30af\uff08\u4e3b\u306b\u7cbe\u795e\u7684\u8981\u56e0\uff09\"\n    else:\n        return \"\u4f4e\u30ea\u30b9\u30af\"\n\n# \u30ea\u30b9\u30af\u5206\u985e\u3092\u8ffd\u52a0\nfactor_scores_df_health['\u30ea\u30b9\u30af\u5206\u985e'] = factor_scores_df_health.apply(\n    lambda row: classify_risk(row['\u8eab\u4f53\u7684\u5065\u5eb7\u56e0\u5b50'], row['\u7cbe\u795e\u7684\u5065\u5eb7\u56e0\u5b50']), axis=1)\n\n# \u30ea\u30b9\u30af\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u4eba\u6570\u3092\u30ab\u30a6\u30f3\u30c8\nrisk_counts = factor_scores_df_health['\u30ea\u30b9\u30af\u5206\u985e'].value_counts()\nprint(\"\\n\u5065\u5eb7\u30ea\u30b9\u30af\u5206\u985e:\")\nprint(risk_counts)\n\n# \u30ea\u30b9\u30af\u30b0\u30eb\u30fc\u30d7\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nrisk_counts.plot(kind='bar', color='skyblue')\nplt.title('\u5065\u5eb7\u30ea\u30b9\u30af\u5206\u985e\u306e\u5206\u5e03')\nplt.xlabel('\u30ea\u30b9\u30af\u5206\u985e')\nplt.ylabel('\u4eba\u6570')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\u3053\u306e\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u5fdc\u7528\u4f8b\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u3092\u884c\u3063\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\u4eba\u5de5\u7684\u306a\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\uff1a</li> <li> <p>\u8eab\u4f53\u7684\u5065\u5eb7\u3068\u7cbe\u795e\u7684\u5065\u5eb7\u306e2\u3064\u306e\u6f5c\u5728\u56e0\u5b50\u3092\u57fa\u306b\u30018\u3064\u306e\u5065\u5eb7\u6307\u6a19\uff08\u4f53\u91cd\u3001BMI\u3001\u8840\u5727\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u3001\u30b9\u30c8\u30ec\u30b9\u3001\u4e0d\u5b89\u3001\u3046\u3064\u75c7\u72b6\u3001\u7761\u7720\u306e\u8cea\uff09\u3092\u751f\u6210</p> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u65bd\uff1a</p> </li> <li>\u76f8\u95a2\u884c\u5217\u306e\u53ef\u8996\u5316\u3001\u9069\u5408\u6027\u306e\u78ba\u8a8d\uff08Bartlett\u306e\u691c\u5b9a\u3001KMO\u6e2c\u5ea6\uff09</li> <li>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3068\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306b\u3088\u308b\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a</li> <li> <p>\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306b\u3088\u308b\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u884c</p> </li> <li> <p>\u7d50\u679c\u306e\u89e3\u91c8\u3068\u5fdc\u7528\uff1a</p> </li> <li>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u53ef\u8996\u5316</li> <li>\u56e0\u5b50\u30b9\u30b3\u30a2\u306e\u6563\u5e03\u56f3\u3068\u305d\u306e\u89e3\u91c8</li> <li>\u5065\u5eb7\u30ea\u30b9\u30af\u8a55\u4fa1\u3078\u306e\u5fdc\u7528\uff08\u56e0\u5b50\u30b9\u30b3\u30a2\u306b\u57fa\u3065\u304f\u30ea\u30b9\u30af\u5206\u985e\uff09</li> </ol> <p>\u3053\u306e\u4f8b\u3067\u306f\u3001\u56e0\u5b50\u5206\u6790\u304c\u5065\u5eb7\u30c7\u30fc\u30bf\u304b\u3089\u6f5c\u5728\u7684\u306a\u5065\u5eb7\u72b6\u614b\uff08\u8eab\u4f53\u7684\u5065\u5eb7\u3068\u7cbe\u795e\u7684\u5065\u5eb7\uff09\u3092\u62bd\u51fa\u3057\u3001\u305d\u308c\u306b\u57fa\u3065\u3044\u3066\u500b\u4eba\u306e\u5065\u5eb7\u30ea\u30b9\u30af\u3092\u8a55\u4fa1\u3059\u308b\u65b9\u6cd5\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u5b9f\u969b\u306e\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u3053\u306e\u3088\u3046\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u5065\u5eb7\u72b6\u614b\u306e\u8a55\u4fa1\u3001\u30ea\u30b9\u30af\u4e88\u6e2c\u3001\u4ecb\u5165\u8a08\u753b\u306e\u7b56\u5b9a\u306a\u3069\u306b\u6d3b\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/45-factor-analysis/#6","title":"6. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/45-factor-analysis/#61","title":"6.1 \u57fa\u672c\u554f\u984c","text":"<ol> <li>\u4ee5\u4e0b\u306e\u76f8\u95a2\u884c\u5217\u306b\u5bfe\u3057\u3066\u56e0\u5b50\u5206\u6790\u3092\u9069\u7528\u3057\u307e\u3059\u3002\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u304b\u3089\u9069\u5207\u306a\u56e0\u5b50\u6570\u3092\u5224\u65ad\u3057\u3001\u305d\u306e\u7406\u7531\u3092\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol> <pre><code>\u76f8\u95a2\u884c\u5217 R:\n[1.00, 0.70, 0.60, 0.30, 0.25]\n[0.70, 1.00, 0.65, 0.35, 0.20]\n[0.60, 0.65, 1.00, 0.40, 0.30]\n[0.30, 0.35, 0.40, 1.00, 0.75]\n[0.25, 0.20, 0.30, 0.75, 1.00]\n</code></pre> <p>\u89e3\u7b54\u4f8b\uff1a    \u3053\u306e\u76f8\u95a2\u884c\u5217\u304b\u3089\u56fa\u6709\u5024\u3092\u8a08\u7b97\u3059\u308b\u3068\u3001[2.68, 1.47, 0.42, 0.29, 0.14]\u306e\u3088\u3046\u306a\u5024\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3067\u306f\u3001\u56fa\u6709\u5024\u304c1.0\u3092\u8d85\u3048\u308b\u56e0\u5b50\u306f2\u3064\uff08\u7b2c1\u56e0\u5b50\u3068\u7b2c2\u56e0\u5b50\uff09\u3067\u3042\u308a\u3001\u7b2c2\u56e0\u5b50\u3068\u7b2c3\u56e0\u5b50\u306e\u9593\u3067\u56fa\u6709\u5024\u306e\u6025\u6fc0\u306a\u6e1b\u5c11\uff08\u300c\u8098\u300d\uff09\u304c\u898b\u3089\u308c\u307e\u3059\u3002\u307e\u305f\u3001\u6700\u521d\u306e2\u56e0\u5b50\u3067\u5168\u5206\u6563\u306e\u7d0483%\uff08(2.68+1.47)/5\u22480.83\uff09\u3092\u8aac\u660e\u3057\u3066\u3044\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001Kaiser\u57fa\u6e96\u3001\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306e\u300c\u8098\u300d\u3001\u7d2f\u7a4d\u5bc4\u4e0e\u7387\u306e\u3044\u305a\u308c\u306e\u89b3\u70b9\u304b\u3089\u3082\u30012\u56e0\u5b50\u30e2\u30c7\u30eb\u304c\u9069\u5207\u3060\u3068\u5224\u65ad\u3067\u304d\u307e\u3059\u3002</p> <ol> <li>2\u56e0\u5b50\u30e2\u30c7\u30eb\u3092\u4eee\u5b9a\u3057\u3001\u4ee5\u4e0b\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\u304c\u5f97\u3089\u308c\u305f\u3068\u3057\u307e\u3059\u3002</li> </ol> <pre><code>\u039b\uff08\u56de\u8ee2\u524d\uff09\uff1a\n[0.80, 0.40]\n[0.75, 0.35]\n[0.70, 0.45]\n[0.50, 0.70]\n[0.45, 0.75]\n</code></pre> <p>\u3053\u306e\u884c\u5217\u306b\u5bfe\u3057\u3066\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u3092\u9069\u7528\u3059\u308b\u3068\u3001\u3069\u306e\u3088\u3046\u306a\u7d50\u679c\u304c\u671f\u5f85\u3055\u308c\u307e\u3059\u304b\uff1f\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u69cb\u9020\u306f\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3067\u304d\u308b\u3067\u3057\u3087\u3046\u304b\uff1f</p> <p>\u89e3\u7b54\u4f8b\uff1a    \u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u3092\u9069\u7528\u3059\u308b\u3068\u3001\u5404\u5909\u6570\u304c\u4e00\u65b9\u306e\u56e0\u5b50\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u3001\u3082\u3046\u4e00\u65b9\u306e\u56e0\u5b50\u306b\u4f4e\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064\u3088\u3046\u306b\u56de\u8ee2\u3055\u308c\u307e\u3059\u3002\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\uff1a</p> <pre><code>\u039b\uff08\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u5f8c\uff09\uff1a\n[0.89, 0.10]\n[0.82, 0.07]\n[0.81, 0.18]\n[0.22, 0.83]\n[0.15, 0.87]\n</code></pre> <p>\u3053\u306e\u56de\u8ee2\u5f8c\u306e\u69cb\u9020\u3067\u306f\u3001\u6700\u521d\u306e3\u3064\u306e\u5909\u6570\u306f\u7b2c1\u56e0\u5b50\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3061\u3001\u6b8b\u308a\u306e2\u3064\u306e\u5909\u6570\u306f\u7b2c2\u56e0\u5b50\u306b\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3061\u307e\u3059\u3002\u3053\u306e\u7d50\u679c\u304b\u3089\u30012\u3064\u306e\u660e\u78ba\u306a\u6f5c\u5728\u56e0\u5b50\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u3082\u3057\u3053\u308c\u304c\u5fc3\u7406\u5b66\u7684\u5c3a\u5ea6\u306e\u5206\u6790\u3067\u3042\u308c\u3070\u3001\u7b2c1\u56e0\u5b50\u306f\u300c\u8a8d\u77e5\u7684\u5074\u9762\u300d\u3001\u7b2c2\u56e0\u5b50\u306f\u300c\u60c5\u7dd2\u7684\u5074\u9762\u300d\u306e\u3088\u3046\u306a\u89e3\u91c8\u304c\u53ef\u80fd\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u307e\u305f\u306f\u5065\u5eb7\u30c7\u30fc\u30bf\u3067\u3042\u308c\u3070\u3001\u7b2c1\u56e0\u5b50\u306f\u300c\u8eab\u4f53\u7684\u5065\u5eb7\u300d\u3001\u7b2c2\u56e0\u5b50\u306f\u300c\u7cbe\u795e\u7684\u5065\u5eb7\u300d\u306e\u3088\u3046\u306a\u89e3\u91c8\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u3002</p> <ol> <li>\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u3068\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306e\u4e3b\u306a\u9055\u3044\u3092\u8aac\u660e\u3057\u3001\u3069\u306e\u3088\u3046\u306a\u72b6\u6cc1\u3067\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u304c\u671b\u307e\u3057\u3044\u304b\u3092\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol> <p>\u89e3\u7b54\u4f8b\uff1a    \u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u3068\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306e\u4e3b\u306a\u9055\u3044\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ol> <li> <p>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306f\u76f4\u4ea4\u56de\u8ee2\u3067\u3042\u308a\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u30bc\u30ed\u306b\u4fdd\u3061\u307e\u3059\u3002\u4e00\u65b9\u3001\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306f\u659c\u4ea4\u56de\u8ee2\u3067\u3042\u308a\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306f\u5404\u56e0\u5b50\u306e\u8ca0\u8377\u91cf\u306e\u4e8c\u4e57\u5024\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u3053\u3068\u3067\u5358\u7d14\u69cb\u9020\u3092\u6c42\u3081\u307e\u3059\u3002\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306f\u307e\u305a\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u3092\u884c\u3044\u3001\u3055\u3089\u306b\u305d\u306e\u7d50\u679c\u3092\u3079\u304d\u4e57\u5909\u63db\uff08\u300c\u30b7\u30e3\u30fc\u30d7\u5316\u300d\uff09\u3059\u308b\u3053\u3068\u3067\u3088\u308a\u5358\u7d14\u306a\u69cb\u9020\u3092\u6c42\u3081\u307e\u3059\u3002</p> </li> <li> <p>\u30d0\u30ea\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u3067\u306f\u3001\u89e3\u91c8\u304c\u5358\u7d14\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u6f5c\u5728\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u5b9f\u969b\u306b\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u305d\u308c\u3092\u7121\u8996\u3059\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u3067\u306f\u3001\u3088\u308a\u73fe\u5b9f\u7684\u306a\u30e2\u30c7\u30eb\u304c\u5f97\u3089\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u3042\u308b\u305f\u3081\u89e3\u91c8\u304c\u3084\u3084\u8907\u96d1\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> </ol> <p>\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u304c\u671b\u307e\u3057\u3044\u72b6\u6cc1\uff1a</p> <ol> <li> <p>\u6f5c\u5728\u56e0\u5b50\u9593\u306b\u76f8\u95a2\u304c\u3042\u308b\u3068\u7406\u8ad6\u7684\u306b\u4e88\u6e2c\u3055\u308c\u308b\u5834\u5408\uff08\u4f8b\uff1a\u5fc3\u7406\u5b66\u7684\u7279\u6027\u3001\u5065\u5eb7\u72b6\u614b\u306e\u69d8\u3005\u306a\u5074\u9762\u306a\u3069\uff09</p> </li> <li> <p>\u3088\u308a\u5358\u7d14\u306a\u56e0\u5b50\u69cb\u9020\u3092\u6c42\u3081\u308b\u5834\u5408\uff08\u5404\u5909\u6570\u304c\u3088\u308a\u660e\u78ba\u306b\u4e00\u3064\u306e\u56e0\u5b50\u306b\u95a2\u9023\u3065\u3051\u3089\u308c\u308b\uff09</p> </li> <li> <p>\u73fe\u5b9f\u306e\u30c7\u30fc\u30bf\u751f\u6210\u904e\u7a0b\u3092\u3088\u308a\u5fe0\u5b9f\u306b\u53cd\u6620\u3057\u305f\u30e2\u30c7\u30eb\u3092\u6c42\u3081\u308b\u5834\u5408</p> </li> </ol> <p>\u7279\u306b\u3001\u5fc3\u7406\u5b66\u3001\u793e\u4f1a\u79d1\u5b66\u3001\u5065\u5eb7\u79d1\u5b66\u306a\u3069\u306e\u591a\u304f\u306e\u5206\u91ce\u3067\u306f\u3001\u5b8c\u5168\u306b\u72ec\u7acb\u3057\u305f\u6f5c\u5728\u56e0\u5b50\u3092\u4eee\u5b9a\u3059\u308b\u306e\u306f\u975e\u73fe\u5b9f\u7684\u3067\u3042\u308b\u3053\u3068\u304c\u591a\u304f\u3001\u30d7\u30ed\u30de\u30c3\u30af\u30b9\u56de\u8ee2\u306a\u3069\u306e\u659c\u4ea4\u56de\u8ee2\u304c\u9069\u5207\u306a\u9078\u629e\u3068\u306a\u308b\u3053\u3068\u304c\u591a\u3044\u3067\u3059\u3002</p> <ol> <li>\u5171\u901a\u6027\u304c\u4f4e\u3044\u5909\u6570\u304c\u56e0\u5b50\u5206\u6790\u306b\u542b\u307e\u308c\u3066\u3044\u308b\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306a\u554f\u984c\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u304b\uff1f\u305d\u306e\u3088\u3046\u306a\u5909\u6570\u306e\u53d6\u308a\u6271\u3044\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol> <p>\u89e3\u7b54\u4f8b\uff1a    \u5171\u901a\u6027\u304c\u4f4e\u3044\u5909\u6570\uff08\u4f8b\uff1a\u5171\u901a\u6027 &lt; 0.3\uff09\u304c\u56e0\u5b50\u5206\u6790\u306b\u542b\u307e\u308c\u3066\u3044\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u554f\u984c\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li> <p>\u305d\u306e\u5909\u6570\u306f\u73fe\u5728\u306e\u56e0\u5b50\u69cb\u9020\u3067\u306f\u3046\u307e\u304f\u8aac\u660e\u3055\u308c\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u3001\u56e0\u5b50\u89e3\u91c8\u306e\u4fe1\u983c\u6027\u304c\u4f4e\u4e0b\u3059\u308b</p> </li> <li> <p>\u4f4e\u3044\u5171\u901a\u6027\u306e\u5909\u6570\u304c\u591a\u3044\u3068\u3001\u30e2\u30c7\u30eb\u5168\u4f53\u306e\u8aac\u660e\u529b\uff08\u7d2f\u7a4d\u5bc4\u4e0e\u7387\uff09\u304c\u4f4e\u4e0b\u3059\u308b</p> </li> <li> <p>\u4f4e\u3044\u5171\u901a\u6027\u306f\u3001\u305d\u306e\u5909\u6570\u304c\u73fe\u5728\u306e\u56e0\u5b50\u3068\u306f\u5225\u306e\u6f5c\u5728\u56e0\u5b50\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u3084\u3001\u6e2c\u5b9a\u8aa4\u5dee\u304c\u5927\u304d\u3044\u53ef\u80fd\u6027\u3092\u793a\u5506\u3059\u308b</p> </li> </ol> <p>\u5171\u901a\u6027\u304c\u4f4e\u3044\u5909\u6570\u306e\u53d6\u308a\u6271\u3044\u306b\u3064\u3044\u3066\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li> <p>\u5909\u6570\u306e\u9664\u5916\uff1a\u5171\u901a\u6027\u304c\u975e\u5e38\u306b\u4f4e\u3044\u5909\u6570\uff08\u4f8b\uff1a&lt; 0.2\uff09\u306f\u3001\u5206\u6790\u304b\u3089\u9664\u5916\u3059\u308b\u3053\u3068\u3092\u691c\u8a0e\u3059\u308b</p> </li> <li> <p>\u56e0\u5b50\u6570\u306e\u5897\u52a0\uff1a\u4f4e\u3044\u5171\u901a\u6027\u306f\u56e0\u5b50\u6570\u304c\u4e0d\u8db3\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u3092\u793a\u5506\u3059\u308b\u305f\u3081\u3001\u56e0\u5b50\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3092\u691c\u8a0e\u3059\u308b</p> </li> <li> <p>\u5909\u6570\u306e\u518d\u691c\u8a0e\uff1a\u5909\u6570\u306e\u6e2c\u5b9a\u65b9\u6cd5\u3084\u5b9a\u7fa9\u306e\u898b\u76f4\u3057\u3092\u884c\u3046</p> </li> <li> <p>\u5225\u306e\u5206\u6790\u624b\u6cd5\u306e\u691c\u8a0e\uff1a\u305d\u306e\u5909\u6570\u304c\u4ed6\u306e\u5909\u6570\u3068\u306f\u7570\u306a\u308b\u69cb\u9020\u3092\u6301\u3064\u5834\u5408\u3001\u5225\u306e\u5206\u6790\u624b\u6cd5\uff08\u4f8b\uff1a\u30af\u30e9\u30b9\u30bf\u30fc\u5206\u6790\u3001\u591a\u6b21\u5143\u5c3a\u5ea6\u6cd5\u306a\u3069\uff09\u306e\u4f75\u7528\u3092\u691c\u8a0e\u3059\u308b</p> </li> </ol> <p>\u306a\u304a\u3001\u5909\u6570\u3092\u9664\u5916\u3059\u308b\u524d\u306b\u3001\u305d\u306e\u5909\u6570\u304c\u7406\u8ad6\u7684\u306b\u91cd\u8981\u3067\u3042\u308b\u304b\u3069\u3046\u304b\u3092\u8003\u616e\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u7406\u8ad6\u7684\u306b\u91cd\u8981\u306a\u5909\u6570\u306e\u5834\u5408\u3001\u4f4e\u3044\u5171\u901a\u6027\u3092\u6301\u3064\u5834\u5408\u3067\u3082\u3001\u5206\u6790\u306b\u542b\u3081\u308b\u3053\u3068\u304c\u6b63\u5f53\u5316\u3055\u308c\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u8b1b\u7fa9\u30ce\u30fc\u30c8 \u7b2c46\u56de","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#_1","title":"\u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c46\u56de \u30c6\u30fc\u30de: \u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u6cd5\uff1at-SNE\u3068UMAP \u95a2\u9023\u9805\u76ee: \u4e3b\u6210\u5206\u5206\u6790\u3001\u6b21\u5143\u524a\u6e1b\u3001\u30de\u30cb\u30d5\u30a9\u30fc\u30eb\u30c9\u5b66\u7fd2 \u4e88\u7fd2\u3059\u3079\u304d\u5185\u5bb9: \u4e3b\u6210\u5206\u5206\u6790\u3001\u7279\u7570\u5024\u5206\u89e3\u3001\u78ba\u7387\u5206\u5e03\u306e\u57fa\u790e\u77e5\u8b58</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#_2","title":"\u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u306e\u9650\u754c\u3092\u7406\u89e3\u3057\u3001\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u306e\u5fc5\u8981\u6027\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>t-SNE\uff08t-distributed Stochastic Neighbor Embedding\uff09\u306e\u7406\u8ad6\u7684\u80cc\u666f\u3068\u52d5\u4f5c\u539f\u7406\u3092\u7406\u89e3\u3059\u308b</li> <li>UMAP\uff08Uniform Manifold Approximation and Projection\uff09\u306e\u57fa\u672c\u6982\u5ff5\u3068\u6570\u5b66\u7684\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u628a\u63e1\u3059\u308b</li> <li>t-SNE\u3068UMAP\u306e\u9055\u3044\u3068\u4f7f\u3044\u5206\u3051\u306e\u57fa\u6e96\u3092\u8aac\u660e\u3067\u304d\u308b</li> <li>\u5065\u5eb7\u30c7\u30fc\u30bf\u79d1\u5b66\u306b\u304a\u3051\u308b\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u306e\u5fdc\u7528\u4f8b\u3092\u7406\u89e3\u3059\u308b</li> </ol>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#1","title":"1. \u57fa\u672c\u6982\u5ff5\uff1a\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u306e\u5fc5\u8981\u6027","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#11","title":"1.1 \u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u306e\u9650\u754c","text":"<p>\u3053\u308c\u307e\u3067\u79c1\u305f\u3061\u306f\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3084\u56e0\u5b50\u5206\u6790\u3068\u3044\u3063\u305f\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u306b\u3064\u3044\u3066\u5b66\u3093\u3067\u304d\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u624b\u6cd5\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u76f4\u4ea4\u8ef8\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u3067\u6b21\u5143\u524a\u6e1b\u3092\u884c\u3044\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u5b9f\u4e16\u754c\u306e\u30c7\u30fc\u30bf\u306f\u5fc5\u305a\u3057\u3082\u7dda\u5f62\u306e\u95a2\u4fc2\u3060\u3051\u3067\u8868\u73fe\u3067\u304d\u308b\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> <p>\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u306e\u9650\u754c: 1. \u975e\u7dda\u5f62\u306e\u95a2\u4fc2\u6027\u3092\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044 2. \u5c40\u6240\u7684\u306a\u69cb\u9020\u3092\u4fdd\u5b58\u3059\u308b\u3053\u3068\u304c\u96e3\u3057\u3044 3. \u30c7\u30fc\u30bf\u304c\u66f2\u9762\uff08\u30de\u30cb\u30d5\u30a9\u30fc\u30eb\u30c9\uff09\u4e0a\u306b\u5206\u5e03\u3057\u3066\u3044\u308b\u5834\u5408\u306b\u9069\u5207\u306b\u8868\u73fe\u3067\u304d\u306a\u3044</p> <p>\u4f8b\u3048\u3070\u3001\u300c\u30b9\u30a4\u30b9\u30ed\u30fc\u30eb\u300d\u3068\u547c\u3070\u308c\u308b\u6709\u540d\u306a3\u6b21\u5143\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u3053\u308c\u306f\u5dfb\u7269\u306e\u3088\u3046\u306a\u5f62\u72b6\u3092\u3057\u3066\u304a\u308a\u3001\u30c7\u30fc\u30bf\u70b9\u306f2\u6b21\u5143\u306e\u66f2\u9762\u4e0a\u306b\u5206\u5e03\u3057\u3066\u3044\u307e\u3059\u3002PCA\u306a\u3069\u306e\u7dda\u5f62\u624b\u6cd5\u3067\u3053\u308c\u30922\u6b21\u5143\u306b\u524a\u6e1b\u3059\u308b\u3068\u3001\u5dfb\u7269\u306e\u69cb\u9020\u304c\u5b8c\u5168\u306b\u5931\u308f\u308c\u3066\u3057\u307e\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#12","title":"1.2 \u30de\u30cb\u30d5\u30a9\u30fc\u30eb\u30c9\u4eee\u8aac","text":"<p>\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u306e\u7406\u8ad6\u7684\u57fa\u790e\u3068\u306a\u3063\u3066\u3044\u308b\u306e\u304c\u300c\u30de\u30cb\u30d5\u30a9\u30fc\u30eb\u30c9\u4eee\u8aac\u300d\u3067\u3059\u3002</p> <p>\u30de\u30cb\u30d5\u30a9\u30fc\u30eb\u30c9\u4eee\u8aac: \u9ad8\u6b21\u5143\u306e\u5b9f\u4e16\u754c\u30c7\u30fc\u30bf\u306f\u3001\u5b9f\u969b\u306b\u306f\u3088\u308a\u4f4e\u3044\u6b21\u5143\u306e\u66f2\u9762\uff08\u30de\u30cb\u30d5\u30a9\u30fc\u30eb\u30c9\uff09\u4e0a\u306b\u5206\u5e03\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u591a\u3044\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u4eba\u9593\u306e\u9854\u753b\u50cf\u306f\u6570\u5343\u301c\u6570\u4e07\u6b21\u5143\u306e\u30d4\u30af\u30bb\u30eb\u7a7a\u9593\u3067\u8868\u73fe\u3055\u308c\u307e\u3059\u304c\u3001\u5b9f\u969b\u306e\u9854\u306e\u5909\u5316\u306f\u6570\u5341\u7a0b\u5ea6\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u5e74\u9f62\u3001\u8868\u60c5\u3001\u89d2\u5ea6\u306a\u3069\uff09\u3067\u8aac\u660e\u3067\u304d\u308b\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#13","title":"1.3 \u5c40\u6240\u7684\u69cb\u9020\u3068\u5927\u57df\u7684\u69cb\u9020\u306e\u4fdd\u5b58","text":"<p>\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u306e\u91cd\u8981\u306a\u7279\u5fb4\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5c40\u6240\u7684\u306a\u69cb\u9020\u3092\u4fdd\u5b58\u3057\u306a\u304c\u3089\u3001\u5927\u57df\u7684\u306a\u69cb\u9020\u3082\u53ef\u80fd\u306a\u9650\u308a\u7dad\u6301\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p> <ul> <li>\u5c40\u6240\u7684\u69cb\u9020: \u8fd1\u63a5\u70b9\u9593\u306e\u95a2\u4fc2\u6027\uff08\u985e\u4f3c\u70b9\u306f\u8fd1\u304f\u306b\u914d\u7f6e\uff09</li> <li>\u5927\u57df\u7684\u69cb\u9020: \u30c7\u30fc\u30bf\u5168\u4f53\u306e\u5206\u5e03\u30d1\u30bf\u30fc\u30f3\uff08\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u5206\u96e2\u306a\u3069\uff09</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#2-t-sne-t-distributed-stochastic-neighbor-embedding","title":"2. t-SNE (t-distributed Stochastic Neighbor Embedding)","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#21","title":"2.1 \u57fa\u672c\u6982\u5ff5\u3068\u958b\u767a\u80cc\u666f","text":"<p>t-SNE\u306f2008\u5e74\u306bLaurens van der Maaten\u3068Geoffrey Hinton\u306b\u3088\u3063\u3066\u958b\u767a\u3055\u308c\u305f\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u3067\u3059\u3002SNE\uff08Stochastic Neighbor Embedding\uff09\u3092\u6539\u826f\u3057\u305f\u3082\u306e\u3067\u3001\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3092\u4f4e\u6b21\u5143\u7a7a\u9593\u306b\u57cb\u3081\u8fbc\u3080\u969b\u306b\u3001\u7279\u306b\u5c40\u6240\u7684\u306a\u69cb\u9020\u3092\u4fdd\u5b58\u3059\u308b\u3053\u3068\u306b\u512a\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>t-SNE\u306e\u76ee\u7684: \u9ad8\u6b21\u5143\u7a7a\u9593\u3067\u306e\u70b9\u9593\u306e\u985e\u4f3c\u5ea6\u95a2\u4fc2\u3092\u3001\u4f4e\u6b21\u5143\u7a7a\u9593\u3067\u3082\u53ef\u80fd\u306a\u9650\u308a\u4fdd\u5b58\u3059\u308b\u3053\u3068\u3002\u7279\u306b\u3001\u8fd1\u508d\u70b9\u9593\u306e\u95a2\u4fc2\u6027\u3092\u91cd\u8996\u3059\u308b\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#22","title":"2.2 \u6570\u5b66\u7684\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af","text":"<p>t-SNE\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u9032\u884c\u3057\u307e\u3059\uff1a</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#221","title":"2.2.1 \u9ad8\u6b21\u5143\u7a7a\u9593\u3067\u306e\u6761\u4ef6\u4ed8\u304d\u78ba\u7387\u306e\u8a08\u7b97","text":"<p>\u307e\u305a\u3001\u9ad8\u6b21\u5143\u7a7a\u9593\u3067\u306e\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u9593\u306e\u985e\u4f3c\u5ea6\u3092\u6761\u4ef6\u4ed8\u304d\u78ba\u7387\u3068\u3057\u3066\u5b9a\u7fa9\u3057\u307e\u3059\u3002</p> <p>\u70b9 \\(x_i\\) \u304b\u3089\u898b\u305f\u70b9 \\(x_j\\) \u306e\u6761\u4ef6\u4ed8\u304d\u78ba\u7387 \\(p_{j|i}\\) \u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\\] <p>\u3053\u3053\u3067 \\(\\sigma_i\\) \u306f\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u5206\u6563\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3001\u70b9 \\(x_i\\) \u306e\u6709\u52b9\u8fd1\u508d\u6570\uff08\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\uff09\u306b\u57fa\u3065\u3044\u3066\u8abf\u6574\u3055\u308c\u307e\u3059\u3002</p> <p>\u7d9a\u3044\u3066\u3001\u5bfe\u79f0\u5316\u3055\u308c\u305f\u7d50\u5408\u78ba\u7387 \\(p_{ij}\\) \u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3057\u307e\u3059\uff1a</p> \\[p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\\] <p>\u3053\u3053\u3067 \\(n\\) \u306f\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u7dcf\u6570\u3067\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#222-t","title":"2.2.2 \u4f4e\u6b21\u5143\u7a7a\u9593\u3067\u306e\u78ba\u7387\u5206\u5e03\uff08t\u5206\u5e03\u306e\u63a1\u7528\uff09","text":"<p>\u4f4e\u6b21\u5143\u7a7a\u9593\uff08\u901a\u5e38\u306f2\u6b21\u5143\u304b3\u6b21\u5143\uff09\u3067\u306e\u5bfe\u5fdc\u3059\u308b\u70b9 \\(y_i\\) \u3068 \\(y_j\\) \u306e\u9593\u306e\u985e\u4f3c\u5ea6\u306f\u3001\u81ea\u7531\u5ea61\u306et\u5206\u5e03\uff08\u30b3\u30fc\u30b7\u30fc\u5206\u5e03\uff09\u3092\u7528\u3044\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\\] <p>t\u5206\u5e03\u304c\u63a1\u7528\u3055\u308c\u305f\u7406\u7531\u306f\u3001\u9ad8\u6b21\u5143\u7a7a\u9593\u3067\u306e\u8fd1\u63a5\u70b9\u304c\u4f4e\u6b21\u5143\u7a7a\u9593\u3067\u3082\u8fd1\u304f\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3055\u308c\u3084\u3059\u304f\u306a\u308b\u300c\u30af\u30e9\u30a6\u30c7\u30a3\u30f3\u30b0\u554f\u984c\u300d\u3092\u8efd\u6e1b\u3059\u308b\u305f\u3081\u3067\u3059\u3002t\u5206\u5e03\u306f\u6b63\u898f\u5206\u5e03\u3088\u308a\u3082\u88fe\u304c\u91cd\u3044\u305f\u3081\u3001\u9060\u3044\u70b9\u540c\u58eb\u3092\u3088\u308a\u96e2\u308c\u305f\u4f4d\u7f6e\u306b\u914d\u7f6e\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#223-kullback-leibler","title":"2.2.3 Kullback-Leibler\u767a\u6563\u306e\u6700\u5c0f\u5316","text":"<p>t-SNE\u306f\u4e8c\u3064\u306e\u78ba\u7387\u5206\u5e03 \\(P\\) \u3068 \\(Q\\) \u306e\u9593\u306eKullback-Leibler\u767a\u6563\u3092\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u6700\u9069\u306a\u4f4e\u6b21\u5143\u8868\u73fe\u3092\u6c42\u3081\u307e\u3059\uff1a</p> \\[C = KL(P||Q) = \\sum_{i \\neq j} p_{ij} \\log\\frac{p_{ij}}{q_{ij}}\\] <p>\u3053\u306e\u76ee\u7684\u95a2\u6570\u306e\u52fe\u914d\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3055\u308c\u307e\u3059\uff1a</p> \\[\\frac{\\partial C}{\\partial y_i} = 4 \\sum_{j \\neq i} (p_{ij} - q_{ij})(y_i - y_j)(1 + \\|y_i - y_j\\|^2)^{-1}\\] <p>\u3053\u306e\u52fe\u914d\u3092\u7528\u3044\u3066\u3001\u52fe\u914d\u964d\u4e0b\u6cd5\u3067\u4f4e\u6b21\u5143\u57cb\u3081\u8fbc\u307f\u3092\u6700\u9069\u5316\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#23","title":"2.3 \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5f79\u5272","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#231-perplexity","title":"2.3.1 \u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3 (Perplexity)","text":"<p>\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\u306f\u3001\u5404\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u306e\u6709\u52b9\u8fd1\u508d\u6570\u3092\u5236\u5fa1\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3001t-SNE\u306e\u6700\u3082\u91cd\u8981\u306a\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3059\u3002</p> <p>\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3: 2\u306e\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306e\u6307\u6570\u3067\u8868\u3055\u308c\u308b\u6709\u52b9\u8fd1\u508d\u6570\u3002\u901a\u5e385\u301c50\u306e\u7bc4\u56f2\u3067\u8a2d\u5b9a\u3055\u308c\u308b\u3002 \\(\\(Perp(P_i) = 2^{H(P_i)}\\)\\) \u3053\u3053\u3067 \\(H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i}\\) \u306f\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3067\u3059\u3002</p> <ul> <li>\u5c0f\u3055\u3044\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\uff085\u301c10\uff09: \u5c40\u6240\u7684\u69cb\u9020\u3092\u91cd\u8996</li> <li>\u5927\u304d\u3044\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\uff0830\u301c50\uff09: \u3088\u308a\u5927\u57df\u7684\u306a\u69cb\u9020\u3092\u91cd\u8996</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#232","title":"2.3.2 \u5b66\u7fd2\u7387\u3068\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570","text":"<ul> <li>\u5b66\u7fd2\u7387: \u52fe\u914d\u964d\u4e0b\u6cd5\u3067\u306e\u30b9\u30c6\u30c3\u30d7\u30b5\u30a4\u30ba\u3002\u901a\u5e38200\u301c1000\u306e\u7bc4\u56f2</li> <li>\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570: 1000\u301c5000\u7a0b\u5ea6\u304c\u4e00\u822c\u7684</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#24","title":"2.4 \u8a08\u7b97\u91cf\u306e\u554f\u984c\u3068\u9ad8\u901f\u5316\u624b\u6cd5","text":"<p>t-SNE\u306e\u8a08\u7b97\u91cf\u306f \\(O(n^2)\\) \u3067\u3001\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u6570 \\(n\\) \u304c\u5897\u3048\u308b\u3068\u8a08\u7b97\u6642\u9593\u304c\u6025\u6fc0\u306b\u5897\u52a0\u3057\u307e\u3059\u3002\u3053\u306e\u305f\u3081\u3001\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u306e\u5229\u7528\u306b\u306f\u9ad8\u901f\u5316\u6280\u8853\u304c\u5fc5\u8981\u3067\u3059\u3002</p> <p>Barnes-Hut\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0: - \u7a7a\u9593\u3092\u56db\u5206\u6728\uff082D\uff09\u3084\u516b\u5206\u6728\uff083D\uff09\u3067\u5206\u5272 - \u9060\u3044\u70b9\u306e\u96c6\u56e3\u3092\u4e00\u3064\u306e\u70b9\u3068\u3057\u3066\u8fd1\u4f3c - \u8a08\u7b97\u91cf\u3092 \\(O(n \\log n)\\) \u306b\u524a\u6e1b</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#3-umap-uniform-manifold-approximation-and-projection","title":"3. UMAP (Uniform Manifold Approximation and Projection)","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#31","title":"3.1 \u57fa\u672c\u6982\u5ff5\u3068\u958b\u767a\u80cc\u666f","text":"<p>UMAP\u306f2018\u5e74\u306bLeland McInnes\u3089\u306b\u3088\u3063\u3066\u958b\u767a\u3055\u308c\u305f\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u3067\u3001t-SNE\u306e\u5f8c\u7d99\u3068\u3057\u3066\u6025\u901f\u306b\u666e\u53ca\u3057\u3066\u3044\u307e\u3059\u3002\u30c8\u30dd\u30ed\u30b8\u30fc\u7406\u8ad6\u306b\u57fa\u3065\u3044\u3066\u304a\u308a\u3001\u5c40\u6240\u7684\u69cb\u9020\u3068\u5927\u57df\u7684\u69cb\u9020\u306e\u4e21\u65b9\u3092\u4fdd\u5b58\u3059\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>UMAP\u306e\u7279\u5fb4: 1. \u8a08\u7b97\u52b9\u7387\u304ct-SNE\u3088\u308a\u9ad8\u3044 2. \u5927\u57df\u7684\u69cb\u9020\u306e\u4fdd\u5b58\u6027\u304c\u512a\u308c\u3066\u3044\u308b 3. \u7406\u8ad6\u7684\u306a\u57fa\u76e4\u304c\u3088\u308a\u5805\u56fa\uff08\u30ea\u30fc\u30de\u30f3\u5e7e\u4f55\u5b66\u3068\u30c8\u30dd\u30ed\u30b8\u30fc\uff09</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#32","title":"3.2 \u6570\u5b66\u7684\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#321","title":"3.2.1 \u30ea\u30fc\u30de\u30f3\u5e7e\u4f55\u5b66\u3068\u30c8\u30dd\u30ed\u30b8\u30fc\u306e\u89b3\u70b9","text":"<p>UMAP\u306f\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u30ea\u30fc\u30de\u30f3\u591a\u69d8\u4f53\u3068\u898b\u306a\u3057\u3001\u305d\u306e\u4f4d\u76f8\u7684\u69cb\u9020\u3092\u4f4e\u6b21\u5143\u7a7a\u9593\u306b\u8fd1\u4f3c\u3057\u3088\u3046\u3068\u3057\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u5404\u70b9\u306e\u5c40\u6240\u7684\u306a\u8fd1\u508d\u3092\u30d5\u30a1\u30b8\u30fc\u5358\u4f53\u8907\u4f53\uff08fuzzy simplicial complex\uff09\u3068\u3057\u3066\u8868\u73fe\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#322","title":"3.2.2 \u5c40\u6240\u7684\u8ddd\u96e2\u95a2\u4fc2\u306e\u30e2\u30c7\u30ea\u30f3\u30b0","text":"<p>\u5404\u70b9 \\(x_i\\) \u306b\u3064\u3044\u3066\u3001\u305d\u306e \\(k\\)-\u8fd1\u508d\u5185\u306e\u70b9 \\(x_j\\) \u307e\u3067\u306e\u8ddd\u96e2\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6b63\u898f\u5316\u3057\u307e\u3059\uff1a</p> \\[d(x_i, x_j) = \\frac{d(x_i, x_j) - \\rho_i}{\\sigma_i}\\] <p>\u3053\u3053\u3067 \\(\\rho_i\\) \u306f\u6700\u3082\u8fd1\u3044\u70b9\u307e\u3067\u306e\u8ddd\u96e2\u3001\\(\\sigma_i\\) \u306f\u8fd1\u508d\u306e\u5e83\u304c\u308a\u3092\u5236\u5fa1\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3059\u3002</p> <p>\u3053\u306e\u6b63\u898f\u5316\u8ddd\u96e2\u304b\u3089\u3001\u9ad8\u6b21\u5143\u7a7a\u9593\u3067\u306e\u5c40\u6240\u7684\u306a\u63a5\u7d9a\u5f37\u5ea6\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a08\u7b97\u3057\u307e\u3059\uff1a</p> \\[v_{ij} = \\exp(-d(x_i, x_j))\\]"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#323","title":"3.2.3 \u30d5\u30a1\u30b8\u30fc\u96c6\u5408\u7406\u8ad6\u306e\u5fdc\u7528","text":"<p>UMAP\u306f\u9ad8\u6b21\u5143\u7a7a\u9593\u3068\u4f4e\u6b21\u5143\u7a7a\u9593\u305d\u308c\u305e\u308c\u3067\u30d5\u30a1\u30b8\u30fc\u96c6\u5408\u3092\u69cb\u7bc9\u3057\u3001\u3053\u308c\u3089\u3092\u8fd1\u4f3c\u3057\u3088\u3046\u3068\u3057\u307e\u3059\u3002\u9ad8\u6b21\u5143\u30d5\u30a1\u30b8\u30fc\u96c6\u5408 \\(\\mu\\) \u3068\u4f4e\u6b21\u5143\u30d5\u30a1\u30b8\u30fc\u96c6\u5408 \\(\\nu\\) \u306e\u9593\u306e\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u6700\u5c0f\u5316\u3057\u307e\u3059\uff1a</p> \\[CE(\\mu, \\nu) = \\sum_{i,j} \\mu_{ij} \\log \\frac{\\mu_{ij}}{\\nu_{ij}} + (1-\\mu_{ij}) \\log \\frac{1-\\mu_{ij}}{1-\\nu_{ij}}\\] <p>\u3053\u3053\u3067\u4f4e\u6b21\u5143\u3067\u306e\u63a5\u7d9a\u5f37\u5ea6 \\(\\nu_{ij}\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[\\nu_{ij} = (1 + a\\|y_i - y_j\\|_2^{2b})^{-1}\\] <p>\\(a\\) \u3068 \\(b\\) \u306f\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3001\u4f4e\u6b21\u5143\u7a7a\u9593\u3067\u306e\u8ddd\u96e2\u306e\u632f\u308b\u821e\u3044\u3092\u5236\u5fa1\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#324","title":"3.2.4 \u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\u306b\u3088\u308b\u6700\u9069\u5316","text":"<p>\u4e0a\u8a18\u306e\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u76ee\u7684\u95a2\u6570\u3092\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\u3067\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u4f4e\u6b21\u5143\u57cb\u3081\u8fbc\u307f\u3092\u5b66\u7fd2\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#33","title":"3.3 \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5f79\u5272","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#331-n_neighbors","title":"3.3.1 \u8fd1\u508d\u6570 (n_neighbors)","text":"<p>\u5404\u70b9\u306e\u5c40\u6240\u7684\u8fd1\u508d\u3092\u5b9a\u7fa9\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3001t-SNE\u306e\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\u306b\u76f8\u5f53\u3057\u307e\u3059\u3002</p> <ul> <li>\u5c0f\u3055\u3044\u5024: \u5c40\u6240\u7684\u69cb\u9020\u3092\u91cd\u8996</li> <li>\u5927\u304d\u3044\u5024: \u5927\u57df\u7684\u69cb\u9020\u3092\u91cd\u8996</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#332-min_dist","title":"3.3.2 \u6700\u5c0f\u8ddd\u96e2 (min_dist)","text":"<p>\u4f4e\u6b21\u5143\u7a7a\u9593\u3067\u306e\u70b9\u306e\u6700\u5c0f\u8ddd\u96e2\u3092\u5236\u5fa1\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u57cb\u3081\u8fbc\u307f\u7a7a\u9593\u3067\u306e\u70b9\u306e\u5bc6\u96c6\u5ea6\u306b\u5f71\u97ff\u3057\u307e\u3059\u3002</p> <ul> <li>\u5c0f\u3055\u3044\u5024: \u70b9\u304c\u5bc6\u96c6\u3057\u305f\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u5f62\u6210</li> <li>\u5927\u304d\u3044\u5024: \u3088\u308a\u5747\u4e00\u306a\u5206\u5e03</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#333","title":"3.3.3 \u30cd\u30ac\u30c6\u30a3\u30d6\u30b5\u30f3\u30d7\u30eb\u6bd4\u7387","text":"<p>\u6700\u9069\u5316\u904e\u7a0b\u3067\u4f7f\u7528\u3059\u308b\u8ca0\u4f8b\u306e\u6570\u3067\u3001\u8a08\u7b97\u52b9\u7387\u3068\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3092\u5236\u5fa1\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#4-t-sneumap","title":"4. t-SNE\u3068UMAP\u306e\u6bd4\u8f03","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#41","title":"4.1 \u7406\u8ad6\u7684\u80cc\u666f\u306e\u9055\u3044","text":"<ul> <li>t-SNE: \u78ba\u7387\u7684\u306a\u70b9\u9593\u306e\u985e\u4f3c\u5ea6\u3092\u4fdd\u5b58</li> <li>UMAP: \u30c8\u30dd\u30ed\u30b8\u30ab\u30eb\u306a\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u4fdd\u5b58</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#42","title":"4.2 \u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306e\u9055\u3044","text":"<ul> <li>\u8a08\u7b97\u52b9\u7387: UMAP\u306ft-SNE\u3088\u308a\u9ad8\u901f\uff08\u7279\u306b\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff09</li> <li>\u5927\u57df\u7684\u69cb\u9020\u306e\u4fdd\u5b58: UMAP\u306e\u65b9\u304c\u512a\u308c\u3066\u3044\u308b\u50be\u5411</li> <li>\u5c40\u6240\u7684\u69cb\u9020\u306e\u4fdd\u5b58: \u3069\u3061\u3089\u3082\u512a\u308c\u3066\u3044\u308b\u304c\u3001\u7279\u6027\u304c\u7570\u306a\u308b</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#43","title":"4.3 \u4f7f\u3044\u5206\u3051\u306e\u57fa\u6e96","text":"<ul> <li>\u63a2\u7d22\u7684\u5206\u6790: \u521d\u671f\u6bb5\u968e\u3067\u306fUMAP\u304c\u52b9\u7387\u7684</li> <li>\u8a73\u7d30\u306a\u5c40\u6240\u69cb\u9020\u306e\u53ef\u8996\u5316: t-SNE\u3082\u8003\u616e</li> <li>\u5927\u898f\u6a21\u30c7\u30fc\u30bf: UMAP\u306e\u65b9\u304c\u9069\u3057\u3066\u3044\u308b</li> <li>\u51e6\u7406\u901f\u5ea6\u304c\u91cd\u8981: UMAP\u3092\u9078\u629e</li> <li>\u5b89\u5b9a\u6027\u304c\u91cd\u8981: UMAP\u306ft-SNE\u3088\u308a\u3082\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u9593\u306e\u5b89\u5b9a\u6027\u304c\u9ad8\u3044</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#5","title":"5. \u5065\u5eb7\u30c7\u30fc\u30bf\u79d1\u5b66\u306b\u304a\u3051\u308b\u5fdc\u7528\u4f8b","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#51-rna","title":"5.1 \u5358\u4e00\u7d30\u80deRNA\u30b7\u30fc\u30b1\u30f3\u30b9\u30c7\u30fc\u30bf\u306e\u8996\u899a\u5316","text":"<p>\u5358\u4e00\u7d30\u80deRNA\u30b7\u30fc\u30b1\u30f3\u30b9\uff08scRNA-seq\uff09\u30c7\u30fc\u30bf\u306f\u3001\u6570\u4e07\u306e\u907a\u4f1d\u5b50\u767a\u73fe\u30ec\u30d9\u30eb\u3067\u5404\u7d30\u80de\u3092\u7279\u5fb4\u3065\u3051\u308b\u975e\u5e38\u306b\u9ad8\u6b21\u5143\u306e\u30c7\u30fc\u30bf\u3067\u3059\u3002</p> <ul> <li>t-SNE\u3068UMAP\u306f\u7d30\u80de\u30bf\u30a4\u30d7\u306e\u540c\u5b9a\u3068\u53ef\u8996\u5316\u306e\u30c7\u30d5\u30a1\u30af\u30c8\u30b9\u30bf\u30f3\u30c0\u30fc\u30c9</li> <li>\u540c\u3058\u7d30\u80de\u30bf\u30a4\u30d7\u306f\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u5f62\u6210</li> <li>\u767a\u751f\u904e\u7a0b\u306e\u30c8\u30e9\u30b8\u30a7\u30af\u30c8\u30ea\u89e3\u6790\u306b\u3082\u5229\u7528</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#52","title":"5.2 \u533b\u7642\u753b\u50cf\u306e\u7279\u5fb4\u7a7a\u9593\u306b\u304a\u3051\u308b\u60a3\u8005\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0","text":"<p>\u533b\u7642\u753b\u50cf\uff08MRI\u3001CT\u3001\u75c5\u7406\u753b\u50cf\u306a\u3069\uff09\u304b\u3089\u62bd\u51fa\u3055\u308c\u305f\u7279\u5fb4\u3092\u4f4e\u6b21\u5143\u306b\u57cb\u3081\u8fbc\u3080\u3053\u3068\u3067\u3001\u60a3\u8005\u30b0\u30eb\u30fc\u30d7\u306e\u81ea\u52d5\u5206\u985e\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p> <ul> <li>\u8133MRI\u753b\u50cf\u304b\u3089\u306e\u30a2\u30eb\u30c4\u30cf\u30a4\u30de\u30fc\u75c5\u306e\u65e9\u671f\u8a3a\u65ad</li> <li>\u76ae\u819a\u75c5\u5909\u306e\u5206\u985e\u3068\u8a3a\u65ad\u652f\u63f4</li> <li>\u75c5\u7406\u7d44\u7e54\u753b\u50cf\u304b\u3089\u306e\u304c\u3093\u30b5\u30d6\u30bf\u30a4\u30d7\u306e\u540c\u5b9a</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#53","title":"5.3 \u751f\u7406\u5b66\u7684\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u30d1\u30bf\u30fc\u30f3\u767a\u898b","text":"<p>\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u3084\u533b\u7642\u30e2\u30cb\u30bf\u30fc\u304b\u3089\u5f97\u3089\u308c\u308b\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3057\u3001\u7570\u5e38\u30d1\u30bf\u30fc\u30f3\u3084\u75be\u60a3\u30ea\u30b9\u30af\u3092\u53ef\u8996\u5316\u3057\u307e\u3059\u3002</p> <ul> <li>\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u304b\u3089\u306e\u4e0d\u6574\u8108\u30d1\u30bf\u30fc\u30f3\u306e\u691c\u51fa</li> <li>\u7761\u7720\u30dd\u30ea\u30b0\u30e9\u30d5\u30c7\u30fc\u30bf\u304b\u3089\u306e\u7761\u7720\u969c\u5bb3\u306e\u5206\u985e</li> <li>\u9023\u7d9a\u8840\u7cd6\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u30c7\u30fc\u30bf\u304b\u3089\u306e\u8840\u7cd6\u30d1\u30bf\u30fc\u30f3\u306e\u540c\u5b9a</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#54","title":"5.4 \u533b\u85ac\u54c1\u958b\u767a\u306b\u304a\u3051\u308b\u5206\u5b50\u69cb\u9020\u306e\u985e\u4f3c\u6027\u30de\u30c3\u30d4\u30f3\u30b0","text":"<p>\u85ac\u7269\u5206\u5b50\u306e\u9ad8\u6b21\u5143\u7279\u5fb4\u8868\u73fe\u3092\u4f4e\u6b21\u5143\u7a7a\u9593\u306b\u57cb\u3081\u8fbc\u307f\u3001\u69cb\u9020\u7684\u30fb\u6a5f\u80fd\u7684\u306b\u985e\u4f3c\u3057\u305f\u5316\u5408\u7269\u3092\u53ef\u8996\u5316\u3057\u307e\u3059\u3002</p> <ul> <li>\u4eee\u60f3\u30b9\u30af\u30ea\u30fc\u30cb\u30f3\u30b0\u3068\u5275\u85ac</li> <li>\u65e2\u5b58\u85ac\u306e\u30ea\u30dd\u30b8\u30b7\u30e7\u30cb\u30f3\u30b0</li> <li>\u85ac\u7269\u526f\u4f5c\u7528\u306e\u4e88\u6e2c</li> </ul>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#6-python","title":"6. Python\u306b\u3088\u308b\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#61","title":"6.1 \u30c7\u30fc\u30bf\u306e\u6e96\u5099\u3068\u524d\u51e6\u7406","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport umap\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\uff08\u624b\u66f8\u304d\u6570\u5b57\uff09\ndigits = load_digits()\nX = digits.data\ny = digits.target\n\n# \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\nX_scaled = StandardScaler().fit_transform(X)\n\nprint(f\"\u5143\u306e\u30c7\u30fc\u30bf\u5f62\u72b6: {X.shape}\")\n</code></pre>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#62-pca","title":"6.2 PCA\uff08\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\uff09\u3068\u306e\u6bd4\u8f03","text":"<pre><code># PCA\u306b\u3088\u308b\u6b21\u5143\u524a\u6e1b\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# \u53ef\u8996\u5316\nplt.figure(figsize=(10, 8))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=50, alpha=0.8)\nplt.colorbar(label='\u6570\u5b57\u306e\u30af\u30e9\u30b9')\nplt.title('PCA\u306b\u3088\u308b\u624b\u66f8\u304d\u6570\u5b57\u306e2\u6b21\u5143\u8868\u793a')\nplt.xlabel(f'\u7b2c1\u4e3b\u6210\u5206 (\u5206\u6563: {pca.explained_variance_ratio_[0]:.2f})')\nplt.ylabel(f'\u7b2c2\u4e3b\u6210\u5206 (\u5206\u6563: {pca.explained_variance_ratio_[1]:.2f})')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# \u7d2f\u7a4d\u5bc4\u4e0e\u7387\nplt.figure(figsize=(8, 5))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\nplt.axhline(y=0.9, color='r', linestyle='--')\nplt.grid(True, alpha=0.3)\nplt.xlabel('\u4e3b\u6210\u5206\u306e\u6570')\nplt.ylabel('\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.title('PCA\u306e\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.show()\n</code></pre>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#63-t-sne","title":"6.3 t-SNE\u306e\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<pre><code># t-SNE\u3092\u5b9f\u884c\n# \u6ce8\u610f: \u8a08\u7b97\u6642\u9593\u304c\u304b\u304b\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nX_tsne = tsne.fit_transform(X_scaled)\n\n# t-SNE\u306e\u7d50\u679c\u3092\u53ef\u8996\u5316\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=50, alpha=0.8)\nplt.colorbar(scatter, label='\u6570\u5b57\u306e\u30af\u30e9\u30b9')\nplt.title('t-SNE\u306b\u3088\u308b\u624b\u66f8\u304d\u6570\u5b57\u306e2\u6b21\u5143\u8868\u793a (\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3=30)')\nplt.xlabel('t-SNE \u6b21\u5143 1')\nplt.ylabel('t-SNE \u6b21\u5143 2')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# \u7570\u306a\u308b\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\u3067\u306e\u6bd4\u8f03\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nperplexities = [5, 30, 50]\n\nfor i, perplexity in enumerate(perplexities):\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n    X_tsne = tsne.fit_transform(X_scaled)\n\n    scatter = axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=30, alpha=0.7)\n    axes[i].set_title(f't-SNE (\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3={perplexity})')\n    axes[i].set_xlabel('t-SNE \u6b21\u5143 1')\n    axes[i].set_ylabel('t-SNE \u6b21\u5143 2')\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#64-umap","title":"6.4 UMAP\u306e\u5b9f\u88c5\u3068\u53ef\u8996\u5316","text":"<pre><code># UMAP\u3092\u5b9f\u884c\numap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\nX_umap = umap_reducer.fit_transform(X_scaled)\n\n# UMAP\u306e\u7d50\u679c\u3092\u53ef\u8996\u5316\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', s=50, alpha=0.8)\nplt.colorbar(scatter, label='\u6570\u5b57\u306e\u30af\u30e9\u30b9')\nplt.title('UMAP\u306b\u3088\u308b\u624b\u66f8\u304d\u6570\u5b57\u306e2\u6b21\u5143\u8868\u793a (n_neighbors=15, min_dist=0.1)')\nplt.xlabel('UMAP \u6b21\u5143 1')\nplt.ylabel('UMAP \u6b21\u5143 2')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# \u7570\u306a\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u306e\u6bd4\u8f03\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\nneighbors = [5, 30]\nmin_dists = [0.1, 0.5]\n\nfor i, n in enumerate(neighbors):\n    for j, d in enumerate(min_dists):\n        umap_reducer = umap.UMAP(n_neighbors=n, min_dist=d, random_state=42)\n        X_umap = umap_reducer.fit_transform(X_scaled)\n\n        scatter = axes[i, j].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', s=30, alpha=0.7)\n        axes[i, j].set_title(f'UMAP (n_neighbors={n}, min_dist={d})')\n        axes[i, j].set_xlabel('UMAP \u6b21\u5143 1')\n        axes[i, j].set_ylabel('UMAP \u6b21\u5143 2')\n        axes[i, j].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#65","title":"6.5 \u5b9f\u884c\u6642\u9593\u306e\u6bd4\u8f03","text":"<pre><code>import time\n\n# \u5b9f\u884c\u6642\u9593\u3092\u8a08\u6e2c\u3059\u308b\u95a2\u6570\ndef measure_time(algorithm, data):\n    start_time = time.time()\n    result = algorithm.fit_transform(data)\n    end_time = time.time()\n    return result, end_time - start_time\n\n# \u5404\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u5b9f\u884c\nprint(\"\u5b9f\u884c\u6642\u9593\u306e\u6bd4\u8f03\uff1a\")\n\n# PCA\npca = PCA(n_components=2)\n_, pca_time = measure_time(pca, X_scaled)\nprint(f\"PCA: {pca_time:.2f}\u79d2\")\n\n# t-SNE\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\n_, tsne_time = measure_time(tsne, X_scaled)\nprint(f\"t-SNE: {tsne_time:.2f}\u79d2\")\n\n# UMAP\numap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n_, umap_time = measure_time(umap_reducer, X_scaled)\nprint(f\"UMAP: {umap_time:.2f}\u79d2\")\n\n# \u5b9f\u884c\u6642\u9593\u306e\u30d0\u30fc\u30d7\u30ed\u30c3\u30c8\nalgorithms = ['PCA', 't-SNE', 'UMAP']\ntimes = [pca_time, tsne_time, umap_time]\n\nplt.figure(figsize=(10, 6))\nplt.bar(algorithms, times, color=['blue', 'green', 'red'])\nplt.ylabel('\u5b9f\u884c\u6642\u9593\uff08\u79d2\uff09')\nplt.title('\u6b21\u5143\u524a\u6e1b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u5b9f\u884c\u6642\u9593\u6bd4\u8f03')\nplt.grid(True, alpha=0.3, axis='y')\nplt.show()\n</code></pre>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#66","title":"6.6 \u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306e\u4f8b: \u7cd6\u5c3f\u75c5\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316","text":"<pre><code>from sklearn.datasets import load_diabetes\n\n# \u7cd6\u5c3f\u75c5\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ndiabetes = load_diabetes()\nX_diabetes = diabetes.data\ny_diabetes = diabetes.target\n\n# \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\nX_diabetes_scaled = StandardScaler().fit_transform(X_diabetes)\n\n# 3\u3064\u306e\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u306e\u9069\u7528\n# PCA\npca_diabetes = PCA(n_components=2).fit_transform(X_diabetes_scaled)\n\n# t-SNE\ntsne_diabetes = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_diabetes_scaled)\n\n# UMAP\numap_diabetes = umap.UMAP(random_state=42).fit_transform(X_diabetes_scaled)\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\uff08\u30ab\u30e9\u30fc\u306f\u75be\u60a3\u9032\u884c\u5ea6\u3092\u793a\u3059\uff09\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# PCA\nscatter1 = axes[0].scatter(pca_diabetes[:, 0], pca_diabetes[:, 1], c=y_diabetes, \n                          cmap='coolwarm', s=40, alpha=0.8)\naxes[0].set_title('PCA: \u7cd6\u5c3f\u75c5\u30c7\u30fc\u30bf')\naxes[0].set_xlabel('\u7b2c1\u4e3b\u6210\u5206')\naxes[0].set_ylabel('\u7b2c2\u4e3b\u6210\u5206')\naxes[0].grid(True, alpha=0.3)\n\n# t-SNE\nscatter2 = axes[1].scatter(tsne_diabetes[:, 0], tsne_diabetes[:, 1], c=y_diabetes, \n                          cmap='coolwarm', s=40, alpha=0.8)\naxes[1].set_title('t-SNE: \u7cd6\u5c3f\u75c5\u30c7\u30fc\u30bf')\naxes[1].set_xlabel('t-SNE \u6b21\u5143 1')\naxes[1].set_ylabel('t-SNE \u6b21\u5143 2')\naxes[1].grid(True, alpha=0.3)\n\n# UMAP\nscatter3 = axes[2].scatter(umap_diabetes[:, 0], umap_diabetes[:, 1], c=y_diabetes, \n                          cmap='coolwarm', s=40, alpha=0.8)\naxes[2].set_title('UMAP: \u7cd6\u5c3f\u75c5\u30c7\u30fc\u30bf')\naxes[2].set_xlabel('UMAP \u6b21\u5143 1')\naxes[2].set_ylabel('UMAP \u6b21\u5143 2')\naxes[2].grid(True, alpha=0.3)\n\n# \u30ab\u30e9\u30fc\u30d0\u30fc\u306e\u8ffd\u52a0\ncbar = fig.colorbar(scatter3, ax=axes.ravel().tolist())\ncbar.set_label('\u75be\u60a3\u9032\u884c\u5ea6')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#7","title":"7. \u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#_3","title":"\u57fa\u672c\u554f\u984c","text":"<ol> <li> <p>\u554f\u984c1: \u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\uff08PCA\uff09\u306e\u9650\u754c\u30923\u3064\u6319\u3052\u3001\u305d\u308c\u305e\u308c\u306b\u3064\u3044\u3066\u7c21\u6f54\u306b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u554f\u984c2: t-SNE\u306b\u304a\u3051\u308b\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5f79\u5272\u3092\u8aac\u660e\u3057\u3001\u5024\u304c\u5c0f\u3055\u3044\u5834\u5408\u3068\u5927\u304d\u3044\u5834\u5408\u3067\u3069\u306e\u3088\u3046\u306a\u9055\u3044\u304c\u751f\u3058\u308b\u304b\u8ff0\u3079\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u554f\u984c3: UMAP\u306e\u7406\u8ad6\u7684\u80cc\u666f\u306b\u3064\u3044\u3066\u3001t-SNE\u3068\u306e\u9055\u3044\u3092\u4e2d\u5fc3\u306b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u554f\u984c4: \u6b21\u306e\u5404\u30b1\u30fc\u30b9\u3067\u306f\u3001PCA\u3001t-SNE\u3001UMAP\u306e\u3046\u3061\u3069\u308c\u304c\u6700\u9069\u304b\u9078\u629e\u3057\u3001\u7406\u7531\u3092\u8aac\u660e\u3057\u306a\u3055\u3044\u3002    a) 10\u4e07\u60a3\u8005\u306e\u96fb\u5b50\u30ab\u30eb\u30c6\u304b\u3089\u62bd\u51fa\u3057\u305f1000\u6b21\u5143\u306e\u7279\u5fb4\u91cf\u3092\u53ef\u8996\u5316\u3057\u305f\u3044    b) \u591a\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u8ef8\u3092\u898b\u3064\u3051\u305f\u3044    c) \u5358\u4e00\u7d30\u80deRNA\u30b7\u30fc\u30b1\u30f3\u30b9\u30c7\u30fc\u30bf\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3044\u305f\u3044    d) \u6b21\u5143\u524a\u6e1b\u5f8c\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001\u65b0\u305f\u306a\u30c7\u30fc\u30bf\u70b9\u3092\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u305f\u3044</p> </li> <li> <p>\u554f\u984c5: t-SNE\u3068UMAP\u306e\u8a08\u7b97\u52b9\u7387\u306e\u9055\u3044\u3092\u8aac\u660e\u3057\u3001\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5834\u5408\u306bUMAP\u304c\u512a\u308c\u3066\u3044\u308b\u7406\u7531\u3092\u8ff0\u3079\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#_4","title":"\u5fdc\u7528\u554f\u984c","text":"<ol> <li>\u554f\u984c6: \u4ee5\u4e0b\u306ePython\u30b3\u30fc\u30c9\u306e\u8aa4\u308a\u3092\u6307\u6458\u3057\u3001\u4fee\u6b63\u3057\u306a\u3055\u3044\u3002</li> </ol> <pre><code># \u6b21\u5143\u524a\u6e1b\u306e\u6bd4\u8f03\u30b3\u30fc\u30c9\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# \u30c7\u30fc\u30bf\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306a\u3057\u3067\u76f4\u63a5\u9069\u7528\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# \u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\u306b\u30de\u30a4\u30ca\u30b9\u5024\u3092\u8a2d\u5b9a\ntsne = TSNE(n_components=2, perplexity=-5)\nX_tsne = tsne.fit_transform(X)\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(X_pca[0], X_pca[1], c=y)  # \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u304c\u8aa4\u3063\u3066\u3044\u308b\nplt.title('PCA')\nplt.subplot(1, 2, 2)\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\nplt.title('t-SNE')\nplt.show()\n</code></pre> <ol> <li> <p>\u554f\u984c7: 2\u6b21\u5143\u5e73\u9762\u4e0a\u306e3\u3064\u306e\u540c\u5fc3\u5186\u304b\u3089\u306a\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08\u5404\u5186\u306b\u306f100\u70b9\u305a\u3064\u3001\u5408\u8a08300\u70b9\uff09\u3092\u751f\u6210\u3057\u3001PCA\u3001t-SNE\u3001UMAP\u306e3\u3064\u306e\u624b\u6cd5\u30672\u6b21\u5143\u306b\u57cb\u3081\u8fbc\u3093\u3060\u7d50\u679c\u3092\u6bd4\u8f03\u3057\u306a\u3055\u3044\u3002\u5404\u624b\u6cd5\u306e\u7d50\u679c\u306e\u9055\u3044\u3092\u8003\u5bdf\u3057\u3001\u3053\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u69cb\u9020\u306b\u9069\u3057\u305f\u624b\u6cd5\u306f\u3069\u308c\u304b\u3001\u7406\u7531\u3068\u3068\u3082\u306b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u554f\u984c8: \u5065\u5eb7\u30c7\u30fc\u30bf\u79d1\u5b66\u306e\u89b3\u70b9\u304b\u3089\u3001\u4ee5\u4e0b\u306e\u30b7\u30ca\u30ea\u30aa\u306b\u5bfe\u3057\u3066\u6700\u9069\u306a\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u3068\u305d\u306e\u5177\u4f53\u7684\u306a\u9069\u7528\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u306a\u3055\u3044\u3002</p> </li> </ol> <p>\u3042\u306a\u305f\u306f\u533b\u7642\u7814\u7a76\u6a5f\u95a2\u3067\u50cd\u3044\u3066\u304a\u308a\u3001\u69d8\u3005\u306a\u751f\u7406\u6307\u6a19\uff08\u8840\u5727\u3001\u5fc3\u62cd\u6570\u3001\u4f53\u6e29\u3001\u8840\u7cd6\u5024\u306a\u306920\u7a2e\u985e\uff09\u3068\u751f\u6d3b\u7fd2\u6163\u30c7\u30fc\u30bf\uff08\u98df\u4e8b\u3001\u904b\u52d5\u3001\u7761\u7720\u306a\u306910\u7a2e\u985e\uff09\u3092\u65e5\u3005\u53ce\u96c6\u3057\u305f3\u5e74\u5206\u306e\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\uff081000\u4eba\u5206\uff09\u3092\u5206\u6790\u3059\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u76ee\u7684\u306f\u3001\u5065\u5eb7\u72b6\u614b\u306e\u5909\u5316\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\u3057\u3001\u5c06\u6765\u306e\u75be\u60a3\u30ea\u30b9\u30af\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306e\u30b5\u30d6\u30b0\u30eb\u30fc\u30d7\u3092\u540c\u5b9a\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#8","title":"8. \u3088\u304f\u3042\u308b\u8cea\u554f\u3068\u89e3\u7b54","text":""},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#q1-pca","title":"Q1: \u306a\u305cPCA\u306a\u3069\u306e\u7dda\u5f62\u624b\u6cd5\u3067\u306f\u306a\u304f\u3001\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u3092\u4f7f\u3046\u3079\u304d\u5834\u5408\u304c\u3042\u308b\u306e\u3067\u3059\u304b\uff1f","text":"<p>A: \u5b9f\u4e16\u754c\u306e\u30c7\u30fc\u30bf\u306f\u591a\u304f\u306e\u5834\u5408\u3001\u975e\u7dda\u5f62\u306e\u95a2\u4fc2\u6027\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u753b\u50cf\u30c7\u30fc\u30bf\u3084\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306a\u3069\u306f\u5358\u7d14\u306a\u7dda\u5f62\u95a2\u4fc2\u3067\u306f\u8868\u73fe\u3067\u304d\u306a\u3044\u8907\u96d1\u306a\u69cb\u9020\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002PCA\u306a\u3069\u306e\u7dda\u5f62\u624b\u6cd5\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u76f4\u4ea4\u8ef8\u3092\u898b\u3064\u3051\u308b\u3060\u3051\u3067\u3042\u308a\u3001\u30c7\u30fc\u30bf\u304c\u66f2\u9762\uff08\u30de\u30cb\u30d5\u30a9\u30fc\u30eb\u30c9\uff09\u4e0a\u306b\u5206\u5e03\u3057\u3066\u3044\u308b\u5834\u5408\u3084\u3001\u5c40\u6240\u7684\u306a\u95a2\u4fc2\u304c\u91cd\u8981\u306a\u5834\u5408\u306b\u306f\u9069\u5207\u306b\u8868\u73fe\u3067\u304d\u307e\u305b\u3093\u3002\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5c40\u6240\u7684\u306a\u95a2\u4fc2\u6027\u3092\u4fdd\u5b58\u3057\u306a\u304c\u3089\u5927\u57df\u7684\u306a\u69cb\u9020\u3082\u7dad\u6301\u3059\u308b\u305f\u3081\u3001\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u306b\u512a\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#q2-t-sneumap","title":"Q2: t-SNE\u3068UMAP\u306e\u3069\u3061\u3089\u3092\u9078\u3079\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A: \u9078\u629e\u306f\u3042\u306a\u305f\u306e\u5206\u6790\u76ee\u7684\u306b\u3088\u308a\u307e\u3059\u3002</p> <ul> <li> <p>t-SNE \u306f\u5c40\u6240\u7684\u306a\u69cb\u9020\u4fdd\u5b58\u306b\u512a\u308c\u3066\u304a\u308a\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u9593\u306e\u8ddd\u96e2\u3088\u308a\u3082\u30af\u30e9\u30b9\u30bf\u30fc\u5185\u306e\u95a2\u4fc2\u6027\u3092\u3088\u304f\u8868\u73fe\u3057\u307e\u3059\u3002\u8a08\u7b97\u6642\u9593\u304c\u9577\u3044\u3053\u3068\u3001\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u306f\u9045\u3044\u3053\u3068\u3001\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u69cb\u9020\u306e\u4fdd\u5b58\u304c\u5f31\u3044\u3053\u3068\u304c\u6b20\u70b9\u3067\u3059\u3002</p> </li> <li> <p>UMAP \u306ft-SNE\u3088\u308a\u3082\u8a08\u7b97\u304c\u901f\u304f\u3001\u5927\u57df\u7684\u69cb\u9020\u3082\u3088\u308a\u826f\u304f\u4fdd\u5b58\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u70b9\u306e\u57cb\u3081\u8fbc\u307f\u3082\u53ef\u80fd\u3067\u3059\u3002\u305f\u3060\u3057\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u8abf\u6574\u304c\u8907\u96d1\u3067\u3001\u7406\u8ad6\u7684\u7406\u89e3\u306b\u306f\u30c8\u30dd\u30ed\u30b8\u30fc\u306e\u77e5\u8b58\u304c\u5fc5\u8981\u3067\u3059\u3002</p> </li> </ul> <p>\u63a2\u7d22\u7684\u5206\u6790\u306e\u521d\u671f\u6bb5\u968e\u3084\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u3067\u306f\u3001\u307e\u305aUMAP\u3092\u8a66\u3057\u3066\u307f\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u3088\u308a\u8a73\u7d30\u306a\u5c40\u6240\u69cb\u9020\u5206\u6790\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f\u3001t-SNE\u3082\u691c\u8a0e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#q3","title":"Q3: \u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u306e\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u306f\u3042\u308a\u307e\u3059\u304b\uff1f","text":"<p>A: \u4e00\u822c\u7684\u306a\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <p>t-SNE\u306e\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3: - \u5c0f\u3055\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08n &lt; 100\uff09: 5\u301c15 - \u4e2d\u898f\u6a21\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08100 &lt; n &lt; 1000\uff09: 15\u301c30 - \u5927\u898f\u6a21\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08n &gt; 1000\uff09: 30\u301c50</p> <p>UMAP\u306e\u30d1\u30e9\u30e1\u30fc\u30bf: - <code>n_neighbors</code>: \u5c40\u6240/\u5927\u57df\u7684\u69cb\u9020\u306e\u30d0\u30e9\u30f3\u30b9\u3092\u5236\u5fa1\uff08\u5c0f\u3055\u3044\u5024\u306f\u5c40\u6240\u7684\u3001\u5927\u304d\u3044\u5024\u306f\u5927\u57df\u7684\uff09 - <code>min_dist</code>: \u57cb\u3081\u8fbc\u307f\u7a7a\u9593\u3067\u306e\u70b9\u306e\u5bc6\u96c6\u5ea6\u3092\u5236\u5fa1\uff08\u5c0f\u3055\u3044\u5024\u306f\u5bc6\u96c6\u3001\u5927\u304d\u3044\u5024\u306f\u5206\u6563\uff09</p> <p>\u3069\u3061\u3089\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u3082\u3001\u7570\u306a\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u3067\u8907\u6570\u56de\u5b9f\u884c\u3057\u3001\u7d50\u679c\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#q4","title":"Q4: \u6b21\u5143\u524a\u6e1b\u306e\u7d50\u679c\u3092\u3069\u306e\u3088\u3046\u306b\u89e3\u91c8\u3059\u308c\u3070\u3088\u3044\u3067\u3059\u304b\uff1f","text":"<p>A: \u6b21\u5143\u524a\u6e1b\u306e\u7d50\u679c\u3092\u89e3\u91c8\u3059\u308b\u969b\u306e\u6ce8\u610f\u70b9\uff1a</p> <ol> <li> <p>\u8ddd\u96e2\u306e\u89e3\u91c8: t-SNE\u3084UMAP\u3067\u306f\u3001\u8fd1\u3044\u70b9\u306f\u9ad8\u6b21\u5143\u7a7a\u9593\u3067\u3082\u8fd1\u3044\u3053\u3068\u3092\u793a\u5506\u3057\u307e\u3059\u304c\u3001\u9060\u3044\u70b9\u540c\u58eb\u306e\u8ddd\u96e2\u306f\u5fc5\u305a\u3057\u3082\u610f\u5473\u3092\u6301\u3061\u307e\u305b\u3093\u3002</p> </li> <li> <p>\u30af\u30e9\u30b9\u30bf\u30fc: \u5f62\u6210\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u30fc\u306f\u3001\u30c7\u30fc\u30bf\u5185\u306e\u81ea\u7136\u306a\u7fa4\u308c\u3092\u8868\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u7279\u6027\u306b\u3088\u308b\u30a2\u30fc\u30c6\u30a3\u30d5\u30a1\u30af\u30c8\u306e\u53ef\u80fd\u6027\u3082\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u8996\u899a\u7684\u691c\u8a3c: \u8907\u6570\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u3084\u8907\u6570\u306e\u624b\u6cd5\u3067\u7d50\u679c\u3092\u6bd4\u8f03\u3057\u3001\u4e00\u8cab\u3057\u305f\u30d1\u30bf\u30fc\u30f3\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p> </li> <li> <p>\u5143\u306e\u7279\u5fb4\u3068\u306e\u95a2\u9023: \u4f4e\u6b21\u5143\u8868\u73fe\u3068\u5143\u306e\u7279\u5fb4\u3068\u306e\u95a2\u9023\u3092\u8abf\u67fb\u3059\u308b\u3053\u3068\u3067\u3001\u5f62\u6210\u3055\u308c\u305f\u30d1\u30bf\u30fc\u30f3\u306e\u610f\u5473\u3092\u7406\u89e3\u3067\u304d\u307e\u3059\u3002</p> </li> </ol>"},{"location":"lectures/LA/46-nonlinear-dimension-reduction/#q5-t-snet","title":"Q5: \u306a\u305ct-SNE\u306f\u6a19\u6e96\u7684\u306a\u6b63\u898f\u5206\u5e03\u3067\u306f\u306a\u304ft\u5206\u5e03\u3092\u63a1\u7528\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304b\uff1f","text":"<p>A: t-SNE\u304ct\u5206\u5e03\u3092\u63a1\u7528\u3057\u3066\u3044\u308b\u4e3b\u306a\u7406\u7531\u306f\u300c\u30af\u30e9\u30a6\u30c7\u30a3\u30f3\u30b0\u554f\u984c\u300d\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u3067\u3059\u3002\u9ad8\u6b21\u5143\u7a7a\u9593\u3067\u306f\u70b9\u9593\u306e\u8ddd\u96e2\u304c\u6bd4\u8f03\u7684\u5747\u4e00\u306b\u306a\u308b\u50be\u5411\uff08\u300c\u6b21\u5143\u306e\u546a\u3044\u300d\uff09\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u3092\u4f4e\u6b21\u5143\u306b\u57cb\u3081\u8fbc\u3080\u969b\u3001\u3059\u3079\u3066\u306e\u8ddd\u96e2\u95a2\u4fc2\u3092\u6b63\u78ba\u306b\u4fdd\u5b58\u3059\u308b\u3053\u3068\u306f\u4e0d\u53ef\u80fd\u3067\u3059\u3002</p> <p>\u901a\u5e38\u306e\u6b63\u898f\u5206\u5e03\uff08SNE\u3067\u4f7f\u7528\uff09\u3067\u306f\u3001\u4f4e\u6b21\u5143\u7a7a\u9593\u3067\u306e\u78ba\u7387\u5206\u5e03\u306e\u88fe\u304c\u6025\u901f\u306b\u6e1b\u5c11\u3059\u308b\u305f\u3081\u3001\u985e\u4f3c\u3057\u3066\u3044\u306a\u3044\u70b9\u3092\u5341\u5206\u306b\u9060\u304f\u306b\u914d\u7f6e\u3067\u304d\u307e\u305b\u3093\u3002t\u5206\u5e03\u306f\u88fe\u304c\u91cd\u3044\uff08heavy-tailed\uff09\u305f\u3081\u3001\u985e\u4f3c\u5ea6\u306e\u4f4e\u3044\u70b9\u3092\u3088\u308a\u9060\u304f\u306b\u914d\u7f6e\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5c40\u6240\u7684\u306a\u69cb\u9020\u304c\u3088\u308a\u660e\u78ba\u306b\u4fdd\u5b58\u3055\u308c\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u304c\u8996\u899a\u7684\u306b\u5206\u96e2\u3057\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/LA/47-wrapup/","title":"\u7dda\u5f62\u4ee3\u6570\u5b66 \u8b1b\u7fa9\u30ce\u30fc\u30c8 - \u7b2c47\u56de","text":""},{"location":"lectures/LA/47-wrapup/#1","title":"1. \u8b1b\u7fa9\u60c5\u5831\u3068\u4e88\u7fd2\u30ac\u30a4\u30c9","text":"<p>\u8b1b\u7fa9\u56de: \u7b2c47\u56de \u30c6\u30fc\u30de: \u7dda\u5f62\u4ee3\u6570\u306e\u5fdc\u7528\u3068\u3057\u3066\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9 \u95a2\u9023\u9805\u76ee: \u56fa\u6709\u5024\u5206\u89e3\u3001\u7279\u7570\u5024\u5206\u89e3\u3001\u4e3b\u6210\u5206\u5206\u6790\u3001\u56e0\u5b50\u5206\u6790\u3001\u6b21\u5143\u524a\u6e1b\u6cd5 \u4e88\u7fd2\u5185\u5bb9: - \u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u6982\u5ff5 - \u7279\u7570\u5024\u5206\u89e3(SVD)\u306e\u57fa\u672c - \u4e3b\u6210\u5206\u5206\u6790(PCA)\u306e\u76ee\u7684\u3068\u624b\u6cd5 - \u56e0\u5b50\u5206\u6790\u306e\u57fa\u672c\u7684\u306a\u8003\u3048\u65b9</p>"},{"location":"lectures/LA/47-wrapup/#2","title":"2. \u5b66\u7fd2\u76ee\u6a19","text":"<ol> <li>\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u4e3b\u8981\u6982\u5ff5\uff08\u56fa\u6709\u5024\u30fb\u7279\u7570\u5024\u5206\u89e3\u30fb\u4e3b\u6210\u5206\u5206\u6790\u30fb\u56e0\u5b50\u5206\u6790\uff09\u306e\u7d71\u5408\u7684\u7406\u89e3</li> <li>\u3053\u308c\u3089\u306e\u624b\u6cd5\u304c\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3001\u7279\u306b\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3044\u3066\u3069\u306e\u3088\u3046\u306b\u5fdc\u7528\u3055\u308c\u308b\u304b\u3092\u7406\u89e3\u3059\u308b</li> <li>\u7dda\u5f62\u624b\u6cd5\u3068\u975e\u7dda\u5f62\u624b\u6cd5\u306e\u9055\u3044\u3068\u4f7f\u3044\u5206\u3051\u306b\u3064\u3044\u3066\u7406\u89e3\u3059\u308b</li> <li>\u6700\u65b0\u306e\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u306b\u3064\u3044\u3066\u6982\u89b3\u3057\u3001\u305d\u306e\u5fdc\u7528\u53ef\u80fd\u6027\u3092\u628a\u63e1\u3059\u308b</li> </ol>"},{"location":"lectures/LA/47-wrapup/#3","title":"3. \u57fa\u672c\u6982\u5ff5\u306e\u7dcf\u62ec","text":""},{"location":"lectures/LA/47-wrapup/#31","title":"3.1 \u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u5fa9\u7fd2","text":"<p>\u5b9a\u7fa9: \u884c\u5217\\(A\\)\u306b\u5bfe\u3057\u3066\u3001\\(Av = \\lambda v\\) (\\(v \\neq 0\\))\u3092\u6e80\u305f\u3059\u30b9\u30ab\u30e9\u30fc\\(\\lambda\\)\u3092\u56fa\u6709\u5024\u3001\u30d9\u30af\u30c8\u30eb\\(v\\)\u3092\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3068\u547c\u3076\u3002</p> <p>\u56fa\u6709\u5024\u30fb\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u3001\u7dda\u5f62\u5909\u63db\u306e\u672c\u8cea\u7684\u306a\u7279\u6027\u3092\u8868\u3057\u3066\u304a\u308a\u3001\u4ee5\u4e0b\u306e\u91cd\u8981\u306a\u6027\u8cea\u3092\u6301\u3061\u307e\u3059\uff1a</p> <ul> <li>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u7dda\u5f62\u5909\u63db\u306e\u4e0b\u3067\u65b9\u5411\u304c\u4fdd\u5b58\u3055\u308c\u308b\u30d9\u30af\u30c8\u30eb</li> <li>\u56fa\u6709\u5024\u306f\u305d\u306e\u30d9\u30af\u30c8\u30eb\u304c\u3069\u308c\u3060\u3051\u4f38\u7e2e\u3055\u308c\u308b\u304b\u3092\u8868\u3059\u4fc2\u6570</li> <li>\\(n \\times n\\)\u884c\u5217\u306f\u6700\u5927\\(n\\)\u500b\u306e\u56fa\u6709\u5024\u3092\u6301\u3064</li> <li>\u5bfe\u79f0\u884c\u5217\u306e\u56fa\u6709\u5024\u306f\u3059\u3079\u3066\u5b9f\u6570\u3067\u3042\u308a\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b</li> </ul> <p>\u56fa\u6709\u5024\u554f\u984c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u5f0f\u5316\u3055\u308c\u307e\u3059\uff1a \\((A - \\lambda I)v = 0\\)</p> <p>\u975e\u81ea\u660e\u306a\u89e3\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u306e\u6761\u4ef6\uff1a \\(\\det(A - \\lambda I) = 0\\)</p> <p>\u3053\u306e\u65b9\u7a0b\u5f0f\u3092\u7279\u6027\u65b9\u7a0b\u5f0f\u3068\u547c\u3073\u3001\u3053\u308c\u3092\u89e3\u304f\u3053\u3068\u3067\u56fa\u6709\u5024\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/LA/47-wrapup/#32-svd","title":"3.2 \u7279\u7570\u5024\u5206\u89e3(SVD)\u306e\u5fa9\u7fd2","text":"<p>\u5b9a\u7fa9: \\(m \\times n\\)\u884c\u5217\\(A\\)\u306e\u7279\u7570\u5024\u5206\u89e3\u306f\u3001\\(A = U\\Sigma V^T\\)\u3068\u8868\u3055\u308c\u308b\u3002\u3053\u3053\u3067\u3001\\(U\\)\u306f\\(m \\times m\\)\u306e\u76f4\u4ea4\u884c\u5217\u3001\\(V\\)\u306f\\(n \\times n\\)\u306e\u76f4\u4ea4\u884c\u5217\u3001\\(\\Sigma\\)\u306f\\(m \\times n\\)\u306e\u5bfe\u89d2\u884c\u5217\u3067\u3001\u5bfe\u89d2\u6210\u5206\u306b\u306f\\(A\\)\u306e\u7279\u7570\u5024\u304c\u5927\u304d\u3044\u9806\u306b\u4e26\u3079\u3089\u308c\u3066\u3044\u308b\u3002</p> <p>\u7279\u7570\u5024\u5206\u89e3\u306e\u91cd\u8981\u306a\u7279\u5fb4\uff1a</p> <ul> <li>\u3059\u3079\u3066\u306e\u884c\u5217\uff08\u5b9f\u6570\u307e\u305f\u306f\u8907\u7d20\u6570\uff09\u306b\u5bfe\u3057\u3066\u7279\u7570\u5024\u5206\u89e3\u304c\u5b58\u5728\u3059\u308b</li> <li>\u7279\u7570\u5024\u306f\\(A^T A\\)\u306e\u56fa\u6709\u5024\u306e\u5e73\u65b9\u6839</li> <li>\u5de6\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff08\\(U\\)\u306e\u5217\uff09\u306f\\(AA^T\\)\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb</li> <li>\u53f3\u7279\u7570\u30d9\u30af\u30c8\u30eb\uff08\\(V\\)\u306e\u5217\uff09\u306f\\(A^T A\\)\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb</li> <li>\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u306e\u6700\u9069\u6027\uff08\u30a8\u30c3\u30ab\u30fc\u30c8\u30fb\u30e4\u30f3\u30b0\u306e\u5b9a\u7406\uff09</li> </ul> <p>SVD\u306f\u4ee5\u4e0b\u306e\u5e7e\u4f55\u5b66\u7684\u89e3\u91c8\u3092\u6301\u3061\u307e\u3059\uff1a 1. \\(V^T\\)\u306b\u3088\u308b\u56de\u8ee2 2. \\(\\Sigma\\)\u306b\u3088\u308b\u8ef8\u65b9\u5411\u306e\u4f38\u7e2e 3. \\(U\\)\u306b\u3088\u308b\u56de\u8ee2</p>"},{"location":"lectures/LA/47-wrapup/#33-pca","title":"3.3 \u4e3b\u6210\u5206\u5206\u6790(PCA)\u306e\u5fa9\u7fd2","text":"<p>\u5b9a\u7fa9: \u4e3b\u6210\u5206\u5206\u6790\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u304c\u6700\u5927\u3068\u306a\u308b\u65b9\u5411\uff08\u4e3b\u6210\u5206\uff09\u3092\u898b\u3064\u3051\u3001\u5143\u306e\u30c7\u30fc\u30bf\u3092\u3088\u308a\u5c11\u306a\u3044\u6b21\u5143\u3067\u8868\u73fe\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002</p> <p>PCA\u306e\u4e3b\u306a\u7279\u5fb4\uff1a</p> <ul> <li>\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u304c\u4e3b\u6210\u5206\u3068\u306a\u308b</li> <li>\u56fa\u6709\u5024\u304c\u5927\u304d\u3044\u9806\u306b\u4e3b\u6210\u5206\u3092\u9078\u3076\u3053\u3068\u3067\u3001\u60c5\u5831\u640d\u5931\u3092\u6700\u5c0f\u5316\u3057\u306a\u304c\u3089\u6b21\u5143\u524a\u6e1b\u304c\u53ef\u80fd</li> <li>\u5404\u4e3b\u6210\u5206\u306f\u4e92\u3044\u306b\u76f4\u4ea4\u3059\u308b</li> <li>\u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\u306e\u6709\u7121\u306b\u3088\u308a\u7d50\u679c\u304c\u5927\u304d\u304f\u5909\u308f\u308b\u5834\u5408\u304c\u3042\u308b</li> </ul> <p>PCA\u306e\u8a08\u7b97\u624b\u9806\uff1a 1. \u30c7\u30fc\u30bf\u884c\u5217\\(X\\)\u3092\u4e2d\u5fc3\u5316\uff08\u5e73\u5747\u30920\u306b\u3059\u308b\uff09 2. \u5171\u5206\u6563\u884c\u5217\\(C = \\frac{1}{n-1}X^T X\\)\u3092\u8a08\u7b97 3. \\(C\\)\u306e\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97 4. \u56fa\u6709\u5024\u306e\u5927\u304d\u3055\u3067\u30bd\u30fc\u30c8\u3057\u3001\u5bfe\u5fdc\u3059\u308b\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u4e3b\u6210\u5206\u3068\u3059\u308b 5. \u9078\u629e\u3057\u305f\u4e3b\u6210\u5206\u306b\u30c7\u30fc\u30bf\u3092\u5c04\u5f71\u3059\u308b</p>"},{"location":"lectures/LA/47-wrapup/#34","title":"3.4 \u56e0\u5b50\u5206\u6790\u306e\u5fa9\u7fd2","text":"<p>\u5b9a\u7fa9: \u56e0\u5b50\u5206\u6790\u306f\u3001\u89b3\u6e2c\u5909\u6570\u306e\u80cc\u5f8c\u306b\u3042\u308b\u6f5c\u5728\u7684\u306a\u56e0\u5b50\uff08\u5171\u901a\u56e0\u5b50\uff09\u3092\u898b\u3064\u3051\u51fa\u3059\u624b\u6cd5\u3067\u3042\u308b\u3002\u89b3\u6e2c\u5909\u6570\\(X\\)\u306f\u3001\u5171\u901a\u56e0\u5b50\\(F\\)\u3068\u56fa\u6709\u56e0\u5b50\\(\\epsilon\\)\u306b\u3088\u308a\u3001\\(X = \\Lambda F + \\epsilon\\)\u3068\u8868\u73fe\u3055\u308c\u308b\u3002</p> <p>\u56e0\u5b50\u5206\u6790\u306e\u4e3b\u306a\u7279\u5fb4\uff1a</p> <ul> <li>\u56e0\u5b50\u8ca0\u8377\u91cf\u884c\u5217\\(\\Lambda\\)\u306f\u3001\u89b3\u6e2c\u5909\u6570\u3068\u56e0\u5b50\u306e\u95a2\u4fc2\u6027\u3092\u8868\u3059</li> <li>\u5171\u901a\u6027\u306f\u3001\u5171\u901a\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u5206\u6563\u306e\u5272\u5408</li> <li>\u72ec\u81ea\u6027\u306f\u3001\u56fa\u6709\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u5206\u6563\u306e\u5272\u5408</li> <li>\u56e0\u5b50\u56de\u8ee2\u306b\u3088\u308a\u3001\u89e3\u91c8\u306e\u3057\u3084\u3059\u3044\u56e0\u5b50\u69cb\u9020\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u308b</li> </ul> <p>PCA\u3068\u56e0\u5b50\u5206\u6790\u306e\u4e3b\u306a\u9055\u3044\uff1a - PCA\u306f\u5206\u6563\u306e\u6700\u5927\u5316\u3092\u76ee\u7684\u3068\u3059\u308b\u304c\u3001\u56e0\u5b50\u5206\u6790\u306f\u5171\u5206\u6563\u69cb\u9020\u306e\u30e2\u30c7\u30eb\u5316\u3092\u76ee\u7684\u3068\u3059\u308b - PCA\u306f\u8a18\u8ff0\u7684\u624b\u6cd5\u3060\u304c\u3001\u56e0\u5b50\u5206\u6790\u306f\u6f5c\u5728\u5909\u6570\u30e2\u30c7\u30eb\u3068\u3044\u3046\u7d71\u8a08\u30e2\u30c7\u30eb\u306b\u57fa\u3065\u304f - \u56e0\u5b50\u5206\u6790\u3067\u306f\u5171\u901a\u56e0\u5b50\u3068\u56fa\u6709\u56e0\u5b50\u3092\u660e\u793a\u7684\u306b\u533a\u5225\u3059\u308b</p>"},{"location":"lectures/LA/47-wrapup/#4","title":"4. \u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u306b\u304a\u3051\u308b\u5fdc\u7528\u4e8b\u4f8b","text":""},{"location":"lectures/LA/47-wrapup/#41-svd","title":"4.1 \u533b\u7642\u753b\u50cf\u89e3\u6790\u306b\u304a\u3051\u308bSVD","text":"<p>\u533b\u7642\u753b\u50cf\u51e6\u7406\u306b\u304a\u3044\u3066\u3001SVD\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u7528\u9014\u3067\u6d3b\u7528\u3055\u308c\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\u753b\u50cf\u306e\u5727\u7e2e\u3068\u30ce\u30a4\u30ba\u9664\u53bb</li> <li>MRI\u3084CT\u30b9\u30ad\u30e3\u30f3\u753b\u50cf\u306e\u4fdd\u5b58\u3068\u8ee2\u9001\u306e\u52b9\u7387\u5316</li> <li> <p>\u30ce\u30a4\u30ba\u306e\u591a\u3044\u753b\u50cf\u304b\u3089\u672c\u8cea\u7684\u306a\u60c5\u5831\u3092\u62bd\u51fa</p> </li> <li> <p>\u7279\u5fb4\u62bd\u51fa</p> </li> <li>\u753b\u50cf\u304b\u3089\u8a3a\u65ad\u306b\u6709\u7528\u306a\u7279\u5fb4\u3092\u62bd\u51fa</li> <li>\u7570\u5e38\u691c\u51fa\u306e\u305f\u3081\u306e\u57fa\u6e96\u3068\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u306e\u8b58\u5225</li> </ol> <p>\u5b9f\u969b\u306e\u5fdc\u7528\u4f8b\uff1a - \u4e73\u623fX\u7dda\u753b\u50cf\uff08\u30de\u30f3\u30e2\u30b0\u30e9\u30e0\uff09\u304b\u3089\u306e\u816b\u760d\u691c\u51fa - \u7db2\u819c\u753b\u50cf\u304b\u3089\u306e\u7cd6\u5c3f\u75c5\u6027\u7db2\u819c\u75c7\u306e\u81ea\u52d5\u8a3a\u65ad - \u8133MRI\u753b\u50cf\u304b\u3089\u306e\u8133\u5352\u4e2d\u3084\u816b\u760d\u306e\u8b58\u5225</p> <p>SVD\u306b\u3088\u308b\u753b\u50cf\u51e6\u7406\u306e\u57fa\u672c\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\uff1a</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# \u753b\u50cf\u3092\u8aad\u307f\u8fbc\u307f\u3001\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u306b\u5909\u63db\nimg = Image.open('medical_image.jpg').convert('L')\nimg_array = np.array(img)\n\n# SVD\u306e\u8a08\u7b97\nU, s, Vt = np.linalg.svd(img_array, full_matrices=False)\n\n# \u7570\u306a\u308b\u7279\u7570\u5024\u306e\u6570\u3067\u753b\u50cf\u3092\u518d\u69cb\u6210\nfor k in [5, 10, 20, 50]:\n    # \u4e0a\u4f4dk\u500b\u306e\u7279\u7570\u5024\u3060\u3051\u3092\u4f7f\u7528\n    s_k = np.zeros_like(s)\n    s_k[:k] = s[:k]\n\n    # \u753b\u50cf\u306e\u518d\u69cb\u6210\n    img_reconstructed = U @ np.diag(s_k) @ Vt\n\n    # \u7d50\u679c\u306e\u8868\u793a\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.imshow(img_array, cmap='gray')\n    plt.title('\u539f\u753b\u50cf')\n    plt.subplot(1, 2, 2)\n    plt.imshow(img_reconstructed, cmap='gray')\n    plt.title(f'\u4e0a\u4f4d{k}\u500b\u306e\u7279\u7570\u5024\u3067\u518d\u69cb\u6210')\n    plt.show()\n</code></pre>"},{"location":"lectures/LA/47-wrapup/#42-pca","title":"4.2 \u751f\u4f53\u4fe1\u53f7\u306ePCA\u306b\u3088\u308b\u7279\u5fb4\u62bd\u51fa","text":"<p>\u751f\u4f53\u4fe1\u53f7\uff08\u5fc3\u96fb\u56f3\u3001\u8133\u6ce2\u3001\u7b4b\u96fb\u56f3\u306a\u3069\uff09\u306f\u9ad8\u6b21\u5143\u304b\u3064\u30ce\u30a4\u30ba\u3092\u542b\u3080\u305f\u3081\u3001PCA\u306b\u3088\u308b\u524d\u51e6\u7406\u304c\u6709\u52b9\u3067\u3059\uff1a</p> <ol> <li>\u30ce\u30a4\u30ba\u9664\u53bb\u3068\u6b21\u5143\u524a\u6e1b</li> <li>\u591a\u30c1\u30e3\u30f3\u30cd\u30eb\u8133\u6ce2(EEG)\u30c7\u30fc\u30bf\u304b\u3089\u610f\u5473\u306e\u3042\u308b\u4fe1\u53f7\u6210\u5206\u306e\u62bd\u51fa</li> <li> <p>\u8a08\u6e2c\u6642\u306e\u30a2\u30fc\u30c6\u30a3\u30d5\u30a1\u30af\u30c8\uff08\u4f53\u52d5\u306a\u3069\uff09\u306e\u9664\u53bb</p> </li> <li> <p>\u30d1\u30bf\u30fc\u30f3\u767a\u898b</p> </li> <li>\u7761\u7720\u6bb5\u968e\u306e\u8b58\u5225</li> <li>\u3066\u3093\u304b\u3093\u767a\u4f5c\u306e\u4e88\u6e2c</li> <li>\u8133-\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9(BCI)\u306e\u7cbe\u5ea6\u5411\u4e0a</li> </ol> <p>\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u306ePCA\u51e6\u7406\u4f8b\uff1a</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# \u591a\u30c1\u30e3\u30f3\u30cd\u30eb\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 (\u4f8b: 12\u8a98\u5c0e\u5fc3\u96fb\u56f3)\n# \u5f62\u72b6: [\u30b5\u30f3\u30d7\u30eb\u6570, \u30c1\u30e3\u30f3\u30cd\u30eb\u6570]\necg_data = np.load('ecg_data.npy')\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\necg_standardized = (ecg_data - np.mean(ecg_data, axis=0)) / np.std(ecg_data, axis=0)\n\n# PCA\u306e\u9069\u7528\npca = PCA()\necg_pca = pca.fit_transform(ecg_standardized)\n\n# \u5bc4\u4e0e\u7387\u306e\u78ba\u8a8d\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\n# \u5bc4\u4e0e\u7387\u306e\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\nplt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'r-o')\nplt.axhline(y=0.9, color='g', linestyle='-')\nplt.xlabel('\u4e3b\u6210\u5206')\nplt.ylabel('\u5bc4\u4e0e\u7387')\nplt.title('\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5206\u6790')\nplt.show()\n\n# \u4e0a\u4f4d2\u6210\u5206\u3067\u306e\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(10, 8))\nplt.scatter(ecg_pca[:, 0], ecg_pca[:, 1])\nplt.xlabel('\u7b2c1\u4e3b\u6210\u5206')\nplt.ylabel('\u7b2c2\u4e3b\u6210\u5206')\nplt.title('\u5fc3\u96fb\u56f3\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\u5e73\u9762\u3078\u306e\u5c04\u5f71')\nplt.show()\n</code></pre>"},{"location":"lectures/LA/47-wrapup/#43","title":"4.3 \u5065\u5eb7\u8cea\u554f\u7968\u306e\u56e0\u5b50\u5206\u6790","text":"<p>\u5065\u5eb7\u95a2\u9023\u8cea\u554f\u7968\u3084\u60a3\u8005\u5831\u544a\u30a2\u30a6\u30c8\u30ab\u30e0(PRO)\u306e\u5206\u6790\u306b\u56e0\u5b50\u5206\u6790\u304c\u5e83\u304f\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\u5c3a\u5ea6\u958b\u767a\u3068\u8a55\u4fa1</li> <li>\u8cea\u554f\u9805\u76ee\u306e\u80cc\u5f8c\u306b\u3042\u308b\u6f5c\u5728\u7684\u306a\u5065\u5eb7\u6982\u5ff5\u306e\u62bd\u51fa</li> <li> <p>\u8cea\u554f\u7968\u306e\u69cb\u9020\u7684\u59a5\u5f53\u6027\u306e\u691c\u8a3c</p> </li> <li> <p>\u5fc3\u7406\u7684\u30fb\u8eab\u4f53\u7684\u5065\u5eb7\u72b6\u614b\u306e\u8a55\u4fa1</p> </li> <li>\u3046\u3064\u75c5\u3084\u4e0d\u5b89\u969c\u5bb3\u306e\u8a55\u4fa1\u5c3a\u5ea6\u306e\u5206\u6790</li> <li>\u751f\u6d3b\u306e\u8cea(QOL)\u8cea\u554f\u7968\u306e\u89e3\u91c8</li> </ol> <p>\u5065\u5eb7\u8cea\u554f\u7968\u306e\u56e0\u5b50\u5206\u6790\u4f8b\uff1a</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom factor_analyzer import FactorAnalyzer\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nfrom factor_analyzer.factor_analyzer import calculate_kmo\n\n# \u5065\u5eb7\u8cea\u554f\u7968\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nhealth_survey = pd.read_csv('health_survey_data.csv')\n\n# Bartlett\u306e\u7403\u9762\u6027\u691c\u5b9a\nchi_square_value, p_value = calculate_bartlett_sphericity(health_survey)\nprint(f'Bartlett\u306e\u7403\u9762\u6027\u691c\u5b9a: chi^2 = {chi_square_value}, p = {p_value}')\n\n# KMO (Kaiser-Meyer-Olkin) \u6307\u6a19\nkmo_all, kmo_model = calculate_kmo(health_survey)\nprint(f'KMO: {kmo_model}')\n\n# \u56e0\u5b50\u6570\u306e\u6c7a\u5b9a (\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8)\nfa = FactorAnalyzer()\nfa.fit(health_survey)\nev, v = fa.get_eigenvalues()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(range(1, len(ev) + 1), ev)\nplt.plot(range(1, len(ev) + 1), ev)\nplt.axhline(y=1, color='r', linestyle='-')\nplt.title('\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8')\nplt.xlabel('\u56e0\u5b50\u6570')\nplt.ylabel('\u56fa\u6709\u5024')\nplt.show()\n\n# \u56e0\u5b50\u5206\u6790\u306e\u5b9f\u65bd\uff08\u56e0\u5b50\u65703\u3068\u4eee\u5b9a\uff09\nfa = FactorAnalyzer(n_factors=3, rotation='varimax')\nfa.fit(health_survey)\n\n# \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u8868\u793a\nfactor_loadings = fa.loadings_\nloadings_df = pd.DataFrame(factor_loadings, \n                          index=health_survey.columns,\n                          columns=[f'\u56e0\u5b50{i+1}' for i in range(3)])\nprint(\"\u56e0\u5b50\u8ca0\u8377\u91cf:\")\nprint(loadings_df)\n\n# \u5171\u901a\u6027\u306e\u8868\u793a\ncommunalities = fa.get_communalities()\ncommunalities_df = pd.DataFrame({'\u9805\u76ee': health_survey.columns, '\u5171\u901a\u6027': communalities})\nprint(\"\\n\u5171\u901a\u6027:\")\nprint(communalities_df)\n</code></pre>"},{"location":"lectures/LA/47-wrapup/#44","title":"4.4 \u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u524a\u6e1b","text":"<p>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306f\u5178\u578b\u7684\u306a\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3067\u3042\u308a\u3001\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u304c\u4e0d\u53ef\u6b20\u3067\u3059\uff1a</p> <ol> <li>\u907a\u4f1d\u5b50\u767a\u73fe\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u306e\u5206\u6790</li> <li>\u6570\u4e07\u306e\u907a\u4f1d\u5b50\u767a\u73fe\u30ec\u30d9\u30eb\u304b\u3089\u91cd\u8981\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u62bd\u51fa</li> <li> <p>\u75be\u60a3\u30b5\u30d6\u30bf\u30a4\u30d7\u306e\u540c\u5b9a\u3084\u4e88\u5f8c\u4e88\u6e2c</p> </li> <li> <p>\u30b7\u30f3\u30b0\u30eb\u30bb\u30ebRNA-seq\u5206\u6790</p> </li> <li>\u7d30\u80de\u30bf\u30a4\u30d7\u306e\u540c\u5b9a</li> <li>\u7d30\u80de\u5206\u5316\u7d4c\u8def\u306e\u89e3\u660e</li> </ol> <p>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u3078\u306ePCA\u9069\u7528\u4f8b\uff1a</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# \u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\n# \u884c\uff1a\u30b5\u30f3\u30d7\u30eb\u3001\u5217\uff1a\u907a\u4f1d\u5b50\ngene_expr = pd.read_csv('gene_expression_data.csv', index_col=0)\nsample_info = pd.read_csv('sample_info.csv', index_col=0)  # \u30b5\u30f3\u30d7\u30eb\u306e\u5c5e\u6027\u60c5\u5831\n\n# \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316\nscaler = StandardScaler()\ngene_expr_scaled = scaler.fit_transform(gene_expr)\n\n# PCA\u306e\u9069\u7528\npca = PCA()\ngene_expr_pca = pca.fit_transform(gene_expr_scaled)\n\n# \u5bc4\u4e0e\u7387\u306e\u30d7\u30ed\u30c3\u30c8\nplt.figure(figsize=(10, 6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.axhline(y=0.7, color='r', linestyle='-')\nplt.xlabel('\u4e3b\u6210\u5206\u6570')\nplt.ylabel('\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.title('\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306ePCA\u7d2f\u7a4d\u5bc4\u4e0e\u7387')\nplt.show()\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u53ef\u8996\u5316\uff08\u4e0a\u4f4d2\u4e3b\u6210\u5206\uff09\nplt.figure(figsize=(12, 10))\nfor category in sample_info['disease_status'].unique():\n    mask = sample_info['disease_status'] == category\n    plt.scatter(gene_expr_pca[mask, 0], gene_expr_pca[mask, 1], label=category, alpha=0.7)\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u306ePCA\u53ef\u8996\u5316')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# \u5bc4\u4e0e\u7387\u306e\u9ad8\u3044\u907a\u4f1d\u5b50\u306e\u7279\u5b9a\nloading_scores = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(5)], index=gene_expr.columns)\ntop_genes = loading_scores.abs().sort_values(by='PC1', ascending=False).index[:20]\nprint(\"PC1\u306b\u5927\u304d\u304f\u5bc4\u4e0e\u3059\u308b\u4e0a\u4f4d20\u907a\u4f1d\u5b50:\")\nprint(top_genes)\n</code></pre>"},{"location":"lectures/LA/47-wrapup/#5","title":"5. \u6700\u65b0\u7814\u7a76\u52d5\u5411","text":""},{"location":"lectures/LA/47-wrapup/#51","title":"5.1 \u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u6cd5","text":"<p>\u7dda\u5f62\u624b\u6cd5\uff08PCA\u3001\u56e0\u5b50\u5206\u6790\uff09\u3067\u306f\u6349\u3048\u304d\u308c\u306a\u3044\u8907\u96d1\u306a\u975e\u7dda\u5f62\u69cb\u9020\u3092\u6301\u3064\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001t-SNE\u3084UMAP\u306a\u3069\u306e\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u6cd5\u304c\u958b\u767a\u3055\u308c\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>t-SNE (t-distributed Stochastic Neighbor Embedding)</li> <li>\u5c40\u6240\u7684\u306a\u985e\u4f3c\u6027\u3092\u4fdd\u5b58\u3059\u308b\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u6cd5</li> <li>\u30c7\u30fc\u30bf\u30dd\u30a4\u30f3\u30c8\u9593\u306e\u6761\u4ef6\u4ed8\u304d\u78ba\u7387\u3092\u4f7f\u7528\u3057\u3066\u3001\u9ad8\u6b21\u5143\u7a7a\u9593\u3067\u306e\u985e\u4f3c\u6027\u3092\u4f4e\u6b21\u5143\u7a7a\u9593\u3067\u518d\u73fe</li> <li> <p>\u96e2\u308c\u305f\u30c7\u30fc\u30bf\u70b9\u9593\u306e\u5927\u57df\u7684\u69cb\u9020\u3088\u308a\u3001\u8fd1\u63a5\u3057\u305f\u30c7\u30fc\u30bf\u70b9\u9593\u306e\u5c40\u6240\u7684\u69cb\u9020\u3092\u512a\u5148</p> </li> <li> <p>UMAP (Uniform Manifold Approximation and Projection)</p> </li> <li>\u30c8\u30dd\u30ed\u30b8\u30fc\u7684\u30c7\u30fc\u30bf\u89e3\u6790\u306b\u57fa\u3065\u304f\u6b21\u5143\u524a\u6e1b\u6cd5</li> <li>\u5c40\u6240\u7684\u69cb\u9020\u3068\u5927\u57df\u7684\u69cb\u9020\u306e\u4e21\u65b9\u3092\u4fdd\u5b58\u3057\u3088\u3046\u3068\u3059\u308b</li> <li>t-SNE\u3088\u308a\u8a08\u7b97\u52b9\u7387\u304c\u826f\u304f\u3001\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u9069\u7528\u53ef\u80fd</li> </ol> <p>\u4e21\u624b\u6cd5\u306e\u6bd4\u8f03\uff1a - t-SNE\u306f\u5c40\u6240\u69cb\u9020\u306e\u4fdd\u5b58\u306b\u512a\u308c\u308b\u304c\u3001\u5927\u57df\u7684\u69cb\u9020\u304c\u5931\u308f\u308c\u308b\u3053\u3068\u304c\u3042\u308b - UMAP\u306ft-SNE\u3088\u308a\u5927\u57df\u7684\u69cb\u9020\u306e\u4fdd\u5b58\u306b\u512a\u308c\u308b\u50be\u5411\u304c\u3042\u308b - UMAP\u306ft-SNE\u3088\u308a\u8a08\u7b97\u304c\u9ad8\u901f - \u3069\u3061\u3089\u3082\u975e\u51f8\u6700\u9069\u5316\u554f\u984c\u3092\u89e3\u304f\u305f\u3081\u3001\u5b9f\u884c\u3054\u3068\u306b\u7d50\u679c\u304c\u5909\u308f\u308a\u3046\u308b</p> <p>t-SNE\u3068UMAP\u306e\u5b9f\u88c5\u4f8b\uff1a</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport umap\n\n# \u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\uff08\u4f8b\uff1a\u5065\u5eb7\u30c7\u30fc\u30bf\uff09\nhigh_dim_data = pd.read_csv('high_dim_health_data.csv')\nlabels = high_dim_data['diagnosis']  # \u8a3a\u65ad\u7d50\u679c\u306a\u3069\ndata = high_dim_data.drop('diagnosis', axis=1)\n\n# t-SNE\u306e\u9069\u7528\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\ntsne_results = tsne.fit_transform(data)\n\n# UMAP\u306e\u9069\u7528\numap_reducer = umap.UMAP(random_state=42)\numap_results = umap_reducer.fit_transform(data)\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\u3068\u6bd4\u8f03\nplt.figure(figsize=(16, 6))\n\n# t-SNE\nplt.subplot(1, 2, 1)\nfor label in np.unique(labels):\n    mask = labels == label\n    plt.scatter(tsne_results[mask, 0], tsne_results[mask, 1], label=label, alpha=0.7)\nplt.title('t-SNE')\nplt.legend()\n\n# UMAP\nplt.subplot(1, 2, 2)\nfor label in np.unique(labels):\n    mask = labels == label\n    plt.scatter(umap_results[mask, 0], umap_results[mask, 1], label=label, alpha=0.7)\nplt.title('UMAP')\nplt.legend()\n\nplt.show()\n</code></pre>"},{"location":"lectures/LA/47-wrapup/#52","title":"5.2 \u30b9\u30d1\u30fc\u30b9\u4e3b\u6210\u5206\u5206\u6790","text":"<p>\u30b9\u30d1\u30fc\u30b9\u4e3b\u6210\u5206\u5206\u6790\u306f\u3001\u901a\u5e38\u306ePCA\u306b\u5236\u7d04\u3092\u52a0\u3048\u3066\u3001\u3088\u308a\u89e3\u91c8\u3057\u3084\u3059\u3044\u4e3b\u6210\u5206\u3092\u5f97\u308b\u624b\u6cd5\u3067\u3059\uff1a</p> <ol> <li>\u4e3b\u306a\u7279\u5fb4</li> <li>\u4e3b\u6210\u5206\u306e\u8a08\u7b97\u306b\u304a\u3044\u3066\u3001\u4e00\u90e8\u306e\u5909\u6570\u306e\u91cd\u307f\u3092\u6b63\u78ba\u306b0\u306b\u3059\u308b\uff08\u30b9\u30d1\u30fc\u30b9\u6027\uff09</li> <li>\u5c11\u6570\u306e\u91cd\u8981\u306a\u5909\u6570\u3060\u3051\u3092\u4f7f\u3063\u3066\u4e3b\u6210\u5206\u3092\u8868\u73fe\u3067\u304d\u308b</li> <li> <p>\u89e3\u91c8\u6027\u3068\u4e88\u6e2c\u6027\u80fd\u306e\u30d0\u30e9\u30f3\u30b9\u3092\u53d6\u308b</p> </li> <li> <p>\u5fdc\u7528\u4f8b</p> </li> <li>\u907a\u4f1d\u5b50\u767a\u73fe\u30c7\u30fc\u30bf\u304b\u3089\u91cd\u8981\u306a\u907a\u4f1d\u5b50\u30bb\u30c3\u30c8\u306e\u540c\u5b9a</li> <li>\u533b\u7642\u753b\u50cf\u306e\u7279\u5fb4\u62bd\u51fa\u306b\u304a\u3051\u308b\u91cd\u8981\u9818\u57df\u306e\u7279\u5b9a</li> <li>\u533b\u7642\u8a3a\u65ad\u306b\u304a\u3051\u308b\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u9078\u5b9a</li> </ol> <p>\u30b9\u30d1\u30fc\u30b9\u4e3b\u6210\u5206\u5206\u6790\u306e\u5b9f\u88c5\u4f8b\uff1a</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import SparsePCA\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ndata = np.load('health_monitoring_data.npy')\n\n# \u30b9\u30d1\u30fc\u30b9\u4e3b\u6210\u5206\u5206\u6790\u306e\u9069\u7528\nspca = SparsePCA(n_components=2, alpha=1.0, ridge_alpha=0.01, random_state=42)\nspca_results = spca.fit_transform(data)\n\n# \u4e3b\u6210\u5206\u306e\u8ca0\u8377\u91cf\uff08\u6210\u5206\uff09\u306e\u78ba\u8a8d\ncomponents = spca.components_\nprint(\"\u30b9\u30d1\u30fc\u30b9\u4e3b\u6210\u5206\u306e\u975e\u30bc\u30ed\u8981\u7d20\u306e\u6570:\")\nprint([np.sum(comp != 0) for comp in components])\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(12, 6))\n\n# \u4e3b\u6210\u52061\u306e\u975e\u30bc\u30ed\u4fc2\u6570\nplt.subplot(1, 2, 1)\nplt.stem(range(len(components[0])), components[0])\nplt.title('\u7b2c1\u30b9\u30d1\u30fc\u30b9\u4e3b\u6210\u5206')\nplt.xlabel('\u5909\u6570\u30a4\u30f3\u30c7\u30c3\u30af\u30b9')\nplt.ylabel('\u8ca0\u8377\u91cf')\n\n# \u4e3b\u6210\u52062\u306e\u975e\u30bc\u30ed\u4fc2\u6570\nplt.subplot(1, 2, 2)\nplt.stem(range(len(components[1])), components[1])\nplt.title('\u7b2c2\u30b9\u30d1\u30fc\u30b9\u4e3b\u6210\u5206')\nplt.xlabel('\u5909\u6570\u30a4\u30f3\u30c7\u30c3\u30af\u30b9')\nplt.ylabel('\u8ca0\u8377\u91cf')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/47-wrapup/#53","title":"5.3 \u30c6\u30f3\u30bd\u30eb\u5206\u89e3\u3068\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf","text":"<p>\u884c\u5217\u3092\u4e00\u822c\u5316\u3057\u305f\u591a\u6b21\u5143\u914d\u5217\u3067\u3042\u308b\u30c6\u30f3\u30bd\u30eb\u306b\u5bfe\u3059\u308b\u5206\u89e3\u624b\u6cd5\u304c\u3001\u8907\u96d1\u306a\u591a\u6b21\u5143\u69cb\u9020\u3092\u6301\u3064\u5065\u5eb7\u30c7\u30fc\u30bf\u5206\u6790\u3067\u6ce8\u76ee\u3055\u308c\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\u30c6\u30f3\u30bd\u30eb\u5206\u89e3\u306e\u4e3b\u306a\u624b\u6cd5</li> <li>CP\u5206\u89e3\uff08CANDECOMP/PARAFAC\u5206\u89e3\uff09</li> <li>Tucker\u5206\u89e3</li> <li> <p>\u30c6\u30f3\u30bd\u30eb\u7279\u7570\u5024\u5206\u89e3\uff08T-SVD\uff09</p> </li> <li> <p>\u5065\u5eb7\u30c7\u30fc\u30bf\u3078\u306e\u5fdc\u7528</p> </li> <li>\u6642\u7cfb\u5217\u533b\u7642\u30c7\u30fc\u30bf\uff08\u60a3\u8005 \u00d7 \u6e2c\u5b9a\u9805\u76ee \u00d7 \u6642\u9593\uff09\u306e\u5206\u6790</li> <li>\u8133\u6a5f\u80fd\u30a4\u30e1\u30fc\u30b8\u30f3\u30b0\uff08\u7a7a\u9593 \u00d7 \u6642\u9593 \u00d7 \u88ab\u9a13\u8005\uff09\u306e\u5206\u6790</li> <li>\u85ac\u7269\u53cd\u5fdc\u30c7\u30fc\u30bf\uff08\u85ac\u7269 \u00d7 \u5206\u5b50\u6a19\u7684 \u00d7 \u7d30\u80de\u7a2e\uff09\u306e\u5206\u6790</li> </ol> <p>\u30c6\u30f3\u30bd\u30eb\u5206\u89e3\u306e\u57fa\u672c\u7684\u306a\u5b9f\u88c5\u4f8b\uff1a</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorly as tl\nfrom tensorly.decomposition import parafac, tucker\n\n# \u4f8b\uff1a3\u6b21\u5143\u30c6\u30f3\u30bd\u30eb\u30c7\u30fc\u30bf\uff08\u60a3\u8005 \u00d7 \u6e2c\u5b9a\u9805\u76ee \u00d7 \u6642\u9593\uff09\ntensor_data = np.load('patient_time_series_tensor.npy')\n\n# CP\u5206\u89e3\nrank = 3  # \u30c6\u30f3\u30bd\u30eb\u306e\u30e9\u30f3\u30af\ncp_factors = parafac(tensor_data, rank=rank, normalize_factors=True)\nweights, factors = cp_factors\n\n# Tucker\u5206\u89e3\nranks = [3, 4, 2]  # \u5404\u30e2\u30fc\u30c9\u306e\u30e9\u30f3\u30af\ntucker_factors = tucker(tensor_data, ranks=ranks, init='random')\ncore, factors = tucker_factors\n\n# \u56e0\u5b50\u306e\u53ef\u8996\u5316\uff08\u4f8b\uff1a\u7b2c1\u30e2\u30fc\u30c9\uff08\u60a3\u8005\uff09\u306e\u56e0\u5b50\uff09\nplt.figure(figsize=(12, 6))\nfor r in range(rank):\n    plt.subplot(1, rank, r+1)\n    plt.plot(factors[0][:, r])\n    plt.title(f'\u60a3\u8005\u56e0\u5b50 {r+1}')\n    plt.xlabel('\u60a3\u8005ID')\n    plt.ylabel('\u56e0\u5b50\u5f97\u70b9')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"lectures/LA/47-wrapup/#54","title":"5.4 \u6df1\u5c64\u5b66\u7fd2\u3068\u306e\u95a2\u9023","text":"<p>\u7dda\u5f62\u4ee3\u6570\u306f\u6df1\u5c64\u5b66\u7fd2\u306e\u57fa\u76e4\u3068\u306a\u3063\u3066\u304a\u308a\u3001\u7279\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u95a2\u9023\u304c\u91cd\u8981\u3067\u3059\uff1a</p> <ol> <li>\u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0</li> <li>\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u306e\u305f\u3081\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb</li> <li>\u5165\u529b\u3092\u4f4e\u6b21\u5143\u306e\u6f5c\u5728\u7a7a\u9593\u306b\u5727\u7e2e\u3057\u3001\u305d\u3053\u304b\u3089\u518d\u69cb\u6210\u3059\u308b</li> <li> <p>\u7dda\u5f62\u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u306fPCA\u3068\u7b49\u4fa1\u306b\u306a\u308b</p> </li> <li> <p>\u884c\u5217\u8a08\u7b97\u3068\u6700\u9069\u5316</p> </li> <li>\u52fe\u914d\u964d\u4e0b\u6cd5\u306b\u304a\u3051\u308b\u884c\u5217\u6f14\u7b97\u306e\u52b9\u7387\u5316</li> <li>\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306b\u304a\u3051\u308b\u5171\u5206\u6563\u884c\u5217\u306e\u8a08\u7b97</li> <li>\u6575\u5bfe\u7684\u751f\u6210\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(GAN)\u306b\u304a\u3051\u308b\u7dda\u5f62\u4ee3\u6570\u306e\u5fdc\u7528</li> </ol> <p>\u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u306e\u5b9f\u88c5\u4f8b\uff1a</p> <pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068\u524d\u51e6\u7406\ndata = np.load('health_monitoring_data.npy')\ndata_scaled = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n\n# \u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u306e\u69cb\u7bc9\ninput_dim = data.shape[1]\nencoding_dim = 2  # \u6f5c\u5728\u7a7a\u9593\u306e\u6b21\u5143\n\n# \u30a8\u30f3\u30b3\u30fc\u30c0\ninput_layer = Input(shape=(input_dim,))\nencoder = Dense(encoding_dim, activation='relu')(input_layer)\n\n# \u30c7\u30b3\u30fc\u30c0\ndecoder = Dense(input_dim, activation='sigmoid')(encoder)\n\n# \u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u30e2\u30c7\u30eb\nautoencoder = Model(inputs=input_layer, outputs=decoder)\nautoencoder.compile(optimizer=Adam(), loss='mse')\n\n# \u30a8\u30f3\u30b3\u30fc\u30c0\u30e2\u30c7\u30eb\uff08\u7279\u5fb4\u62bd\u51fa\u7528\uff09\nencoder_model = Model(inputs=input_layer, outputs=encoder)\n\n# \u30e2\u30c7\u30eb\u306e\u8a13\u7df4\nhistory = autoencoder.fit(\n    data_scaled, data_scaled,\n    epochs=50,\n    batch_size=32,\n    shuffle=True,\n    validation_split=0.2\n)\n\n# \u30a8\u30f3\u30b3\u30fc\u30c9\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\nencoded_data = encoder_model.predict(data_scaled)\n\nplt.figure(figsize=(10, 8))\nplt.scatter(encoded_data[:, 0], encoded_data[:, 1], alpha=0.7)\nplt.colorbar()\nplt.title('\u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u306b\u3088\u308b2\u6b21\u5143\u6f5c\u5728\u7a7a\u9593')\nplt.xlabel('\u6f5c\u5728\u6b21\u51431')\nplt.ylabel('\u6f5c\u5728\u6b21\u51432')\nplt.show()\n</code></pre>"},{"location":"lectures/LA/47-wrapup/#6","title":"6. \u7dcf\u5408\u6f14\u7fd2\u554f\u984c","text":""},{"location":"lectures/LA/47-wrapup/#_1","title":"\u57fa\u672c\u554f\u984c\uff08\u7406\u89e3\u5ea6\u78ba\u8a8d\uff09","text":"<ol> <li> <p>SVD\u3001PCA\u3001\u56e0\u5b50\u5206\u6790\u306e\u4e3b\u306a\u9055\u3044\u3092\u8aac\u660e\u3057\u3001\u305d\u308c\u305e\u308c\u304c\u3069\u306e\u3088\u3046\u306a\u5834\u5408\u306b\u9069\u3057\u3066\u3044\u308b\u304b\u3092\u8ff0\u3079\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u753b\u50cf\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308bSVD\u306e\u5fdc\u7528\u306b\u3064\u3044\u3066\u3001\u7279\u7570\u5024\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u3067\u3069\u306e\u3088\u3046\u306a\u52b9\u679c\u304c\u5f97\u3089\u308c\u308b\u304b\u8aac\u660e\u3057\u306a\u3055\u3044\u3002</p> </li> <li> <p>\u4ee5\u4e0b\u306e3\u00d73\u884c\u5217\u306eSVD\u3092\u8a08\u7b97\u3057\u3001\u7279\u7570\u5024\u3068\u7279\u7570\u30d9\u30af\u30c8\u30eb\u3092\u6c42\u3081\u306a\u3055\u3044\u3002\u307e\u305f\u3001\u30e9\u30f3\u30af1\u8fd1\u4f3c\u3092\u6c42\u3081\u306a\u3055\u3044\u3002    \\(\\(A = \\begin{pmatrix} 3 &amp; 1 &amp; 1 \\\\ 1 &amp; 3 &amp; 1 \\\\ 1 &amp; 1 &amp; 3 \\end{pmatrix}\\)\\)</p> </li> <li> <p>PCA\u3068\u975e\u7dda\u5f62\u6b21\u5143\u524a\u6e1b\u6cd5\uff08t-SNE\u307e\u305f\u306fUMAP\uff09\u306e\u9055\u3044\u3092\u8aac\u660e\u3057\u3001\u305d\u308c\u305e\u308c\u304c\u9069\u3057\u3066\u3044\u308b\u72b6\u6cc1\u306b\u3064\u3044\u3066\u8ff0\u3079\u306a\u3055\u3044\u3002</p> </li> </ol>"},{"location":"lectures/LA/48-final-report/","title":"\u6700\u7d42\u30ec\u30dd\u30fc\u30c8\u8ab2\u984c","text":"<p>\u8abf\u6574\u4e2d</p>"},{"location":"lectures/OC/","title":"\u751f\u6210AI\u306b\u3088\u308b\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8","text":""},{"location":"lectures/OC/#1","title":"1. \u30a4\u30f3\u30c8\u30ed\u30c0\u30af\u30b7\u30e7\u30f3","text":"<p>\u3053\u306e\u8b1b\u7fa9\u3067\u306f\u30012024\u5e74\u4ee5\u964d\u306e\u751f\u6210AI\u306e\u9032\u5316\u3092\u6982\u89b3\u3057\u3001\u305d\u308c\u306b\u57fa\u3065\u3044\u3066\u79c1\u305f\u3061\u306f\u3053\u308c\u304b\u3089\u306e\u6642\u4ee3\u306b\u4f55\u3092\u5b66\u3076\u3079\u304d\u3067\u3001\u4f55\u3092\u5b66\u3076\u3079\u304d\u3067\u306f\u306a\u3044\u306e\u304b\u3068\u3044\u3046\u70b9\u306b\u3064\u3044\u3066\u304a\u8a71\u3057\u3057\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u300cAI\u304c\u3053\u308c\u307b\u3069\u8ce2\u304f\u306a\u3063\u305f\u6642\u4ee3\u306b\u3001\u306a\u305c\u79c1\u305f\u3061\u306f\u5927\u5b66\u3067\u5b66\u3076\u5fc5\u8981\u304c\u3042\u308b\u306e\u304b\uff1f\u300d\u306b\u3064\u3044\u3066\u8003\u3048\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u751f\u6210AI\u306e\u767b\u5834\u4ee5\u524d\u306b\u4fa1\u5024\u304c\u9ad8\u304b\u3063\u305f\u80fd\u529b\u3068\u3001\u751f\u6210AI\u304c\u9ad8\u3044\u80fd\u529b\u3092\u7372\u5f97\u3057\u3064\u3064\u3042\u308b\u4eca\u306e\u6642\u4ee3\u3067\u4fa1\u5024\u304c\u5411\u4e0a\u3059\u308b\u80fd\u529b\u304c\u5909\u308f\u308a\u3064\u3064\u3042\u308b\u8ee2\u63db\u70b9\u3092\u8fce\u3048\u3066\u3044\u308b\u3068\u3044\u3046\u304a\u8a71\u3092\u3057\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u3053\u306e\u8b1b\u7fa9\u3092\u901a\u3058\u3066\u3001\u5927\u5b66\u306b\u304a\u3044\u3066\u5b66\u3076\u3053\u3068\u306e\u610f\u7fa9\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3057\u3066\u3044\u304f\u304b\u3068\u3044\u3046\u3053\u3068\u306b\u3064\u3044\u3066\u7406\u89e3\u3092\u6df1\u3081\u3066\u3082\u3089\u3048\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002</p> <p>2022\u5e7410\u6708\u306bChatGPT\u304c\u767a\u8868\u3055\u308c\u3066\u4ee5\u6765\u3001\u4eba\u3068\u4f1a\u8a71\u3067\u304d\u308bAI\u304c\u751f\u307e\u308c\u305f\u3053\u3068\u306b\u3068\u3066\u3082\u9a5a\u304d\u3092\u899a\u3048\u307e\u3057\u305f\u3002\u7279\u306b\u6700\u521d\u306e\u3046\u3061\u306f\u30cf\u30eb\u30b7\u30cd\u30fc\u30b7\u30e7\u30f3\u304c\u591a\u304f\u898b\u3089\u308c\u307e\u3057\u305f\u304c\u3001\u305f\u3060\u6700\u521d\u306e\u5927\u304d\u306a\u5909\u5316\u306f\u300c\u82f1\u8a9e\u300d\u3078\u306e\u7ffb\u8a33\u306e\u610f\u5473\u3067\u306f\u3001\u5f93\u6765\u306e\u30e2\u30c7\u30eb\u3068\u6bd4\u8f03\u3057\u3066\u5727\u5012\u7684\u306b\u512a\u79c0\u3067\u3042\u308a\u3001\u300c\u82f1\u8a9e\u300d\u5b66\u7fd2\u306e\u610f\u5473\u304c\u554f\u3044\u76f4\u3055\u308c\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3057\u305f\u3002</p> <p>\u305d\u306e\u3042\u3068\u3001ChatGPT\u304b\u3089\u3055\u307e\u3056\u307e\u306a\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3059\u308b\u3068\u3044\u3046Plugin\u306e\u5229\u7528\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001\u98df\u3079\u30ed\u30b0\u306a\u3069\u306e\u5916\u90e8\u30b5\u30fc\u30d3\u30b9\u3068\u306e\u9023\u643a\u304c\u56f3\u3089\u308c\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u305d\u3057\u3066\u30012023\u5e74\u306e\u4e2d\u65ec\u301c\u4e0b\u65ec\u306b\u306a\u308b\u3068\u3001\u65e5\u672c\u8a9e\u80fd\u529b\u3082\u5411\u4e0a\u3057\u3001\u4eba\u9593\u304c\u30c6\u30fc\u30de\u3092\u4e0e\u3048\u308b\u3068\u305d\u308c\u306b\u3064\u3044\u3066\u4ee3\u7b46\u3059\u308b\u7a0b\u5ea6\u306e\u3053\u3068\u306f\u53ef\u80fd\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u3053\u308c\u306b\u3068\u3082\u306a\u3063\u3066\u3001\u79c1\u305f\u3061\u6559\u54e1\u306f\u3001\u751f\u6210AI\u3092\u4f7f\u3063\u3066\u5b66\u751f\u304c\u66f8\u3044\u305f\u30ec\u30dd\u30fc\u30c8\u3092\u3069\u3046\u8a55\u4fa1\u3059\u308b\u306e\u304b\u3068\u3044\u3046\u554f\u3044\u304c\u4e0e\u3048\u3089\u308c\u307e\u3057\u305f\u3002\u3053\u3068\u6559\u80b2\u306b\u304a\u3044\u3066\u306f\u3001\u751f\u6210AI\u306e\u5229\u7528\u304c\u5b66\u529b\u306e\u4f4e\u4e0b\u30fb\u601d\u8003\u529b\u306e\u4f4e\u4e0b\u3092\u62db\u304f\u3068\u3044\u3046\u61f8\u5ff5\u304c\u5927\u3005\u7684\u306b\u5831\u3058\u3089\u308c\u307e\u3057\u305f\u3002\u4e00\u65b9\u3067\u3001\u751f\u6210AI\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u4e8b\u52d9\u7684\u4f5c\u696d\u52b9\u7387\u304c\u5411\u4e0a\u3057\u305f\u308a\u3001\u307e\u305f\u751f\u6210AI\u3092\u5bb6\u5ead\u6559\u5e2b\u306e\u3088\u3046\u306b\u4f7f\u3046\u3053\u3068\u3067\u4ee5\u524d\u3088\u308a\u3082\u5b66\u529b\u304c\u4f38\u3073\u3084\u3059\u304f\u306a\u3063\u305f\u3068\u3044\u3046\u4e8b\u4f8b\u3082\u5831\u544a\u3055\u308c\u307e\u3057\u305f\u3002</p> <p>\u305d\u3057\u3066\u3001\u7686\u3055\u3093\u304c\u90e8\u6d3b\u3084\u53d7\u9a13\u52c9\u5f37\u3092\u9811\u5f35\u3063\u3066\u3044\u305f2024\u5e74\u306f\u3001\u751f\u6210AI\u306e\u80fd\u529b\u304c\u98db\u8e8d\u3057\u305f\u5e74\u3067\u3059\u3002\u77e5\u80fd\u30ec\u30d9\u30eb\u304c\u5287\u7684\u306b\u5411\u4e0a\u3057\u3001\u533b\u5e2b\u56fd\u5bb6\u8a66\u9a13\u306b\u5408\u683c\u3057\u305f\u308a\u3001\u85ac\u5264\u5e2b\u8a66\u9a13\u306b\u5408\u683c\u3059\u308b\u3068\u3044\u3046\u56fd\u5bb6\u8a66\u9a13\u30ec\u30d9\u30eb\u306e\u554f\u984c\u306b\u3082\u5bfe\u51e6\u3067\u304d\u308b\u3060\u3051\u306e\u77e5\u8b58\u3092\u6301\u3064\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u5f53\u6642\u306eAI\u306f\u82f1\u8a9e\u3084\u56fd\u8a9e\u306e\u77e5\u8b58\u3001\u6587\u7ae0\u554f\u984c\u306b\u5bfe\u3059\u308b\u80fd\u529b\u306f\u9ad8\u304b\u3063\u305f\u306e\u3067\u3059\u304c\u3001\u4e00\u65b9\u3067STEM\u9818\u57df\u306e\u554f\u984c\u3092\u89e3\u304f\u80fd\u529b\u306f\u307b\u3068\u3093\u3069\u3042\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u307e\u305f\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u80fd\u529b\u3082\u5f90\u3005\u306b\u6539\u5584\u3057\u3066\u306f\u3044\u307e\u3057\u305f\u304c\u3001\u307e\u3060\u307e\u3060\u4f7f\u3044\u7269\u306b\u306a\u3089\u306a\u3044\u306d\u3068\u3044\u3046\u58f0\u304c\u307b\u3068\u3093\u3069\u3067\u3057\u305f\u3002\u79c1\u305f\u3061\u7814\u7a76\u8005\u306e\u4e2d\u306b\u3082\u3001\u307e\u3060\u3053\u306e\u9803\u306e\u751f\u6210AI\u306e\u80fd\u529b\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u3072\u304d\u3065\u3063\u3066\u3044\u3066\u30012025\u5e74\u306e\u4eca\u306e\u751f\u6210AI\u306e\u5b9f\u529b\u3092\u904e\u5c0f\u8a55\u4fa1\u3057\u3066\u3044\u308b\u4eba\u306f\u591a\u3044\u3067\u3059\u3002</p> <p>\u751f\u6210AI\u306e\u80fd\u529b\u5024\u304c\u7206\u767a\u7684\u306b\u3001\u975e\u7dda\u5f62\u306b\u5411\u4e0a\u3057\u305f\u306e\u306f2024\u5e74\u306e9\u6708\u306b\u767a\u8868\u3055\u308c\u305f Open AI o1-preview \u3068\u547c\u3070\u308c\u308b\u5f93\u6765\u306e\u751f\u6210AI\u306e\u30e2\u30c7\u30eb\u304c\u767a\u8868\u3055\u308c\u305f\u4ee5\u964d\u306e\u8a71\u3067\u3059\u3002\u3053\u306e\u3068\u304d\u3001\u4f55\u304c\u8d77\u3053\u3063\u305f\u306e\u304b\u3068\u3044\u3046\u3068\u3001\u751f\u6210AI\u304c\u601d\u8003\u529b\u3092\u6301\u3061\u59cb\u3081\u305f\u306e\u3067\u3059\u3002\u601d\u8003\u3059\u308b\u3068\u306f\u3001\u6587\u7ae0\u3092\u4f55\u5ea6\u3082\u7cbe\u67fb\u3057\u3066\u3088\u308a\u826f\u3044\u6587\u7ae0\u3092\u81ea\u5206\u3067\u4f5c\u308a\u76f4\u3059\u3088\u3046\u306b\u306a\u3063\u305f\u3068\u3044\u3046\u3053\u3068\u3067\u3059\uff08\u5f8c\u3067\u8aac\u660e\u3057\u307e\u3059\uff09\u3002\u305d\u3057\u3066\u3001\u3053\u306e\u8fba\u308a\u304b\u3089STEM\u9818\u57df\u3068\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u80fd\u529b\u304c\u3001\u660e\u3089\u304b\u306b2024\u5e749\u6708\u4ee5\u524d\u3068\u6bd4\u3079\u3066\u98db\u8e8d\u7684\u306a\u5dee\u304c\u751f\u307e\u308c\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002</p> <p></p> <p>\u3053\u306e\u9803\u304b\u3089\u3001\u533b\u5e2b\u56fd\u5bb6\u8a66\u9a13\u3001\u85ac\u5264\u5e2b\u8a66\u9a13\u306e\u6210\u7e3e\u306f\u3001\u5408\u683c\u304b\u4e0d\u5408\u683c\u304b\u3068\u3044\u3046\u30ec\u30d9\u30eb\u304b\u3089\u3001\u6b63\u7b54\u7387\u304c\u3069\u3053\u307e\u3067\u81f3\u308c\u308b\u304b\u3068\u3044\u3046\u5225\u306e\u6b21\u5143\u306e\u8a71\u3078\u3068\u5f15\u304d\u4e0a\u3052\u3089\u308c\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u5f90\u3005\u306bSTEM\u9818\u57df\u306e\u89e3\u51cd\u80fd\u529b\u304c\u4f38\u3073\u59cb\u3081\u307e\u3059\u3002\u305d\u306e\u3042\u3068\u30012024\u5e7412\u6708 OpenAI o1 \u304c\u767a\u8868\u3055\u308c\u3001\u540c\u6642\u306b\u6570\u5b66\u3084\u7269\u7406\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306b\u7279\u5316\u3057\u305f\u63a8\u8ad6\u304c\u3067\u304d\u308b o1-pro \u304c\u767a\u8868\u3055\u308c\u307e\u3057\u305f\u3002\u3053\u306e\u9803\u306b\u306a\u308b\u3068\u30012018\u5e74\u304f\u3089\u3044\u306e\u8ad6\u6587\u306b\u66f8\u304b\u308c\u3066\u3044\u308b\u3053\u3068\u306b\u3064\u3044\u3066\u3001\u307b\u3068\u3093\u3069\u6b63\u78ba\u306b\u7b54\u3048\u3089\u308c\u308b\u3088\u3046\u306b\u306a\u308a\u3064\u3064\u3042\u308a\u307e\u3057\u305f\u3002\u305d\u3057\u3066\u9ad8\u5ea6\u306a\u30d7\u30ed\u30b0\u30e9\u30e0\u3082\u66f8\u3051\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u3053\u306e\u6a5f\u80fd\u3092\u4f7f\u3044\u653e\u984c\u306b\u306a\u308b ChatGPT Pro \u304c\u6708200\u30c9\u30eb\u306e\u30d7\u30e9\u30f3\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\u3002\u307e\u305f\u3001\u540c\u6642\u671f\u306bClaude\u306bMCP\u304c\u767a\u8868\u3055\u308c\u3066\u304a\u308a\u3001\u30ed\u30fc\u30ab\u30eb\u306a\u77e5\u8b58\u3092\u6d3b\u7528\u3057\u305f\u56de\u7b54\u304c\u751f\u6210\u3067\u304d\u308b\u30b7\u30b9\u30c6\u30e0\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306fRAG\u3068\u547c\u3070\u308c\u308b\u6280\u8853\u3092Claude\u304c\u7d71\u5408\u3057\u305f\u3068\u8a00\u3048\u307e\u3059\u3002</p> <p></p> <p>2025\u5e74\u306f\u3001\u3055\u3089\u306b\u5927\u304d\u306a\u51fa\u6765\u4e8b\u3068\u3057\u3066\u3001DeepSeek \u304c\u767a\u8868\u3055\u308c\u3001\u5b9f\u306f\u30ed\u30fc\u30ab\u30eb\u30de\u30b7\u30fc\u30f3\u3067\u3082OpenAI o1\u7a0b\u5ea6\u306e\u529b\u306e\u3042\u308bLLM\u304c\u52d5\u304f\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u305d\u3057\u3066\u30012\u6708\u306b\u306f OpenAI\u304c o3 \u30e2\u30c7\u30eb\u3092\u30ea\u30ea\u30fc\u30b9\u3057\u307e\u3057\u305f\u3002\u305d\u3057\u3066\u3001OpenAI Deep Research \u304c\u767a\u8868\u3055\u308c\u307e\u3057\u305f\u3002\u3053\u306e\u6a5f\u80fd\u306f\u3001\u5c02\u9580\u5bb6\u304c1\u65e5\u30841\u9031\u9593\u304b\u3051\u3066\u884c\u3046\u8abf\u67fb\u3092\u30015\u5206\u301c30\u5206\u3067\u5b8c\u4e86\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3082\u306e\u3067\u3059\u3002\u305d\u3057\u3066\u3001\u5185\u5bb9\u306b\u3064\u3044\u3066\u6839\u62e0\u8cc7\u6599\u3092\u3082\u3063\u3066\u7b54\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3068\u3044\u3046\u70b9\u3067\u3001\u975e\u5e38\u306b\u6709\u7528\u3067\u3059\u3002\u305d\u3057\u3066\u30012\u670825\u65e5\u306bAnthropic \u304b\u3089 Claude 3.7 Sonnet\u304c\u767a\u8868\u3055\u308c\u307e\u3057\u305f\u3002\u60c5\u5831\u3092\u307e\u3068\u3081\u4e0a\u3052\u308b\u3053\u3068\u3001\u6570\u5b66\u306e\u8a08\u7b97\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3069\u308c\u3092\u3068\u3063\u3066\u3082\u3001\u50d5\u3089\u3088\u308a\u3082\u306f\u308b\u304b\u306b\u8ce2\u3044AI\u3067\u3059\u3002</p> <p>\u3067\u306f\u3001\u305d\u3093\u306a\u6642\u4ee3\u304c\u6765\u305f\u4eca\u3001\u79c1\u305f\u3061\u306f\u4f55\u3092\u512a\u5148\u7684\u306b\u5b66\u3073\u3001\u4f55\u3092\u6368\u3066\u308b\u3079\u304d\u3067\u3057\u3087\u3046\u304b\uff1f\u5927\u5b66\u306b\u304a\u3044\u3066\u5b66\u3076\u3053\u3068\u306e\u610f\u7fa9\u306f\u306a\u3093\u3067\u3057\u3087\u3046\u304b\u3002\u305d\u308c\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u3001\u307e\u305a\u306f\u4eca\u306e\u751f\u6210AI\u306e\u80fd\u529b\u3068\u3001\u4f55\u304c\u3067\u304d\u308b\u306e\u304b\u3092\u6b63\u3057\u304f\u8a8d\u8b58\u3057\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/OC/#2-2025ai","title":"2. 2025\u5e74\u306e\u751f\u6210AI\u306e\u80fd\u529b","text":"<p>\u751f\u6210AI\u306b\u3088\u3063\u3066\u4f55\u304c\u3067\u304d\u308b\u306e\u304b\u3068\u3044\u3046\u8a71\u3092\u3055\u307e\u3056\u307e\u306a\u89d2\u5ea6\u304b\u3089\u3053\u3053\u3067\u306f\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\u307e\u305a\u306f\u3001\u751f\u6210AI\u306e\u57fa\u76e4\u6280\u8853\u3067\u3042\u308b  Transformer \u3068\u3001\u751f\u6210AI\u306b\u601d\u8003\u529b\u3092\u52a0\u3048\u308b\u305f\u3081\u306e\u5f37\u5316\u5b66\u7fd2\u306b\u3064\u3044\u3066\u7d39\u4ecb\u3057\u307e\u3059\u3002\u305d\u306e\u3042\u3068\u3067\u3001\u73fe\u5728\u306e\u751f\u6210AI\u306e\u80fd\u529b\u3092\u4e8b\u4f8b\u3092\u7528\u3044\u3066\u7d39\u4ecb\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u901a\u3057\u3066\u3001\u6b63\u3057\u3044\u8a8d\u8b58\u3092\u7372\u5f97\u3059\u308b\u3053\u3068\u3092\u76ee\u6a19\u306b\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/OC/#21-transformer","title":"2.1 Transformer \u3068 \u63a8\u8ad6\u30e2\u30c7\u30eb","text":""},{"location":"lectures/OC/#211-transformer-and-self-attention-mechanism","title":"2.1.1 Transformer and Self-Attention Mechanism","text":"<p>Attention is all you need\u3068\u3044\u3046\u8ad6\u6587\u3067\u63d0\u6848\u3055\u308c\u305f\u624b\u6cd5\u3067\u3059\u3002\u56f3\u306b\u3059\u308b\u3068\u3001\u3053\u3093\u306a\u611f\u3058\u306b\u306e\u4ed5\u7d44\u307f\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u3060\u3051\u3092\u898b\u3066\u3082\u3001\u306a\u306b\u3053\u308c\uff1f\u3063\u3066\u611f\u3058\u3067\u3059\u3088\u306d\u3002\u3053\u306e\u4e2d\u306b\u4f7f\u308f\u308c\u3066\u3044\u308b\u3001Multi-head Attention\u3068\u547c\u3070\u308c\u308b\u6a5f\u69cb\u304c\u975e\u5e38\u306b\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3066\u3044\u307e\u3059\u3002</p> <p></p> <p>\u4e00\u822c\u7684\u306b\u79c1\u305f\u3061\u304c\u6587\u7ae0\u3092\u57f7\u7b46\u3059\u308b\u3053\u3068\u3092\u60f3\u50cf\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4f8b\u3048\u3070\u300c\u65e5\u672c\u306e\u4eba\u53e3\u30d4\u30e9\u30df\u30c3\u30c9\u306e\u5909\u5316\u3092\u8003\u3048\u305f\u4e0a\u3067\u3001\u533b\u7642\u8cbb\u306e\u8ca0\u62c5\u306e\u73fe\u72b6\u3068\u4eca\u5f8c\u306b\u3064\u3044\u3066\u300d\u3068\u3044\u3046\u30c6\u30fc\u30de\u306b\u3064\u3044\u3066\u6587\u7ae0\u3092\u57f7\u7b46\u3057\u306a\u3055\u3044\u3068\u3044\u3046\u554f\u984c\u3092\u8003\u3048\u307e\u3057\u3087\u3046\u3002\u305d\u306e\u5834\u5408\u3001\u57f7\u7b46\u3092\u3059\u308b\u306b\u306f\u4f55\u3092\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u304b\u3068\u3044\u3046\u3068\u3001\u300c\u30c6\u30fc\u30de\u3092\u8a18\u61b6\u300d\u3057\u3064\u3064\u3001\u300c\u6b21\u306b\u7d9a\u304f\u6587\u7ae0\u3092\u4e88\u6e2c\u300d\u3057\u3066\u57f7\u7b46\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u4ed5\u7d44\u307f\u3092\u5b9f\u88c5\u3057\u3066\u3044\u308b\u306e\u304c\u3001Transformer\u3068\u547c\u3070\u308c\u308b\u4ed5\u7d44\u307f\u3067\u3059\u3002\u305d\u3057\u3066\u3082\u30461\u3064\u91cd\u8981\u306a\u306e\u306f\u3001\u4eba\u9593\u304c\u81ea\u7136\u306b\u884c\u306a\u3063\u3066\u3044\u308b\u300c\u6587\u7ae0\u306e\u610f\u5473\u7406\u89e3\u300d\u3092\u3069\u306e\u3088\u3046\u306b\u3057\u3066\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u306b\u884c\u308f\u305b\u308b\u306e\u304b\u3068\u3044\u3046\u90e8\u5206\u3067\u3059\u3002\u305d\u3053\u3067\u91cd\u8981\u306a\u306e\u304c\u3001\u3053\u306e\u56f3\u306eMulti-head attention\u3068\u3044\u3046\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u6a5f\u68b0\u3068\u79c1\u305f\u3061\u306e\u9593\u306b\u3042\u308b\u672c\u8cea\u7684\u306a\u9055\u3044\u3068\u3044\u3046\u306e\u306f\u3001\u6587\u5b57\u3068\u30a4\u30e1\u30fc\u30b8\u304c\u7d50\u3073\u3064\u3044\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3068\u3044\u3046\u90e8\u5206\u3067\u3059\u3002\u3057\u307e\u3057\u307e\u306a\u6a21\u69d8\uff0b\u30a6\u30de\u3068\u3044\u3046\u8db3\u3057\u7b97\u306f\u3001\u79c1\u305f\u3061\u306e\u4e2d\u3067\u306f\u3057\u307e\u3046\u307e\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u304c\u3001\u6a5f\u68b0\u306b\u3068\u3063\u3066\u306f\u5168\u7136\u308f\u304b\u3089\u306a\u3044\u3082\u306e\u3067\u3059\u3002\u305d\u3053\u3067\u6a5f\u68b0\u306b\u3053\u306e\u80fd\u529b\u3092\u7372\u5f97\u3055\u305b\u308b\u305f\u3081\u306b\u306f\u3001\u5358\u8a9e\u3068\u5358\u8a9e\u306e\u8ddd\u96e2\u3084\u95a2\u4fc2\u6027\u3092\u5b66\u7fd2\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u6587\u7ae0\u3067\u3042\u308c\u3070\u4e26\u3073\u9806\u3092\u8003\u3048\u308b\u5fc5\u8981\u3082\u3042\u308a\u307e\u3059\u3002\u3055\u3089\u306b\u3001\u6b21\u306b\u7d9a\u304f\u6587\u7ae0\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u8cea\u554f\u6587\u306e\u3069\u3053\u306b\u5927\u4e8b\u306a\u5358\u8a9e\u304c\u3042\u308b\u306e\u304b\u3001\u305d\u308c\u306f\u4ed6\u306e\u5358\u8a9e\u3068\u3069\u3046\u7e4b\u304c\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u7406\u89e3\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u3092\u53ef\u80fd\u306b\u3057\u305f\u306e\u304c\u3001Multi-Head Attention\u306e\u4ed5\u7d44\u307f\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#212","title":"2.1.2 \u5f37\u5316\u5b66\u7fd2\u306e\u7d71\u5408\uff1a","text":"<p>\u8a73\u3057\u3044\u8aac\u660e\u306f\u3001\u3053\u3061\u3089\u306e\u89e3\u8aac\u8a18\u4e8b\u304c\u3068\u3066\u3082\u53c2\u8003\u306b\u306a\u308a\u307e\u3059\u3002\u5f37\u5316\u5b66\u7fd2\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u6349\u3048\u3066\u3044\u305f\u3060\u304f\u305f\u3081\u306b\u3001\u3053\u3061\u3089\u306e\u56f3\u3060\u3051\u5f15\u7528\u3057\u307e\u3059\u3002</p> <p>\u6b21\u306e\u5927\u304d\u306a\u554f\u984c\u306f\u3001\u751f\u6210\u3057\u305f\u6587\u7ae0\u306e\u826f\u3057\u60aa\u3057\u3092\u8a55\u4fa1\u3057\u3066\u3044\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u4e00\u5ea6\u751f\u6210\u3055\u308c\u305f\u6587\u7ae0\u3092\u3001\u751f\u6210AI\u304c\u898b\u76f4\u3059\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u305d\u308c\u3060\u3068\u3001\u3042\u304f\u307e\u3067\u3082\u78ba\u7387\u306b\u57fa\u3065\u3044\u305f\u751f\u6210\u3092\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u5618\u306e\u60c5\u5831\u3084\u8cea\u306e\u4f4e\u3044\u6587\u7ae0\u304c\u751f\u6210\u3055\u308c\u3066\u3057\u307e\u3044\u307e\u3059\u3002\u4eba\u9593\u3067\u3082\u540c\u3058\u3067\u3059\u3002\u4e00\u56de\u66f8\u3044\u305f\u3060\u3051\u306e\u6587\u66f8\u3092\u63d0\u51fa\u306f\u3057\u306a\u3044\u3067\u3057\u3087\u3046\u3002</p> <p>\u4eba\u9593\u304c\u57f7\u7b46\u3059\u308b\u3001\u307e\u305f\u306f\u554f\u984c\u3092\u89e3\u304f\u969b\u306b\u306f\u3001\u4f55\u5ea6\u3082\u898b\u76f4\u3057\u3092\u3057\u3066\u6587\u7ae0\u306e\u524d\u5f8c\u95a2\u4fc2\u3092\u7cbe\u67fb\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u3067\u6587\u7ae0\u306e\u8cea\u3092\u9ad8\u3081\u307e\u3059\u3088\u306d\u3002\u540c\u69d8\u306b\u751f\u6210AI\u3082\u3053\u308c\u3092\u884c\u3044\u307e\u3059\u3002\u6587\u7ae0\u306e\u8cea\u306e\u826f\u3057\u60aa\u3057\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306a\u4fa1\u5024\u5224\u65ad\u3092\u3059\u308bAI\u3092\u4f5c\u308b\u3053\u3068\u3067\u3001\u751f\u6210\u3055\u308c\u305f\u6587\u7ae0\u306e\u826f\u3057\u60aa\u3057\u3092\u5224\u5225\u3055\u305b\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u3053\u306e\u4fa1\u5024\u5224\u65ad\u3092\u3059\u308b\u3068\u3044\u3046\u306e\u306f\u3001\u30d7\u30ed\u30f3\u30d7\u30c8\uff08\u6307\u793a\u6587\uff09\uff0b\u751f\u6210AI\u304c\u51fa\u529b\u3057\u305f\u6587\u7ae0\u3092\u5165\u529b\u3059\u308b\u3068\u3001\u70b9\u6570\u3065\u3051\u3092\u3057\u3066\u304f\u308c\u308b\u3088\u3046\u306aAI\u3060\u3068\u601d\u3046\u3068\u308f\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002</p> <p></p> <p></p> <p>\u305d\u3057\u3066\u3001\u3053\u306e\u6587\u7ae0\u306e\u70b9\u6570\u3092\u3064\u3051\u308bAI\u304c\u3001\u6570\u5b66\u3084\u30d7\u30ed\u30b0\u30e9\u30e0\u306a\u3069\u306b\u5fdc\u3058\u3066\u3055\u307e\u3056\u307e\u306a\u3082\u306e\u3092\u7528\u610f\u3057\u3066\u304a\u304f\u3068\u3001\u6570\u5b66\u3084\u30d7\u30ed\u30b0\u30e9\u30e0\u751f\u6210\u306b\u304a\u3044\u3066\u3001\u8cea\u554f\u306b\u5bfe\u3057\u3066\u5341\u5206\u306b\u8cea\u306e\u9ad8\u3044\u56de\u7b54\u304c\u5f97\u3089\u308c\u308b\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u65b9\u6cd5\u3092\u7528\u3044\u3066\u3001\u5f37\u5316\u3055\u308c\u305f\u73fe\u5728\u306e\u751f\u6210AI\u306e\u80fd\u529b\u306e\u9ad8\u3055\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/OC/#22","title":"2.2 \u77e5\u8b58\u554f\u984c\u306b\u5bfe\u3059\u308b\u89e3\u7b54\u529b","text":"<p>\u3053\u3061\u3089\u306e\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u3067\u306f\u30012025\u5e74\u306b\u5b9f\u65bd\u3055\u308c\u305f\u533b\u5e2b\u56fd\u5bb6\u8a66\u9a13\u3092\u3001\u73fe\u5728\u306e\u751f\u6210AI\u306b\u89e3\u304b\u305b\u3066\u6b63\u7b54\u7387\u3092\u78ba\u8a8d\u3057\u305f\u7d50\u679c\u304c\u63b2\u8f09\u3055\u308c\u3066\u3044\u308b\u3002o3-mini-high\u306b\u3088\u308b\u5f97\u70b9\u306f\u3001\u5fc5\u4fee\u306e\u5f97\u70b9\u306f\u4e0a\u4f4d10%\u7a0b\u5ea6\u3067\u3001\u4e00\u822c\u81e8\u5e8a\u554f\u984c\u3067\u306f\u7b2c3\u4f4d\u306e\u6210\u7e3e\u306b\u8a72\u5f53\u3059\u308b\u3068\u767a\u8868\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p></p> <p>\u540c\u69d8\u306b\u3001\u85ac\u5264\u5e2b\u56fd\u5bb6\u8a66\u9a13\u306b\u3064\u3044\u3066\u30822024\u5e749\u6708\u6642\u70b9\u3067\u3053\u3061\u3089\u3067\u60c5\u5831\u304c\u767a\u8868\u3055\u308c\u307e\u3057\u305f\u304c\u3001o1-preview \u306b\u89e3\u304b\u305b\u305f\u3068\u3053\u308d\u6b63\u7b54\u7387\u306f100%\u3060\u3063\u305f\u3068\u306e\u3053\u3068\u3067\u3001\u77e5\u8b58\u554f\u984c\u306b\u5bfe\u3059\u308b\u6b63\u7b54\u7387\u306f\u5341\u5206\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u56fd\u5bb6\u8a66\u9a13\u3092\u53d7\u3051\u308b\u4eba\u306f\u3001\u3053\u308c\u3089\u306eAI\u304c\u3069\u306e\u3088\u3046\u306b\u601d\u8003\u3057\u3066\u89e3\u3044\u3066\u3044\u308b\u306e\u304b\u3092\u30c8\u30ec\u30fc\u30b9\u3059\u308b\u3053\u3068\u3067\u6b63\u7b54\u7387\u3092\u4e0a\u3052\u308b\u3053\u3068\u304c\u53ef\u80fd\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/OC/#23-ai","title":"2.3 \u6771\u5927\u6570\u5b66\u30fb\u4eac\u5927\u6570\u5b66\u306b\u5bfe\u3059\u308b\u751f\u6210AI\u306e\u80fd\u529b","text":"<p>2011\u5e74\u304b\u3089\u30ed\u30dc\u30c3\u30c8\u306f\u6771\u4eac\u5927\u5b66\u306b\u5165\u308c\u308b\u304b\uff1f\u3068\u3044\u3046\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304c\u3001\u65e5\u672c\u306e\u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240\u304c\u4e2d\u5fc3\u306b\u306a\u3063\u3066\u5b9f\u65bd\u3055\u308c\u3066\u3044\u307e\u3057\u305f\u30022016\u5e74\u306b\u5927\u5b66\u5165\u8a66\u30bb\u30f3\u30bf\u30fc\u8a66\u9a13\u30012021\u5e74\u306b\u6771\u5927\u5165\u8a66\u7a81\u7834\u3092\u76ee\u6307\u3057\u3066\u304d\u3066\u3044\u305f\u306e\u3067\u3059\u304c\u30012016\u5e74\u306b\u51cd\u7d50\u3055\u308c\u3066\u3057\u307e\u3063\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u4e2d\u3067\u3082\u6700\u3082\u96e3\u95a2\u3060\u3063\u305f\u306e\u306f\u6570\u5b66\u3067\u3042\u308a\u30011\u554f\u3082\u6b63\u89e3\u3067\u304d\u306a\u3044\u3068\u3044\u3046\u30ec\u30d9\u30eb\u3067\u3057\u305f\u3002</p> <p>\u4eca\u5e74\u30012025\u5e74\u306e\u5165\u8a66\u554f\u984c\u306e\u6570\u5b66\u3067\u306f\u3069\u3046\u3060\u3063\u305f\u306e\u304b\uff1f\u3068\u3044\u3048\u3070\u3001\u6771\u4eac\u5927\u5b66\u306e\u6570\u5b66\u306e\u554f\u984c\u306f\u5927\u554f\u304c6\u3064\u3042\u308a\u3001\u305d\u308c\u30925\u5b8c\u534a\u3057\u305f\u3068\u3044\u3046\u6295\u7a3f\u304c\u3042\u308a\u307e\u3057\u305f\u30ea\u30f3\u30af\u3002\u3055\u3089\u306b\u3001\u4eac\u90fd\u5927\u5b66\u306e\u554f\u984c\u306f\u5168\u3066\u306e\u554f\u984c\u3092\u6b63\u89e3\u3057\u305f\uff08\u30ea\u30f3\u30af\uff09\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u3053\u306e\u3053\u3068\u304b\u3089\u3001\u3044\u3088\u3044\u3088\u4e0e\u3048\u3089\u308c\u305f\u554f\u984c\u3092\u89e3\u304f\u3068\u3044\u3046\u529b\u306b\u3064\u3044\u3066\u306f\u3001\u540c\u4e16\u4ee3\u306e\u5b66\u751f\u306e\u4e0a\u4f4d1\uff05\u306b\u306f\u5230\u9054\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3088\u306d\uff08\u50d5\u306f\u305d\u3082\u305d\u30825\u554f\u3082\u89e3\u3051\u308b\u81ea\u4fe1\u304c\u306a\u3044...\uff09\u3002\u3053\u306e\u3053\u3068\u304c\u793a\u3057\u3066\u3044\u308b\u306e\u306f\u3001\u6570\u5b66\u7684\u306b\u6b63\u3057\u304f\u4e0e\u3048\u3089\u308c\u305f\u554f\u984c\u306b\u3064\u3044\u3066\u306f\u3001\u751f\u6210AI\u306f\u89e3\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#24","title":"2.4 \u30b3\u30fc\u30c9\u3092\u66f8\u304f\u80fd\u529b","text":"<p>\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306f\u3001\u30b9\u30dd\u30fc\u30c4\u9078\u624b\u306e\u52d5\u4f5c\u5206\u6790\u3084\u3001\u30ea\u30cf\u30d3\u30ea\u30c6\u30fc\u30b7\u30e7\u30f3\u3084\u4ecb\u8b77\u306e\u52d5\u4f5c\u89e3\u6790\u3092\u884c\u3046\u3068\u3044\u3046\u7814\u7a76\u30c6\u30fc\u30de\u3092\u6271\u3044\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u624b\u6cd5\u3092\u3001\u5b9f\u969b\u306b\u81ea\u5206\u3067\u30bc\u30ed\u304b\u3089\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u3059\u308b\u3068\u306a\u308b\u3068\u3001\u975e\u5e38\u306b\u56f0\u96e3\u3067\u3055\u307e\u3056\u307e\u306a\u77e5\u8b58\u304c\u8981\u6c42\u3055\u308c\u308b\u308f\u3051\u3067\u3059\u304c\u3001\u79c1\u305f\u3061\u306b\u306f\u5f37\u3044\u5473\u65b9\u3067\u3042\u308b\u751f\u6210AI\u304c\u3042\u308a\u307e\u3059\u3002\u307e\u305a\u306f\u3001\u73fe\u5728\u306e\u751f\u6210AI\u306e\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u80fd\u529b\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <p>\u751f\u6210AI\u306e\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u80fd\u529b\u3092\u6e2c\u308b\u305f\u3081\u306e\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3068\u3057\u3066\u3001SWE-bench \u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002SWE-bench\u306f\u5b9f\u969b\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u958b\u767a\u904e\u7a0b\u3067\u767a\u751f\u3059\u308b\u8ab2\u984c\u306b\u5bfe\u3057\u3066\u3001AI\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u3067\u3069\u3053\u307e\u3067\u5bfe\u5fdc\u3067\u304d\u308b\u304b\u306e\u6e2c\u5b9a\u3092\u76ee\u7684\u3068\u3057\u3066\u304a\u308a\u3001\u4ee5\u4e0b\u306e\u7279\u5fb4\u304c\u3042\u308a\u307e\u3059\u3002</p> <ul> <li>\u5b9f\u969b\u306eGitHub Issue\u3068Pull Request(\u73fe\u5728\u306f\u4e3b\u306bPython\u306b\u95a2\u9023\u3057\u305fPyPl\u30d1\u30c3\u30b1\u30fc\u30b8\u95a2\u9023\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u304c\u30bf\u30fc\u30b2\u30c3\u30c8)\u304b\u3089\u62bd\u51fa\u3055\u308c\u305f\u73fe\u5b9f\u7684\u306a\u30bf\u30b9\u30af\u89e3\u6c7a\u3092\u8a66\u307f\u308b</li> <li>\u5358\u306a\u308b\u30b3\u30fc\u30c9\u751f\u6210\u3060\u3051\u3067\u306a\u304f\u3001\u30d0\u30b0\u4fee\u6b63\u3084\u6a5f\u80fd\u5b9f\u88c5\u306a\u3069\u591a\u69d8\u306a\u30bf\u30b9\u30af\u3092\u542b\u3080</li> <li>\u81ea\u52d5\u8a55\u4fa1\u30b7\u30b9\u30c6\u30e0\u306b\u3088\u308b\u5ba2\u89b3\u7684\u306a\u6027\u80fd\u6e2c\u5b9a</li> </ul> <p>\u3064\u307e\u308a\u73fe\u6bb5\u968e\u306e\u6700\u65b0\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb(AI)\u304c\u3001\u3000\u73fe\u4ee3\u306e\u30ea\u30a2\u30eb\u306a\u30a2\u30d7\u30ea\u958b\u767a\u3084\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u306e\u554f\u984c\u3092\u3069\u3053\u307e\u3067\u4eba\u9593\u3068\u540c\u3058\u3088\u3046\u306b\u3053\u306a\u305b\u308b\u304b\u3092\u5b9a\u91cf\u7684\u306b\u6e2c\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u305f\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u30b9\u30b3\u30a2\u3068\u3044\u3046\u3053\u3068\u3067\u3059\uff08\u30ea\u30f3\u30af\uff09\u3002\u3053\u308c\u306b\u304a\u3044\u3066 Claude 3.7 Sonnet \u306e\u80fd\u529b\u306f\u4ee5\u4e0b\u306e\u7d50\u679c\u304b\u3089\u3082\u308f\u304b\u308b\u3088\u3046\u306b\u975e\u5e38\u306b\u9ad8\u304f\u3001\u3053\u308c\u3092\u8e0f\u307e\u3048\u308b\u3068\u3001\u73fe\u5728\u306e\u4eba\u9593\u306e\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u4e0a\u4f4d1\uff05\u3088\u308a\u3082\u4e0a\u306b\u4f4d\u7f6e\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u305d\u3046\u3067\u3059\u3002</p> <p></p> <p>\u3053\u3053\u304b\u3089\u4f55\u304c\u8a00\u3048\u308b\u306e\u304b\u3068\u3044\u3046\u3068\u3001\u76ee\u7684\u3068\u306a\u308b\u30b3\u30fc\u30c9\u306e\u65b9\u5411\u6027\u304c\u308f\u304b\u3063\u3066\u3044\u308b\u306a\u3089\u3001\u81ea\u5206\u304c\u66f8\u304b\u306a\u304f\u3066\u3082\u3001\u9069\u5207\u306b\u5236\u5fa1\u3055\u3048\u3059\u308c\u3070AI\u304c\u66f8\u3044\u3066\u304f\u308c\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p> <p>\u305d\u3053\u3067\u3001\u59ff\u52e2\u63a8\u5b9a\u3092\u884c\u3046\u30bf\u30b9\u30af\u3092 claude 3.7 Sonnet \u3092\u7528\u3044\u3066\u81ea\u4f5c\u3057\u3066\u307f\u305f\u7d50\u679c\u3092\u793a\u305d\u3046\u3068\u601d\u3044\u307e\u3059\uff08Google Colab\uff09\u3002\u59ff\u52e2\u63a8\u5b9a\u306b\u306f\u3001mediapipe\u3084\u3001YOLO\u306a\u3069\u3055\u307e\u3056\u307e\u306a\u30e2\u30c7\u30eb\u304c\u3042\u308a\u307e\u3059\u3002\u4eca\u56de\u306fmediapipe\u3092\u4f7f\u3063\u3066\u3001\u9759\u6b62\u59ff\u52e2\u3092\u64ae\u5f71\u3057\u3001\u305d\u306e\u7d50\u679c\u304b\u3089\u59ff\u52e2\u306e\u72b6\u6cc1\u3092\u7b54\u3048\u3066\u304f\u308c\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u958b\u767a\u3057\u307e\u3059\u3002</p> <p>\u5b9f\u306f\u3053\u306e\u65b9\u6cd5\u306f\u3001\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u306e\u30c7\u30fc\u30bf\u306b\u3082\u5fdc\u7528\u3067\u304d\u308b\u305f\u3081\u3001\u52d5\u304d\u3092\u4f34\u3046\u52d5\u4f5c\u306e\u89e3\u6790\u3082\u53ef\u80fd\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u5de6\u8155\u3068\u53f3\u8155\u306e\u53ef\u52d5\u57df\u306b\u5dee\u304c\u3042\u308b\u304b\u3069\u3046\u304b\u3092\u691c\u51fa\u3057\u305f\u308a\u3067\u304d\u307e\u3059\u3002\u3053\u3061\u3089\u306f\u3001claude 3.7 Sonnet \u3067\u4f5c\u6210\u3057\u305f\u30a4\u30f3\u30d5\u30a9\u30b0\u30e9\u30d5\u30a3\u30c3\u30af\u3067\u3059\u3002</p> <ul> <li>\u59ff\u52e2\u63a8\u5b9a\u6280\u8853\uff1a</li> <li>\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\uff08PoseNet\u3001AlphaPose\u3001OpenPose\uff09</li> <li>2D\u30683D\u306e\u30dd\u30fc\u30ba\u63a8\u5b9a\u65b9\u6cd5\u8ad6\u3068\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5</li> <li>\u8907\u96d1\u306a\u74b0\u5883\u306b\u304a\u3051\u308b\u8907\u6570\u4eba\u8ffd\u8de1\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0</li> <li> <p>\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u6027\u80fd\u6700\u9069\u5316\u6280\u8853</p> </li> <li> <p>\u30b9\u30dd\u30fc\u30c4\u3078\u306e\u5fdc\u7528\uff1a</p> </li> <li>\u30a2\u30b9\u30ea\u30fc\u30c8\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306b\u304a\u3051\u308b\u52d5\u4f5c\u30d1\u30bf\u30fc\u30f3\u5206\u6790</li> <li>\u751f\u4f53\u529b\u5b66\u7684\u504f\u5dee\u304b\u3089\u306e\u50b7\u5bb3\u30ea\u30b9\u30af\u8a55\u4fa1</li> <li>\u5373\u6642\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u306b\u3088\u308b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u6280\u8853\u306e\u6700\u9069\u5316</li> <li> <p>\u30b1\u30fc\u30b9\u30b9\u30bf\u30c7\u30a3\uff1a\u30d7\u30ed\u91ce\u7403\u306e\u6295\u7403\u30e1\u30ab\u30cb\u30af\u30b9\u3001\u6c34\u6cf3\u306e\u30b9\u30c8\u30ed\u30fc\u30af\u5206\u6790</p> </li> <li> <p>\u5065\u5eb7\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u30b7\u30b9\u30c6\u30e0\uff1a</p> </li> <li>\u795e\u7d4c\u5b66\u7684\u75c7\u72b6\u306e\u65e9\u671f\u767a\u898b\u306e\u305f\u3081\u306e\u6b69\u884c\u5206\u6790</li> <li>\u30ea\u30cf\u30d3\u30ea\u30c6\u30fc\u30b7\u30e7\u30f3\u9032\u884c\u306e\u5b9a\u91cf\u5316</li> <li>\u9ad8\u9f62\u8005\u30b1\u30a2\u306e\u305f\u3081\u306e\u30ea\u30e2\u30fc\u30c8\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3</li> <li> <p>\u96fb\u5b50\u5065\u5eb7\u8a18\u9332\u3068\u306e\u7d71\u5408</p> </li> <li> <p>\u30c7\u30e2\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\uff1a</p> </li> <li>\u53c2\u52a0\u8005\u306e\u52d5\u4f5c\u30d1\u30bf\u30fc\u30f3\u306e\u30e9\u30a4\u30d6\u30ad\u30e3\u30d7\u30c1\u30e3\u3068\u5206\u6790</li> <li>\u751f\u4f53\u529b\u5b66\u7684\u6307\u6a19\u306e\u8996\u899a\u5316</li> <li>\u53c2\u7167\u52d5\u4f5c\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u3068\u306e\u6bd4\u8f03</li> <li>\u30a8\u30e9\u30fc\u691c\u51fa\u3068\u4fee\u6b63\u63a8\u5968</li> </ul>"},{"location":"lectures/OC/#25","title":"2.5 \u30ea\u30b5\u30fc\u30c1\u80fd\u529b","text":"<p>\u4ed5\u4e8b\u3084\u7814\u7a76\u306b\u304a\u3044\u3066\u5927\u534a\u306e\u6642\u9593\u3092\u4f7f\u3046\u3068\u8a00\u3063\u3066\u3082\u904e\u8a00\u3067\u306f\u306a\u3044\u306e\u306f\u3001\u5148\u884c\u4e8b\u4f8b\u8abf\u67fb\u3068\u547c\u3070\u308c\u308b\u4f5c\u696d\u3067\u3059\u3002\u3053\u308c\u306f\u3001\u4f55\u304b\u3092\u884c\u3046\u3068\u304d\u306b\u81ea\u5206\u305f\u3061\u306e\u65b9\u6cd5\u304c\u3001\u3069\u3046\u3044\u3046\u7acb\u3061\u4f4d\u7f6e\u306a\u306e\u304b\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u4f5c\u696d\u306a\u306e\u3067\u3059\u304c\u3001\u3053\u308c\u306b\u306f\u975e\u5e38\u306b\u6642\u9593\u3092\u53d6\u3089\u308c\u307e\u3059\u3002\u307e\u305f\u3001\u5927\u5b66\u30fb\u4f1a\u793e\u3069\u3093\u306a\u7d44\u7e54\u3067\u3082\u3001\u4f55\u304b\u65b0\u3057\u3044\u53d6\u308a\u7d44\u307f\u3092\u3059\u308b\u3068\u3044\u3046\u3068\u304d\u306b\u306f\u3001\u4ed6\u306e\u4f1a\u793e\u306f\u3069\u3046\u3044\u3046\u3075\u3046\u306b\u3057\u3066\u3044\u308b\u304b\u3092\u8abf\u67fb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u8abf\u67fb\u3092\u5b9f\u884c\u3059\u308b\u969b\u306b\u306f\u3001\u308f\u305f\u3057\u305f\u3061\u306f google \u306e\u7a93\u3084 youtube \u306e\u7a93\u3092\u53e9\u304d\u307e\u304f\u3063\u3066\u8abf\u67fb\u3057\u3066\u304d\u307e\u3057\u305f\u3002</p> <p>\u3053\u308c\u306f\u904e\u53bb\u306b\u3082\u751f\u6210AI\u3067\u53d6\u308a\u7d44\u307e\u308c\u3066\u304d\u305f\u3053\u3068\u3067\u306f\u3042\u3063\u305f\u306e\u3067\u3059\u304c\u3001\u305d\u3082\u305d\u3082\u60c5\u5831\u304c\u306a\u3044\u3082\u306e\u3092\u3042\u308b\u304b\u306e\u3088\u3046\u306b\u3046\u305d\u3076\u3044\u305f\u308a\u3059\u308b\u305f\u3081\u306b\u3001\u6587\u732e\u8abf\u67fb\u3092\u3059\u308b\u306e\u306b\u306f\u9069\u3057\u3066\u3044\u306a\u3044\u3068\u3044\u3046\u306e\u304c\u4e00\u822c\u8a8d\u8b58\u3060\u3063\u305f\u306e\u3067\u3059\u3002\u3057\u304b\u3057\u3001o3-mini-high \u306b\u642d\u8f09\u3055\u308c\u305f\u300cDeep Research\u300d\u306f\u3001\u3053\u306e\u554f\u984c\u3092\u89e3\u6d88\u3057\u3066\u304a\u308a\u3001\u8abf\u67fb\u3057\u305f\u7d50\u679c\u306e\u6839\u62e0\u3092URL\u3067\u63d0\u793a\u3057\u305f\u308a\u3001\u66f8\u7c4d\u3092\u63d0\u793a\u3055\u305b\u305f\u308a\u3067\u304d\u307e\u3059\u3002\u304a\u3088\u305d\u4eba\u9593\u304c2\u6642\u9593\u301c3\u6642\u9593\u304b\u304b\u308b\u4f5c\u696d\u309210\u5206\u7a0b\u5ea6\u3067\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u5927\u5e45\u306a\u6642\u9593\u77ed\u7e2e\u3092\u53ef\u80fd\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u6a5f\u80fd\u306f 2025\u5e74\u306e2\u6708\u306b\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u3066\u3001\u7121\u6599\u30e6\u30fc\u30b6\u30fc\u306f\u73fe\u5728\u4f7f\u7528\u3067\u304d\u307e\u305b\u3093\u304c\u3001\u4eca\u5f8c\u306f\u89e3\u653e\u3055\u308c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p> <p>\u5b9f\u969b\u306e\u30ec\u30dd\u30fc\u30c8\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u4eca\u56de\u306f\u3001\u9806\u5929\u5802\u5927\u5b66\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5b66\u90e8\u306b\u3064\u3044\u3066\u3001Deep Research \u3067\u8a55\u5224\u3092\u8abf\u67fb\u3057\u3066\u307f\u307e\u3059\u3002\uff08\u30ea\u30f3\u30af\uff09\u3002</p> <p>\u6b21\u306b\u3001\u3053\u306e\u60c5\u5831\u3092\u52a0\u5de5\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u3053\u306e\u7d50\u679c\u3092 Claude 3.7 Sonnet \u306b\u4e0e\u3048\u308b\u3053\u3068\u3067\u3001\u7dba\u9e97\u306b\u52a0\u5de5\u3057\u3066\u307f\u307e\u3059\uff08\u4f7f\u3046\u30d7\u30ed\u30f3\u30d7\u30c8\u306f\u3053\u3061\u3089\uff09\u3002\u5b8c\u6210\u3057\u305f\u3082\u306e\u304c\u3053\u3061\u3089\u306b\u306a\u308a\u307e\u3059\u3002\u60c5\u5831\u304b\u3089\u30a4\u30f3\u30d5\u30a9\u30b0\u30e9\u30d5\u30a3\u30c3\u30af\u306e\u4f5c\u6210\u307e\u3067\u3001\u4e00\u6c17\u901a\u8cab\u306b\u4f5c\u6210\u3067\u304d\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#26-rag-model-context-protocol","title":"2.6 RAG \u304b\u3089 Model Context Protocol\u3078","text":"<p>\u6b21\u306b\u30012024\u5e74\u306b\u3067\u304d\u3066\u6ce8\u76ee\u3057\u305f\u3044\u306e\u306f\u3001\u691c\u7d22\u62e1\u5f35\u751f\u6210\uff08RAG\uff09\u3068\u547c\u3070\u308c\u308b\u6280\u8853\u3068\u3001Model context protocol (Anthropic) \u3068\u547c\u3070\u308c\u308b\u6280\u8853\u3067\u3059\u3002\u7279\u306b\u3001\u3053\u3053\u3067\u306fMCP\u306b\u6ce8\u76ee\u3057\u307e\u3059\u3002\u4e00\u822c\u7684\u306b\u5927\u5b66\u3084\u4f01\u696d\u3067\u306f\u30c7\u30fc\u30bf\u3092\u30aa\u30fc\u30d7\u30f3\u306b\u3057\u305f\u304f\u306f\u306a\u3044\u3068\u3044\u3046\u554f\u984c\u304c\u3042\u308a\u307e\u3059\u3002\u305d\u3053\u3067\u3001\u30ed\u30fc\u30ab\u30eb\u30c7\u30fc\u30bf\u30bd\u30fc\u30b9\u3092\u7528\u610f\u3057\u3066\u3001\u305d\u308c\u3092\u53c2\u7167\u3057\u3066Claude \u306b\u56de\u7b54\u3055\u305b\u308b\u3068\u3044\u3046\u30b7\u30b9\u30c6\u30e0\u304cMCP\u3067\u3059\uff08\u3053\u3061\u3089 \u306e\u8a18\u4e8b\u304c\u308f\u304b\u308a\u3084\u3059\u3044\u306e\u3067\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\uff09\u3002</p> <p></p> <p>\u30a4\u30f3\u30d5\u30a9\u30b0\u30e9\u30d5\u30a3\u30c3\u30af\u3082\u7528\u610f\u3057\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/OC/#3","title":"3. \u6839\u672c\u7684\u306a\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8","text":"<p>\u3053\u3053\u307e\u3067\u898b\u3066\u304d\u305f\u3088\u3046\u306b\u3001\u751f\u6210AI\u306f\u79c1\u305f\u3061\u4eba\u9593\u304c\u884c\u3063\u3066\u304d\u305f\u591a\u304f\u306e\u77e5\u7684\u4f5c\u696d\u3092\u4ee3\u66ff\u3001\u3042\u308b\u3044\u306f\u62e1\u5f35\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3092\u898b\u308b\u3068\u300e\u79c1\u305f\u3061\u306f\u5927\u5b66\u3067\u4f55\u3092\u5b66\u3079\u3070\u3044\u3044\u306e\u304b\uff1f\u300f\u3068\u3044\u3046\u7591\u554f\u304c\u751f\u3058\u308b\u306e\u306f\u5f53\u7136\u3067\u3059\u3002\u3057\u304b\u3057\u3001\u3053\u306e\u6280\u8853\u9769\u65b0\u306f\u5358\u306b\u4eba\u9593\u306e\u4ed5\u4e8b\u3092\u596a\u3046\u306e\u3067\u306f\u306a\u304f\u3001\u79c1\u305f\u3061\u306e\u5f79\u5272\u3092\u6839\u672c\u7684\u306b\u5909\u3048\u308b\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8\u3092\u5f15\u304d\u8d77\u3053\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u5909\u5316\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u304c\u3001\u5927\u5b66\u6559\u80b2\u306e\u65b0\u305f\u306a\u610f\u7fa9\u3092\u898b\u51fa\u3059\u9375\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u307e\u305a\u79c1\u305f\u3061\u306e\u4eba\u9593\u793e\u4f1a\u306e\u5927\u539f\u5247\u306b\u3064\u3044\u3066\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u6700\u3082\u91cd\u8981\u304b\u3064\u6839\u672c\u7684\u306a\u539f\u5247\u306fAI\u304c\u3069\u308c\u3060\u3051\u9032\u6b69\u3057\u3088\u3046\u3068\u3082\u3001AI\u51fa\u529b\u306b\u5bfe\u3059\u308b\u4eba\u9593\u306e\u8cac\u4efb\u306f\u306a\u304f\u306a\u3089\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u3053\u306e\u70b9\u304c\u5909\u5316\u3057\u3066\u3057\u307e\u3063\u305f\u306e\u306a\u3089\u3001\u793e\u4f1a\u306e\u30eb\u30fc\u30eb\u306f\u65b0\u3057\u304f\u8003\u3048\u76f4\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u4eca\u306e\u73fe\u72b6\u306f\u3001\u3053\u306e\u539f\u5247\u304c\u793e\u4f1a\u306b\u9069\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u4eba\u3068\u793e\u4f1a\u3067\u306f\u3001\u3044\u3064\u3067\u3082\u7d50\u679c\u306e\u6b63\u3057\u3055\u306e\u691c\u8a3c\u3068\u4fdd\u8a3c\u3092\u5fc5\u8981\u3068\u3057\u3066\u3044\u307e\u3059\u3002\u3060\u304b\u3089\u3053\u305d\u3001\u4eba\u9593\u304c\u6700\u5f8c\u306e\u7d50\u679c\u304c\u6b63\u3057\u3044\u3053\u3068\u306b\u8cac\u4efb\u3092\u6301\u3061\u88dc\u511f\u3092\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u306e\u3067\u3059\u3002</p> <p>\u305d\u3057\u3066\u3001\u3082\u30461\u3064\u91cd\u8981\u306a\u539f\u5247\u304c\u3042\u308a\u307e\u3059\u3002\u554f\u984c\u3068\u3044\u3046\u306e\u306f\u3001\u3069\u3093\u306a\u3082\u306e\u3067\u3082\u4eba\u304c\u4f5c\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u6570\u5b66\u3067\u89e3\u304d\u305f\u3044\u554f\u984c\u3082\u3001\u4eba\u9593\u95a2\u4fc2\u304b\u3089\u304f\u308b\u60a9\u307f\u4e8b\u3082\u3001\u30b9\u30dd\u30fc\u30c4\u304c\u4e0a\u9054\u3057\u306a\u3044\u3053\u3068\u3082\u3001\u75c5\u6c17\u304c\u826f\u304f\u306a\u3089\u306a\u3044\u3053\u3068\u3082\u3001\u4f55\u304b\u306b\u5bfe\u3057\u3066\u9811\u5f35\u308c\u306a\u3044\u81ea\u5206\u304c\u5acc\u306b\u306a\u308b\u3082\u306e\u3001\u3059\u3079\u3066\u4eba\u9593\u304c\u751f\u307f\u51fa\u3057\u3066\u3044\u308b\u554f\u984c\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u4f55\u304c\u554f\u984c\u306a\u306e\u304b\uff1f\u3001\u3069\u306e\u554f\u984c\u3092\u89e3\u304f\u306e\u304b\u306fAI\u304c\u6c7a\u3081\u308b\u3082\u306e\u3067\u306f\u306a\u304f\u3001\u4eba\u9593\u304c\u6c7a\u3081\u308b\u3053\u3068\u3067\u3059\u3002</p> <p>\u751f\u6210AI\u306f\u306a\u3093\u3067\u3082\u3067\u304d\u307e\u3059\u3002\u305f\u304f\u3055\u3093\u306e\u554f\u984c\u3084\u56f0\u308a\u4e8b\u3092\u89e3\u6d88\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u539f\u5247\u3068\u3057\u3066\u3001\u4eba\u9593\u304c\u89e3\u304f\u554f\u984c\u3092\u6c7a\u3081\u3066\u3044\u3066\u3001\u4eba\u9593\u304c\u4f55\u3092\u3059\u308b\u304b\u3092\u8cac\u4efb\u3092\u6301\u3063\u3066\u6c7a\u3059\u308b\u3068\u3044\u3046\u524d\u63d0\u304c\u3042\u308b\u306e\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u89e3\u6c7a\u80fd\u529b\u304c\u9ad8\u3044AI\u304c\u3042\u308b\u73fe\u72b6\u306b\u304a\u3044\u3066\u306f\u3001\u60f3\u5b9a\u7684\u306b\u6b21\u306e2\u3064\u306e\u91cd\u8981\u6027\u304c\u76f8\u5bfe\u7684\u306b\u5927\u304d\u304f\u306a\u308a\u307e\u3059\u3002</p> <ul> <li>\u554f\u984c\u5b9a\u7fa9\uff1a\u554f\u984c\u306e\u7bc4\u56f2\u3068\u5236\u7d04\u306e\u78ba\u7acb</li> <li>\u51fa\u529b\u691c\u8a3c\uff1a\u89e3\u6c7a\u7b56\u3092\u610f\u601d\u6c7a\u5b9a\u3057\u3001\u305d\u308c\u306b\u5bfe\u3059\u308b\u8cac\u4efb\u3092\u8ca0\u3046</li> </ul> <p>\u305d\u3057\u3066\u3001\u3082\u30461\u3064\u6c17\u306b\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u91cd\u8981\u306a\u3053\u3068\u306f\u3001\u4e16\u754c\u306b\u3042\u308b\u554f\u984c\u306f\u3059\u3079\u3066AI\u304c\u89e3\u3051\u308b\u5f62\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304b\uff1f \u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u7b54\u3048\u306f\u3001\u306a\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u3059\u3002\u4eba\u9593\u304c\u611f\u3058\u305f\u554f\u984c\u3092\u89e3\u6d88\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u6b21\u306e2\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002</p> <ul> <li> <ol> <li>\u305d\u306e\u307e\u307e\u4eba\u9593\u304c\u89e3\u304f</li> </ol> </li> <li> <ol> <li>AI\u304c\u89e3\u3051\u308b\u5f62\u306b\u4eba\u9593\u304c\u6574\u5f62\u3059\u308b\u3002</li> </ol> </li> </ul> <p>AI\u306e\u529b\u3092\u5f15\u304d\u51fa\u3059\u305f\u3081\u306b\u306f\u3001AI\u304c\u89e3\u3051\u308b\u304b\u305f\u3061\u306b\u3001AI\u304c\u89e3\u304f\u3053\u3068\u3092\u30b5\u30dd\u30fc\u30c8\u3067\u304d\u308b\u304b\u305f\u3061\u306b\u554f\u984c\u3092\u7ffb\u8a33\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u554f\u984c\u3092\u89e3\u304f\u80fd\u529b\u304c\u4f4e\u3044\u5834\u5408\u306b\u306f\u3001AI\u306e\u305f\u3081\u306b\u554f\u984c\u3092\u89e3\u3051\u308b\u304b\u305f\u3061\u306b\u7ffb\u8a33\u3059\u308b\u5fc5\u8981\u306f\u306a\u304b\u3063\u305f\u3067\u3059\u304c\u3001\u554f\u984c\u3092\u89e3\u304f\u80fd\u529b\u304c\u9ad8\u304f\u306a\u3063\u3066\u304d\u305f\u5834\u5408\u306b\u306f\u3001\u3053\u306e\u80fd\u529b\u304c\u975e\u5e38\u306b\u5927\u304d\u306a\u610f\u5473\u3092\u6301\u3061\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u79c1\u305f\u3061\u306b\u6c42\u3081\u3089\u308c\u308b\u3082\u30461\u3064\u306e\u529b\u306f\u3001\u554f\u984c\u3092AI\u304c\u89e3\u3051\u308b\u304b\u305f\u3061\u306b\u7ffb\u8a33\u3059\u308b\u529b\u3067\u3059\u3002</p> <ul> <li>\u554f\u984c\u7ffb\u8a33 : \u554f\u984c\u306e\u672c\u8cea\u3092\u898b\u629c\u304d\u3001AI\u304c\u89e3\u3051\u308b\u5f62\u306b\u6301\u3063\u3066\u3044\u304f\u3053\u3068</li> </ul> <p>\u3053\u306e\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8\u306b\u3088\u3063\u3066\u3001\u554f\u984c\u89e3\u6c7a\u306e\u6d41\u308c\u306f\u300e\u554f\u984c\u5b9a\u7fa9\u2192\u554f\u984c\u7ffb\u8a33\u2192AI\u3067\u89e3\u6c7a\u2192\u51fa\u529b\u691c\u8a3c\u300f\u3068\u3044\u3046\u65b0\u3057\u3044\u30d7\u30ed\u30bb\u30b9\u306b\u5909\u308f\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u5358\u306a\u308b\u624b\u9806\u306e\u5909\u66f4\u3067\u306f\u306a\u304f\u3001\u79c1\u305f\u3061\u4eba\u9593\u306b\u6c42\u3081\u3089\u308c\u308b\u80fd\u529b\u306e\u672c\u8cea\u7684\u306a\u5909\u5316\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u3067\u306f\u3001\u3053\u306e\u65b0\u3057\u3044\u30d7\u30ed\u30bb\u30b9\u306e\u4e2d\u3067\u3001\u3069\u306e\u3088\u3046\u306a\u80fd\u529b\u304c\u3053\u308c\u304b\u3089\u306e\u6642\u4ee3\u306b\u6c42\u3081\u3089\u308c\u308b\u306e\u3067\u3057\u3087\u3046\u304b\uff1f</p>"},{"location":"lectures/OC/#4-ai","title":"4. \u89e3\u304f\u529b\u304b\u3089\u554f\u3046\u529b\u3078\uff1aAI\u3068\u5171\u5275\u3059\u308b\u554f\u984c\u89e3\u6c7a\u306e\u65b0\u6642\u4ee3","text":""},{"location":"lectures/OC/#41","title":"4.1 \u5f93\u6765\u306e\u554f\u984c\u89e3\u6c7a\u30d7\u30ed\u30bb\u30b9\u306e\u9650\u754c","text":"<p>\u3053\u308c\u307e\u3067\u306e\u793e\u4f1a\u3067\u306f\u3001\u4e0e\u3048\u3089\u308c\u305f\u554f\u984c\u3092\u300c\u3044\u304b\u306b\u65e9\u304f\u300d\u300c\u3044\u304b\u306b\u6b63\u78ba\u306b\u300d\u89e3\u304f\u304b\u3068\u3044\u3046\u80fd\u529b\u304c\u91cd\u8996\u3055\u308c\u3066\u304d\u307e\u3057\u305f\u3002\u30c6\u30b9\u30c8\u3084\u5165\u5b66\u8a66\u9a13\u3001\u8cc7\u683c\u8a66\u9a13\u306a\u3069\u306f\u307e\u3055\u306b\u305d\u306e\u5178\u578b\u3067\u3059\u3002\u591a\u304f\u306e\u4f01\u696d\u3067\u3082\u3001\u7279\u5b9a\u306e\u30bf\u30b9\u30af\u3092\u3053\u306a\u3059\u901f\u3055\u3068\u6b63\u78ba\u3055\u304c\u8a55\u4fa1\u306e\u5bfe\u8c61\u3067\u3057\u305f\u3002</p> <p>\u3057\u304b\u3057\u3001\u524d\u7ae0\u3067\u898b\u3066\u304d\u305f\u3088\u3046\u306b\u3001\u73fe\u72b6\u306e\u751f\u6210AI\u306f\u533b\u5e2b\u56fd\u5bb6\u8a66\u9a13\u3084\u6771\u5927\u6570\u5b66\u306a\u3069\u306e\u96e3\u554f\u3092\u89e3\u304f\u80fd\u529b\u306b\u304a\u3044\u3066\u3001\u3059\u3067\u306b\u4eba\u9593\u306e\u4e0a\u4f4d1%\u7a0b\u5ea6\u306e\u982d\u8133\u306b\u5339\u6575\u3057\u307e\u3059\u3002\u3053\u308c\u304b\u3089\u767a\u5c55\u3057\u3066\u3044\u3051\u3070\u3001\u5927\u62b5\u306e\u4eba\u985e\u3088\u308a\u3082\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u306b\u512a\u308c\u308b\u77e5\u80fd\u304c\u3001\u8ab0\u3082\u304c\u5229\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002</p> <p>\u3053\u306e\u3053\u3068\u306f\u672c\u8cea\u7684\u306a\u5909\u5316\u3092\u3082\u305f\u3089\u3057\u307e\u3059\u3002\u3053\u308c\u307e\u3067\u306f\u300c\u7b54\u3048\u3092\u51fa\u3059\u4eba\u300d\u304c\u4fa1\u5024\u3092\u6301\u3063\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u3053\u308c\u304b\u3089\u306f\u300c\u554f\u3044\u3092\u7acb\u3066\u308b\u4eba\u300d\u300c\u554f\u984c\u3092\u5b9a\u7fa9\u3059\u308b\u4eba\u300d\u300cAI\u306e\u51fa\u529b\u3092\u8a55\u4fa1\u3067\u304d\u308b\u4eba\u300d\u306b\u4fa1\u5024\u304c\u30b7\u30d5\u30c8\u3059\u308b\u306e\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#_1","title":"\u3010\u5177\u4f53\u4f8b\uff1a\u30c7\u30fc\u30bf\u5206\u6790\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3011","text":"<p>\u5f93\u6765\u306e\u65b9\u6cd5\uff1a \u300c\u3053\u306e\u8cfc\u8cb7\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3057\u3066\u3001\u4f55\u304b\u9762\u767d\u3044\u50be\u5411\u3092\u898b\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u300d \u2192 \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u304c\u4f55\u65e5\u3082\u304b\u3051\u3066\u30c7\u30fc\u30bf\u3092\u63a2\u7d22\u3001\u53ef\u8996\u5316\u3057\u3001\u3044\u304f\u3064\u304b\u306e\u6d1e\u5bdf\u3092\u5f97\u308b</p> <p>AI\u306e\u6642\u4ee3\uff1a \u300c\u3053\u306e\u5c0f\u58f2\u5e97\u306e\u8cfc\u8cb7\u30c7\u30fc\u30bf\u304b\u3089\u3001\u5929\u5019\u3068\u8cfc\u8cb7\u30d1\u30bf\u30fc\u30f3\u306e\u95a2\u9023\u6027\u3001\u7279\u306b\u96e8\u5929\u6642\u306e\u5546\u54c1\u30ab\u30c6\u30b4\u30ea\u5225\u306e\u58f2\u4e0a\u5909\u52d5\u3092\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u300d \u2192 AI\u304c\u6570\u5206\u3067\u57fa\u672c\u5206\u6790\u3092\u5b9f\u884c\u3001\u4eba\u9593\u306f\u305d\u306e\u7d50\u679c\u3092\u7cbe\u67fb\u3057\u3001\u300c\u306a\u305c\u96e8\u306e\u65e5\u306b\u7279\u5b9a\u5546\u54c1\u306e\u58f2\u4e0a\u304c\u4e0a\u304c\u308b\u306e\u304b\u300d\u300c\u898b\u843d\u3068\u3057\u3066\u3044\u308b\u8981\u56e0\u306f\u306a\u3044\u304b\u300d\u3092\u8003\u5bdf</p>"},{"location":"lectures/OC/#42-ai","title":"4.2 AI\u3068\u306e\u5354\u50cd\u306b\u3088\u308b\u65b0\u3057\u3044\u554f\u984c\u89e3\u6c7a\u30b5\u30a4\u30af\u30eb","text":"<p>\u751f\u6210AI\u306e\u767b\u5834\u306b\u3088\u308a\u3001\u554f\u984c\u89e3\u6c7a\u306e\u30d7\u30ed\u30bb\u30b9\u306f\u6839\u672c\u7684\u306b\u5909\u308f\u308a\u307e\u3059\u3002\u5f93\u6765\u306f\u4e00\u3064\u306e\u554f\u984c\u3092\u89e3\u3044\u3066\u7d42\u308f\u308a\u3067\u3057\u305f\u304c\u3001\u751f\u6210AI\u306f\u307b\u307c\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u306b\u89e3\u7b54\u3092\u751f\u6210\u3067\u304d\u308b\u305f\u3081\u3001\u8907\u6570\u56de\u306e\u8a66\u884c\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/OC/#ai_1","title":"AI\u3068\u306e\u554f\u984c\u89e3\u6c7a\u30b5\u30a4\u30af\u30eb\uff1a","text":"<ol> <li>\u554f\u984c\u306e\u8a8d\u8b58\uff08\u4eba\u9593\uff09\uff1a\u73fe\u5b9f\u4e16\u754c\u306e\u72b6\u6cc1\u304b\u3089\u300c\u3053\u3053\u306b\u8ab2\u984c\u304c\u3042\u308b\u300d\u3068\u6c17\u3065\u304f</li> <li>\u554f\u984c\u306e\u5b9a\u7fa9\u3068\u5f62\u5f0f\u5316\uff08\u4eba\u9593\uff09\uff1aAI\u304c\u89e3\u3051\u308b\u5f62\u306b\u554f\u984c\u3092\u6574\u7406\u30fb\u7ffb\u8a33\u3059\u308b</li> <li>\u521d\u671f\u89e3\u6790\u30fb\u89e3\u7b54\u751f\u6210\uff08AI\uff09\uff1a\u554f\u984c\u306b\u5bfe\u3059\u308b\u89e3\u7b54\u3084\u5206\u6790\u3092\u751f\u6210</li> <li>\u7d50\u679c\u306e\u8a55\u4fa1\u3068\u554f\u984c\u306e\u518d\u5b9a\u7fa9\uff08\u4eba\u9593\uff09\uff1aAI\u306e\u51fa\u529b\u3092\u8a55\u4fa1\u3057\u3001\u554f\u984c\u3092\u7cbe\u7dfb\u5316\u3059\u308b</li> <li>\u8a73\u7d30\u89e3\u6790\u30fb\u89e3\u7b54\u306e\u6539\u5584\uff08AI\uff09\uff1a\u7cbe\u7dfb\u5316\u3055\u308c\u305f\u554f\u984c\u306b\u5bfe\u3057\u3066\u518d\u5ea6\u89e3\u7b54</li> <li>\u6700\u7d42\u8a55\u4fa1\u3068\u610f\u601d\u6c7a\u5b9a\uff08\u4eba\u9593\uff09\uff1a\u8907\u6570\u306e\u89e3\u7b54\u304b\u3089\u6700\u9069\u306a\u9078\u629e\u3092\u3059\u308b</li> <li>\u5b9f\u88c5\u3068\u691c\u8a3c\uff08\u4eba\u9593\u3068AI\uff09\uff1a\u5b9f\u969b\u306b\u9069\u7528\u3057\u3066\u7d50\u679c\u3092\u78ba\u8a8d\u3059\u308b</li> </ol> <p>\u2192 \u65b0\u305f\u306a\u554f\u984c\u306e\u8a8d\u8b58\u3078</p> <p>\u3053\u306e\u30b5\u30a4\u30af\u30eb\u3092\u9ad8\u901f\u306b\u56de\u8ee2\u3055\u305b\u308b\u3053\u3068\u304c\u3001\u3053\u308c\u304b\u3089\u306e\u554f\u984c\u89e3\u6c7a\u306e\u9375\u3068\u306a\u308a\u307e\u3059\u3002\u304b\u3064\u3066\u306f\u540c\u50da\u3084\u4e0a\u53f8\u3068\u306e\u5bfe\u8a71\u3067\u884c\u3063\u3066\u3044\u305f\u3053\u306e\u30d7\u30ed\u30bb\u30b9\u304c\u3001\u751f\u6210AI\u3068\u306e\u5bfe\u8a71\u306b\u7f6e\u304d\u63db\u308f\u308b\u306e\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#_2","title":"\u3010\u5177\u4f53\u4f8b\uff1a\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u5206\u6790\u3011","text":"<p>\u5927\u5b66\u306e\u30b9\u30dd\u30fc\u30c4\u79d1\u5b66\u7814\u7a76\u3067\u30d5\u30a3\u30c3\u30c8\u30cd\u30b9\u30c8\u30e9\u30c3\u30ab\u30fc\u306e\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3059\u308b\u5834\u5408\uff1a</p> <p>\u5f93\u6765\uff08\u30b5\u30a4\u30af\u30eb\u304c\u9045\u3044\uff09\uff1a 1. \u30c7\u30fc\u30bf\u3092\u96c6\u3081\u308b\uff08\u6570\u9031\u9593\uff09 2. \u5206\u6790\u65b9\u6cd5\u3092\u6c7a\u3081\u308b\uff08\u6570\u65e5\uff09 3. \u5206\u6790\u3092\u5b9f\u884c\uff08\u6570\u65e5\uff09 4. \u7d50\u679c\u3092\u89e3\u91c8\uff08\u6570\u65e5\uff09 5. \u5831\u544a\u66f8\u4f5c\u6210\uff08\u6570\u65e5\uff09 \u2192 \u5168\u4f53\u30671\u301c2\u30f6\u6708</p> <p>AI\u3068\u306e\u5354\u50cd\uff08\u30b5\u30a4\u30af\u30eb\u304c\u901f\u3044\uff09\uff1a 1. \u30c7\u30fc\u30bf\u3092\u96c6\u3081\u308b\uff08\u540c\u3058\u304f\u6570\u9031\u9593\uff09 2. \u521d\u671f\u5206\u6790\u65b9\u91dd\u3092AI\u3068\u8b70\u8ad6\uff08\u6570\u6642\u9593\uff09 3. AI\u304c\u8907\u6570\u306e\u5206\u6790\u6848\u3092\u63d0\u793a\uff08\u6570\u5206\uff09 4. \u4eba\u9593\u304c\u8a55\u4fa1\u3057\u65b9\u5411\u6027\u3092\u4fee\u6b63\uff08\u6570\u6642\u9593\uff09 5. AI\u304c\u8a73\u7d30\u5206\u6790\u3092\u5b9f\u884c\uff08\u6570\u5206\uff09 6. \u4eba\u9593\u304c\u7d50\u679c\u3092\u89e3\u91c8\u30fb\u8a55\u4fa1\uff08\u6570\u6642\u9593\uff09 7. AI\u304c\u5831\u544a\u66f8\u306e\u8349\u6848\u3092\u4f5c\u6210\uff08\u6570\u5206\uff09 8. \u4eba\u9593\u304c\u6700\u7d42\u8abf\u6574\uff08\u6570\u6642\u9593\uff09 \u2192 \u30c7\u30fc\u30bf\u53ce\u96c6\u5f8c\u306f1\u301c2\u65e5\u3067\u5b8c\u4e86</p> <p>\u3053\u306e\u3088\u3046\u306b\u3001AI\u3068\u306e\u5354\u50cd\u306b\u3088\u308a\u554f\u984c\u89e3\u6c7a\u306e\u30b5\u30a4\u30af\u30eb\u304c\u5287\u7684\u306b\u9ad8\u901f\u5316\u3055\u308c\u3001\u3088\u308a\u591a\u304f\u306e\u8a66\u884c\u932f\u8aa4\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/OC/#43","title":"4.3 \u554f\u984c\u5b9a\u7fa9\u80fd\u529b\u3068\u7cbe\u67fb\u80fd\u529b\u306e\u91cd\u8981\u6027","text":"<p>\u3053\u306e\u3088\u3046\u306a\u6642\u4ee3\u306b\u304a\u3044\u3066\u3001\u7279\u306b\u91cd\u8981\u306b\u306a\u308b\u306e\u304c\u300c\u554f\u984c\u5b9a\u7fa9\u80fd\u529b\u300d\u3068\u300cAI\u306e\u51fa\u529b\u3092\u7cbe\u67fb\u3059\u308b\u80fd\u529b\u300d\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#3_1","title":"\u554f\u984c\u5b9a\u7fa9\u80fd\u529b\u306e3\u3064\u306e\u30ec\u30d9\u30eb\uff1a","text":"<ol> <li>\u57fa\u672c\u30ec\u30d9\u30eb\uff1a\u300c\u5065\u5eb7\u30c7\u30fc\u30bf\u304c\u3042\u308b\u304b\u3089\u4f55\u304b\u5206\u6790\u3057\u305f\u3044\u300d</li> <li> <p>\u3042\u3044\u307e\u3044\u3067\u65b9\u5411\u6027\u304c\u306a\u3044\u3002AI\u306f\u5e83\u7bc4\u306a\u63d0\u6848\u3092\u3059\u308b\u304c\u7126\u70b9\u304c\u306a\u3044\u3002</p> </li> <li> <p>\u5177\u4f53\u5316\u30ec\u30d9\u30eb\uff1a\u300c20\u4ee3\u306e\u5927\u5b66\u751f\u306e\u7761\u7720\u30d1\u30bf\u30fc\u30f3\u3068\u5b66\u696d\u6210\u7e3e\u306e\u95a2\u4fc2\u3092\u5206\u6790\u3057\u305f\u3044\u300d</p> </li> <li> <p>\u5bfe\u8c61\u3068\u76ee\u7684\u304c\u660e\u78ba\u3002AI\u306f\u3088\u308a\u5177\u4f53\u7684\u306a\u5206\u6790\u304c\u3067\u304d\u308b\u3002</p> </li> <li> <p>\u89e3\u6790\u53ef\u80fd\u30ec\u30d9\u30eb\uff1a\u300c20\u4ee3\u5927\u5b66\u751f100\u540d\u306e1\u30f6\u6708\u9593\u306e\u7761\u7720\u30c7\u30fc\u30bf\uff08\u7761\u7720\u6642\u9593\u3001\u8cea\u3001\u4e2d\u65ad\u56de\u6570\uff09\u3068\u671f\u672b\u8a66\u9a13\u6210\u7e3e\u306e\u76f8\u95a2\u5206\u6790\u3092\u884c\u3044\u3001\u7279\u306b\u7761\u7720\u306e\u8cea\u304c\u8a66\u9a13\u524d\u65e5\u30681\u9031\u9593\u524d\u3067\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3059\u308b\u304b\u3092\u91cd\u56de\u5e30\u5206\u6790\u3067\u691c\u8a3c\u3057\u305f\u3044\u300d</p> </li> <li>AI\u304c\u6700\u9069\u306a\u624b\u6cd5\u3067\u5206\u6790\u3067\u304d\u308b\u5f62\u306b\u7ffb\u8a33\u3055\u308c\u3066\u3044\u308b\u3002</li> </ol> <p>\u554f\u984c\u3092\u660e\u78ba\u306b\u5b9a\u7fa9\u3059\u308b\u80fd\u529b\u306f\u3001AI\u304b\u3089\u306e\u51fa\u529b\u306e\u8cea\u3092\u6839\u672c\u7684\u306b\u5de6\u53f3\u3057\u307e\u3059\u3002\u66d6\u6627\u306a\u554f\u984c\u8a2d\u5b9a\u304b\u3089\u306f\u66d6\u6627\u306a\u56de\u7b54\u3057\u304b\u5f97\u3089\u308c\u307e\u305b\u3093\u3002</p>"},{"location":"lectures/OC/#ai_2","title":"AI\u51fa\u529b\u306e\u7cbe\u67fb\u80fd\u529b\uff1a","text":"<p>AI\u304c\u751f\u307f\u51fa\u3057\u305f\u89e3\u7b54\u3084\u5206\u6790\u7d50\u679c\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u30dd\u30a4\u30f3\u30c8\uff1a</p> <ol> <li>\u8ad6\u7406\u7684\u4e00\u8cab\u6027\uff1a\u7d50\u8ad6\u306b\u81f3\u308b\u63a8\u8ad6\u904e\u7a0b\u306b\u98db\u8e8d\u3084\u77db\u76fe\u306f\u306a\u3044\u304b</li> <li>\u30c7\u30fc\u30bf\u9069\u5408\u6027\uff1a\u4f7f\u7528\u3055\u308c\u305f\u30c7\u30fc\u30bf\u3084\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u306f\u554f\u984c\u306b\u9069\u5207\u304b</li> <li>\u4eee\u5b9a\u306e\u59a5\u5f53\u6027\uff1a\u5206\u6790\u306e\u524d\u63d0\u6761\u4ef6\u306f\u73fe\u5b9f\u7684\u304b</li> <li>\u7d50\u679c\u306e\u89e3\u91c8\uff1a\u7d71\u8a08\u7684\u6709\u610f\u6027\u3068\u5b9f\u52d9\u7684\u91cd\u8981\u6027\u3092\u533a\u5225\u3067\u304d\u3066\u3044\u308b\u304b</li> <li>\u4ee3\u66ff\u8aac\u660e\uff1a\u4ed6\u306e\u89e3\u91c8\u306e\u53ef\u80fd\u6027\u3092\u691c\u8a0e\u3057\u3066\u3044\u308b\u304b</li> </ol>"},{"location":"lectures/OC/#_3","title":"\u3010\u5177\u4f53\u4f8b\uff1a\u7761\u7720\u3068\u5b66\u7fd2\u52b9\u7387\u306e\u5206\u6790\u3011","text":"<p>AI\u304c\u300c\u7761\u7720\u6642\u9593\u3068\u5b66\u7fd2\u52b9\u7387\u306b\u306f\u5f37\u3044\u6b63\u306e\u76f8\u95a2\u304c\u3042\u308b\u300d\u3068\u7d50\u8ad6\u4ed8\u3051\u305f\u5834\u5408\u306e\u7cbe\u67fb\uff1a</p> <p>\u4e0d\u5341\u5206\u306a\u7cbe\u67fb\uff1a \u300cAI\u304c\u8a00\u3063\u3066\u3044\u308b\u304b\u3089\u6b63\u3057\u3044\u3060\u308d\u3046\u300d\u2192 \u7d50\u8ad6\u3092\u305d\u306e\u307e\u307e\u53d7\u3051\u5165\u308c\u308b</p> <p>\u9069\u5207\u306a\u7cbe\u67fb\uff1a - \u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u306f\u5341\u5206\u304b\uff1f\uff0830\u4eba\u3067\u306f\u5c11\u306a\u3044\uff09 - \u4ea4\u7d61\u56e0\u5b50\u306f\u8003\u616e\u3055\u308c\u3066\u3044\u308b\u304b\uff1f\uff08\u4f8b\uff1a\u30ab\u30d5\u30a7\u30a4\u30f3\u6442\u53d6\u3001\u30b9\u30c8\u30ec\u30b9\u30ec\u30d9\u30eb\uff09 - \u56e0\u679c\u95a2\u4fc2\u3068\u76f8\u95a2\u95a2\u4fc2\u306e\u533a\u5225\u306f\u3064\u3044\u3066\u3044\u308b\u304b\uff1f - \u500b\u4eba\u5dee\u306f\u8003\u616e\u3055\u308c\u3066\u3044\u308b\u304b\uff1f\uff08\u77ed\u6642\u9593\u7761\u7720\u3067\u3082\u52b9\u7387\u7684\u306b\u5b66\u7fd2\u3067\u304d\u308b\u4eba\u3082\u3044\u308b\uff09 - \u30c7\u30fc\u30bf\u306e\u53ce\u96c6\u65b9\u6cd5\u306f\u4fe1\u983c\u3067\u304d\u308b\u304b\uff1f\uff08\u81ea\u5df1\u7533\u544avs\u5ba2\u89b3\u7684\u6e2c\u5b9a\uff09</p> <p>\u3053\u306e\u7cbe\u67fb\u80fd\u529b\u304c\u3042\u308b\u304b\u306a\u3044\u304b\u3067\u3001AI\u3068\u306e\u5354\u50cd\u306e\u8cea\u304c\u5927\u304d\u304f\u5909\u308f\u308a\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u3053\u306e\u80fd\u529b\u3092\u990a\u3046\u305f\u3081\u306b\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u790e\u77e5\u8b58\u304c\u4e0d\u53ef\u6b20\u306a\u306e\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#44","title":"4.4 \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5b66\u90e8\u3067\u78e8\u304f\u3079\u304d\u80fd\u529b","text":"<p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5b66\u90e8\u306e\u5b66\u751f\u3068\u3057\u3066\u7279\u306b\u6ce8\u76ee\u3059\u3079\u304d\u306f\u3001AI\u306e\u6642\u4ee3\u306b\u304a\u3051\u308b\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u306e\u5f79\u5272\u306e\u5909\u5316\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#_4","title":"\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u306e\u5f79\u5272\u306e\u5909\u5316\uff1a","text":"<ul> <li>\u5f93\u6765\uff1a\u300c\u30c7\u30fc\u30bf\u306e\u53ce\u96c6\u30fb\u6574\u7406\u30fb\u5206\u6790\u30fb\u53ef\u8996\u5316\u300d\u304c\u4e3b\u306a\u4ed5\u4e8b</li> <li>\u3053\u308c\u304b\u3089\uff1a\u300c\u554f\u984c\u5b9a\u7fa9\u30fb\u4eee\u8aac\u69cb\u7bc9\u30fbAI\u51fa\u529b\u306e\u691c\u8a3c\u30fb\u610f\u601d\u6c7a\u5b9a\u652f\u63f4\u300d\u304c\u30b3\u30a2\u306e\u4fa1\u5024\u306b</li> </ul> <p>\u3053\u308c\u304b\u3089\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u306b\u6c42\u3081\u3089\u308c\u308b\u80fd\u529b\u306f\uff1a</p> <ol> <li>\u9818\u57df\u77e5\u8b58\u3068\u6570\u7406\u7684\u601d\u8003\u306e\u878d\u5408\uff1a</li> <li>\u533b\u7642\u3001\u30b9\u30dd\u30fc\u30c4\u3001\u30d3\u30b8\u30cd\u30b9\u306a\u3069\u7279\u5b9a\u5206\u91ce\u306e\u77e5\u8b58\u3068\u3001\u6570\u5b66\u30fb\u7d71\u8a08\u306e\u77e5\u8b58\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u529b</li> <li> <p>\u4f8b\uff1a\u300c\u3053\u306e\u904b\u52d5\u30d1\u30bf\u30fc\u30f3\u30c7\u30fc\u30bf\u304b\u3089\u602a\u6211\u306e\u30ea\u30b9\u30af\u3092\u4e88\u6e2c\u3059\u308b\u306b\u306f\u3001\u3069\u306e\u5909\u6570\u304c\u91cd\u8981\u304b\u300d\u3092\u5224\u65ad\u3067\u304d\u308b</p> </li> <li> <p>\u7d71\u8a08\u7684\u306a\u6279\u5224\u7684\u601d\u8003\uff1a</p> </li> <li>\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3001\u30d0\u30a4\u30a2\u30b9\u3001\u56e0\u679c\u95a2\u4fc2\u3068\u76f8\u95a2\u95a2\u4fc2\u306e\u533a\u5225\u3092\u7406\u89e3\u3059\u308b\u529b</li> <li> <p>\u4f8b\uff1a\u300c\u3053\u306eAI\u30e2\u30c7\u30eb\u304c\u793a\u3059\u95a2\u4fc2\u6027\u306f\u3001\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30d0\u30a4\u30a2\u30b9\u306b\u3088\u308b\u898b\u305b\u304b\u3051\u306e\u76f8\u95a2\u3067\u306f\u306a\u3044\u304b\u300d\u3092\u5224\u65ad\u3067\u304d\u308b</p> </li> <li> <p>\u554f\u984c\u306e\u6570\u7406\u7684\u8868\u73fe\u529b\uff1a    \u524d\u7ae0\u3067\u793a\u3057\u305f\u4f8b\u306e\u3088\u3046\u306b\u3001\u73fe\u5b9f\u306e\u554f\u984c\u3092\u6570\u5b66\u7684\u30fb\u7d71\u8a08\u7684\u306b\u8868\u73fe\u3059\u308b\u529b</p> </li> <li>\u300c\u533b\u7642\u8cbb\u306e\u8ca0\u62c5\u5897\u52a0\u300d\u2192\u300c\u5e74\u9f62\u5c64\u5225\u533b\u7642\u8cbb\u652f\u51fa\u306e\u6642\u7cfb\u5217\u5206\u6790\u3068\u5c06\u6765\u63a8\u8a08\u300d</li> <li> <p>\u300c\u5065\u5eb7\u5897\u9032\u52b9\u679c\u300d\u2192\u300c\u4ecb\u5165\u524d\u5f8c\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u5909\u5316\u306e\u7d71\u8a08\u7684\u691c\u5b9a\u300d</p> </li> <li> <p>\u5b9f\u9a13\u8a2d\u8a08\u80fd\u529b\uff1a</p> </li> <li>AI\u306e\u4e88\u6e2c\u3084\u5206\u6790\u7d50\u679c\u3092\u691c\u8a3c\u3059\u308b\u305f\u3081\u306e\u5b9f\u9a13\u3092\u8a2d\u8a08\u3059\u308b\u529b</li> <li>\u4f8b\uff1a\u300c\u3053\u306eAI\u304c\u4e88\u6e2c\u3059\u308b\u904b\u52d5\u52b9\u679c\u3092\u691c\u8a3c\u3059\u308b\u306b\u306f\u3001\u3069\u306e\u3088\u3046\u306a\u5bfe\u7167\u5b9f\u9a13\u304c\u5fc5\u8981\u304b\u300d</li> </ol> <p>\u3053\u308c\u3089\u306e\u80fd\u529b\u306f\u3001AI\u304c\u554f\u984c\u3092\u89e3\u304f\u80fd\u529b\u304c\u9ad8\u307e\u308b\u307b\u3069\u3001\u76f8\u5bfe\u7684\u306b\u4fa1\u5024\u304c\u5897\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u306a\u305c\u306a\u3089\u3001AI\u306f\u4e0e\u3048\u3089\u308c\u305f\u554f\u984c\u306f\u89e3\u3051\u3066\u3082\u3001\u300c\u4f55\u3092\u554f\u984c\u3068\u3059\u3079\u304d\u304b\u300d\u300c\u3069\u3046\u691c\u8a3c\u3059\u3079\u304d\u304b\u300d\u3092\u81ea\u3089\u5224\u65ad\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u306a\u3044\u304b\u3089\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#45","title":"4.5 \u65b0\u6642\u4ee3\u306e\u554f\u984c\u89e3\u6c7a\u8005\u306b\u306a\u308b\u305f\u3081\u306b","text":"<p>\u751f\u6210AI\u306e\u6642\u4ee3\u306b\u5fc5\u8981\u306a\u306e\u306f\u3001AI\u3068\u5354\u50cd\u3057\u3066\u554f\u984c\u3092\u89e3\u6c7a\u3059\u308b\u300c\u554f\u984c\u89e3\u6c7a\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c8\u300d\u3068\u3057\u3066\u306e\u80fd\u529b\u3067\u3059\u3002\u305d\u306e\u305f\u3081\u306b\u5927\u5b66\u3067\u5b66\u3076\u3079\u304d\u3053\u3068\u306f\uff1a</p> <ol> <li>\u57fa\u790e\u77e5\u8b58\u306e\u7fd2\u5f97\uff1a</li> <li>\u6570\u5b66\u30fb\u7d71\u8a08\u5b66\u306e\u57fa\u790e\u6982\u5ff5</li> <li>\u30c7\u30fc\u30bf\u5206\u6790\u306e\u65b9\u6cd5\u8ad6</li> <li>\u5c02\u9580\u9818\u57df\uff08\u5065\u5eb7\u79d1\u5b66\u306a\u3069\uff09\u306e\u77e5\u8b58</li> </ol> <p>\u3053\u308c\u3089\u306f\u554f\u984c\u3092\u5b9a\u7fa9\u3057\u3001AI\u306e\u51fa\u529b\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u300c\u7269\u5dee\u3057\u300d\u3068\u306a\u308a\u307e\u3059\u3002</p> <ol> <li>\u6279\u5224\u7684\u601d\u8003\u306e\u8a13\u7df4\uff1a</li> <li>\u8ad6\u7406\u7684\u306b\u8b70\u8ad6\u3092\u5206\u6790\u3059\u308b</li> <li>\u4eee\u8aac\u3092\u7acb\u3066\u691c\u8a3c\u3059\u308b</li> <li>\u591a\u89d2\u7684\u306b\u554f\u984c\u3092\u898b\u308b</li> </ol> <p>\u3053\u308c\u3089\u306fAI\u306e\u51fa\u529b\u3092\u9d5c\u5451\u307f\u306b\u305b\u305a\u3001\u9069\u5207\u306b\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u3067\u3059\u3002</p> <ol> <li>\u554f\u984c\u8a2d\u5b9a\u306e\u7df4\u7fd2\uff1a</li> <li>\u3042\u3044\u307e\u3044\u306a\u72b6\u6cc1\u304b\u3089\u554f\u984c\u3092\u660e\u78ba\u5316\u3059\u308b</li> <li>\u5927\u304d\u306a\u554f\u984c\u3092\u5c0f\u3055\u306a\u554f\u3044\u306b\u5206\u89e3\u3059\u308b</li> <li>AI\u304c\u89e3\u3051\u308b\u5f62\u306b\u554f\u984c\u3092\u7ffb\u8a33\u3059\u308b</li> </ol> <p>\u3053\u308c\u3089\u306f\u751f\u6210AI\u306e\u80fd\u529b\u3092\u6700\u5927\u9650\u306b\u5f15\u304d\u51fa\u3059\u305f\u3081\u306b\u4e0d\u53ef\u6b20\u3067\u3059\u3002</p> <p>\u91cd\u8981\u306a\u306e\u306f\u3001\u3053\u308c\u3089\u306e\u80fd\u529b\u306f\u300cAI\u3092\u4f7f\u308f\u305a\u306b\u5168\u90e8\u81ea\u5206\u3067\u3084\u308b\u300d\u3053\u3068\u3067\u8eab\u306b\u3064\u304f\u308f\u3051\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u3080\u3057\u308d\u3001AI\u3068\u5354\u50cd\u3057\u306a\u304c\u3089\u3001\u305d\u306e\u51fa\u529b\u3092\u8a55\u4fa1\u3057\u3001\u554f\u984c\u3092\u518d\u5b9a\u7fa9\u3059\u308b\u30b5\u30a4\u30af\u30eb\u3092\u7e70\u308a\u8fd4\u3059\u3053\u3068\u3067\u78e8\u304b\u308c\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>\u5927\u5b66\u3067\u306e\u5b66\u3073\u306f\u3001\u5358\u306b\u30c4\u30fc\u30eb\u306e\u4f7f\u3044\u65b9\u3092\u899a\u3048\u308b\u3053\u3068\u3067\u306f\u306a\u304f\u3001AI\u3068\u5171\u306b\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3066\u3044\u304f\u305f\u3081\u306e\u77e5\u7684\u57fa\u76e4\u3092\u7bc9\u304f\u30d7\u30ed\u30bb\u30b9\u306a\u306e\u3067\u3059\u3002\u3053\u306e\u77e5\u7684\u57fa\u76e4\u304c\u3042\u3063\u3066\u3053\u305d\u3001\u7686\u3055\u3093\u306f\u300cAI\u306e\u8a00\u3046\u3053\u3068\u3092\u9d5c\u5451\u307f\u306b\u3059\u308b\u4eba\u300d\u3067\u306f\u306a\u304f\u3001\u300cAI\u3092\u4f7f\u3044\u3053\u306a\u3059\u554f\u984c\u89e3\u6c7a\u8005\u300d\u306b\u306a\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#5-ai","title":"5. \u751f\u6210AI\u6642\u4ee3\u306e\u5927\u5b66\u6559\u80b2\uff1a\u554f\u3046\u529b\u3092\u80b2\u3080\u77e5\u7684\u57fa\u76e4\u306e\u69cb\u7bc9","text":"<p>\u3053\u3053\u307e\u3067\u898b\u3066\u304d\u305f\u3088\u3046\u306b\u3001\u751f\u6210AI\u306e\u767b\u5834\u306b\u3088\u3063\u3066\u79c1\u305f\u3061\u306e\u554f\u984c\u89e3\u6c7a\u306e\u3042\u308a\u65b9\u306f\u6839\u672c\u7684\u306b\u5909\u308f\u308a\u3064\u3064\u3042\u308a\u307e\u3059\u3002\u300c\u89e3\u304f\u529b\u300d\u306f\u751f\u6210AI\u306b\u4efb\u305b\u3089\u308c\u308b\u4e00\u65b9\u3067\u3001\u300c\u554f\u3046\u529b\u300d\u300c\u691c\u8a3c\u3059\u308b\u529b\u300d\u304c\u3088\u308a\u91cd\u8981\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u3067\u306f\u3001\u3053\u306e\u5909\u5316\u3092\u8e0f\u307e\u3048\u3066\u3001\u5927\u5b66\u3067\u5b66\u3076\u3053\u3068\u306e\u610f\u7fa9\u3068\u306f\u4f55\u3067\u3057\u3087\u3046\u304b\uff1f\u305d\u3057\u3066\u3001\u7686\u3055\u3093\u304c\u3053\u308c\u304b\u30894\u5e74\u9593\u3067\u8eab\u306b\u3064\u3051\u308b\u3079\u304d\u80fd\u529b\u306f\u4f55\u306a\u306e\u3067\u3057\u3087\u3046\u304b\uff1f</p>"},{"location":"lectures/OC/#51","title":"5.1 \u77e5\u8b58\u306e\u65b0\u305f\u306a\u4fa1\u5024\uff1a\u554f\u3046\u305f\u3081\u306e\u57fa\u76e4\u3068\u3057\u3066","text":"<p>\u5f93\u6765\u306e\u6559\u80b2\u89b3\u3067\u306f\u300c\u77e5\u8b58\u3092\u8eab\u306b\u3064\u3051\u308b\u300d\u3053\u3068\u304c\u91cd\u8996\u3055\u308c\u3066\u304d\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u3084\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u306e\u666e\u53ca\u306b\u3088\u308a\u3001\u300c\u77e5\u3063\u3066\u3044\u308b\u3053\u3068\u300d\u306e\u4fa1\u5024\u306f\u76f8\u5bfe\u7684\u306b\u4f4e\u4e0b\u3057\u3066\u3044\u308b\u3068\u8a00\u308f\u308c\u3066\u304d\u307e\u3057\u305f\u3002</p> <p>\u3057\u304b\u3057\u3001\u751f\u6210AI\u306e\u6642\u4ee3\u306b\u304a\u3044\u3066\u3001\u5b9f\u306f\u77e5\u8b58\u306e\u4fa1\u5024\u306f\u5927\u304d\u304f\u9ad8\u307e\u3063\u3066\u3044\u308b\u3068\u8a00\u3048\u307e\u3059\u3002\u306a\u305c\u306a\u3089\uff1a</p> <ol> <li> <p>\u554f\u984c\u3092\u5b9a\u7fa9\u3059\u308b\u305f\u3081\u306e\u524d\u63d0\uff1a    \u826f\u3044\u554f\u3044\u3092\u7acb\u3066\u308b\u305f\u3081\u306b\u306f\u3001\u305d\u306e\u5206\u91ce\u306e\u57fa\u672c\u6982\u5ff5\u3084\u8ab2\u984c\u3092\u7406\u89e3\u3057\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u5065\u5eb7\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3059\u308b\u969b\u3001\u300c\u5fc3\u62cd\u5909\u52d5\u300d\u3084\u300c\u6d3b\u52d5\u91cf\u300d\u306a\u3069\u306e\u57fa\u672c\u6982\u5ff5\u3092\u7406\u89e3\u3057\u3066\u3044\u306a\u3051\u308c\u3070\u3001\u9069\u5207\u306a\u554f\u3044\u3092\u7acb\u3066\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093\u3002</p> </li> <li> <p>AI\u306e\u51fa\u529b\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u7269\u5dee\u3057\uff1a    AI\u306e\u56de\u7b54\u304c\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u3092\u5224\u65ad\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u305d\u306e\u5206\u91ce\u306e\u57fa\u790e\u77e5\u8b58\u304c\u4e0d\u53ef\u6b20\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001AI\u304c\u300c\u3053\u306e\u7d71\u8a08\u5206\u6790\u306e\u7d50\u679c\u304b\u3089\u3001\u4ecb\u5165\u52b9\u679c\u306f\u6709\u610f\u3067\u3042\u308b\u300d\u3068\u8a00\u3063\u305f\u3068\u304d\u3001\u305d\u306e\u5224\u65ad\u304c\u6b63\u3057\u3044\u304b\u3092\u8a55\u4fa1\u3059\u308b\u306b\u306f\u7d71\u8a08\u5b66\u306e\u57fa\u790e\u77e5\u8b58\u304c\u5fc5\u8981\u3067\u3059\u3002</p> </li> <li> <p>\u9818\u57df\u3092\u6a2a\u65ad\u3059\u308b\u601d\u8003\u306e\u57fa\u76e4\uff1a    \u7570\u306a\u308b\u5206\u91ce\u306e\u77e5\u8b58\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u65b0\u3057\u3044\u554f\u3044\u3092\u7acb\u3066\u308b\u3053\u3068\u306f\u3001AI\u306b\u306f\u307e\u3060\u96e3\u3057\u3044\u4eba\u9593\u306a\u3089\u3067\u306f\u306e\u80fd\u529b\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u5fc3\u7406\u5b66\u306e\u77e5\u898b\u3068\u30a6\u30a7\u30a2\u30e9\u30d6\u30eb\u30c7\u30d0\u30a4\u30b9\u306e\u6280\u8853\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u65b0\u3057\u3044\u5065\u5eb7\u5897\u9032\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u69cb\u60f3\u3059\u308b\u306b\u306f\u3001\u4e21\u65b9\u306e\u5206\u91ce\u306e\u77e5\u8b58\u304c\u5fc5\u8981\u3067\u3059\u3002</p> </li> </ol> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u77e5\u8b58\u306f\u5358\u306a\u308b\u300c\u7b54\u3048\u300d\u3067\u306f\u306a\u304f\u3001\u300c\u554f\u3044\u3092\u751f\u307f\u51fa\u3059\u6e90\u6cc9\u300d\u300cAI\u3068\u306e\u5bfe\u8a71\u3092\u6df1\u3081\u308b\u57fa\u76e4\u300d\u3068\u3057\u3066\u3001\u65b0\u305f\u306a\u4fa1\u5024\u3092\u6301\u3064\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/OC/#_5","title":"\u3010\u5177\u4f53\u4f8b\uff1a\u5065\u5eb7\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u3067\u306e\u77e5\u8b58\u306e\u6d3b\u7528\u3011","text":"<p>\u751f\u6210AI\u306b\u300c\u904b\u52d5\u3068\u5065\u5eb7\u306e\u95a2\u4fc2\u3092\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u300d\u3068\u6307\u793a\u3057\u305f\u5834\u5408\uff1a</p> <p>\u77e5\u8b58\u304c\u306a\u3044\u5b66\u751f\u306e\u5834\u5408\uff1a - \u6f20\u7136\u3068\u3057\u305f\u6307\u793a\u3057\u304b\u3067\u304d\u306a\u3044 - AI\u306e\u56de\u7b54\uff08\u300c\u6709\u9178\u7d20\u904b\u52d5\u306f\u5fc3\u8840\u7ba1\u7cfb\u306e\u5065\u5eb7\u306b\u826f\u3044\u300d\u306a\u3069\uff09\u3092\u9d5c\u5451\u307f\u306b\u3059\u308b\u3057\u304b\u306a\u3044 - \u5206\u6790\u306e\u6df1\u6398\u308a\u304c\u3067\u304d\u306a\u3044</p> <p>\u77e5\u8b58\u304c\u3042\u308b\u5b66\u751f\u306e\u5834\u5408\uff1a - \u300c30\u4ee3\u5973\u6027\u306e\u90313\u56de\u306e\u9ad8\u5f37\u5ea6\u30a4\u30f3\u30bf\u30fc\u30d0\u30eb\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u304c\u3001\u5b89\u9759\u6642\u5fc3\u62cd\u6570\u3068\u7761\u7720\u52b9\u7387\u306b\u4e0e\u3048\u308b6\u9031\u9593\u306e\u52b9\u679c\u3092\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7279\u306b\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u6642\u9593\u5e2f\uff08\u671dvs\u5915\u65b9\uff09\u306b\u3088\u308b\u5dee\u7570\u306b\u6ce8\u76ee\u3057\u3066\u300d\u306a\u3069\u5177\u4f53\u7684\u306a\u6307\u793a\u304c\u3067\u304d\u308b - AI\u306e\u56de\u7b54\u306b\u5bfe\u3057\u3066\u300c\u3053\u306e\u5206\u6790\u3067\u306f\u4ea4\u7d61\u56e0\u5b50\u3068\u3057\u3066\u6804\u990a\u6442\u53d6\u72b6\u6cc1\u3092\u8003\u616e\u3059\u3079\u304d\u3067\u306f\uff1f\u300d\u3068\u3044\u3063\u305f\u9069\u5207\u306a\u8cea\u554f\u304c\u3067\u304d\u308b - \u300c\u3053\u306e\u7d50\u679c\u3068\u8fd1\u5e74\u306e\u7b4b\u30c8\u30ec\u306b\u3088\u308b\u30de\u30a4\u30aa\u30ab\u30a4\u30f3\u306e\u7814\u7a76\u3092\u95a2\u9023\u4ed8\u3051\u308b\u3068\u3001\u3069\u306e\u3088\u3046\u306a\u4eee\u8aac\u304c\u7acb\u3066\u3089\u308c\u307e\u3059\u304b\uff1f\u300d\u306a\u3069\u3001\u5206\u6790\u3092\u6df1\u6398\u308a\u3067\u304d\u308b</p>"},{"location":"lectures/OC/#52-ai","title":"5.2 \u751f\u6210AI\u6642\u4ee3\u306b\u5927\u5b66\u3067\u57f9\u3046\u672c\u8cea\u7684\u306a\u529b","text":"<p>\u6700\u5f8c\u306b\u3001\u751f\u6210AI\u6642\u4ee3\u306b\u5927\u5b66\u6559\u80b2\u3067\u57f9\u3046\u3079\u304d\u672c\u8cea\u7684\u306a\u529b\u306b\u3064\u3044\u3066\u307e\u3068\u3081\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/OC/#1_1","title":"1. \u6df1\u3044\u6587\u8108\u7406\u89e3\u529b","text":"<p>\u8868\u9762\u7684\u306a\u60c5\u5831\u3060\u3051\u3067\u306a\u304f\u3001\u305d\u306e\u80cc\u666f\u306b\u3042\u308b\u7406\u8ad6\u3084\u6b74\u53f2\u3001\u793e\u4f1a\u7684\u6587\u8108\u3092\u7406\u89e3\u3059\u308b\u529b\u3067\u3059\u3002AI\u306f\u60c5\u5831\u3092\u51e6\u7406\u3067\u304d\u307e\u3059\u304c\u3001\u305d\u306e\u610f\u5473\u3092\u6df1\u304f\u7406\u89e3\u3059\u308b\u306e\u306f\u4eba\u9593\u306e\u5f79\u5272\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u5065\u5eb7\u30c7\u30fc\u30bf\u306e\u5909\u52d5\u304c\u5358\u306a\u308b\u6570\u5024\u5909\u5316\u3067\u306f\u306a\u304f\u3001\u751f\u6d3b\u7fd2\u6163\u306e\u5909\u5316\u3084\u793e\u4f1a\u74b0\u5883\u306e\u5f71\u97ff\u3092\u53cd\u6620\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u7406\u89e3\u3067\u304d\u308b\u306e\u306f\u3001\u6587\u8108\u3092\u628a\u63e1\u3059\u308b\u4eba\u9593\u306e\u529b\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#2","title":"2. \u9818\u57df\u6a2a\u65ad\u7684\u306a\u7d71\u5408\u529b","text":"<p>\u7570\u306a\u308b\u5206\u91ce\u306e\u77e5\u8b58\u3092\u6709\u6a5f\u7684\u306b\u7d50\u3073\u3064\u3051\u3001\u65b0\u3057\u3044\u8996\u70b9\u3092\u751f\u307f\u51fa\u3059\u529b\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u30b9\u30dd\u30fc\u30c4\u79d1\u5b66\u306e\u77e5\u898b\u3068\u884c\u52d5\u7d4c\u6e08\u5b66\u306e\u7406\u8ad6\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3001\u3088\u308a\u52b9\u679c\u7684\u306a\u5065\u5eb7\u5897\u9032\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8a2d\u8a08\u3059\u308b\u3068\u3044\u3063\u305f\u601d\u8003\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/OC/#3_2","title":"3. \u77e5\u7684\u8b19\u865a\u3055\u3068\u6279\u5224\u7684\u601d\u8003\u529b","text":"<p>\u81ea\u5206\u306e\u77e5\u8b58\u306e\u9650\u754c\u3092\u7406\u89e3\u3057\u3001\u5e38\u306b\u691c\u8a3c\u3059\u308b\u59ff\u52e2\u3092\u6301\u3064\u3053\u3068\u3067\u3059\u3002AI\u306e\u56de\u7b54\u306b\u5bfe\u3057\u3066\u3082\u3001\u81ea\u5206\u306e\u8003\u3048\u306b\u5bfe\u3057\u3066\u3082\u3001\u300c\u672c\u5f53\u306b\u305d\u3046\u306a\u306e\u304b\uff1f\u300d\u3068\u554f\u3044\u7d9a\u3051\u308b\u7fd2\u6163\u304c\u91cd\u8981\u3067\u3059\u3002\u77e5\u8b58\u304c\u5897\u3048\u308b\u307b\u3069\u300c\u308f\u304b\u3089\u306a\u3044\u3053\u3068\u300d\u304c\u5897\u3048\u308b\u3053\u3068\u3092\u7406\u89e3\u3057\u3001\u5b66\u3073\u7d9a\u3051\u308b\u8b19\u865a\u3055\u304c\u5fc5\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/OC/#4","title":"4. \u5275\u9020\u7684\u306a\u554f\u984c\u8a2d\u5b9a\u529b","text":"<p>\u65e2\u5b58\u306e\u67a0\u7d44\u307f\u306b\u3068\u3089\u308f\u308c\u305a\u3001\u65b0\u3057\u3044\u554f\u3044\u3092\u7acb\u3066\u308b\u529b\u3067\u3059\u3002\u300c\u3053\u3093\u306a\u5206\u6790\u306f\u3067\u304d\u306a\u3044\u3060\u308d\u3046\u304b\u300d\u300c\u3053\u306e\u73fe\u8c61\u306e\u80cc\u5f8c\u306b\u306f\u3069\u3093\u306a\u30e1\u30ab\u30cb\u30ba\u30e0\u304c\u3042\u308b\u306e\u3060\u308d\u3046\u304b\u300d\u3068\u3044\u3063\u305f\u597d\u5947\u5fc3\u304b\u3089\u751f\u307e\u308c\u308b\u554f\u3044\u304c\u3001\u30a4\u30ce\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u6e90\u6cc9\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/OC/#53-ai","title":"5.3 \u7d50\u8ad6\uff1aAI\u3068\u5171\u306b\u6210\u9577\u3059\u308b\u77e5\u7684\u5192\u967a\u8005\u306b\u306a\u308b","text":"<p>\u751f\u6210AI\u306e\u6642\u4ee3\u306b\u5927\u5b66\u3067\u5b66\u3076\u610f\u7fa9\u306f\u3001\u5358\u306b\u30b9\u30ad\u30eb\u3084\u77e5\u8b58\u3092\u8eab\u306b\u3064\u3051\u308b\u3053\u3068\u3067\u306f\u306a\u304f\u3001\u300c\u554f\u3046\u529b\u300d\u3092\u6301\u3063\u305f\u77e5\u7684\u5192\u967a\u8005\u306b\u306a\u308b\u3053\u3068\u306b\u3042\u308a\u307e\u3059\u3002\u5e38\u306b\u597d\u5947\u5fc3\u3092\u6301\u3061\u3001\u6df1\u304f\u8003\u3048\u3001AI\u3068\u5bfe\u8a71\u3057\u306a\u304c\u3089\u81ea\u3089\u306e\u601d\u8003\u3092\u6df1\u3081\u3066\u3044\u304f\u3002\u305d\u3057\u3066\u3001\u305d\u306e\u904e\u7a0b\u3067\u5f97\u305f\u77e5\u898b\u3092\u793e\u4f1a\u306b\u9084\u5143\u3057\u3066\u3044\u304f\u3002</p> <p>\u7686\u3055\u3093\u306b\u306f\u3001\u751f\u6210AI\u3092\u9053\u5177\u3068\u3057\u3066\u4f7f\u3044\u3053\u306a\u3057\u306a\u304c\u3089\u3001\u4eba\u9593\u306b\u3057\u304b\u3067\u304d\u306a\u3044\u300c\u554f\u3044\u300d\u3092\u7acb\u3066\u7d9a\u3051\u308b\u3053\u3068\u3092\u671f\u5f85\u3057\u3066\u3044\u307e\u3059\u3002\u5927\u5b66\u3067\u306e4\u5e74\u9593\u306f\u3001\u305d\u306e\u77e5\u7684\u5192\u967a\u306e\u7b2c\u4e00\u6b69\u3067\u3059\u3002AI\u304c\u89e3\u7b54\u3092\u63d0\u4f9b\u3059\u308b\u6642\u4ee3\u3060\u304b\u3089\u3053\u305d\u3001\u3088\u308a\u826f\u3044\u554f\u3044\u3092\u7acb\u3066\u3089\u308c\u308b\u4eba\u306b\u306a\u308b\u305f\u3081\u306e\u8cb4\u91cd\u306a\u6642\u9593\u3068\u3057\u3066\u6d3b\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>\u6700\u5f8c\u306b\u3001\u3053\u306e\u8b1b\u7fa9\u306e\u5185\u5bb9\u3092\u4e00\u8a00\u3067\u307e\u3068\u3081\u308b\u306a\u3089\uff1a\u300cAI\u304c\u7b54\u3048\u3092\u51fa\u3059\u6642\u4ee3\u306b\u306f\u3001\u3088\u308a\u826f\u3044\u554f\u3044\u3092\u7acb\u3066\u308b\u4eba\u304c\u4e16\u754c\u3092\u5909\u3048\u308b\u300d\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u7686\u3055\u3093\u304c\u305d\u306e\u3088\u3046\u306a\u300c\u554f\u3044\u306e\u529b\u300d\u3092\u6301\u3063\u305f\u4eba\u6750\u306b\u6210\u9577\u3055\u308c\u308b\u3053\u3068\u3092\u9858\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/OC/presentation/","title":"\u751f\u6210AI\u306b\u3088\u308b\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8","text":""},{"location":"lectures/OC/presentation/#ai_1","title":"\u751f\u6210AI\u6642\u4ee3\u306b\u5fc5\u8981\u306a\u80fd\u529b","text":""},{"location":"lectures/OC/presentation/#_1","title":"\u76ee\u6b21","text":"<ol> <li>\u30a4\u30f3\u30c8\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\uff1a\u751f\u6210AI\u306e\u9032\u5316\u3068\u8ee2\u63db\u70b9</li> <li>AI\u306e\u4ed5\u7d44\u307f\u3068\u6280\u8853\u7684\u57fa\u76e4</li> <li>\u6839\u672c\u7684\u306a\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8</li> <li>\u554f\u984c\u5b9a\u7fa9\u30fb\u89e3\u6c7a\u80fd\u529b\u306e\u8cea\u304c\u554f\u308f\u308c\u308b\u6642\u4ee3</li> <li>\u307e\u3068\u3081\uff1a\u751f\u6210AI\u6642\u4ee3\u306e\u5b66\u3073\u306e\u65b9\u5411\u6027</li> </ol>"},{"location":"lectures/OC/presentation/#1-ai","title":"1. \u30a4\u30f3\u30c8\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\uff1a\u751f\u6210AI\u306e\u9032\u5316","text":"<ul> <li>2022: ChatGPT\u306e\u767b\u5834 - \u4f1a\u8a71\u578bAI\u306e\u8a95\u751f\u3068\u8a00\u8a9e\u51e6\u7406\u306e\u9769\u65b0</li> <li>2023\u672b: \u65e5\u672c\u8a9e\u80fd\u529b\u306e\u5411\u4e0a\u3001\u4ee3\u7b46\u30ec\u30d9\u30eb\u306b\u5230\u9054</li> <li>2024\u521d: \u533b\u5e2b\u56fd\u5bb6\u8a66\u9a13\u3084\u85ac\u5264\u5e2b\u8a66\u9a13\u306b\u5408\u683c\u3059\u308b\u30ec\u30d9\u30eb\u306b\u5230\u9054</li> <li>2024.9: OpenAI o1-preview - \u300c\u9577\u8003\u300d\u6a5f\u80fd\u306b\u3088\u308bSTEM\u9818\u57df\u3067\u306e\u9769\u65b0</li> <li>2025\u73fe\u5728: Claude 3.7 Sonnet\u3001GPT-4.5\u3001o3-mini-high - \u4eba\u9593\u4e0a\u4f4d1%\u306e\u77e5\u7684\u80fd\u529b</li> </ul>"},{"location":"lectures/OC/presentation/#ai_2","title":"\u8ee2\u63db\u70b9\uff1a\u5f93\u6765\u306eAI\u3068\u73fe\u5728\u306e\u80fd\u529b\u5dee","text":"**\u5f93\u6765\u306e\u5f31\u70b9**: - STEM\u9818\u57df\u306e\u554f\u984c\u89e3\u6c7a\u529b\u306e\u6b20\u5982 - \u9650\u5b9a\u7684\u306a\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u80fd\u529b - \u77ed\u671f\u7684\u601d\u8003\u306b\u9650\u5b9a  **\u73fe\u5728\u306e\u80fd\u529b**: - \u9577\u8003\u306b\u3088\u308b\u554f\u984c\u89e3\u6c7a\u80fd\u529b - \u535a\u58eb\u8ab2\u7a0b\u30ec\u30d9\u30eb\u306e\u8cea\u554f\u3078\u306e\u5bfe\u5fdc - \u4eba\u9593\u30a8\u30f3\u30b8\u30cb\u30a2\u4e0a\u4f4d1%\u30ec\u30d9\u30eb\u306e\u30b3\u30fc\u30c9\u751f\u6210     ![width:100%](image-1.png)    \ud83d\udca1 **\u8003\u5bdf**: \u3053\u306e\u3088\u3046\u306a\u80fd\u529b\u5909\u5316\u306f\u3001\u3042\u306a\u305f\u306e\u5c02\u9580\u5206\u91ce\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u3067\u3057\u3087\u3046\u304b\uff1f    # 2. AI\u306e\u4ed5\u7d44\u307f\u3068\u6280\u8853\u7684\u57fa\u76e4 ## \u512a\u308c\u305f\u554f\u984c\u89e3\u6c7a\u8005\u3092\u7406\u89e3\u3059\u308b"},{"location":"lectures/OC/presentation/#21-transformer","title":"2.1 Transformer\u3068\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb","text":"<ul> <li>Transformer\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3: \u300cAttention is all you need\u300d(2017)</li> <li>\u30c6\u30fc\u30de\u306e\u8a18\u61b6\u3068\u6587\u8108\u3092\u8e0f\u307e\u3048\u305f\u6b21\u5358\u8a9e\u4e88\u6e2c</li> <li>Multi-head Attention: \u5358\u8a9e\u9593\u306e\u8907\u96d1\u306a\u95a2\u4fc2\u6027\u3092\u540c\u6642\u306b\u5b66\u7fd2</li> </ul>  **\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb (Self-Attention)**: \u6587\u7ae0\u5185\u306e\u5404\u5358\u8a9e\u304c\u4ed6\u306e\u3059\u3079\u3066\u306e\u5358\u8a9e\u306b\u3069\u306e\u7a0b\u5ea6\u300c\u6ce8\u610f\u300d\u3092\u6255\u3046\u3079\u304d\u304b\u3092\u8a08\u7b97\u3057\u3001\u6587\u8108\u306b\u5fdc\u3058\u305f\u610f\u5473\u3092\u7372\u5f97\u3059\u308b\u6a5f\u69cb"},{"location":"lectures/OC/presentation/#22","title":"2.2 \u5f37\u5316\u5b66\u7fd2\u306e\u7d71\u5408","text":"<ul> <li>\u751f\u6210\u3057\u305f\u6587\u7ae0\u306e\u8cea\u3092\u8a55\u4fa1\u3059\u308b\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u30eb\u30fc\u30d7</li> <li>RLHF (Reinforcement Learning from Human Feedback): \u4eba\u9593\u306e\u9078\u597d\u306b\u57fa\u3065\u304f\u5831\u916c\u30e2\u30c7\u30eb</li> <li>\u81ea\u5df1\u6539\u5584\u30e1\u30ab\u30cb\u30ba\u30e0\uff08\u81ea\u5df1\u6279\u8a55\u30fb\u4fee\u6b63\uff09\u306b\u3088\u308a\u54c1\u8cea\u5411\u4e0a</li> </ul>   ![width:100%](image-2.png)     ![width:100%](image-3.png)    **RLHF**: \u4eba\u9593\u306e\u8a55\u4fa1\u8005\u304c\u751f\u6210\u3055\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u306e\u8cea\u3092\u5224\u65ad\u3057\u3001\u305d\u306e\u8a55\u4fa1\u3092\u3082\u3068\u306bAI\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u3059\u308b\u624b\u6cd5"},{"location":"lectures/OC/presentation/#23-12","title":"2.3 \u5c02\u9580\u77e5\u8b58\u8a66\u9a13\u306b\u304a\u3051\u308b\u9ad8\u6027\u80fd\u5316 (1/2)","text":""},{"location":"lectures/OC/presentation/#2025","title":"\u533b\u5e2b\u56fd\u5bb6\u8a66\u9a13 (2025)","text":"<ul> <li>o3-mini-high\u306b\u3088\u308b\u6210\u7e3e:</li> <li>\u5fc5\u4fee\u554f\u984c: \u4e0a\u4f4d10%\u76f8\u5f53</li> <li>\u4e00\u822c\u81e8\u5e8a\u554f\u984c: \u5168\u53d7\u9a13\u8005\u4e2d\u7b2c3\u4f4d\u76f8\u5f53</li> <li>\u5c02\u9580\u77e5\u8b58\u9818\u57df\u306b\u304a\u3044\u3066\u30c8\u30c3\u30d7\u30ec\u30d9\u30eb\u306e\u6210\u7e3e</li> </ul>"},{"location":"lectures/OC/presentation/#2024","title":"\u85ac\u5264\u5e2b\u56fd\u5bb6\u8a66\u9a13 (2024)","text":"<ul> <li>o1-preview\u306b\u3088\u308b \u6b63\u7b54\u7387100%</li> </ul>"},{"location":"lectures/OC/presentation/#23-22","title":"2.3 \u5c02\u9580\u77e5\u8b58\u8a66\u9a13\u306b\u304a\u3051\u308b\u9ad8\u6027\u80fd\u5316 (2/2)","text":""},{"location":"lectures/OC/presentation/#2025_1","title":"\u6771\u5927\u30fb\u4eac\u5927\u6570\u5b66\u5165\u8a66 (2025)","text":"<ul> <li>\u6771\u5927\u6570\u5b66: \u5927\u554f6\u554f\u4e2d5.5\u554f\u6b63\u89e3</li> <li>\u4eac\u5927\u6570\u5b66: \u5168\u554f\u6b63\u89e3\uff08\u4e0a\u4f4d1%\u30ec\u30d9\u30eb\uff09</li> </ul>  **\u9577\u8003**: \u751f\u6210AI\u304c\u81ea\u8eab\u306e\u89e3\u7b54\u3092\u5185\u90e8\u3067\u691c\u8a3c\u30fb\u4fee\u6b63\u3057\u306a\u304c\u3089\u3001\u8907\u96d1\u306a\u554f\u984c\u306b\u5bfe\u3057\u3066\u6bb5\u968e\u7684\u306b\u89e3\u3092\u5c0e\u51fa\u3059\u308b\u30d7\u30ed\u30bb\u30b9   \ud83d\udca1 **\u8a0e\u8ad6**: \u5927\u5b66\u5165\u8a66\u306b\u304a\u3044\u3066\u3001AI\u304c\u4eba\u9593\u3088\u308a\u9ad8\u3044\u6210\u7e3e\u3092\u53ce\u3081\u308b\u3053\u3068\u306e\u793e\u4f1a\u7684\u5f71\u97ff\u306f\uff1f"},{"location":"lectures/OC/presentation/#24","title":"2.4 \u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u80fd\u529b\u306e\u98db\u8e8d\u7684\u5411\u4e0a","text":""},{"location":"lectures/OC/presentation/#swe-bench","title":"SWE-bench\u8a55\u4fa1","text":"<ul> <li>\u5b9f\u969b\u306eGitHub\u306e\u8ab2\u984c\u89e3\u6c7a\u80fd\u529b\u3092\u5b9a\u91cf\u7684\u306b\u6e2c\u5b9a</li> <li>\u30d0\u30b0\u4fee\u6b63\u3001\u6a5f\u80fd\u5b9f\u88c5\u3001\u30b3\u30fc\u30c9\u751f\u6210\u306e\u7dcf\u5408\u8a55\u4fa1</li> <li>Claude 3.7 Sonnet\u306e\u80fd\u529b: \u4eba\u9593\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u4e0a\u4f4d1%\u4ee5\u4e0a</li> </ul>"},{"location":"lectures/OC/presentation/#_2","title":"\u5b9f\u4f8b\uff1a\u59ff\u52e2\u63a8\u5b9a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u958b\u767a","text":"<pre><code># Claude 3.7 Sonnet\u3067\u751f\u6210\u3057\u305f\u59ff\u52e2\u63a8\u5b9a\u30b3\u30fc\u30c9\u4f8b\nimport mediapipe as mp\nimport cv2\nimport numpy as np\n\n# MediaPipe Pose\u30e2\u30c7\u30eb\u306e\u521d\u671f\u5316\nmp_pose = mp.solutions.pose\npose = mp_pose.Pose(\n    static_image_mode=True,\n    model_complexity=2,\n    enable_segmentation=True\n)\nmp_drawing = mp.solutions.drawing_utils\n\ndef analyze_posture(image_path):\n    # \u753b\u50cf\u8aad\u307f\u8fbc\u307f\n    image = cv2.imread(image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # \u59ff\u52e2\u63a8\u5b9a\u5b9f\u884c\n    results = pose.process(image_rgb)\n\n    # \u7d50\u679c\u306e\u89e3\u6790\u3068\u59ff\u52e2\u8a55\u4fa1...\n</code></pre>"},{"location":"lectures/OC/presentation/#_3","title":"\u767a\u5c55\u7684\u5fdc\u7528\uff1a\u59ff\u52e2\u63a8\u5b9a\u6280\u8853\u306e\u6d3b\u7528\u9818\u57df","text":"**\u59ff\u52e2\u63a8\u5b9a\u6280\u8853\u57fa\u76e4**: - \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3 - 2D/3D\u30dd\u30fc\u30ba\u63a8\u5b9a\u65b9\u6cd5 - \u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u51e6\u7406\u6700\u9069\u5316  **\u30b9\u30dd\u30fc\u30c4\u79d1\u5b66\u5fdc\u7528**: - \u30a2\u30b9\u30ea\u30fc\u30c8\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u5206\u6790 - \u50b7\u5bb3\u30ea\u30b9\u30af\u8a55\u4fa1 - \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u6700\u9069\u5316     **\u5065\u5eb7\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0**: - \u795e\u7d4c\u5b66\u7684\u75c7\u72b6\u306e\u65e9\u671f\u767a\u898b - \u30ea\u30cf\u30d3\u30ea\u30c6\u30fc\u30b7\u30e7\u30f3\u9032\u884c\u306e\u5b9a\u91cf\u5316 - \u9ad8\u9f62\u8005\u30b1\u30a2\u306e\u30ea\u30e2\u30fc\u30c8\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0 - \u96fb\u5b50\u5065\u5eb7\u8a18\u9332\u3068\u306e\u7d71\u5408"},{"location":"lectures/OC/presentation/#25-deep-research","title":"2.5 \u9ad8\u5ea6\u306a\u30ea\u30b5\u30fc\u30c1\u80fd\u529b\uff1aDeep Research","text":"<ul> <li>\u5148\u884c\u4e8b\u4f8b\u8abf\u67fb\u306e\u81ea\u52d5\u5316\u3068\u52b9\u7387\u5316</li> <li>2\u301c3\u6642\u9593\u306e\u8abf\u67fb\u4f5c\u696d\u309210\u5206\u7a0b\u5ea6\u306b\u77ed\u7e2e</li> <li>\u8abf\u67fb\u7d50\u679c\u306e\u6839\u62e0\uff08URL\u3001\u6587\u732e\uff09\u306e\u660e\u793a</li> <li>\u60c5\u5831\u53ce\u96c6\u304b\u3089\u52a0\u5de5\u307e\u3067\u306e\u4e00\u8cab\u51e6\u7406</li> <li>\u6587\u732e\u8abf\u67fb\u306b\u304a\u3051\u308b\u30cf\u30eb\u30b7\u30cd\u30fc\u30b7\u30e7\u30f3\u554f\u984c\u306e\u89e3\u6c7a</li> </ul>  **Deep Research**: GPT-4.5/o3-mini-high\u306b\u642d\u8f09\u3055\u308c\u305f\u6a5f\u80fd\u3067\u3001\u8abf\u67fb\u3057\u305f\u60c5\u5831\u306e\u4fe1\u983c\u6027\u3092\u62c5\u4fdd\u3059\u308b\u305f\u3081\u306b\u51fa\u5178\u3092\u660e\u793a\u3059\u308bWeb\u30d6\u30e9\u30a6\u30b8\u30f3\u30b0\u80fd\u529b"},{"location":"lectures/OC/presentation/#26-mcp","title":"2.6 \u4f01\u696d/\u7814\u7a76\u6a5f\u95a2\u5411\u3051\u9032\u5316\uff1aMCP\u306e\u767b\u5834","text":"- **\u691c\u7d22\u62e1\u5f35\u751f\u6210\uff08RAG\uff09**:    - \u5916\u90e8\u30c7\u30fc\u30bf\u30bd\u30fc\u30b9\u3068\u306e\u9023\u643a   - \u60c5\u5831\u9bae\u5ea6\u306e\u7dad\u6301  - **Model Context Protocol (MCP)**:   - \u30ed\u30fc\u30ab\u30eb\u30c7\u30fc\u30bf\u306e\u5b89\u5168\u306a\u6d3b\u7528   - \u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u3092\u62c5\u4fdd\u3057\u305f\u77e5\u8b58\u62e1\u5f35   - \u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u30c7\u30fc\u30bf\u306e\u7d71\u5408     ![width:100%](image-6.png)    **MCP**: Anthropic\u304c\u958b\u767a\u3057\u305f\u6280\u8853\u3067\u3001\u793e\u5185\u306e\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u30c7\u30fc\u30bf\u3092\u5b89\u5168\u306bAI\u306b\u53c2\u7167\u3055\u305b\u308b\u305f\u3081\u306e\u30d7\u30ed\u30c8\u30b3\u30eb    # 3. \u6839\u672c\u7684\u306a\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8 ## \u5909\u308f\u308b\u4eba\u9593\u306e\u5f79\u5272"},{"location":"lectures/OC/presentation/#ai_3","title":"AI\u304c\u3069\u308c\u3060\u3051\u9032\u6b69\u3057\u3066\u3082\u5909\u308f\u3089\u306a\u3044\u6839\u672c\u539f\u5247","text":"### 1. \u4eba\u9593\u306e\u8cac\u4efb\u539f\u5247 - AI\u51fa\u529b\u306b\u5bfe\u3059\u308b\u6700\u7d42\u7684\u306a\u8cac\u4efb\u306f\u4eba\u9593\u306b\u3042\u308b - \u6b63\u3057\u3055\u306e\u691c\u8a3c\u306f\u5e38\u306b\u5fc5\u9808 - \u6cd5\u7684\u30fb\u502b\u7406\u7684\u8cac\u4efb\u306e\u6240\u5728\u306f\u5909\u308f\u3089\u306a\u3044     ### 2. \u554f\u984c\u5b9a\u7fa9\u306e\u4e3b\u4f53\u6027 - \u554f\u984c\u306f\u4eba\u9593\u306e\u6d3b\u52d5\u304b\u3089\u751f\u307e\u308c\u308b - \u4f55\u304c\u554f\u984c\u304b\u306fAI\u3067\u306f\u306a\u304f\u4eba\u9593\u304c\u6c7a\u3081\u308b - \u4fa1\u5024\u5224\u65ad\u306f\u4eba\u9593\u306e\u9818\u57df    \ud83d\udca1 **\u8003\u5bdf**: \u3042\u306a\u305f\u306e\u7814\u7a76\u9818\u57df\u3067\u306f\u3001\u3069\u306e\u3088\u3046\u306a\u554f\u984c\u304cAI\u3067\u306f\u7279\u5b9a\u3067\u304d\u306a\u3044\u3067\u3057\u3087\u3046\u304b\uff1f"},{"location":"lectures/OC/presentation/#_4","title":"\u4eba\u9593\u306e\u5f79\u5272\u306e\u672c\u8cea\u7684\u30b7\u30d5\u30c8","text":""},{"location":"lectures/OC/presentation/#_5","title":"\u65e7\u30d1\u30e9\u30c0\u30a4\u30e0","text":"<ul> <li>\u554f\u984c\u89e3\u6c7a\u8005</li> <li>\u77e5\u8b58\u306e\u4fdd\u6301\u8005</li> <li>\u8a08\u7b97\u5b9f\u884c\u8005</li> </ul>"},{"location":"lectures/OC/presentation/#_6","title":"\u65b0\u30d1\u30e9\u30c0\u30a4\u30e0","text":"<ul> <li>\u554f\u984c\u5b9a\u7fa9\u8005: \u7bc4\u56f2\u3068\u5236\u7d04\u306e\u78ba\u7acb</li> <li>\u554f\u984c\u7ffb\u8a33\u8005: AI\u304c\u89e3\u3051\u308b\u5f62\u3078\u306e\u5909\u63db</li> <li>\u51fa\u529b\u691c\u8a3c\u8005: \u89e3\u6c7a\u7b56\u306b\u5bfe\u3059\u308b\u8cac\u4efb</li> </ul>"},{"location":"lectures/OC/presentation/#-ai","title":"\u4eba\u9593-AI\u5354\u50cd\u306e\u65b0\u3057\u3044\u65b9\u7a0b\u5f0f","text":"\\[\\text{\u554f\u984c\u89e3\u6c7a} = \\underbrace{\\text{\u554f\u984c\u5b9a\u7fa9}}_{\\text{\u4eba\u9593}} + \\underbrace{\\text{\u554f\u984c\u7ffb\u8a33}}_{\\text{\u4eba\u9593}} + \\underbrace{\\text{\u89e3\u7b54\u751f\u6210}}_{\\text{AI}} + \\underbrace{\\text{\u51fa\u529b\u691c\u8a3c}}_{\\text{\u4eba\u9593}}\\]   ### \u5f93\u6765\u578b\u5b66\u7fd2\u306e\u7126\u70b9 - \u89e3\u6cd5\u306e\u6697\u8a18\u3068\u9069\u7528 - \u8a08\u7b97\u30b9\u30ad\u30eb - \u8a18\u61b6\u529b     ### \u65b0\u6642\u4ee3\u306e\u5b66\u7fd2\u7126\u70b9 - \u554f\u984c\u306e\u672c\u8cea\u7406\u89e3 - AI\u5bfe\u8a71\u30fb\u6307\u793a\u80fd\u529b - \u6279\u5224\u7684\u691c\u8a3c\u80fd\u529b"},{"location":"lectures/OC/presentation/#12","title":"\u4f8b\uff1a\u6570\u5b66\u6559\u80b2\u30d1\u30e9\u30c0\u30a4\u30e0\u306e\u5909\u5316 (1/2)","text":""},{"location":"lectures/OC/presentation/#_7","title":"\u5f93\u6765\u306e\u554f\u984c\u30a2\u30d7\u30ed\u30fc\u30c1:","text":"<p>\u6b21\u306e\u4e0d\u5b9a\u7a4d\u5206\u3092\u6c42\u3081\u306a\u3055\u3044\u3002 \\(\\(\\int \\frac{x}{(1+x^2)^2} \\, dx.\\)\\)</p>   **\u8a55\u4fa1\u3055\u308c\u308b\u80fd\u529b**: - \u7f6e\u63db\u7a4d\u5206\u306e\u77e5\u8b58 - \u8a08\u7b97\u306e\u6b63\u78ba\u3055 - \u89e3\u6cd5\u306e\u8a18\u61b6     **\u6559\u80b2\u306e\u7126\u70b9**: - \u516c\u5f0f\u306e\u6697\u8a18 - \u89e3\u6cd5\u30d1\u30bf\u30fc\u30f3\u306e\u7fd2\u5f97 - \u8a08\u7b97\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0"},{"location":"lectures/OC/presentation/#22_1","title":"\u4f8b\uff1a\u6570\u5b66\u6559\u80b2\u30d1\u30e9\u30c0\u30a4\u30e0\u306e\u5909\u5316 (2/2)","text":""},{"location":"lectures/OC/presentation/#_8","title":"\u65b0\u6642\u4ee3\u306e\u554f\u984c\u30a2\u30d7\u30ed\u30fc\u30c1:","text":"<p>\u4ee5\u4e0b\u306e\u7a4d\u5206\u89e3\u7b54\u4f8b\u306e\u8aa4\u308a\u3092\u6307\u6458\u3057\u3001\u6700\u7d42\u7b54\u3048\u3078\u306e\u5f71\u97ff\u3092\u8aac\u660e\u305b\u3088\u3002</p> <ol> <li>\u7f6e\u63db \\(u = 1+x^2\\) \u3068\u304a\u304f\u3068\u3001\\(du = 2x\\,dx\\) \u3068\u306a\u308b\u3002</li> <li>\\(x\\,dx = du\\) \u3068\u7f6e\u304d\u63db\u3048\u3001\\(\\int \\frac{x}{(1+x^2)^2} \\, dx = \\int \\frac{1}{u^2}\\,du\\)</li> <li>\\(\\int \\frac{1}{u^2}\\,du = -\\frac{1}{u}+C = -\\frac{1}{1+x^2}+C\\)</li> </ol>   **\u8a55\u4fa1\u3055\u308c\u308b\u80fd\u529b**: - \u8aa4\u308a\u306e\u8ad6\u7406\u7684\u691c\u51fa - \u6570\u5b66\u7684\u6279\u5224\u7684\u601d\u8003 - \u5f71\u97ff\u306e\u8a55\u4fa1\u80fd\u529b     **\u6559\u80b2\u306e\u7126\u70b9**: - \u7406\u89e3\u306e\u6df1\u3055 - \u6279\u5224\u7684\u601d\u8003 - \u691c\u8a3c\u30b9\u30ad\u30eb"},{"location":"lectures/OC/presentation/#_9","title":"\u4f8b\uff1a\u793e\u4f1a\u79d1\u5b66\u306e\u30ec\u30dd\u30fc\u30c8\u8ab2\u984c\u306e\u5909\u5316","text":"### \u5f93\u6765\u306e\u554f\u984c: \u5e74\u53ce103\u4e07\u5186\u306e\u58c1\u3068\u306f\u4f55\u304b\u8aac\u660e\u3057\u3001\u3053\u306e\u5e74\u53ce103\u4e07\u5186\u306e\u58c1\u306b\u3088\u308b\u554f\u984c\u70b9\u3092\u8ff0\u3079\u306a\u3055\u3044\u3002  **\u8a55\u4fa1\u3055\u308c\u308b\u80fd\u529b**: - \u77e5\u8b58\u306e\u8a18\u61b6\u3068\u518d\u73fe - \u5b9a\u578b\u7684\u306a\u8a18\u8ff0\u529b - \u57fa\u672c\u7684\u5206\u6790\u80fd\u529b     ### \u65b0\u6642\u4ee3\u306e\u554f\u984c: \u5e74\u53ce103\u4e07\u5186\u306e\u58c1\u306b\u3064\u3044\u3066\u8abf\u67fb\u3057\u305f\u30ec\u30dd\u30fc\u30c8\u3092\u8aad\u307f\u3001\u3053\u306e\u5185\u5bb9\u306e\u9069\u5207\u6027\u3092\u691c\u8a3c\u3057\u305f\u4e0a\u3067\u3001\u3042\u306a\u305f\u306e\u7acb\u5834\u3092\u660e\u78ba\u306b\u3057\u3066\u610f\u898b\u3092\u8ff0\u3079\u306a\u3055\u3044\u3002  **\u8a55\u4fa1\u3055\u308c\u308b\u80fd\u529b**: - \u60c5\u5831\u306e\u59a5\u5f53\u6027\u8a55\u4fa1 - \u30e1\u30bf\u5206\u6790\u80fd\u529b - \u72ec\u81ea\u8996\u70b9\u306e\u69cb\u7bc9\u529b     # 4. \u554f\u984c\u5b9a\u7fa9\u30fb\u89e3\u6c7a\u80fd\u529b\u306e\u8cea\u304c\u554f\u308f\u308c\u308b\u6642\u4ee3 ## \u5faa\u74b0\u578b\u554f\u984c\u89e3\u6c7a\u30d7\u30ed\u30bb\u30b9\u306e\u6642\u4ee3\u3078"},{"location":"lectures/OC/presentation/#_10","title":"\u8ab2\u984c\u89e3\u6c7a\u30d7\u30ed\u30bb\u30b9\u306e\u672c\u8cea\u7684\u5909\u5316","text":"### \u5f93\u6765\u306e\u8ab2\u984c\u89e3\u6c7a\u30d7\u30ed\u30bb\u30b9 1. \u5b9a\u7fa9\u3055\u308c\u305f\u554f\u984c\u3092\u7406\u89e3 2. \u89e3\u6cd5\u3092\u9069\u7528 3. \u89e3\u7b54\u3092\u63d0\u51fa 4. \u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3092\u5f85\u3064 5. *(\u6570\u65e5\u301c\u6570\u9031\u9593\u5f8c)* 6. \u6b21\u306e\u554f\u984c\u3078  **\u7279\u5fb4**: \u4e00\u65b9\u5411\u7684\u30fb\u4f4e\u983b\u5ea6\u30b5\u30a4\u30af\u30eb     ### \u751f\u6210AI\u6642\u4ee3\u306e\u8ab2\u984c\u89e3\u6c7a\u30d7\u30ed\u30bb\u30b9 1. \u554f\u984c\u306e\u672c\u8cea\u3092\u62bd\u51fa 2. AI\u304c\u89e3\u3051\u308b\u5f62\u306b\u554f\u984c\u3092\u7ffb\u8a33 3. AI\u89e3\u7b54\u3092\u5373\u6642\u53d6\u5f97 4. \u89e3\u7b54\u3092\u691c\u8a3c\u30fb\u8a55\u4fa1 5. \u554f\u984c\u5b9a\u7fa9\u3092\u6d17\u7df4 6. *(\u6570\u5206\u301c\u6570\u6642\u9593\u3067)* 7. \u7e70\u308a\u8fd4\u3057\u6539\u5584  **\u7279\u5fb4**: \u5faa\u74b0\u7684\u30fb\u9ad8\u983b\u5ea6\u30b5\u30a4\u30af\u30eb"},{"location":"lectures/OC/presentation/#_11","title":"\u65b0\u6642\u4ee3\u306b\u91cd\u8981\u3068\u306a\u308b\u30b9\u30ad\u30eb\u30bb\u30c3\u30c8","text":"### \u751f\u6210AI\u3068\u306e\u5bfe\u8a71\u80fd\u529b - \u30d7\u30ed\u30f3\u30d7\u30c8\u8a2d\u8a08\u529b - \u52b9\u679c\u7684\u306a\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af - \u554f\u984c\u306e\u9069\u5207\u306a\u5206\u89e3\u80fd\u529b  ### \u89e3\u7b54\u306e\u7cbe\u67fb\u80fd\u529b - \u6279\u5224\u7684\u601d\u8003 - \u30c9\u30e1\u30a4\u30f3\u77e5\u8b58\u306b\u57fa\u3065\u304f\u691c\u8a3c - \u30a8\u30c3\u30b8\u30b1\u30fc\u30b9\u306e\u7279\u5b9a     ### \u554f\u984c\u306e\u8a00\u3044\u63db\u3048\u30fb\u7ffb\u8a33\u80fd\u529b - \u66d6\u6627\u3055\u306e\u9664\u53bb - \u6570\u5b66\u7684\u30fb\u8ad6\u7406\u7684\u5b9a\u5f0f\u5316 - \u89e3\u50cf\u5ea6\u306e\u9ad8\u3044\u554f\u984c\u8a2d\u5b9a  ### \u30e1\u30bf\u8a8d\u77e5\u80fd\u529b - \u81ea\u5df1\u306e\u77e5\u8b58\u5883\u754c\u306e\u8a8d\u8b58 - \u554f\u984c\u30a2\u30d7\u30ed\u30fc\u30c1\u306e\u6226\u7565\u9078\u629e - \u691c\u8a3c\u624b\u6cd5\u306e\u6700\u9069\u5316"},{"location":"lectures/OC/presentation/#12_1","title":"\u554f\u984c\u89e3\u50cf\u5ea6\u3092\u9ad8\u3081\u308b\u4f8b\uff1a\u66d6\u6627\u306a\u554f\u984c\u306e\u7ffb\u8a33 (1/2)","text":""},{"location":"lectures/OC/presentation/#_12","title":"\u66d6\u6627\u306a\u554f\u984c\u63d0\u8d77:","text":"<p>\u65e2\u5b58\u306e\u753b\u50cf\u3092\u6c34\u5897\u3057\u3059\u308bA\u3068\u3044\u3046\u65b9\u6cd5\u3092\u4f5c\u308a\u3001\u305d\u306e\u6c34\u5897\u3057\u30c7\u30fc\u30bf\u3067\u5225\u30c7\u30fc\u30bfB\u3068\u6bd4\u8f03\u3067\u304d\u308b\u304b\uff1f</p>   **\u554f\u984c\u70b9**: - \u300c\u6c34\u5897\u3057\u300d\u306e\u5b9a\u7fa9\u304c\u4e0d\u660e\u78ba - \u6bd4\u8f03\u306e\u76ee\u7684\u304c\u4e0d\u660e - \u7d71\u8a08\u7684\u67a0\u7d44\u307f\u306e\u6b20\u5982 - \u8a55\u4fa1\u57fa\u6e96\u306e\u4e0d\u5728     **AI\u3078\u306e\u6307\u793a\u3068\u3057\u3066\u4e0d\u9069\u5207\u306a\u7406\u7531**: - \u591a\u7fa9\u7684\u306a\u89e3\u91c8\u304c\u53ef\u80fd - \u6570\u7406\u7684\u5b9a\u5f0f\u5316\u304c\u306a\u3044 - \u89e3\u304f\u3079\u304d\u554f\u984c\u306e\u5883\u754c\u304c\u4e0d\u660e\u78ba"},{"location":"lectures/OC/presentation/#22_2","title":"\u554f\u984c\u89e3\u50cf\u5ea6\u3092\u9ad8\u3081\u308b\u4f8b\uff1a\u66d6\u6627\u306a\u554f\u984c\u306e\u7ffb\u8a33 (2/2)","text":""},{"location":"lectures/OC/presentation/#_13","title":"\u9ad8\u89e3\u50cf\u5ea6\u306e\u554f\u984c\u5b9a\u7fa9:","text":"<p>\u753b\u50cf\u306e\u80cc\u5f8c\u306b\u3042\u308b\u751f\u6210\u904e\u7a0b\u3092 \\(f(X)\\) \u3068\u3059\u308b\u3002\u3053\u306e \\(f(X)\\) \u3092\u3001\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 D \u3092\u7528\u3044\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08VAE, GAN, U-net, Diffusion\u30e2\u30c7\u30eb\uff09\u3067\u5b66\u7fd2\u3057\u305f\u3082\u306e\u3092 \\(\\hat{f}(z|D)\\) \u3068\u3059\u308b\u3002\u3053\u306e\u3068\u304d\u3001\\(\\hat{f}(z|D)\\) \u3092\u7528\u3044\u3066\u751f\u6210\u3057\u305f\u753b\u50cf\u3092 \\(D'\\) \u3068\u3057\u3066\u3001\u65b0\u305f\u306a\u30c7\u30fc\u30bf \\(B\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u6642\u3001\\(D'\\) \u3068 \\(B\\) \u3092\u7d71\u8a08\u7684\u4eee\u8aac\u691c\u5b9a\u3067\u5dee\u306e\u691c\u5b9a\u3092\u884c\u3046\u3068\u3001\u6709\u52b9\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u3001Type I Error, Type II Error \u306e\u89b3\u70b9\u304b\u3089\u59a5\u5f53\u306a\u691c\u5b9a\u3068\u306a\u308a\u3046\u308b\u304b\uff1f</p>  \ud83d\udca1 **\u5b9f\u8df5**: \u3042\u306a\u305f\u306e\u7814\u7a76\u5206\u91ce\u3067\u66d6\u6627\u306a\u554f\u984c\u30921\u3064\u6319\u3052\u3001\u9ad8\u89e3\u50cf\u5ea6\u306b\u7ffb\u8a33\u3057\u3066\u307f\u307e\u3057\u3087\u3046"},{"location":"lectures/OC/presentation/#_14","title":"\u82f1\u8a9e\u529b\u306e\u5fc5\u8981\u6027\u306e\u518d\u5b9a\u7fa9","text":"### \u76f8\u5bfe\u7684\u91cd\u8981\u5ea6\u304c\u4f4e\u4e0b\u3059\u308b\u9762 - \u5358\u7d14\u306a\u82f1\u8a9e\u306e\u6587\u7ae0\u4f5c\u6210\uff08Writing\uff09 - \u4e00\u822c\u7684\u306a\u4f1a\u8a71\uff08Speaking\uff09 - \u57fa\u672c\u7684\u306a\u30ea\u30b9\u30cb\u30f3\u30b0  ### \u3088\u308a\u91cd\u8981\u306b\u306a\u308b\u9762 - \u82f1\u6587\u6cd5\uff08Grammar\uff09\u306e\u7406\u89e3 - \u82f1\u8a9e\u8aad\u89e3\u529b\uff08Reading\uff09 - \u5c02\u9580\u9818\u57df\u306b\u304a\u3051\u308b\u82f1\u8a9e\u8868\u73fe - AI\u51fa\u529b\u82f1\u8a9e\u306e\u691c\u8a3c\u80fd\u529b     ![bg 80%](https://via.placeholder.com/400x300/e8eaf6/3949ab?text=English+Skills+Shift)   \\[ \\text{\u5fc5\u8981\u306a\u82f1\u8a9e\u529b} = \\text{\u5c02\u9580\u7528\u8a9e\u7406\u89e3} + \\text{\u69cb\u6587\u628a\u63e1} + \\text{AI\u51fa\u529b\u691c\u8a3c} \\]   # 5. \u751f\u6210AI\u6642\u4ee3\u306b\u5927\u5b66\u3067\u5b66\u3076\u610f\u7fa9 ## \u5b66\u3073\u306e\u672c\u8cea\u7684\u4fa1\u5024\u306e\u518d\u5b9a\u7fa9"},{"location":"lectures/OC/presentation/#_15","title":"\u5927\u5b66\u6559\u80b2\u306e\u672c\u8cea\u7684\u4fa1\u5024\u306e\u5909\u5316","text":"### \u554f\u984c\u306e\u89e3\u50cf\u5ea6\u3092\u9ad8\u3081\u308b\u30c4\u30fc\u30eb\u7372\u5f97 - \u5c02\u9580\u9818\u57df\u306e\u4f53\u7cfb\u7684\u77e5\u8b58 - \u30c9\u30e1\u30a4\u30f3\u56fa\u6709\u306e\u601d\u8003\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af - \u8907\u96d1\u6027\u3092\u6271\u3046\u6982\u5ff5\u7684\u30c4\u30fc\u30eb - \u554f\u984c\u5206\u89e3\u306e\u65b9\u6cd5\u8ad6     ### \u89e3\u7b54\u306e\u7cbe\u67fb\u80fd\u529b\u3092\u9ad8\u3081\u308b\u30c4\u30fc\u30eb\u7372\u5f97 - \u6279\u5224\u7684\u601d\u8003\u6cd5\u306e\u8a13\u7df4 - \u691c\u8a3c\u306e\u305f\u3081\u306e\u30e1\u30bd\u30c9\u30ed\u30b8\u30fc - \u30a8\u30e9\u30fc\u691c\u51fa\u30d1\u30bf\u30fc\u30f3\u306e\u7fd2\u5f97 - \u54c1\u8cea\u8a55\u4fa1\u306e\u5224\u65ad\u57fa\u6e96    \ud83d\udca1 **\u8a0e\u8ad6**: \u5927\u5b66\u6559\u80b2\u306f\u751f\u6210AI\u6642\u4ee3\u306b\u3069\u306e\u3088\u3046\u306b\u5909\u9769\u3059\u3079\u304d\u3067\u3057\u3087\u3046\u304b\uff1f"},{"location":"lectures/OC/presentation/#ai_4","title":"\u307e\u3068\u3081\uff1a\u751f\u6210AI\u6642\u4ee3\u306e\u5b66\u3073\u306e\u65b9\u5411\u6027","text":"### \u77e5\u8b58\u306e\u4fa1\u5024\u306e\u518d\u5b9a\u7fa9 - \u5358\u306a\u308b\u6697\u8a18\u304b\u3089\u6279\u5224\u7684\u8a55\u4fa1\u3078 - \u77e5\u8b58\u306f\u691c\u8a3c\u30c4\u30fc\u30eb\u3068\u3057\u3066\u91cd\u8981\u6027\u5897\u5927  ### \u5b66\u3073\u306e\u76ee\u7684\u306e\u5909\u5316 - \u89e3\u6cd5\u306e\u7fd2\u5f97\u304b\u3089\u554f\u984c\u5b9a\u7fa9\u80fd\u529b\u3078 - \u7b54\u3048\u306e\u518d\u73fe\u304b\u3089\u601d\u8003\u30d7\u30ed\u30bb\u30b9\u306e\u7fd2\u5f97\u3078     ### \u4eba-AI\u5354\u50cd\u306e\u9ad8\u901f\u30eb\u30fc\u30d7\u7fd2\u5f97 - \u554f\u984c\u767a\u898b\u2192AI\u89e3\u6c7a\u2192\u691c\u8a3c\u2192\u554f\u984c\u7cbe\u7dfb\u5316 - \u3053\u306e\u5faa\u74b0\u306e\u8cea\u3092\u9ad8\u3081\u308b\u57fa\u76e4\u77e5\u8b58 - \u8a66\u884c\u932f\u8aa4\u306e\u52b9\u7387\u5316  ### \u30e1\u30bf\u30b9\u30ad\u30eb\u306e\u91cd\u8981\u6027 - \u5b66\u3073\u65b9\u3092\u5b66\u3076\u529b - AI\u3068\u306e\u5354\u50cd\u65b9\u6cd5\u306e\u6700\u9069\u5316 - \u6279\u5224\u7684\u601d\u8003\u306e\u4f53\u7cfb\u7684\u7fd2\u5f97"},{"location":"lectures/OC/presentation/#ai_5","title":"\u6700\u7d42\u30e1\u30c3\u30bb\u30fc\u30b8\uff1a\u751f\u6210AI\u3068\u5171\u306b\u6210\u9577\u3059\u308b","text":""},{"location":"lectures/OC/presentation/#_16","title":"\u5909\u308f\u3089\u306a\u3044\u672c\u8cea","text":"<ul> <li>\u4eba\u9593\u306e\u6d1e\u5bdf\u3068\u5224\u65ad\u306e\u91cd\u8981\u6027</li> <li>\u554f\u984c\u3092\u898b\u3064\u3051\u308b\u5275\u9020\u6027\u306e\u4fa1\u5024</li> <li>\u691c\u8a3c\u3068\u8cac\u4efb\u306e\u6240\u5728</li> </ul>"},{"location":"lectures/OC/presentation/#_17","title":"\u3053\u308c\u304b\u3089\u306e\u6642\u4ee3\u306e\u6210\u529f\u8981\u56e0","text":"<ul> <li>AI\u3092\u4f7f\u3044\u3053\u306a\u3059\u601d\u8003\u6cd5\u306e\u7fd2\u5f97</li> <li>\u9ad8\u901f\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u30eb\u30fc\u30d7\u306e\u69cb\u7bc9</li> <li>\u6279\u5224\u7684\u601d\u8003\u306b\u3088\u308b\u8cea\u306e\u62c5\u4fdd</li> </ul>"},{"location":"lectures/OC/presentation/#_18","title":"\u3054\u6e05\u8074\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f","text":""},{"location":"lectures/OC/presentation/#_19","title":"\u53c2\u8003\u6587\u732e\u30fb\u30ea\u30bd\u30fc\u30b9","text":"<ul> <li>\"Attention is all you need\" (Vaswani et al., 2017)</li> <li>\"\u89e3\u50cf\u5ea6\u3092\u9ad8\u3081\u308b\" (https://speakerdeck.com/tumada/jie-xiang-du-wogao-meru)</li> <li>\"SWE-bench: \u751f\u6210AI\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u80fd\u529b\u8a55\u4fa1\" (https://qiita.com/tosenbo/items/57ed6ded19da2b24d900)</li> <li>\"Reinforcement Learning from Human Feedback: A Review\" (Casper et al., 2023)</li> <li>\"The AI Revolution in Scientific Discovery\" (Nature, 2024)</li> </ul>"},{"location":"lectures/SIWS/","title":"Index","text":""},{"location":"lectures/SIWS/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> \\[ f(x) = \\int_{-\\infty}^\\infty     \\hat f(\\xi)\\,e^{2 \\pi i \\xi x}     \\,d\\xi \\] <p>inline tex codes \\(E=mc^2\\).</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]"},{"location":"lectures/SIWS/01-getting-started/","title":"Python\u3067\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3092\u59cb\u3081\u3088\u3046","text":"<p>Python\u3092\u4f7f\u3063\u3066\u30c7\u30fc\u30bf\u3092\u63a2\u7d22\u3059\u308b\u524d\u306b\u3001\u307e\u305a\u7406\u89e3\u3057\u3066\u304a\u304f\u3079\u304d\u91cd\u8981\u306a\u6982\u5ff5\u304c\u3042\u308a\u307e\u3059\u3002</p> <ol> <li>Python\u3068Google Colaboratory\u3068\u306f\u4f55\u304b\uff1f</li> <li>Python\u306e\u30b3\u30fc\u30c9\u306e\u66f8\u304d\u65b9</li> <li>Python\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u306f\uff1f</li> </ol> <p>\u3053\u308c\u3089\u306e\u6982\u5ff5\u306b\u3064\u3044\u3066\u3001\u6b21\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u8a73\u3057\u304f\u8aac\u660e\u3057\u307e\u3059\uff08\u30bb\u30af\u30b7\u30e7\u30f3aaa\uff09\u3002\u3059\u3067\u306b\u3042\u308b\u7a0b\u5ea6\u306e\u77e5\u8b58\u304c\u3042\u308b\u5834\u5408\u306f\u3001\u30bb\u30af\u30b7\u30e7\u30f3aaa \u306b\u9032\u3093\u3067\u3001\u6700\u521d\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7d39\u4ecb\u3059\u308b\u90e8\u5206\u304b\u3089\u8aad\u307f\u59cb\u3081\u3066\u3082\u69cb\u3044\u307e\u305b\u3093\u3002\u672c\u66f8\u3067\u306f\u30012013\u5e74\u306b\u30cb\u30e5\u30fc\u30e8\u30fc\u30af\u5e02\uff08NYC\uff09\u306e\u4e3b\u89813\u7a7a\u6e2f\u304b\u3089\u51fa\u767a\u3057\u305f\u3059\u3079\u3066\u306e\u56fd\u5185\u7dda\u30d5\u30e9\u30a4\u30c8\u306e\u30c7\u30fc\u30bf \u3092\u6271\u3044\u3001\u4ee5\u964d\u306e\u7ae0\u3067\u8a73\u3057\u304f\u5206\u6790\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#python-google-colaboratory","title":"Python \u3068 Google Colaboratory\u3068\u306f\uff1f","text":"<p>\u672c\u66f8\u3067\u306f\u3001Python\u3092Google Colaboratory\uff08\u4ee5\u4e0b\u3001Google Colab\uff09 \u3092\u901a\u3058\u3066\u4f7f\u7528\u3059\u308b\u3053\u3068\u3092\u524d\u63d0\u3068\u3057\u307e\u3059\u3002\u521d\u3081\u3066\u306e\u65b9\u306e\u4e2d\u306b\u306f\u3001Python\u3068Google Colab\u306e\u9055\u3044\u304c\u308f\u304b\u308a\u306b\u304f\u3044\u3068\u611f\u3058\u308b\u3053\u3068\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3053\u308c\u3092\u308f\u304b\u308a\u3084\u3059\u304f\u8aac\u660e\u3059\u308b\u3068\u3001Python\u306f\u8eca\u306e\u30a8\u30f3\u30b8\u30f3\u3001Google Colab\u306f\u8eca\u306e\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9 \u306e\u3088\u3046\u306a\u95a2\u4fc2\u306b\u3042\u308a\u307e\u3059\uff08\u56f3\u3092\u53c2\u7167\uff09\u3002</p> <p>Note</p> <p>\u3053\u3053\u306b\u30a4\u30e1\u30fc\u30b8\u56f3\u3092\u633f\u5165</p> <p>\u3082\u3046\u5c11\u3057\u8a73\u3057\u304f\u8aac\u660e\u3059\u308b\u3068\u3001Python\u306f\u8a08\u7b97\u3092\u5b9f\u884c\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e \u3067\u3042\u308a\u3001\u4e00\u65b9\u3067Google Colab\u306f\u3001Google\u793e\u304c\u63d0\u4f9b\u3059\u308b\u6a5f\u68b0\u5b66\u7fd2\u5411\u3051\u306e\u7d71\u5408\u958b\u767a\u74b0\u5883\uff08IDE\uff09 \u3067\u3059\u3002Google Colab\u306b\u306f\u3001Python\u3092\u4fbf\u5229\u306b\u4f7f\u3046\u305f\u3081\u306e\u3055\u307e\u3056\u307e\u306a\u6a5f\u80fd\u3084\u30c4\u30fc\u30eb\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>\u3053\u308c\u306f\u3001\u904b\u8ee2\u3059\u308b\u3068\u304d\u306b\u30b9\u30d4\u30fc\u30c9\u30e1\u30fc\u30bf\u30fc\u3084\u30d0\u30c3\u30af\u30df\u30e9\u30fc\u3001\u30ca\u30d3\u30b2\u30fc\u30b7\u30e7\u30f3\u30b7\u30b9\u30c6\u30e0\u304c\u3042\u308b\u3053\u3068\u3067\u5b89\u5168\u304b\u3064\u30b9\u30e0\u30fc\u30ba\u306b\u904b\u8ee2\u3067\u304d\u308b\u306e\u3068\u540c\u3058\u3088\u3046\u306b\u3001Google Colab\u3092\u4f7f\u3046\u3053\u3068\u3067Python\u3092\u3088\u308a\u76f4\u611f\u7684\u304b\u3064\u52b9\u7387\u7684\u306b\u64cd\u4f5c\u3067\u304d\u308b \u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#google-colaboratory-python","title":"Google Colaboratory \u3067 Python \u3092\u59cb\u3081\u308b","text":"<p>Google Colaboratory\uff08Google Colab\uff09 \u306f\u3001Google \u304c\u63d0\u4f9b\u3059\u308b\u7121\u6599\u306e\u30af\u30e9\u30a6\u30c9\u74b0\u5883\u3067\u3001Python \u3092\u30d6\u30e9\u30a6\u30b6\u4e0a\u3067\u5b9f\u884c\u3067\u304d\u308b\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3067\u3059\u3002\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u4e0d\u8981\u3067\u4f7f\u3048\u308b\u305f\u3081\u3001\u521d\u5fc3\u8005\u3067\u3082\u7c21\u5358\u306b Python \u3092\u59cb\u3081\u3089\u308c\u307e\u3059\u3002</p> <ol> <li>Google Colab \u3092\u4f7f\u3046\u6e96\u5099</li> </ol> <p>Google Colab \u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001Google \u30a2\u30ab\u30a6\u30f3\u30c8\u304c\u5fc5\u8981\u3067\u3059\u3002\u4ee5\u4e0b\u306e\u624b\u9806\u306b\u5f93\u3063\u3066\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3092\u884c\u3044\u307e\u3057\u3087\u3046\u3002</p> <ol> <li> <p>Google Colab \u306b\u30a2\u30af\u30bb\u30b9</p> </li> <li> <p>Google Colaboratory \u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3059\u3002</p> </li> <li> <p>Google \u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u30ed\u30b0\u30a4\u30f3\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u65b0\u3057\u3044\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u4f5c\u6210</p> </li> <li> <p>Google Colab \u306e\u30db\u30fc\u30e0\u753b\u9762\u3067 \u300c\u65b0\u3057\u3044\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u300d \u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u65b0\u3057\u3044 Python \u306e Jupyter Notebook \u304c\u958b\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u7c21\u5358\u306a Python \u30b3\u30fc\u30c9\u3092\u5b9f\u884c</p> </li> <li> <p>\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306e\u30bb\u30eb\u306b\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5165\u529b\u3057\u307e\u3059\uff1a   <pre><code>print(\"Hello, Google Colab!\")\n</code></pre></p> </li> <li>Shift + Enter \u3092\u62bc\u3059\u304b\u3001\u30bb\u30eb\u306e\u5de6\u5074\u306b\u3042\u308b\u518d\u751f\u30dc\u30bf\u30f3 \u25b6 \u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u5b9f\u884c\u3057\u307e\u3059\u3002</li> <li>\u51fa\u529b\u3068\u3057\u3066 <code>Hello, Google Colab!</code> \u3068\u8868\u793a\u3055\u308c\u308c\u3070\u6210\u529f\u3067\u3059\u3002</li> </ol>"},{"location":"lectures/SIWS/01-getting-started/#google-colaboratory-python_1","title":"Google Colaboratory \u3067 Python \u3092\u4f7f\u3046","text":"<p>\u4ee5\u524d\u306e\u8eca\u306e\u30a2\u30ca\u30ed\u30b8\u30fc\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u79c1\u305f\u3061\u306f\u30a8\u30f3\u30b8\u30f3\u3092\u76f4\u63a5\u64cd\u4f5c\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u4e0a\u306e\u8981\u7d20\u3092\u4f7f\u3063\u3066\u8eca\u3092\u904b\u8ee2\u3057\u307e\u3059\u3002\u540c\u69d8\u306b\u3001Python \u3092\u76f4\u63a5\u64cd\u4f5c\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001Google Colaboratory\uff08Google Colab\uff09 \u3092\u4f7f\u7528\u3057\u3066 Python \u3092\u5b9f\u884c\u3057\u307e\u3059\u3002</p> <p>Google Colab \u306f\u3001Google \u304c\u63d0\u4f9b\u3059\u308b\u30af\u30e9\u30a6\u30c9\u30d9\u30fc\u30b9\u306e Jupyter Notebook \u74b0\u5883\u3067\u3059\u3002\u3053\u308c\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u3001Python \u306e\u74b0\u5883\u69cb\u7bc9\u3092\u884c\u3046\u3053\u3068\u306a\u304f\u3001\u30d6\u30e9\u30a6\u30b6\u4e0a\u3067\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u3001\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>Google Colab \u3092\u958b\u304f\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p> <p>Note</p> <p>\u3053\u3053\u306b Google Colab \u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306e\u753b\u50cf\u3092\u633f\u5165</p> <p>Google Colab \u306e\u753b\u9762\u306f\u3001\u5927\u304d\u304f\u5206\u3051\u3066 3 \u3064\u306e\u90e8\u5206\u306b\u5206\u304b\u308c\u3066\u3044\u307e\u3059\u3002</p> <ol> <li>\u30b3\u30fc\u30c9\u30bb\u30eb: \u3053\u3053\u306b Python \u30b3\u30fc\u30c9\u3092\u8a18\u8ff0\u3057\u3001\u5b9f\u884c\u3057\u307e\u3059\u3002</li> <li>\u51fa\u529b\u30a8\u30ea\u30a2: \u30b3\u30fc\u30c9\u306e\u5b9f\u884c\u7d50\u679c\u304c\u8868\u793a\u3055\u308c\u308b\u5834\u6240\u3067\u3059\u3002</li> <li>\u30d5\u30a1\u30a4\u30eb\u7ba1\u7406\u30a8\u30ea\u30a2: Google \u30c9\u30e9\u30a4\u30d6\u3084\u30ed\u30fc\u30ab\u30eb\u30d5\u30a1\u30a4\u30eb\u3092\u7ba1\u7406\u3067\u304d\u307e\u3059\u3002</li> </ol>"},{"location":"lectures/SIWS/01-getting-started/#python_1","title":"Python \u306e\u57fa\u672c\u7684\u306a\u66f8\u304d\u65b9","text":"<p>Python \u3092\u4f7f\u3044\u59cb\u3081\u308b\u3068\u3001\u300cPython \u306f\u3069\u3046\u3084\u3063\u3066\u4f7f\u3046\u306e\uff1f\u300d\u3068\u3044\u3046\u7591\u554f\u304c\u6d6e\u304b\u3076\u3067\u3057\u3087\u3046\u3002Python \u306f \u30a4\u30f3\u30bf\u30d7\u30ea\u30bf\u578b\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e \u3067\u3042\u308a\u3001Excel \u3084 SPSS \u306e\u3088\u3046\u306a \u30dd\u30a4\u30f3\u30c8 &amp; \u30af\u30ea\u30c3\u30af \u64cd\u4f5c\u3067\u306f\u306a\u304f\u3001\u30b3\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u5b9f\u884c\u3059\u308b \u3053\u3068\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002</p> <p>Python \u3092\u4f7f\u3046\u306b\u306f\u3001\u57fa\u672c\u7684\u306a\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u672c\u66f8\u306f\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306b\u7279\u5316\u3057\u305f\u66f8\u7c4d\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u30c7\u30fc\u30bf\u3092\u63a2\u7d22\u30fb\u5206\u6790\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u6700\u4f4e\u9650\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u77e5\u8b58\u3092\u5b66\u3093\u3067\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_1","title":"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u57fa\u790e\u3068\u6982\u5ff5","text":"<p>\u57fa\u672c\u7684\u306a\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u6982\u5ff5\u3068\u7528\u8a9e\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002\u3059\u3079\u3066\u3092\u6697\u8a18\u3059\u308b\u5fc5\u8981\u306f\u306a\u304f\u3001\u300c\u5b9f\u969b\u306b\u3084\u308a\u306a\u304c\u3089\u5b66\u3076\u300d\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\u3002\u3053\u306e\u30ac\u30a4\u30c9\u3067\u306f\u3001\u901a\u5e38\u306e\u6587\u7ae0\u3068 <code>computer_code</code> \u3092\u533a\u5225\u3059\u308b\u305f\u3081\u306b\u7570\u306a\u308b\u30d5\u30a9\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p> <p>\u5b66\u7fd2\u3092\u9032\u3081\u308b\u4e0a\u3067\u3001Python \u3068 Google Colaboratory \u3092\u6d3b\u7528\u3057\u306a\u304c\u3089\u3001\u7e70\u308a\u8fd4\u3057\u7df4\u7fd2\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_2","title":"\u57fa\u672c\u6982\u5ff5","text":"<ul> <li>\u30b3\u30fc\u30c9\u30bb\u30eb (Code Cell): Google Colaboratory \u3067\u30b3\u30fc\u30c9\u3092\u5165\u529b\u3057\u3001\u5b9f\u884c\u3059\u308b\u5834\u6240\u3002</li> <li>\u30b3\u30fc\u30c9\u306e\u5b9f\u884c: Python \u306b\u547d\u4ee4\u3092\u4e0e\u3048\u3001\u5b9f\u969b\u306b\u51e6\u7406\u3092\u884c\u308f\u305b\u308b\u3053\u3068\u3002</li> <li>\u5909\u6570 (Variables): \u5024\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002\u5909\u6570\u306b\u5024\u3092 \u4ee3\u5165 \u3057\u3001\u305d\u306e\u5185\u5bb9\u3092\u8868\u793a\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002</li> <li>\u30c7\u30fc\u30bf\u578b (Data Types): <code>int</code> (\u6574\u6570), <code>float</code> (\u6d6e\u52d5\u5c0f\u6570\u70b9\u6570), <code>bool</code> (\u8ad6\u7406\u578b), <code>str</code> (\u6587\u5b57\u5217) \u306a\u3069\u3002</li> <li>\u6574\u6570 (<code>int</code>): <code>-1, 0, 2, 4092</code> \u306a\u3069\u3002</li> <li>\u6d6e\u52d5\u5c0f\u6570\u70b9\u6570 (<code>float</code>): <code>-24.932, 0.8</code> \u306a\u3069\u3002</li> <li>\u8ad6\u7406\u578b (<code>bool</code>): <code>True</code> \u307e\u305f\u306f <code>False</code>\u3002</li> <li>\u6587\u5b57\u5217 (<code>str</code>): <code>\"cabbage\"</code>, <code>\"Hamilton\"</code>, <code>\"This ramen is delicious.\"</code> \u306a\u3069\u3002</li> </ul>"},{"location":"lectures/SIWS/01-getting-started/#list","title":"\u30ea\u30b9\u30c8 (List)","text":"<p>\u8907\u6570\u306e\u5024\u3092\u307e\u3068\u3081\u305f\u30c7\u30fc\u30bf\u69cb\u9020\u3067\u3001<code>[]</code> \u3092\u4f7f\u3063\u3066\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <pre><code>numbers = [6, 11, 13, 31, 90, 92]\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#dataframe","title":"\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0 (DataFrame)","text":"<p>\u8868\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u3067\u3042\u308a\u3001<code>pandas</code> \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3057\u3066\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <pre><code>import pandas as pd\n\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': [25, 30, 35],\n        'City': ['New York', 'Los Angeles', 'Chicago']}\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#conditionals","title":"\u6761\u4ef6\u5206\u5c90 (Conditionals)","text":"<ul> <li><code>==</code> \u3092\u4f7f\u3063\u3066\u7b49\u4fa1\u6027\u3092\u6bd4\u8f03 (<code>=</code> \u306f\u4ee3\u5165\u306e\u305f\u3081\u306b\u4f7f\u7528)\u3002   <pre><code>print(2 + 1 == 3)  # True\n</code></pre></li> <li>\u30d6\u30fc\u30eb\u6f14\u7b97: <code>True</code> / <code>False</code> \u306e\u8a55\u4fa1\u3002   <pre><code>print(4 + 2 &gt;= 3)  # True\nprint(3 + 5 &lt;= 1)  # False\n</code></pre></li> <li>\u8ad6\u7406\u6f14\u7b97\u5b50: <code>and</code> (\u304b\u3064), <code>or</code> (\u307e\u305f\u306f)\u3002   <pre><code>print((2 + 1 == 3) and (2 + 1 == 4))  # False\nprint((2 + 1 == 3) or (2 + 1 == 4))   # True\n</code></pre></li> </ul>"},{"location":"lectures/SIWS/01-getting-started/#functions","title":"\u95a2\u6570 (Functions)","text":"<p>\u95a2\u6570\u306f\u7279\u5b9a\u306e\u30bf\u30b9\u30af\u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306e\u3082\u306e\u3067\u3059\u3002Python \u3067\u306f <code>def</code> \u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u95a2\u6570\u3092\u5b9a\u7fa9\u3067\u304d\u307e\u3059\u3002</p> <pre><code>def greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Alice\"))\n</code></pre> <p>\u307e\u305f\u3001\u7d44\u307f\u8fbc\u307f\u95a2\u6570\u306e <code>range()</code> \u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u6570\u5024\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002</p> <pre><code>list(range(2, 6))  # [2, 3, 4, 5]\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#messages","title":"\u30a8\u30e9\u30fc\u3001\u8b66\u544a\u3001\u30e1\u30c3\u30bb\u30fc\u30b8 {#messages}","text":"<p>Google Colaboratory \u3067 Python \u3092\u4f7f\u3046\u969b\u3001\u30a8\u30e9\u30fc\u3001\u8b66\u544a\u3001\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u8868\u793a\u3055\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u306f\u8d64\u5b57\u3067\u8868\u793a\u3055\u308c\u308b\u305f\u3081\u3001\u6700\u521d\u306f\u6238\u60d1\u3046\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u51b7\u9759\u306b\u5bfe\u5fdc\u3059\u308c\u3070\u554f\u984c\u3042\u308a\u307e\u305b\u3093\u3002</p> <p>Python \u3067\u8868\u793a\u3055\u308c\u308b\u8d64\u5b57\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u306b\u306f\u4ee5\u4e0b\u306e\u7a2e\u985e\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#errors","title":"\u30a8\u30e9\u30fc (Errors)","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u6b63\u5e38\u306b\u52d5\u4f5c\u3057\u306a\u3044\u5834\u5408\u306b\u767a\u751f\u3057\u3001\u30b3\u30fc\u30c9\u306e\u5b9f\u884c\u304c\u505c\u6b62\u3057\u307e\u3059\u3002</p> <pre><code>print(1 / 0)  # ZeroDivisionError: division by zero\n</code></pre> <p>\u3053\u306e\u5834\u5408\u3001<code>0</code> \u3067\u306e\u5272\u308a\u7b97\u304c\u8a31\u53ef\u3055\u308c\u3066\u3044\u306a\u3044\u305f\u3081\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#warnings","title":"\u8b66\u544a (Warnings)","text":"<p>\u30b3\u30fc\u30c9\u306e\u5b9f\u884c\u306f\u7d99\u7d9a\u3067\u304d\u308b\u3082\u306e\u306e\u3001\u6ce8\u610f\u3059\u3079\u304d\u3053\u3068\u304c\u3042\u308b\u5834\u5408\u306b\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p> <pre><code>import warnings\nwarnings.warn(\"This is a warning message!\")\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#messages_1","title":"\u30e1\u30c3\u30bb\u30fc\u30b8 (Messages)","text":"<p>\u901a\u5e38\u306e\u8a3a\u65ad\u60c5\u5831\u3084\u51e6\u7406\u7d50\u679c\u304c\u8868\u793a\u3055\u308c\u308b\u5834\u5408\u3002</p> <pre><code>print(\"Data loaded successfully!\")\n</code></pre> <p>\u30a8\u30e9\u30fc\u3084\u8b66\u544a\u304c\u8868\u793a\u3055\u308c\u3066\u3082\u7126\u3089\u305a\u3001</p> <ul> <li>\u30a8\u30e9\u30fc: \u30b3\u30fc\u30c9\u304c\u5b9f\u884c\u3067\u304d\u306a\u3044\u306e\u3067\u4fee\u6b63\u304c\u5fc5\u8981 \u2192 \u8d64\u4fe1\u53f7: \u505c\u6b62\u3057\u3066\u554f\u984c\u3092\u78ba\u8a8d</li> <li>\u8b66\u544a: \u5b9f\u884c\u3067\u304d\u308b\u304c\u6ce8\u610f\u304c\u5fc5\u8981 \u2192 \u9ec4\u8272\u4fe1\u53f7: \u6ce8\u610f\u3057\u306a\u304c\u3089\u9032\u884c</li> <li>\u30e1\u30c3\u30bb\u30fc\u30b8: \u5358\u306a\u308b\u60c5\u5831\u63d0\u4f9b \u2192 \u7dd1\u4fe1\u53f7: \u554f\u984c\u306a\u3057</li> </ul> <p>\u3068\u8003\u3048\u308b\u3068\u3088\u3044\u3067\u3057\u3087\u3046\u3002</p> <p>Python \u3068 Google Colaboratory \u3092\u6d3b\u7528\u3057\u306a\u304c\u3089\u3001\u7e70\u308a\u8fd4\u3057\u7df4\u7fd2\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff01</p>"},{"location":"lectures/SIWS/01-getting-started/#_3","title":"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u5b66\u3076\u4e0a\u3067\u306e\u5fc3\u5f97","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u5b66\u3076\u3053\u3068\u306f\u3001\u5916\u56fd\u8a9e\u3092\u5b66\u3076\u3053\u3068\u306b\u4f3c\u3066\u3044\u307e\u3059\u3002\u6700\u521d\u306f\u96e3\u3057\u304f\u3001\u632b\u6298\u3059\u308b\u3053\u3068\u3082\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u3001\u9593\u9055\u3044\u3092\u6050\u308c\u305a\u306b\u52aa\u529b\u3092\u7d9a\u3051\u308c\u3070\u3001\u8ab0\u3067\u3082\u5b66\u3073\u3001\u4e0a\u9054\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u5b66\u3076\u969b\u306b\u5f79\u7acb\u3064\u3044\u304f\u3064\u304b\u306e\u30dd\u30a4\u30f3\u30c8\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p> <ul> <li> <p>\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u306f\u5b9f\u306f\u305d\u308c\u307b\u3069\u8ce2\u304f\u306a\u3044: \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3084\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u306f\u300c\u8ce2\u3044\u300d\u3068\u601d\u308f\u308c\u304c\u3061\u3067\u3059\u304c\u3001\u305d\u308c\u306f\u4eba\u9593\u304c\u591a\u304f\u306e\u6642\u9593\u3068\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u8cbb\u3084\u3057\u3066\u8a2d\u8a08\u3057\u305f\u7d50\u679c\u3067\u3059\u3002\u5b9f\u969b\u306b\u306f\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u306b\u306f\u3059\u3079\u3066\u306e\u6307\u793a\u3092\u660e\u78ba\u304b\u3064\u6b63\u78ba\u306b\u4f1d\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u66d6\u6627\u306a\u6307\u793a\u3084\u30df\u30b9\u304c\u3042\u308b\u3068\u3001\u6b63\u3057\u304f\u52d5\u4f5c\u3057\u307e\u305b\u3093\u3002</p> </li> <li> <p>\u300c\u30b3\u30d4\u30fc\u3001\u30da\u30fc\u30b9\u30c8\u3001\u4fee\u6b63\u300d\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u6d3b\u7528\u3059\u308b: \u6700\u521d\u306b\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3092\u5b66\u3076\u3068\u304d\u3084\u3001\u7279\u306b\u8907\u96d1\u306a\u30b3\u30fc\u30c9\u3092\u7406\u89e3\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3068\u304d\u306f\u3001\u65e2\u5b58\u306e\u52d5\u4f5c\u3059\u308b\u30b3\u30fc\u30c9\u3092\u30b3\u30d4\u30fc\u3057\u3001\u81ea\u5206\u306e\u76ee\u7684\u306b\u5408\u308f\u305b\u3066\u4fee\u6b63\u3059\u308b\u306e\u304c\u52b9\u7387\u7684\u3067\u3059\u3002\u3053\u308c\u3092 \u300c\u30b3\u30d4\u30fc\u3001\u30da\u30fc\u30b9\u30c8\u3001\u4fee\u6b63\u300d \u30a2\u30d7\u30ed\u30fc\u30c1\u3068\u547c\u3073\u307e\u3059\u3002\u6700\u521d\u306f\u6697\u8a18\u3057\u3066\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u306e\u3067\u306f\u306a\u304f\u3001\u63d0\u4f9b\u3055\u308c\u305f\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u30b3\u30d4\u30fc\u3057\u3001\u305d\u308c\u3092\u4fee\u6b63\u3057\u306a\u304c\u3089\u5b66\u3076\u3053\u3068\u3092\u63a8\u5968\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u81ea\u8ee2\u8eca\u306e\u88dc\u52a9\u8f2a\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3001\u6163\u308c\u3066\u304f\u308c\u3070\u5f90\u3005\u306b\u88dc\u52a9\u306a\u3057\u3067\u30b3\u30fc\u30c9\u3092\u66f8\u3051\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u5b9f\u8df5\u3092\u901a\u3058\u3066\u5b66\u3076\u306e\u304c\u6700\u826f\u306e\u65b9\u6cd5: \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u30b9\u30ad\u30eb\u3092\u5411\u4e0a\u3055\u305b\u308b\u6700\u3082\u52b9\u679c\u7684\u306a\u65b9\u6cd5\u306f\u3001\u5b9f\u969b\u306b\u624b\u3092\u52d5\u304b\u3057\u3066\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u3053\u3068\u3067\u3059\u3002\u7279\u306b\u3001\u81ea\u5206\u304c\u8208\u5473\u306e\u3042\u308b\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3059\u308b\u306a\u3069\u3001\u5177\u4f53\u7684\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u6301\u3064\u3068\u3001\u5b66\u7fd2\u304c\u30b9\u30e0\u30fc\u30ba\u306b\u9032\u307f\u307e\u3059\u3002</p> </li> <li> <p>\u7df4\u7fd2\u304c\u9375: \u5916\u56fd\u8a9e\u3092\u4e0a\u9054\u3055\u305b\u308b\u552f\u4e00\u306e\u65b9\u6cd5\u304c\u7e70\u308a\u8fd4\u3057\u8a71\u3059\u3053\u3068\u3067\u3042\u308b\u3088\u3046\u306b\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3092\u4e0a\u9054\u3055\u305b\u308b\u552f\u4e00\u306e\u65b9\u6cd5\u3082\u591a\u304f\u306e\u7df4\u7fd2\u3092\u7a4d\u3080\u3053\u3068\u3067\u3059\u3002\u5fc3\u914d\u3057\u306a\u3044\u3067\u304f\u3060\u3055\u3044\uff01 \u79c1\u305f\u3061\u306f\u5341\u5206\u306a\u7df4\u7fd2\u306e\u6a5f\u4f1a\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> </li> </ul> <p>Python \u3068 Google Colaboratory \u3092\u6d3b\u7528\u3057\u306a\u304c\u3089\u3001\u7e70\u308a\u8fd4\u3057\u7df4\u7fd2\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff01</p>"},{"location":"lectures/SIWS/01-getting-started/#python_2","title":"Python\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u306f\uff1f","text":"<p>Python\u521d\u5fc3\u8005\u304c\u6700\u521d\u306b\u6238\u60d1\u3046\u6982\u5ff5\u306e\u3072\u3068\u3064\u306b\u3001Python\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\uff08\u307e\u305f\u306f\u30d1\u30c3\u30b1\u30fc\u30b8\uff09\u304c\u3042\u308a\u307e\u3059\u3002Python\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u3001\u8ffd\u52a0\u306e\u95a2\u6570\u3001\u30c7\u30fc\u30bf\u3001\u305d\u3057\u3066\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3067\u3001Python\u306e\u6a5f\u80fd\u3092\u62e1\u5f35\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u3001\u4e16\u754c\u4e2d\u306ePython\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u4f5c\u3089\u308c\u3066\u304a\u308a\u3001\u901a\u5e38\u306fPyPI\uff08Python Package Index\uff09\u306a\u3069\u304b\u3089\u7121\u6599\u3067\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u672c\u66f8\u3067\u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u4e2d\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3082\u306e\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ul> <li>matplotlib: \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u306e\u305f\u3081\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002</li> <li>Pandas: \u30c7\u30fc\u30bf\u306e\u6574\u5f62\u3084\u64cd\u4f5c\u306e\u305f\u3081\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002</li> <li>NumPy: \u6570\u5024\u8a08\u7b97\u3084\u914d\u5217\u64cd\u4f5c\u306e\u305f\u3081\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002</li> </ul> <p>Python\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u3001\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u308b\u30a2\u30d7\u30ea\u306b\u4f8b\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3064\u307e\u308a\u3001Python\u81ea\u4f53\u306f\u65b0\u3057\u3044\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002\u6700\u521d\u306f\u57fa\u672c\u7684\u306a\u6a5f\u80fd\u306f\u5099\u308f\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u3059\u3079\u3066\u306e\u6a5f\u80fd\u304c\u63c3\u3063\u3066\u3044\u308b\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002Python\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u3001\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u306bApp Store\u3084Google Play\u304b\u3089\u30a2\u30d7\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u306e\u3068\u540c\u3058\u3088\u3046\u306b\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u3042\u306a\u305f\u306e\u4f5c\u696d\u74b0\u5883\u3092\u62e1\u5f35\u3057\u3066\u304f\u308c\u307e\u3059\u3002</p> <p>\u3053\u306e\u30a2\u30ca\u30ed\u30b8\u30fc\u3092\u7d9a\u3051\u308b\u305f\u3081\u306b\u3001\u5199\u771f\u306e\u7de8\u96c6\u3084\u5171\u6709\u306b\u4f7f\u3046Instagram\u30a2\u30d7\u30ea\u3092\u4f8b\u306b\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u305f\u3068\u3048\u3070\u3001\u65b0\u3057\u3044\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u3092\u8cfc\u5165\u3057\u3066\u3001\u64ae\u3063\u305f\u5199\u771f\u3092\u53cb\u4eba\u3084\u5bb6\u65cf\u3068Instagram\u3067\u5171\u6709\u3057\u305f\u3044\u3068\u3057\u307e\u3059\u3002\u305d\u306e\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u30b9\u30c6\u30c3\u30d7\u304c\u5fc5\u8981\u3067\u3059\uff1a</p> <ol> <li> <p>\u30a2\u30d7\u30ea\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb:\u65b0\u3057\u3044\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u306b\u306fInstagram\u30a2\u30d7\u30ea\u304c\u30d7\u30ea\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3044\u305f\u3081\u3001App Store\u3084Google Play\u304b\u3089\u30a2\u30d7\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u4e00\u5ea6\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308c\u3070\u3001\u305d\u306e\u5f8c\u306f\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u304c\u3042\u308b\u307e\u3067\u518d\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> </li> <li> <p>\u30a2\u30d7\u30ea\u306e\u8d77\u52d5:\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5b8c\u4e86\u3057\u305f\u3089\u3001Instagram\u30a2\u30d7\u30ea\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002</p> </li> </ol> <p>Instagram\u30a2\u30d7\u30ea\u3092\u8d77\u52d5\u3059\u308c\u3070\u3001\u5199\u771f\u3092\u53cb\u4eba\u3084\u5bb6\u65cf\u3068\u5171\u6709\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u3068\u540c\u69d8\u306b\u3001Python\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u3082\u6b21\u306e2\u3064\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u8e0f\u307f\u307e\u3059\uff1a</p> <ol> <li> <p>\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb:\u3053\u308c\u306f\u3001\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u306b\u30a2\u30d7\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u306e\u3068\u540c\u3058\u3067\u3059\u3002\u591a\u304f\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u3001Python\u3084Google Colaboratory\u306e\u521d\u671f\u72b6\u614b\u306b\u306f\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u305f\u3081\u3001\u521d\u3081\u3066\u4f7f\u7528\u3059\u308b\u969b\u306b\u306fpip\u306a\u3069\u3092\u4f7f\u3063\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u4e00\u5ea6\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308c\u3070\u3001\u901a\u5e38\u306f\u66f4\u65b0\u304c\u5fc5\u8981\u306b\u306a\u308b\u307e\u3067\u518d\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> </li> <li> <p>\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8:\u3053\u308c\u306f\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u30a2\u30d7\u30ea\u3092\u8d77\u52d5\u3059\u308b\u306e\u3068\u540c\u3058\u3067\u3059\u3002Python\u3067\u306f\u3001\u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30d7\u30ed\u30b0\u30e9\u30e0\u5185\u3067\u6bce\u56deimport\u6587\u3092\u7528\u3044\u3066\u8aad\u307f\u8fbc\u3080\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> </ol> <p>\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u306e\u305f\u3081\u306ematplotlib\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f8b\u306b\u3001\u3053\u306e2\u3064\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u969b\u306b\u884c\u3063\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code># matplotlib\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n!pip install matplotlib\n\n# matplotlib\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\nimport matplotlib.pyplot as plt\n\n# \u7c21\u5358\u306a\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u306e\u63cf\u753b\nx = [1, 2, 3, 4, 5]\ny = [10, 20, 25, 30, 40]\nplt.plot(x, y)\nplt.xlabel(\"X\u8ef8\")\nplt.ylabel(\"Y\u8ef8\")\nplt.title(\"Matplotlib\u306e\u30b5\u30f3\u30d7\u30eb\u30b0\u30e9\u30d5\")\nplt.show()\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#python_3","title":"Python\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>Google Colaboratory\u306b\u3064\u3044\u3066\u306e\u6ce8\u610f:  Google Colaboratory\u3067\u306f\u3001\u591a\u304f\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u4e8b\u524d\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u8ffd\u52a0\u3067\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u81ea\u5206\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002Google Colaboratory\u3092\u4f7f\u308f\u305a\u306b\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067Python\u3092\u5b9f\u884c\u3059\u308b\u5834\u5408\u3082\u3001\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u624b\u9806\u3092\u77e5\u3063\u3066\u304a\u304f\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\uff08\u4eca\u56de\u306f\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u306f\u4f7f\u7528\u3057\u307e\u305b\u3093\u304c\u3001\u4eca\u5f8c\u5229\u7528\u3059\u308b\u969b\u306e\u53c2\u8003\u306b\u8a18\u8f09\u3057\u3066\u304a\u304d\u307e\u3059\uff09\u3002</p> <p>Python\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u65b9\u6cd5\u306f2\u7a2e\u985e\u3042\u308a\u307e\u3059\u3002</p> <ol> <li>\u7c21\u5358\u306a\u65b9\u6cd5\uff08Google Colaboratory\u307e\u305f\u306fJupyter Notebook\u3067\u5b9f\u884c\uff09:     <code>!pip install</code> \u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002</li> </ol> <pre><code>!pip install seaborn pandas numpy matplotlib\n</code></pre> <ol> <li>\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\uff08\u30bf\u30fc\u30df\u30ca\u30eb\u307e\u305f\u306f\u30b3\u30de\u30f3\u30c9\u30d7\u30ed\u30f3\u30d7\u30c8\u3067\u5b9f\u884c\uff1a\u4eca\u56de\u306f\u5229\u7528\u3057\u306a\u3044\uff09:</li> </ol> <pre><code>pip install seaborn pandas numpy matplotlib\n</code></pre> <p>\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u306b\u304a\u3044\u3066\u306f\u3001\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u306e\u30a2\u30d7\u30ea\u3068\u540c\u69d8\u3001\u4e00\u5ea6\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308c\u3070\u518d\u5ea6\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001Google Colaboratory\u306e\u74b0\u5883\u306b\u304a\u3044\u3066\u306f\u3001\u6642\u9593\u304c\u7d4c\u904e\u3059\u308b\u3068\u30ea\u30bb\u30c3\u30c8\u3055\u308c\u308b\u305f\u3081\u3001\u518d\u5ea6\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5fc5\u8981\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u30a4\u30e1\u30fc\u30b8\u3068\u3057\u3066\u306f\u3001\u8cb8\u3057\u51fa\u3057\u30b9\u30de\u30db\u3067\u3059\u3002\u4e00\u5ea6\u5e97\u306b\u8fd4\u5374\u3059\u308b\u3068\u4e2d\u8eab\u306e\u30c7\u30fc\u30bf\u304c\u6d88\u3055\u308c\u3066\u3044\u3066\u3001\u518d\u5ea6\u30a2\u30d7\u30ea\u3092\u5165\u308c\u76f4\u3059\u5fc5\u8981\u304c\u3042\u308b\u306e\u3068\u540c\u3058\u3067\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_4","title":"\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u8aad\u307f\u8fbc\u307f","text":"<p>\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u5f8c\u3001\u305d\u308c\u3092Python\u30d7\u30ed\u30b0\u30e9\u30e0\u5185\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u306f\u300c\u8aad\u307f\u8fbc\u3080\u300d\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067\u3001\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u8aad\u307f\u8fbc\u3080\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <pre><code># library\u306e\u8aad\u307f\u8fbc\u307f\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <p>\u3082\u3057\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u304c\u8868\u793a\u3055\u308c\u305f\u5834\u5408:</p> <pre><code>ModuleNotFoundError: No module named 'seaborn'\n</code></pre> <p>\u3053\u308c\u306f\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u5834\u5408\u306f\u3001\u524d\u8ff0\u306e<code>pip install</code>\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>Note</p> <p><code>pip</code> \u3092\u7528\u3044\u3066\u3001<code>seaborn</code> \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_5","title":"\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u5229\u7528","text":"<p>Python\u3067\u30c7\u30fc\u30bf\u5206\u6790\u3092\u884c\u3046\u969b\u3001\u9069\u5207\u306b\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3001\u8aad\u307f\u8fbc\u3080\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001<code>seaborn</code> \u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u3092\u884c\u3046\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3068\u30b7\u30f3\u30d7\u30eb\u306a\u6563\u5e03\u56f3\u3092\u63cf\u753b\u3067\u304d\u307e\u3059\u3002</p> <pre><code># \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\nnp.random.seed(10)\ndata = pd.DataFrame({\n    \"x\": np.random.rand(50),\n    \"y\": np.random.rand(50)\n})\n\n# Seaborn\u3092\u4f7f\u3063\u305f\u6563\u5e03\u56f3\u306e\u63cf\u753b\nsns.scatterplot(x=\"x\", y=\"y\", data=data)\nplt.title(\"Seaborn\u3092\u7528\u3044\u305f\u6563\u5e03\u56f3\u306e\u4f8b\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u6b63\u3057\u304f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3001\u8aad\u307f\u8fbc\u3080\u3053\u3068\u3067\u3001Python\u3067\u306e\u30c7\u30fc\u30bf\u5206\u6790\u3084\u53ef\u8996\u5316\u304c\u7c21\u5358\u306b\u884c\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_6","title":"\u521d\u3081\u3066\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u63a2\u7d22\u3057\u3066\u307f\u3088\u3046","text":"<p>\u3053\u308c\u307e\u3067\u5b66\u3093\u3060\u3053\u3068\u3092\u6d3b\u7528\u3057\u3066\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3092\u63a2\u7d22\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u30c7\u30fc\u30bf\u306f\u753b\u50cf\u3001\u30c6\u30ad\u30b9\u30c8\u3001\u6570\u5024\u306a\u3069\u3055\u307e\u3056\u307e\u306a\u5f62\u5f0f\u3067\u5b58\u5728\u3057\u307e\u3059\u304c\u3001\u672c\u66f8\u3067\u306f\u4e3b\u306b\u300c\u30b9\u30d7\u30ec\u30c3\u30c9\u30b7\u30fc\u30c8\u300d\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u7126\u70b9\u3092\u5f53\u3066\u307e\u3059\u3002\u3053\u308c\u306f\u591a\u304f\u306e\u5206\u91ce\u3067\u30c7\u30fc\u30bf\u304c\u53ce\u96c6\u30fb\u4fdd\u5b58\u3055\u308c\u308b\u6700\u3082\u4e00\u822c\u7684\u306a\u65b9\u6cd5\u3067\u3059\u3002Python\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u300c\u30b9\u30d7\u30ec\u30c3\u30c9\u30b7\u30fc\u30c8\u300d\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3068\u547c\u3073\u307e\u3059\u3002\u4ee5\u964d\u3001\u672c\u66f8\u3067\u306f\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3068\u3057\u3066\u4fdd\u5b58\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u64cd\u4f5c\u306b\u6ce8\u76ee\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>\u307e\u305a\u3001\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#_7","title":"\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f","text":"<p>\u4eca\u56de\u306f\u3001\u5e83\u544a\u8cbb\u7528\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\u3092\u793a\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u306f\u3001TV\u3001\u30e9\u30b8\u30aa\u3001\u65b0\u805e\u306b\u6295\u3058\u305f\u4e88\u7b97\u3068\u58f2\u4e0a\u306e\u60c5\u5831\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>url = 'https://raw.githubusercontent.com/a-martyn/ISL-python/ee156568a8f7307be71dad5390bae12b51dcd93f/Notebooks/data/Advertising.csv'\ndata = pd.read_csv(url, index_col=0)\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#_8","title":"\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u69cb\u9020\u3092\u78ba\u8a8d\u3059\u308b","text":"<p>\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u69cb\u9020\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u3001\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code>data.info()\n</code></pre> <p>\u3053\u306e\u51fa\u529b\u304b\u3089\u3001\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u306f200\u884c\u30684\u5217\u304c\u3042\u308a\u3001\u5404\u5217\u306e\u30c7\u30fc\u30bf\u578b\u3068\u6b20\u640d\u5024\u306e\u6709\u7121\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p> <p>\u6b21\u306b\u3001\u30c7\u30fc\u30bf\u306e\u6700\u521d\u306e\u6570\u884c\u3092\u8868\u793a\u3057\u3066\u3001\u30c7\u30fc\u30bf\u306e\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> <pre><code>data.head()\n</code></pre> <p>\u3053\u306e\u51fa\u529b\u304b\u3089\u3001\u5404\u5217\u306e\u540d\u524d\u3068\u6700\u521d\u306e5\u884c\u306e\u30c7\u30fc\u30bf\u304c\u8868\u793a\u3055\u308c\u3001\u30c7\u30fc\u30bf\u306e\u6982\u8981\u3092\u628a\u63e1\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_9","title":"\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u63a2\u7d22","text":"<p>\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u63a2\u7d22\u3059\u308b\u65b9\u6cd5\u306f\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u30013\u3064\u306e\u65b9\u6cd5\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p> <ol> <li> <p><code>head()</code>\u30e1\u30bd\u30c3\u30c9: \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u6700\u521d\u306e\u6570\u884c\u3092\u8868\u793a\u3057\u307e\u3059\u3002</p> <pre><code>data.head()\n</code></pre> </li> <li> <p><code>describe()</code>\u30e1\u30bd\u30c3\u30c9: \u6570\u5024\u30c7\u30fc\u30bf\u306e\u57fa\u672c\u7684\u306a\u7d71\u8a08\u91cf\u3092\u8868\u793a\u3057\u307e\u3059\u3002</p> <pre><code>data.describe()\n</code></pre> </li> <li> <p><code>columns</code>\u5c5e\u6027: \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u5217\u540d\u3092\u8868\u793a\u3057\u307e\u3059\u3002</p> <pre><code>data.columns\n</code></pre> </li> </ol> <p>\u3053\u308c\u3089\u306e\u65b9\u6cd5\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3001\u30c7\u30fc\u30bf\u306e\u6982\u8981\u3092\u628a\u63e1\u3057\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_10","title":"\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316","text":"<p>\u30c7\u30fc\u30bf\u306e\u95a2\u4fc2\u6027\u3092\u8996\u899a\u7684\u306b\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u3001\u6563\u5e03\u56f3\u3092\u4f5c\u6210\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u4f8b\u3048\u3070\u3001TV\u5e83\u544a\u8cbb\u7528\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\u3092\u78ba\u8a8d\u3059\u308b\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002</p> <pre><code>plt.scatter(data['TV'], data['Sales'])\nplt.xlabel('TV Advertising Budget (in thousands of dollars)')\nplt.ylabel('Sales (in thousands of units)')\nplt.title('TV Advertising vs Sales')\nplt.show()\n</code></pre> <p>\u3053\u306e\u6563\u5e03\u56f3\u304b\u3089\u3001TV\u5e83\u544a\u8cbb\u7528\u3068\u58f2\u4e0a\u306e\u9593\u306b\u6b63\u306e\u76f8\u95a2\u304c\u3042\u308b\u3053\u3068\u304c\u8996\u899a\u7684\u306b\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p> <p>\u540c\u69d8\u306b\u3001\u30e9\u30b8\u30aa\u3084\u65b0\u805e\u306e\u5e83\u544a\u8cbb\u7528\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\u3082\u6563\u5e03\u56f3\u3067\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code># \u30e9\u30b8\u30aa\u5e83\u544a\u8cbb\u7528\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\nplt.scatter(data['Radio'], data['Sales'])\nplt.xlabel('Radio Advertising Budget (in thousands of dollars)')\nplt.ylabel('Sales (in thousands of units)')\nplt.title('Radio Advertising vs Sales')\nplt.show()\n\n# \u65b0\u805e\u5e83\u544a\u8cbb\u7528\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\nplt.scatter(data['Newspaper'], data['Sales'])\nplt.xlabel('Newspaper Advertising Budget (in thousands of dollars)')\nplt.ylabel('Sales (in thousands of units)')\nplt.title('Newspaper Advertising vs Sales')\nplt.show()\n</code></pre> <p>\u3053\u308c\u3089\u306e\u6563\u5e03\u56f3\u3092\u901a\u3058\u3066\u3001\u5404\u5e83\u544a\u5a92\u4f53\u306e\u4e88\u7b97\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\u6027\u3092\u8996\u899a\u7684\u306b\u7406\u89e3\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_11","title":"\u30c7\u30fc\u30bf\u306e\u57fa\u672c\u7d71\u8a08\u91cf","text":"<p>\u30c7\u30fc\u30bf\u306e\u57fa\u672c\u7684\u306a\u7d71\u8a08\u91cf\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3084\u4e2d\u5fc3\u50be\u5411\u3092\u7406\u89e3\u3067\u304d\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code>data.describe()\n</code></pre> <p>\u3053\u306e\u51fa\u529b\u304b\u3089\u3001\u5404\u5909\u6570\u306e\u5e73\u5747\u5024\u3001\u6a19\u6e96\u504f\u5dee\u3001\u6700\u5c0f\u5024\u3001\u6700\u5927\u5024\u3001\u56db\u5206\u4f4d\u6570\u306a\u3069\u306e\u60c5\u5831\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u4ee5\u4e0a\u306e\u624b\u9806\u3067\u3001Python\u3068Pandas\u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u307f\u3001\u305d\u306e\u69cb\u9020\u3092\u7406\u89e3\u3057\u3001\u57fa\u672c\u7684\u306a\u7d71\u8a08\u91cf\u3092\u78ba\u8a8d\u3057\u3001\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u3092\u884c\u3044\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u65b9\u6cd5\u3092\u6d3b\u7528\u3057\u3066\u3001\u3055\u307e\u3056\u307e\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u63a2\u7d22\u3057\u3001\u5206\u6790\u306e\u57fa\u790e\u3092\u7bc9\u3044\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_12","title":"\u307e\u3068\u3081","text":"<p>\u672c\u7ae0\u3067\u306f\u3001Python\u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u3092\u63a2\u7d22\u3059\u308b\u305f\u3081\u306e\u57fa\u672c\u7684\u306a\u30c4\u30fc\u30eb\u30bb\u30c3\u30c8\u3092\u7d39\u4ecb\u3057\u307e\u3057\u305f\u3002\u3053\u306e\u7ae0\u306b\u3059\u3079\u3066\u306e\u77e5\u8b58\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3059\u3079\u3066\u3092\u76db\u308a\u8fbc\u3080\u3068\u81a8\u5927\u306a\u91cf\u306b\u306a\u308a\u3001\u5b9f\u7528\u7684\u3067\u306f\u306a\u304f\u306a\u3063\u3066\u3057\u307e\u3046\u304b\u3089\u3067\u3059\uff01\u6700\u3082\u91cd\u8981\u306a\u306e\u306f\u3001Google Colaboratory\u4e0a\u3067\u5b9f\u969b\u306b\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3001\u8a66\u884c\u932f\u8aa4\u3092\u7e70\u308a\u8fd4\u3057\u306a\u304c\u3089\u5b66\u3076\u3053\u3068\u3067\u3059\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#_13","title":"\u8ffd\u52a0\u30ea\u30bd\u30fc\u30b9","text":"<p>\u3082\u3057\u3001Python\u3084Google Colaboratory\u3001\u30c7\u30fc\u30bf\u5206\u6790\u306b\u4e0d\u6163\u308c\u3067\u3001\u3088\u308a\u8a73\u7d30\u306a\u5165\u9580\u66f8\u3092\u6c42\u3081\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u4ee5\u4e0b\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u53c2\u7167\u3059\u308b\u3053\u3068\u3092\u304a\u3059\u3059\u3081\u3057\u307e\u3059\u3002</p> <ul> <li> <p>Python Data Science Handbook   NumPy\u3001Pandas\u3001Matplotlib\u3001Scikit-Learn\u306a\u3069\u3001Python\u3092\u4f7f\u3063\u305f\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u790e\u3092\u5b66\u3076\u305f\u3081\u306e\u5305\u62ec\u7684\u306a\u30ac\u30a4\u30c9\u3067\u3059\u3002</p> </li> <li> <p>Google Colaboratory\u306e\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8   Google Colab\u306e\u57fa\u672c\u7684\u306a\u4f7f\u3044\u65b9\u3092\u5b66\u3076\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u30af\u30e9\u30a6\u30c9\u74b0\u5883\u3067Python\u3092\u5b9f\u884c\u3067\u304d\u308b\u305f\u3081\u3001\u74b0\u5883\u69cb\u7bc9\u306a\u3057\u306b\u3059\u3050\u306b\u59cb\u3081\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>Pandas\u306e\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8   \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u64cd\u4f5c\u306e\u8a73\u7d30\u306a\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u3067\u3059\u3002</p> </li> <li> <p>Matplotlib\u306e\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8   \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u306b\u95a2\u3059\u308b\u8a73\u7d30\u306a\u60c5\u5831\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> </li> </ul>"},{"location":"lectures/SIWS/01-getting-started/#_14","title":"\u4eca\u5f8c\u306e\u5c55\u958b","text":"<p>\u6b21\u306e\u7ae0\u3067\u306f\u3001\u300c\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u6700\u3082\u91cd\u8981\u306a\u30c4\u30fc\u30eb\u306e1\u3064\u300d\u3068\u3082\u8a00\u3048\u308b\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002Pandas\u3068Matplotlib\u3092\u6d3b\u7528\u3057\u3001\u8996\u899a\u7684\u306b\u30c7\u30fc\u30bf\u3092\u63a2\u7d22\u3059\u308b\u65b9\u6cd5\u3092\u63a2\u6c42\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>\u30c7\u30fc\u30bf\u53ef\u8996\u5316\u306f\u3001<code>head()</code> \u3084 <code>describe()</code> \u306e\u3088\u3046\u306a\u95a2\u6570\u3067\u306f\u6349\u3048\u304d\u308c\u306a\u3044\u30d1\u30bf\u30fc\u30f3\u3084\u50be\u5411\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u3067\u3059\u3002\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u57fa\u672c\u7684\u306a\u30d7\u30ed\u30c3\u30c8\u3092\u5b66\u3073\u3001\u30c7\u30fc\u30bf\u306e\u80cc\u5f8c\u306b\u3042\u308b\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u8aad\u307f\u89e3\u304f\u529b\u3092\u990a\u3044\u307e\u3057\u3087\u3046\u3002</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nurl = 'https://raw.githubusercontent.com/a-martyn/ISL-python/ee156568a8f7307be71dad5390bae12b51dcd93f/Notebooks/data/Advertising.csv'\ndata = pd.read_csv(url, index_col=0)\n\n# \u6563\u5e03\u56f3\u306e\u4f5c\u6210\uff08TV\u5e83\u544a\u8cbb\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\uff09\nplt.scatter(data['TV'], data['Sales'])\nplt.xlabel('TV Advertising Budget (in thousands of dollars)')\nplt.ylabel('Sales (in thousands of units)')\nplt.title('TV Advertising vs Sales')\nplt.show()\n</code></pre> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u6b21\u306e\u7ae0\u3067\u306f\u30c7\u30fc\u30bf\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u3088\u308a\u76f4\u611f\u7684\u306b\u7406\u89e3\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u65c5\u3092\u7d9a\u3051\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff01</p> <p>\u6b21\u306e\u7ae0\u3067\u306f\u3001\u300c\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b\u6700\u3082\u91cd\u8981\u306a\u30c4\u30fc\u30eb\u306e1\u3064\u300d\u3068\u3082\u8a00\u3048\u308b\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002Pandas\u3068Matplotlib\u3092\u6d3b\u7528\u3057\u3001\u8996\u899a\u7684\u306b\u30c7\u30fc\u30bf\u3092\u63a2\u7d22\u3059\u308b\u65b9\u6cd5\u3092\u63a2\u6c42\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>\u30c7\u30fc\u30bf\u53ef\u8996\u5316\u306f\u3001<code>head()</code> \u3084 <code>describe()</code> \u306e\u3088\u3046\u306a\u95a2\u6570\u3067\u306f\u6349\u3048\u304d\u308c\u306a\u3044\u30d1\u30bf\u30fc\u30f3\u3084\u50be\u5411\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u3067\u3059\u3002\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u57fa\u672c\u7684\u306a\u30d7\u30ed\u30c3\u30c8\u3092\u5b66\u3073\u3001\u30c7\u30fc\u30bf\u306e\u80cc\u5f8c\u306b\u3042\u308b\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u8aad\u307f\u89e3\u304f\u529b\u3092\u990a\u3044\u307e\u3057\u3087\u3046\u3002</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nurl = 'https://raw.githubusercontent.com/a-martyn/ISL-python/ee156568a8f7307be71dad5390bae12b51dcd93f/Notebooks/data/Advertising.csv'\ndata = pd.read_csv(url, index_col=0)\n\n# \u6563\u5e03\u56f3\u306e\u4f5c\u6210\uff08TV\u5e83\u544a\u8cbb\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\uff09\nplt.scatter(data['TV'], data['Sales'])\nplt.xlabel('TV Advertising Budget (in thousands of dollars)')\nplt.ylabel('Sales (in thousands of units)')\nplt.title('TV Advertising vs Sales')\nplt.show()\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#_15","title":"\u7406\u89e3\u5ea6\u30c1\u30a7\u30c3\u30af\u8ab2\u984c","text":"<p>\u4ee5\u4e0b\u306e\u554f\u984c\u3092\u89e3\u3044\u3066\u3001Python\u3001Google Colaboratory\u3001Pandas\u3001Matplotlib\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u63a2\u7d22\u306e\u7406\u89e3\u3092\u6df1\u3081\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#1","title":"\u554f\u984c 1: \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u3069\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u304b\uff1f \u30c7\u30fc\u30bf\u306e\u6982\u8981\u3092\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>import pandas as pd\n\nurl = 'https://raw.githubusercontent.com/a-martyn/ISL-python/ee156568a8f7307be71dad5390bae12b51dcd93f/Notebooks/data/Advertising.csv'\ndata = pd.read_csv(url, index_col=0)\nprint(data.head())\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#2","title":"\u554f\u984c 2: \u30c7\u30fc\u30bf\u306e\u7d71\u8a08\u60c5\u5831","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u3069\u306e\u3088\u3046\u306a\u60c5\u5831\u304c\u5f97\u3089\u308c\u307e\u3059\u304b\uff1f \u5f97\u3089\u308c\u305f\u60c5\u5831\u304b\u3089\u3001\u3069\u306e\u5e83\u544a\u5a92\u4f53\u304c\u58f2\u4e0a\u306b\u6700\u3082\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u304b\uff1f</p> <pre><code>print(data.describe())\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#3","title":"\u554f\u984c 3: \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3001TV\u5e83\u544a\u8cbb\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u30b0\u30e9\u30d5\u306e\u50be\u5411\u3092\u8aac\u660e\u3057\u3001\u58f2\u4e0a\u306b\u5bfe\u3059\u308bTV\u5e83\u544a\u306e\u5f71\u97ff\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.scatter(data['TV'], data['Sales'])\nplt.xlabel('TV Advertising Budget (in thousands of dollars)')\nplt.ylabel('Sales (in thousands of units)')\nplt.title('TV Advertising vs Sales')\nplt.show()\n</code></pre>"},{"location":"lectures/SIWS/01-getting-started/#4","title":"\u554f\u984c 4: \u4ed6\u306e\u5e83\u544a\u5a92\u4f53\u3068\u306e\u95a2\u4fc2","text":"<p>\u30e9\u30b8\u30aa (<code>Radio</code>) \u3084\u65b0\u805e (<code>Newspaper</code>) \u306e\u5e83\u544a\u8cbb\u3068\u58f2\u4e0a\u306e\u95a2\u4fc2\u3092\u8abf\u3079\u308b\u305f\u3081\u306b\u3001 \u4e0a\u8a18\u306e\u30b3\u30fc\u30c9\u3092\u4fee\u6b63\u3057\u3066\u3001\u305d\u308c\u305e\u308c\u306e\u6563\u5e03\u56f3\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u7d50\u679c\u304b\u3089\u3001\u3069\u306e\u5e83\u544a\u5a92\u4f53\u304c\u58f2\u4e0a\u306b\u5bfe\u3057\u3066\u6700\u3082\u5927\u304d\u306a\u5f71\u97ff\u3092\u6301\u3064\u304b\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/SIWS/01-getting-started/#5","title":"\u554f\u984c 5: \u30c7\u30fc\u30bf\u306e\u5fdc\u7528","text":"<p>\u4f01\u696d\u304c\u5e83\u544a\u8cbb\u3092\u6700\u9069\u306b\u914d\u5206\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u3069\u306e\u3088\u3046\u306a\u5206\u6790\u3092\u884c\u3046\u3079\u304d\u3067\u3057\u3087\u3046\u304b\uff1f \u307e\u305f\u3001Pandas\u3084Matplotlib\u3092\u6d3b\u7528\u3057\u3066\u3001\u3069\u306e\u3088\u3046\u306a\u8ffd\u52a0\u306e\u8996\u899a\u5316\u3084\u7d71\u8a08\u5206\u6790\u304c\u3067\u304d\u308b\u304b\u63d0\u6848\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>\u4ee5\u4e0a\u306e\u554f\u984c\u3092\u89e3\u304f\u3053\u3068\u3067\u3001Python\u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u306e\u63a2\u7d22\u3068\u53ef\u8996\u5316\u306e\u57fa\u790e\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002 \u5b9f\u969b\u306b\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3001\u5f97\u3089\u308c\u305f\u7d50\u679c\u3092\u3082\u3068\u306b\u8003\u5bdf\u3092\u6df1\u3081\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/SIWS/02-visualization/","title":"Python \u3067\u30c7\u30fc\u30bf\u3092\u53ef\u8996\u5316\u3088\u3046","text":"<p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u30c4\u30fc\u30eb\u30dc\u30c3\u30af\u30b9\u3092\u69cb\u7bc9\u3059\u308b\u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u3068\u3057\u3066\u3001\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u3092\u5b66\u3073\u307e\u3059\u3002\u30c7\u30fc\u30bf\u3092\u8996\u899a\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u5358\u306a\u308b\u6570\u5024\u306e\u7f85\u5217\u3067\u306f\u898b\u3048\u306a\u304b\u3063\u305f\u30d1\u30bf\u30fc\u30f3\u3084\u50be\u5411\u3092\u767a\u898b\u3067\u304d\u307e\u3059\u3002Python\u3067\u306f\u3001<code>matplotlib</code> \u3084 <code>seaborn</code> \u3092\u5229\u7528\u3057\u3066\u30c7\u30fc\u30bf\u3092\u53ef\u8996\u5316\u3067\u304d\u307e\u3059\u3002\u672c\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u6d3b\u7528\u3057\u3001\u57fa\u672c\u7684\u306a\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002</p> <p>\u30b0\u30e9\u30d5\uff08\u30d7\u30ed\u30c3\u30c8\u3084\u30c1\u30e3\u30fc\u30c8\u3068\u3082\u547c\u3073\u307e\u3059\uff09\u306f\u3001\u30c7\u30fc\u30bf\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u63a2\u308b\u305f\u3081\u306e\u5f37\u529b\u306a\u624b\u6cd5\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u5916\u308c\u5024\u306e\u7279\u5b9a\u3001\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3001\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u3092\u7406\u89e3\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\u9069\u5207\u306a\u53ef\u8996\u5316\u3092\u884c\u3046\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u304b\u3089\u5f97\u3089\u308c\u308b\u6d1e\u5bdf\u3092\u52b9\u679c\u7684\u306b\u4f1d\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u60c5\u5831\u3092\u8a70\u3081\u8fbc\u307f\u3059\u304e\u308b\u3068\u3001\u304b\u3048\u3063\u3066\u7406\u89e3\u3057\u306b\u304f\u304f\u306a\u308b\u3053\u3068\u3082\u3042\u308b\u305f\u3081\u3001\u9069\u5207\u306a\u30d0\u30e9\u30f3\u30b9\u3092\u53d6\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_1","title":"\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea","text":"<p>\u672c\u7ae0\u3067\u306f\u3001gapminder\u3068\u547c\u3070\u308c\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5229\u7528\u3057\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u306b\u3001\u307e\u305a\u306f\u4ee5\u4e0b\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>!pip install gapminder\n</code></pre> <p>\u6b21\u306b\u3001\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u307e\u3059\u3002</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom gapminder import gapminder\n</code></pre>"},{"location":"lectures/SIWS/02-visualization/#_2","title":"\u30b0\u30e9\u30d5\u30a3\u30c3\u30af\u306e\u57fa\u790e\u7406\u8ad6","text":"<p>\u300c\u30b0\u30e9\u30d5\u30a3\u30c3\u30af\u306e\u6587\u6cd5\uff08Grammar of Graphics\uff09\u300d\u3068\u306f\u3001\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u3092\u4f53\u7cfb\u7684\u306b\u6574\u7406\u3059\u308b\u305f\u3081\u306e\u7406\u8ad6\u3067\u3059\u3002\u3053\u306e\u6982\u5ff5\u306f Leland Wilkinson \u306b\u3088\u3063\u3066\u63d0\u5531\u3055\u308c\u3001R\u306e <code>ggplot2</code> \u3084 Python\u306e <code>plotly</code> \u306a\u3069\u306e\u53ef\u8996\u5316\u30e9\u30a4\u30d6\u30e9\u30ea\u306b\u3082\u5fdc\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>\u3053\u306e\u7406\u8ad6\u306b\u3088\u308b\u3068\u3001\u7d71\u8a08\u30b0\u30e9\u30d5\u306f\u6b21\u306e3\u3064\u306e\u8981\u7d20\u304b\u3089\u69cb\u6210\u3055\u308c\u307e\u3059\u3002</p> <ol> <li>\u30c7\u30fc\u30bf (<code>data</code>): \u53ef\u8996\u5316\u306e\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3002</li> <li>\u5e7e\u4f55\u30aa\u30d6\u30b8\u30a7\u30af\u30c8 (<code>geom</code>): \u30b0\u30e9\u30d5\u306b\u63cf\u753b\u3055\u308c\u308b\u57fa\u672c\u8981\u7d20\uff08\u70b9\u3001\u7dda\u3001\u68d2\u306a\u3069\uff09\u3002</li> <li>\u7f8e\u7684\u8981\u7d20 (<code>aes</code>): \u5e7e\u4f55\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u898b\u305f\u76ee\u3092\u6c7a\u3081\u308b\u8981\u7d20\uff08\u4f4d\u7f6e\u3001\u8272\u3001\u5927\u304d\u3055\u306a\u3069\uff09\u3002</li> </ol> <p>\u3053\u308c\u3089\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u3092\u69cb\u7bc9\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#gapminder","title":"Gapminder\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316","text":"<p>Gapminder\u306f\u3001\u4e16\u754c\u306e\u7d4c\u6e08\u30fb\u5065\u5eb7\u30fb\u767a\u5c55\u72b6\u6cc1\u306b\u95a2\u3059\u308b\u30c7\u30fc\u30bf\u3092\u63d0\u4f9b\u3059\u308b\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u306f\u3001\u5404\u56fd\u306eGDP\u3001\u5bff\u547d\u3001\u4eba\u53e3\u306a\u3069\u306e\u60c5\u5831\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_3","title":"\u30c7\u30fc\u30bf\u306e\u6e96\u5099","text":"<p>\u307e\u305a\u3001Gapminder\u30c7\u30fc\u30bf\u30922007\u5e74\u306e\u30c7\u30fc\u30bf\u306b\u7d5e\u308a\u8fbc\u3093\u3067\u8868\u793a\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code># 2007\u5e74\u306e\u30c7\u30fc\u30bf\u306e\u307f\u3092\u53d6\u5f97\ngapminder_2007 = gapminder[gapminder['year'] == 2007]\n\n# \u5fc5\u8981\u306a\u5217\u306e\u307f\u9078\u629e\ngapminder_2007 = gapminder_2007[['country', 'continent', 'lifeExp', 'pop', 'gdpPercap']]\ngapminder_2007.columns = ['Country', 'Continent', 'Life Expectancy', 'Population', 'GDP per Capita']\n\n# \u30c7\u30fc\u30bf\u306e\u5148\u982d\u3092\u8868\u793a\ngapminder_2007.head()\n</code></pre> <p>\u3053\u306e\u8868\u306e\u5404\u884c\u306f1\u3064\u306e\u56fd\u3092\u8868\u3057\u3001\u6b21\u306e\u60c5\u5831\u3092\u542b\u307f\u307e\u3059\u3002</p> <ol> <li>Country: \u56fd\u306e\u540d\u524d\u3002</li> <li>Continent: 5\u3064\u306e\u5927\u9678\u306e\u3044\u305a\u308c\u304b\uff08\u300cAmericas\u300d\u306f\u5317\u7c73\u3068\u5357\u7c73\u3092\u542b\u307f\u3001\u5357\u6975\u306f\u9664\u5916\uff09\u3002</li> <li>Life Expectancy: \u5e73\u5747\u5bff\u547d\uff08\u5e74\uff09\u3002</li> <li>Population: \u4eba\u53e3\uff08\u4eba\uff09\u3002</li> <li>GDP per Capita: 1\u4eba\u5f53\u305f\u308aGDP\uff08\u7c73\u30c9\u30eb\uff09\u3002</li> </ol>"},{"location":"lectures/SIWS/02-visualization/#_4","title":"\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316","text":"<p>2007\u5e74\u306eGDP\u3068\u5e73\u5747\u5bff\u547d\u306e\u95a2\u4fc2\u3092\u53ef\u8996\u5316\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> <pre><code>plt.figure(figsize=(10, 6))\n\nsns.scatterplot(\n    data=gapminder_2007,\n    x='GDP per Capita',\n    y='Life Expectancy',\n    size='Population',\n    hue='Continent',\n    sizes=(10, 200),\n    alpha=0.7\n)\n\nplt.xscale('log')  # GDP\u306f\u5bfe\u6570\u30b9\u30b1\u30fc\u30eb\u306b\u5909\u63db\nplt.xlabel('GDP per Capita (log scale)')\nplt.ylabel('Life Expectancy (years)')\nplt.title('2007\u5e74\u306b\u304a\u3051\u308bGDP\u3068\u5e73\u5747\u5bff\u547d\u306e\u95a2\u4fc2')\nplt.legend(title='Continent', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n</code></pre> <p>\u3053\u306e\u3068\u304d\u3001\u30b0\u30e9\u30d5\u306e\u30bf\u30a4\u30c8\u30eb\u3092\u307f\u308b\u3068\u3001\u65e5\u672c\u8a9e\u304c\u8868\u793a\u3067\u304d\u3066\u3044\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u306d\u3002\u3053\u308c\u306f\u3001\u975e\u5e38\u306b\u3088\u304f\u8d77\u304d\u308b\u554f\u984c\u3067\u3059\u3002python\u3067\u306f\u3001\u3053\u308c\u3092\u4ee5\u4e0b\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3053\u3068\u3067\u89e3\u6d88\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <pre><code>!pip install japanize-matplotlib\n</code></pre> <p>\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u5f8c\u3067\u3001\u6700\u521d\u306e <code>import</code> \u90e8\u5206\u3092\u6b21\u306e\u3088\u3046\u306b\u5909\u66f4\u3057\u307e\u3057\u3087\u3046\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport japanize_matplotlib\nimport seaborn as sns\nfrom gapminder import gapminder\n</code></pre> <p>\u3053\u308c\u3067\u65e5\u672c\u8a9e\u304c\u8868\u793a\u3055\u308c\u308b\u306f\u305a\u3067\u3059\u3002\u6b21\u306b\u3001\u3053\u306e\u30d7\u30ed\u30c3\u30c8\u3092\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002</p> <ul> <li>\u5909\u6570 GDP per Capita\uff081\u4eba\u5f53\u305f\u308aGDP\uff09 \u306f\u3001x\u8ef8\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3055\u308c\u3066\u3044\u307e\u3059\u3002</li> <li>\u5909\u6570 Life Expectancy\uff08\u5e73\u5747\u5bff\u547d\uff09 \u306f\u3001\u30dd\u30a4\u30f3\u30c8\u306e y\u8ef8\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3055\u308c\u3066\u3044\u307e\u3059\u3002</li> <li>\u5909\u6570 Population\uff08\u4eba\u53e3\uff09 \u306f\u3001\u30dd\u30a4\u30f3\u30c8\u306e \u30b5\u30a4\u30ba\uff08size aesthetic\uff09 \u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3055\u308c\u307e\u3059\u3002</li> <li>\u5909\u6570 Continent\uff08\u5927\u9678\uff09 \u306f\u3001\u30dd\u30a4\u30f3\u30c8\u306e \u8272\uff08color aesthetic\uff09 \u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3055\u308c\u307e\u3059\u3002</li> </ul> <p>\u3053\u3053\u3067\u3001<code>data</code>\uff08\u30c7\u30fc\u30bf\uff09\u3068\u306f python\u306e <code>dataframe</code>\u306b\u5bfe\u5fdc\u3057\u3001\u30c7\u30fc\u30bf\u5909\u6570\uff08data variables\uff09 \u304c\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u5185\u306e\u7279\u5b9a\u306e\u5217\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u3002\u7279\u306b\u3053\u306e\u30d7\u30ed\u30c3\u30c8\u3067\u306f\u30014\u3064\u306e\u60c5\u5831\u3092\u540c\u6642\u306b\u96c6\u7d04\u3057\u3066\u3044\u307e\u3059\u3002x\u8ef8\u306b\u4e00\u4eba\u3042\u305f\u308a\u306eGDP\u3001y\u8ef8\u306b\u5e73\u5747\u5bff\u547d\u3001\u3055\u3089\u306b\u70b9\u306e\u5927\u304d\u3055\u3067\u4eba\u53e3\u3001\u305d\u3057\u3066\u3001\u70b9\u306e\u8272\u3067\u5927\u9678\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u30d7\u30ed\u30c3\u30c8\u3092\u901a\u3058\u3066\u3001\u4ee5\u4e0b\u306e\u50be\u5411\u304c\u8aad\u307f\u53d6\u308c\u307e\u3059\u3002</p> <ul> <li>GDP\u304c\u9ad8\u3044\u56fd\u307b\u3069\u3001\u5e73\u5747\u5bff\u547d\u304c\u9577\u3044\u50be\u5411\u304c\u3042\u308b\u3002</li> <li>\u5927\u9678\u3054\u3068\u306b\u5206\u5e03\u304c\u7570\u306a\u308a\u3001\u7279\u306b\u30a2\u30d5\u30ea\u30ab\uff08Africa\uff09\u306e\u56fd\u3005\u306fGDP\u3068\u5e73\u5747\u5bff\u547d\u304c\u4f4e\u3044\u50be\u5411\u304c\u3042\u308b\u3002</li> <li>\u56fd\u3054\u3068\u306e\u4eba\u53e3\uff08\u70b9\u306e\u5927\u304d\u3055\uff09\u3082\u8996\u899a\u7684\u306b\u628a\u63e1\u3067\u304d\u308b\u3002</li> </ul> <p>\u3053\u306e\u3088\u3046\u306b\u81ea\u5206\u304c\u793a\u3057\u305f\u3044\u3082\u306e\u304c\u4f55\u304b\u3068\u3044\u3046\u3053\u3068\u3092\u610f\u8b58\u3057\u3066\u3001\u9069\u5207\u306a\u56f3\u3092\u66f8\u304f\u3053\u3068\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u57fa\u672c\u3068\u306a\u308b\u3082\u306e\u3067\u3059\u3002\u307e\u305f\u3001\u30c7\u30fc\u30bf\u306e\u7a2e\u985e\u306b\u3088\u3063\u3066\u3082\u3001\u6700\u9069\u306a\u56f3\u306e\u66f8\u304d\u65b9\u306f\u7570\u306a\u308a\u307e\u3059\u3002\u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u3068\u3057\u3066\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3051\u308b5\u3064\u306e\u57fa\u672c\u7684\u306a\u56f3\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#python-5-5ng","title":"Python\u306b\u3088\u308b\u30c7\u30fc\u30bf\u53ef\u8996\u5316\u5165\u9580: 5\u3064\u306e\u4ee3\u8868\u7684\u306a\u30b0\u30e9\u30d5 (5NG)","text":"<p>\u3053\u306e\u30c6\u30ad\u30b9\u30c8\u3067\u306f\u3001Google Colaboratory\u4e0a\u3067Python\u3001NumPy\u3001Pandas\u3001matplotlib\u3001seaborn\u3092\u7528\u3044\u3066\u3001\u30c7\u30fc\u30bf\u53ef\u8996\u5316\u306e\u57fa\u672c\u3068\u306a\u308b5\u7a2e\u985e\u306e\u30b0\u30e9\u30d5\uff085NG\uff09\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001seaborn\u306b\u7d44\u307f\u8fbc\u307e\u308c\u3066\u3044\u308biris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f8b\u3068\u3057\u3066\u4f7f\u7528\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#5-5ng","title":"5\u3064\u306e\u4ee3\u8868\u7684\u306a\u30b0\u30e9\u30d5 \u2013 5NG","text":"<p>\u672c\u66f8\u3067\u306f\u3001\u4ee5\u4e0b\u306e5\u7a2e\u985e\u306e\u30b0\u30e9\u30d5\u306b\u6ce8\u76ee\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306f\u4e00\u822c\u7684\u306b\u540d\u524d\u304c\u4ed8\u3051\u3089\u308c\u3066\u304a\u308a\u3001\u4ee5\u964d\u300c5NG\u300d\u3068\u547c\u3073\u307e\u3059\uff1a</p> <ul> <li>\u6563\u5e03\u56f3 (scatterplots)</li> <li>\u6298\u308c\u7dda\u30b0\u30e9\u30d5 (linegraphs)</li> <li>\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0 (histograms)</li> <li>\u7bb1\u3072\u3052\u56f3 (boxplots)</li> <li>\u68d2\u30b0\u30e9\u30d5 (barplots)</li> </ul> <p>\u3053\u308c\u3089\u306e\u57fa\u672c\u7684\u306a\u30b0\u30e9\u30d5\u3092\u899a\u3048\u3066\u304a\u304f\u3053\u3068\u3067\u3001\u3055\u307e\u3056\u307e\u306a\u7a2e\u985e\u306e\u5909\u6570\u3092\u8996\u899a\u7684\u306b\u8868\u73fe\u3059\u308b\u969b\u306b\u975e\u5e38\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\u306a\u304a\u3001\u3042\u308b\u30b0\u30e9\u30d5\u306f\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306b\u9069\u3057\u3066\u304a\u308a\u3001\u307e\u305f\u5225\u306e\u30b0\u30e9\u30d5\u306f\u6570\u5024\u5909\u6570\u306b\u9069\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#5ng1-scatterplots","title":"5NG#1: \u6563\u5e03\u56f3 (Scatterplots)","text":"<p>\u6563\u5e03\u56f3\u306f\u30012\u3064\u306e\u6570\u5024\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u3092\u8996\u899a\u5316\u3059\u308b\u6700\u3082\u57fa\u672c\u7684\u306a\u30b0\u30e9\u30d5\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4ee5\u4e0b\u306e2\u3064\u306e\u6570\u5024\u5909\u6570\u306e\u95a2\u4fc2\u3092\u6563\u5e03\u56f3\u3067\u8868\u73fe\u3057\u307e\u3059\uff1a</p> <ul> <li>sepal_length\uff1ax\u8ef8</li> <li>petal_length\uff1ay\u8ef8</li> </ul>"},{"location":"lectures/SIWS/02-visualization/#_5","title":"\u57fa\u672c\u7684\u306a\u6563\u5e03\u56f3\u306e\u4f5c\u6210","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306f\u3001iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u307f\u3001<code>sepal_length</code> \u3068 <code>petal_length</code> \u306e\u95a2\u4fc2\u3092\u6563\u5e03\u56f3\u3067\u53ef\u8996\u5316\u3059\u308b\u4f8b\u3067\u3059\u3002</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\niris = sns.load_dataset(\"iris\")\n\n# \u57fa\u672c\u7684\u306a\u6563\u5e03\u56f3\u306e\u4f5c\u6210\nsns.scatterplot(data=iris, x=\"sepal_length\", y=\"petal_length\")\nplt.title(\"Iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8: sepal_length\u3068petal_length\u306e\u6563\u5e03\u56f3\")\nplt.xlabel(\"sepal_length\")\nplt.ylabel(\"petal_length\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u30b3\u30fc\u30c9\u3067\u306f\u3001sns.scatterplot \u95a2\u6570\u3092\u7528\u3044\u3066\u3001iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5185\u306esepal_length\uff08x\u8ef8\uff09\u3068petal_length\uff08y\u8ef8\uff09\u306e\u5404\u30b5\u30f3\u30d7\u30eb\u3092\u70b9\u3068\u3057\u3066\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\u56f3\u3067\u306f\u3001\u5404\u70b9\u304c1\u3064\u306e\u82b1\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u8868\u3057\u3066\u304a\u308a\u3001\u4e21\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u304c\u8996\u899a\u7684\u306b\u628a\u63e1\u3067\u304d\u307e\u3059\u3002</p> <p>Note</p> <p>(LC2.1) Iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092Pandas\u306eDataFrame\u3068\u3057\u3066\u8868\u793a\u3057\u3001\u5404\u5909\u6570\uff08sepal_length, sepal_width, petal_length, petal_width, species\uff09\u306e\u578b\u3068\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_6","title":"\u904e\u5270\u306a\u30d7\u30ed\u30c3\u30c8\uff08\u30aa\u30fc\u30d0\u30fc\u30d7\u30ed\u30c3\u30c6\u30a3\u30f3\u30b0\uff09\u3078\u306e\u5bfe\u51e6","text":"<p>\u30c7\u30fc\u30bf\u6570\u304c\u975e\u5e38\u306b\u591a\u3044\u5834\u5408\u3001\u540c\u3058\u4f4d\u7f6e\u306b\u70b9\u304c\u91cd\u306a\u3063\u3066\u3057\u307e\u3044\u3001\u5b9f\u969b\u306b\u30d7\u30ed\u30c3\u30c8\u3055\u308c\u3066\u3044\u308b\u70b9\u306e\u6570\u304c\u5206\u304b\u308a\u306b\u304f\u304f\u306a\u308b\u300c\u30aa\u30fc\u30d0\u30fc\u30d7\u30ed\u30c3\u30c6\u30a3\u30f3\u30b0\u300d\u304c\u767a\u751f\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u5bfe\u51e6\u3059\u308b\u65b9\u6cd5\u306f\u4e3b\u306b2\u3064\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#1-alpha","title":"\u65b9\u6cd51: \u900f\u660e\u5ea6 (alpha) \u306e\u8abf\u6574","text":"<p>\u70b9\u306e\u900f\u660e\u5ea6\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u3001\u91cd\u306a\u3063\u305f\u90e8\u5206\u304c\u6fc3\u304f\u898b\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u3001\u70b9\u306e\u5bc6\u96c6\u5ea6\u304c\u8996\u899a\u7684\u306b\u5206\u304b\u308a\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306f\u900f\u660e\u5ea6\u3092\u8a2d\u5b9a\u3057\u305f\u6563\u5e03\u56f3\u306e\u4f8b\u3067\u3059\u3002</p> <p>python \u30b3\u30d4\u30fc\u3059\u308b sns.scatterplot(data=iris, x=\"sepal_length\", y=\"petal_length\", alpha=0.5) plt.title(\"\u900f\u660e\u5ea6\u3092\u8abf\u6574\u3057\u305f\u6563\u5e03\u56f3 (alpha=0.5)\") plt.xlabel(\"sepal_length\") plt.ylabel(\"petal_length\") plt.show() \u3053\u3053\u3067\u306f\u3001alpha \u30d1\u30e9\u30e1\u30fc\u30bf\u30920.5\u306b\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u5404\u70b9\u306e\u4e0d\u900f\u660e\u5ea6\u304c50%\u3068\u306a\u308a\u3001\u91cd\u306a\u3063\u3066\u3044\u308b\u90e8\u5206\u306f\u3088\u308a\u6fc3\u304f\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#2-jitter","title":"\u65b9\u6cd52: \u30b8\u30c3\u30bf\u30fc (Jitter) \u306e\u8ffd\u52a0","text":"<p>\u30b8\u30c3\u30bf\u30fc\u306f\u3001\u5404\u70b9\u306b\u5c0f\u3055\u306a\u30e9\u30f3\u30c0\u30e0\u306a\u305a\u308c\uff08\u30ce\u30a4\u30ba\uff09\u3092\u52a0\u3048\u3066\u3001\u540c\u3058\u4f4d\u7f6e\u306b\u91cd\u306a\u3063\u3066\u8868\u793a\u3055\u308c\u308b\u70b9\u3092\u5c11\u3057\u305a\u3089\u3059\u624b\u6cd5\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u91cd\u306a\u308a\u5408\u3063\u3066\u3044\u308b\u70b9\u304c\u500b\u5225\u306b\u898b\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306f\u30b8\u30c3\u30bf\u30fc\u3092\u52a0\u3048\u305f\u6563\u5e03\u56f3\u306e\u4f8b\u3067\u3059\u3002</p> <pre><code>import numpy as np\n\n# \u30b8\u30c3\u30bf\u30fc\u306e\u5f37\u3055\uff08\u8abf\u6574\u53ef\u80fd\uff09\njitter_strength = 0.1\n\n# sepal_length\u3068petal_length\u306b\u30e9\u30f3\u30c0\u30e0\u306a\u30ce\u30a4\u30ba\u3092\u52a0\u3048\u308b\nx_jittered = iris[\"sepal_length\"] + np.random.uniform(-jitter_strength, jitter_strength, size=len(iris))\ny_jittered = iris[\"petal_length\"] + np.random.uniform(-jitter_strength, jitter_strength, size=len(iris))\n\nplt.figure()\nplt.scatter(x_jittered, y_jittered, alpha=0.7)\nplt.title(\"\u30b8\u30c3\u30bf\u30fc\u3092\u52a0\u3048\u305f\u6563\u5e03\u56f3\")\nplt.xlabel(\"sepal_length (jittered)\")\nplt.ylabel(\"petal_length (jittered)\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u4f8b\u3067\u306f\u3001np.random.uniform \u3092\u7528\u3044\u3066\u3001\u5404\u70b9\u306b -0.1 \u304b\u3089 0.1 \u306e\u7bc4\u56f2\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u5024\u3092\u52a0\u3048\u3066\u3044\u307e\u3059\u3002\u30b8\u30c3\u30bf\u30fc\u306f\u3042\u304f\u307e\u3067\u8996\u899a\u5316\u306e\u305f\u3081\u306e\u624b\u6cd5\u3067\u3042\u308a\u3001\u5143\u306e\u30c7\u30fc\u30bf\u81ea\u4f53\u306f\u5909\u66f4\u3055\u308c\u307e\u305b\u3093\u3002</p> <p>Learning Check</p> <p>(LC2.2) \u900f\u660e\u5ea6\uff08alpha\uff09\u306e\u8a2d\u5b9a\u304c\u3001\u91cd\u306a\u308a\u5408\u3046\u70b9\u306e\u5bc6\u96c6\u5ea6\u3092\u3069\u306e\u3088\u3046\u306b\u8868\u73fe\u3059\u308b\u304b\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002 (LC2.3) \u30b8\u30c3\u30bf\u30fc\u306b\u3088\u3063\u3066\u70b9\u304c\u308f\u305a\u304b\u306b\u305a\u308c\u308b\u3068\u3001\u5143\u306e\u30c7\u30fc\u30bf\u306e\u30d1\u30bf\u30fc\u30f3\u306f\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u3067\u3057\u3087\u3046\u304b\uff1f\u307e\u305f\u3001\u30b8\u30c3\u30bf\u30fc\u306e\u5f37\u3055\u306f\u3069\u306e\u3088\u3046\u306b\u8abf\u6574\u3059\u308b\u306e\u304c\u9069\u5207\u304b\u691c\u8a0e\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_7","title":"\u30c1\u30a7\u30c3\u30af\u9805\u76ee","text":"<ul> <li>(LC2.4) \u900f\u660e\u5ea6\u306e\u8abf\u6574\u3068\u30b8\u30c3\u30bf\u30fc\u306e\u8ffd\u52a0\u3001\u305d\u308c\u305e\u308c\u306e\u624b\u6cd5\u306e\u30e1\u30ea\u30c3\u30c8\u30fb\u30c7\u30e1\u30ea\u30c3\u30c8\u3092\u6bd4\u8f03\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</li> <li>(LC2.5) Iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5185\u306e\u4ed6\u306e\u6570\u5024\u5909\u6570\uff08\u4f8b\u3048\u3070\u3001sepal_width\u3084petal_width\uff09\u3092\u7528\u3044\u3066\u3001\u5225\u306e\u6563\u5e03\u56f3\u3092\u4f5c\u6210\u3057\u3001\u5f97\u3089\u308c\u308b\u30d1\u30bf\u30fc\u30f3\u306e\u9055\u3044\u3092\u89b3\u5bdf\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul> <p>\u3053\u306e\u3088\u3046\u306b\u3001Python\u3068Google Colaboratory\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u3001R\u3084RStudio\u3067\u884c\u3063\u3066\u3044\u305f\u3088\u3046\u306a\u30b0\u30e9\u30d5\u30a3\u30c3\u30af\u30b9\u306e\u57fa\u672c\u64cd\u4f5c\u3092\u624b\u8efd\u306b\u518d\u73fe\u3057\u3001\u3055\u307e\u3056\u307e\u306a\u624b\u6cd5\u3067\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002\u6b21\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u306f\u3001\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u3001\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3001\u7bb1\u3072\u3052\u56f3\u3001\u68d2\u30b0\u30e9\u30d5\u306e\u4f5c\u6210\u65b9\u6cd5\u306b\u3064\u3044\u3066\u3082\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_8","title":"\u307e\u3068\u3081","text":"<p>\u6563\u5e03\u56f3\u306f\u30012\u3064\u306e\u6570\u5024\u5909\u6570\u9593\u306e\u95a2\u4fc2\u3092\u76f4\u611f\u7684\u306b\u628a\u63e1\u3059\u308b\u305f\u3081\u306e\u57fa\u672c\u7684\u306a\u53ef\u8996\u5316\u624b\u6cd5\u3067\u3059\u3002\u7279\u306b\u30c7\u30fc\u30bf\u91cf\u304c\u591a\u3044\u5834\u5408\u3001\u900f\u660e\u5ea6\u306e\u8abf\u6574\u3084\u30b8\u30c3\u30bf\u30fc\u306e\u8ffd\u52a0\u3068\u3044\u3063\u305f\u5de5\u592b\u3092\u3059\u308b\u3053\u3068\u3067\u3001\u91cd\u306a\u308a\u5408\u3063\u305f\u30c7\u30fc\u30bf\u70b9\u306e\u60c5\u5831\u3092\u3088\u308a\u660e\u78ba\u306b\u4f1d\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u3069\u306e\u624b\u6cd5\u3092\u7528\u3044\u308b\u304b\u306f\u3001\u30c7\u30fc\u30bf\u306e\u7279\u6027\u3084\u4f1d\u3048\u305f\u3044\u5185\u5bb9\u306b\u5fdc\u3058\u3066\u5224\u65ad\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#5ng2-linegraphs","title":"5NG#2: \u6298\u308c\u7dda\u30b0\u30e9\u30d5 (Linegraphs)","text":"<p>\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u306f\u3001x\u8ef8\u306b\u9806\u5e8f\u6027\uff08\u7279\u306b\u6642\u9593\u306a\u3069\u306e\u9023\u7d9a\u7684\u306a\u60c5\u5831\uff09\u304c\u3042\u308b\u5834\u5408\u306b\u30012\u3064\u306e\u6570\u5024\u5909\u6570\u9593\u306e\u95a2\u4fc2\u3092\u8996\u899a\u5316\u3059\u308b\u305f\u3081\u306e\u30b0\u30e9\u30d5\u3067\u3059\u3002\u96a3\u63a5\u3059\u308b\u30c7\u30fc\u30bf\u70b9\u3092\u7dda\u3067\u7d50\u3076\u3053\u3068\u3067\u3001\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5909\u5316\u3084\u30c8\u30ec\u30f3\u30c9\u3092\u660e\u78ba\u306b\u793a\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u3053\u3053\u3067\u306f\u3001seaborn\u306b\u7d44\u307f\u8fbc\u307e\u308c\u3066\u3044\u308bflights\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u3066\u30011949\u5e74\u304b\u30891960\u5e74\u307e\u3067\u306e\u6708\u3054\u3068\u306e\u4e57\u5ba2\u6570\u306e\u63a8\u79fb\u3092\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u3067\u53ef\u8996\u5316\u3059\u308b\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002 flights\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u3001\u4ee5\u4e0b\u306e\u5909\u6570\u3092\u542b\u3093\u3067\u3044\u307e\u3059\uff1a</p> <ul> <li>year: \u5e74\uff081949\uff5e1960\uff09</li> <li>month: \u6708\uff08\"Jan\", \"Feb\", \u2026, \"Dec\" \u3068\u3044\u3063\u305f\u6587\u5b57\u5217\uff09</li> <li>passengers: \u5404\u6708\u306e\u4e57\u5ba2\u6570</li> </ul>"},{"location":"lectures/SIWS/02-visualization/#flights","title":"flights\u30c7\u30fc\u30bf\u306e\u6e96\u5099\u3068\u65e5\u6642\u5909\u6570\u306e\u4f5c\u6210","text":"<p>\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u306f\u3001x\u8ef8\u306b\u9806\u5e8f\u6027\u306e\u3042\u308b\u5909\u6570\uff08\u3053\u3053\u3067\u306f\u65e5\u6642\uff09\u304c\u5fc5\u8981\u3067\u3059\u3002 flights\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u306f\u3001<code>year</code> \u3068 <code>month</code> \u306e2\u3064\u306e\u5909\u6570\u304b\u3089\u65e5\u6642\u60c5\u5831\u3092\u4f5c\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u306f\u3001Pandas\u3092\u7528\u3044\u3066\u3053\u308c\u3089\u306e\u5909\u6570\u3092\u7d44\u307f\u5408\u308f\u305b\u3001<code>date</code> \u3068\u3044\u3046\u65b0\u3057\u3044\u65e5\u6642\u578b\u306e\u5909\u6570\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# flights\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\nflights = sns.load_dataset(\"flights\")\nprint(flights.head())\n\n# year\u3068month\u304b\u3089datetime\u578b\u306e\u5909\u6570\u3092\u4f5c\u6210\n# flights\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306emonth\u5217\u306f \"Jan\", \"Feb\", ... \u306e\u5f62\u5f0f\u306e\u305f\u3081\u3001'%Y-%b' \u3092\u6307\u5b9a\nflights['date'] = pd.to_datetime(flights['year'].astype(str) + '-' + flights['month'], format='%Y-%b')\nprint(flights.head())\n</code></pre>"},{"location":"lectures/SIWS/02-visualization/#_9","title":"\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u306e\u4f5c\u6210","text":"<p>\u4f5c\u6210\u3057\u305fdate\u5909\u6570\u3092x\u8ef8\u3001passengers\u5909\u6570\u3092y\u8ef8\u3068\u3057\u3066\u3001seaborn\u306elineplot()\u95a2\u6570\u3092\u4f7f\u3044\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u3092\u63cf\u3044\u3066\u307f\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u306f\u3001\u56f3\u306e\u30b5\u30a4\u30ba\u3092\u8abf\u6574\u3057\u3001\u30bf\u30a4\u30c8\u30eb\u3084\u8ef8\u30e9\u30d9\u30eb\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002</p> <pre><code>plt.figure(figsize=(12,6))\nsns.lineplot(data=flights, x=\"date\", y=\"passengers\")\nplt.title(\"1949\u5e74\uff5e1960\u5e74\u306e\u6708\u3054\u3068\u306e\u4e57\u5ba2\u6570\u306e\u63a8\u79fb\")\nplt.xlabel(\"\u65e5\u4ed8\")\nplt.ylabel(\"\u4e57\u5ba2\u6570\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u30b3\u30fc\u30c9\u3067\u306f\u3001date\uff08\u65e5\u6642\u578b\u306e\u5909\u6570\uff09\u306b\u6cbf\u3063\u3066passengers\uff08\u4e57\u5ba2\u6570\uff09\u306e\u5404\u30c7\u30fc\u30bf\u70b9\u3092\u7dda\u3067\u7d50\u3093\u3067\u3044\u307e\u3059\u3002\u65e5\u6642\u306e\u9806\u5e8f\u304c\u4fdd\u305f\u308c\u308b\u305f\u3081\u3001\u9023\u7d9a\u3057\u305f\u30c7\u30fc\u30bf\u306e\u5909\u5316\u3092\u76f4\u611f\u7684\u306b\u628a\u63e1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_10","title":"\u30c1\u30a7\u30c3\u30af\u9805\u76ee","text":"<p>(LC1) flights\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306emonth\u5217\u306f\u3069\u306e\u3088\u3046\u306a\u5f62\u5f0f\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u304b\uff1f\u307e\u305f\u3001\u306a\u305cyear\u3068month\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u65e5\u6642\u578b\u306e\u5909\u6570\u3092\u4f5c\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u3057\u3087\u3046\u304b\uff1f (LC2) \u6298\u308c\u7dda\u30b0\u30e9\u30d5\u3067\u30c7\u30fc\u30bf\u70b9\u3092\u7dda\u3067\u7d50\u3076\u3053\u3068\u306f\u3001\u3069\u306e\u3088\u3046\u306a\u60c5\u5831\uff08\u4f8b\uff1a\u50be\u5411\u3084\u5909\u5316\u306e\u65b9\u5411\u6027\uff09\u3092\u8996\u899a\u7684\u306b\u4f1d\u3048\u308b\u306e\u3067\u3057\u3087\u3046\u304b\uff1f (LC3) \u4ed6\u306e\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\uff08\u4f8b\uff1a\u3042\u308b\u90fd\u5e02\u306e\u6708\u5225\u5e73\u5747\u6c17\u6e29\u306a\u3069\uff09\u306b\u304a\u3044\u3066\u3001\u540c\u69d8\u306e\u65b9\u6cd5\u3067\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u306f\u53ef\u80fd\u3067\u3057\u3087\u3046\u304b\uff1f\u305d\u306e\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306a\u70b9\u306b\u6ce8\u610f\u3059\u3079\u304d\u3067\u3057\u3087\u3046\u304b\uff1f</p>"},{"location":"lectures/SIWS/02-visualization/#_11","title":"\u307e\u3068\u3081","text":"<p>\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u306f\u3001\u6642\u9593\u3084\u9806\u5e8f\u6027\u306e\u3042\u308b\u5909\u6570\u3092x\u8ef8\u306b\u3068\u308b\u3053\u3068\u3067\u3001\u9023\u7d9a\u3057\u305f\u30c7\u30fc\u30bf\u306e\u5909\u5316\u3092\u52b9\u679c\u7684\u306b\u8868\u73fe\u3067\u304d\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001seaborn\u306eflights\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u3001year \u3068 month \u304b\u3089\u4f5c\u6210\u3057\u305f\u65e5\u6642\u578b\u306e\u5909\u6570\u3092\u7528\u3044\u3066\u30011949\u5e74\u304b\u30891960\u5e74\u307e\u3067\u306e\u6708\u3054\u3068\u306e\u4e57\u5ba2\u6570\u306e\u63a8\u79fb\u3092\u7dda\u30b0\u30e9\u30d5\u3067\u53ef\u8996\u5316\u3057\u307e\u3057\u305f\u3002 \u3053\u306e\u624b\u6cd5\u306f\u3001\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5206\u6790\u3084\u30c8\u30ec\u30f3\u30c9\u306e\u628a\u63e1\u306b\u975e\u5e38\u306b\u6709\u7528\u3067\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#5ng3-histograms","title":"5NG#3: \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0 (Histograms)","text":"<p>\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306f\u30011\u3064\u306e\u6570\u5024\u5909\u6570\u306e\u5206\u5e03\u3092\u8996\u899a\u5316\u3059\u308b\u305f\u3081\u306e\u30b0\u30e9\u30d5\u3067\u3059\u3002  \u3053\u3053\u3067\u306f\u3001seaborn \u306b\u7d44\u307f\u8fbc\u307e\u308c\u3066\u3044\u308b titanic \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3001\u4e57\u5ba2\u306e age\uff08\u5e74\u9f62\uff09\u306e\u5206\u5e03\u306b\u6ce8\u76ee\u3057\u307e\u3059\u3002  \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u70b9\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p> <ol> <li>\u6700\u5c0f\u5024\u3068\u6700\u5927\u5024\u306f\u3069\u3053\u304b\uff1f</li> <li>\u4e2d\u5fc3\u3084\u300c\u6700\u3082\u5178\u578b\u7684\u306a\u300d\u5024\u306f\u3069\u3053\u304b\uff1f</li> <li>\u5024\u306e\u5e83\u304c\u308a\u306f\u3069\u306e\u7a0b\u5ea6\u304b\uff1f</li> <li>\u983b\u51fa\u5024\u3068\u7a00\u306a\u5024\u306f\u3069\u3053\u304b\uff1f</li> </ol>"},{"location":"lectures/SIWS/02-visualization/#1","title":"1. \u5206\u5e03\u306e\u6982\u8981\u3092\u8996\u899a\u5316\u3059\u308b","text":"<p>\u307e\u305a\u3001\u5e74\u9f62\u306e\u5404\u89b3\u6e2c\u5024\u3092\u6a2a\u4e00\u5217\u306b\u4e26\u3079\u305f\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u3001\u5206\u5e03\u306e\u5927\u307e\u304b\u306a\u69d8\u5b50\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002 \uff08\u3053\u306e\u30d7\u30ed\u30c3\u30c8\u306f\u3001\u5f8c\u8ff0\u3059\u308b\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3068\u6bd4\u3079\u308b\u3068\u30aa\u30fc\u30d0\u30fc\u30d7\u30ed\u30c3\u30c8\u304c\u767a\u751f\u3059\u308b\u305f\u3081\u3001\u7d30\u304b\u3044\u983b\u5ea6\u306f\u8aad\u307f\u53d6\u308a\u306b\u304f\u3044\u3067\u3059\u304c\u3001\u5168\u4f53\u306e\u5e83\u304c\u308a\u3092\u76f4\u611f\u7684\u306b\u628a\u63e1\u3059\u308b\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\uff09</p> <p><pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# titanic\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\ntitanic = sns.load_dataset(\"titanic\")\n\n# NaN\u306e\u9664\u5916\uff08age\u5217\u306b\u306f\u6b20\u640d\u5024\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\uff09\ntitanic_age = titanic.dropna(subset=[\"age\"])\n\n# \u5404\u5e74\u9f62\u306e\u5024\u3092\u6a2a\u4e00\u5217\u306b\u4e26\u3079\u308b\uff08y\u5ea7\u6a19\u306f\u56fa\u5b9a\u5024\uff09\nplt.figure(figsize=(10, 1))\nsns.stripplot(x=\"age\", data=titanic_age, jitter=False, color=\"gray\")\nplt.xlabel(\"Age\")\nplt.yticks([])  # y\u8ef8\u306e\u76ee\u76db\u308a\u3092\u975e\u8868\u793a\u306b\nplt.title(\"Titanic\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff1a\u4e57\u5ba2\u306e\u5e74\u9f62\u306e\u6563\u5e03\u56f3\uff08\u6a2a\u4e00\u5217\uff09\")\nplt.show()\n</code></pre> \u3053\u306e\u30d7\u30ed\u30c3\u30c8\u3067\u306f\u3001\u5404\u70b9\u304c\u500b\u3005\u306e\u4e57\u5ba2\u306e\u5e74\u9f62\u3092\u793a\u3057\u3066\u304a\u308a\u3001\u5e74\u9f62\u306e\u5206\u5e03\u306e\u6982\u5f62\u3092\u628a\u63e1\u3067\u304d\u307e\u3059\u3002 \u305f\u3060\u3057\u3001\u591a\u304f\u306e\u70b9\u304c\u91cd\u306a\u3063\u3066\u8868\u793a\u3055\u308c\u308b\u305f\u3081\u3001\u7d30\u304b\u3044\u983b\u5ea6\u306f\u898b\u3048\u306b\u304f\u3044\u3067\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#2","title":"2. \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u4f5c\u6210","text":"<p>\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3067\u306f\u3001x\u8ef8\uff08\u3053\u3053\u3067\u306f\u5e74\u9f62\uff09\u3092\u3044\u304f\u3064\u304b\u306e\u300c\u30d3\u30f3\u300d\uff08\u533a\u9593\uff09\u306b\u5206\u5272\u3057\u3001\u5404\u533a\u9593\u306b\u542b\u307e\u308c\u308b\u89b3\u6e2c\u5024\u306e\u6570\uff08\u5ea6\u6570\uff09\u3092\u68d2\u30b0\u30e9\u30d5\u3067\u793a\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u3067\u5e74\u9f62\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u4f5c\u6210\u3059\u308b\u4f8b\u3067\u3059\u3002</p> <pre><code>plt.figure(figsize=(10,6))\nsns.histplot(data=titanic_age, x=\"age\")\nplt.title(\"Titanic\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff1a\u4e57\u5ba2\u306e\u5e74\u9f62\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0 (\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a)\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u30d7\u30ed\u30c3\u30c8\u3067\u306f\u3001seaborn \u304c\u81ea\u52d5\u7684\u306b30\u500b\u7a0b\u5ea6\u306e\u30d3\u30f3\u306b\u5206\u5272\u3057\u3066\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u63cf\u753b\u3057\u3066\u3044\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u30d3\u30f3\u306e\u6570\u304c\u591a\u3059\u304e\u308b\u5834\u5408\u3001\u5404\u30d3\u30f3\u306e\u5e45\u304c\u72ed\u304f\u306a\u308a\u3001\u5206\u5e03\u304c\u300c\u3054\u3061\u3083\u3054\u3061\u3083\u300d\u3057\u3066\u898b\u3048\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#3","title":"3. \u30d3\u30f3\u306e\u8abf\u6574","text":"<p>\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u89e3\u91c8\u3092\u5bb9\u6613\u306b\u3059\u308b\u305f\u3081\u306b\u3001\u30d3\u30f3\u306e\u6570\u3084\u5e45\u3092\u8abf\u6574\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4ee5\u4e0b\u306b\u3001\u30d3\u30f3\u306e\u6570\u3068\u30d3\u30f3\u5e45\u3092\u5909\u66f4\u3057\u305f\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#1_1","title":"(1) \u30d3\u30f3\u306e\u6570\u3092\u6307\u5b9a\u3059\u308b\u65b9\u6cd5","text":"<p>\u3053\u3053\u3067\u306f\u3001bins=20 \u3068\u3057\u3066\u30d3\u30f3\u306e\u6570\u309220\u306b\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002 \u307e\u305f\u3001edgecolor=\"white\" \u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u5404\u30d3\u30f3\u306e\u5883\u754c\u7dda\u3092\u767d\u8272\u306b\u3057\u3001\u533a\u5207\u308a\u3092\u660e\u78ba\u306b\u3057\u3066\u3044\u307e\u3059\u3002</p> <pre><code>plt.figure(figsize=(10,6))\nsns.histplot(data=titanic_age, x=\"age\", bins=20, color=\"steelblue\", edgecolor=\"white\")\nplt.title(\"Titanic\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff1a\u4e57\u5ba2\u306e\u5e74\u9f62\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0 (20\u30d3\u30f3)\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.show()\n</code></pre>"},{"location":"lectures/SIWS/02-visualization/#2_1","title":"(2) \u30d3\u30f3\u5e45\u3092\u6307\u5b9a\u3059\u308b\u65b9\u6cd5","text":"<p>seaborn \u306e histplot() \u3067\u306f\u3001binwidth \u5f15\u6570\u3092\u4f7f\u3063\u3066\u5404\u30d3\u30f3\u306e\u5e45\u3092\u76f4\u63a5\u6307\u5b9a\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u30d3\u30f3\u5e45\u30925\u6b73\u306b\u8a2d\u5b9a\u3059\u308b\u5834\u5408\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p> <pre><code>plt.figure(figsize=(10,6))\nsns.histplot(data=titanic_age, x=\"age\", binwidth=5, color=\"steelblue\", edgecolor=\"white\")\nplt.title(\"Titanic\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff1a\u4e57\u5ba2\u306e\u5e74\u9f62\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0 (\u30d3\u30f3\u5e45=5)\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u4f8b\u3067\u306f\u3001\u5e74\u9f62\u304c5\u6b73\u523b\u307f\u3067\u533a\u5207\u3089\u308c\u3001\u5404\u533a\u9593\u306b\u304a\u3051\u308b\u4e57\u5ba2\u6570\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#4-faceting","title":"4. Faceting\uff08\u30d5\u30a1\u30bb\u30c3\u30c8\uff09\u306b\u3088\u308b\u5206\u5272\u8868\u793a","text":"<p>\u30d5\u30a1\u30bb\u30c3\u30c8\u306f\u3001\u3042\u308b\u5909\u6570\u306e\u5024\u3054\u3068\u306b\u540c\u3058\u7a2e\u985e\u306e\u30d7\u30ed\u30c3\u30c8\u3092\u8907\u6570\u63cf\u753b\u3057\u3001\u6bd4\u8f03\u3057\u3084\u3059\u304f\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001sex\uff08\u6027\u5225\uff09\u306b\u3088\u3063\u3066\u4e57\u5ba2\u306e\u5e74\u9f62\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u5206\u5272\u3057\u3066\u8868\u793a\u3057\u3066\u307f\u307e\u3059\u3002</p> <pre><code># seaborn \u306e displot() \u306f\u3001\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u30d5\u30a1\u30bb\u30c3\u30c8\u8868\u793a\u306b\u9069\u3057\u3066\u3044\u307e\u3059\nsns.displot(data=titanic_age, x=\"age\", bins=20, color=\"steelblue\", edgecolor=\"white\", col=\"sex\", height=4, aspect=1.2)\nplt.suptitle(\"Titanic\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff1a\u6027\u5225\u3054\u3068\u306e\u4e57\u5ba2\u306e\u5e74\u9f62\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\", y=1.03)\nplt.show()\n</code></pre> <p>\u3053\u306e\u30b3\u30fc\u30c9\u3067\u306f\u3001col=\"sex\" \u306b\u3088\u308a\u3001\u6027\u5225\uff08male \u3068 female\uff09\u3054\u3068\u306b\u5225\u3005\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u304c\u63cf\u753b\u3055\u308c\u3001\u5404\u30b0\u30eb\u30fc\u30d7\u5185\u3067\u5e74\u9f62\u306e\u5206\u5e03\u3092\u6bd4\u8f03\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_12","title":"\u30c1\u30a7\u30c3\u30af\u9805\u76ee","text":"<ul> <li> <p>(LC1) titanic \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u304a\u3051\u308b age \u5909\u6570\u306e\u6700\u5c0f\u5024\u3001\u6700\u5927\u5024\u3001\u4e2d\u592e\u5024\u306f\u305d\u308c\u305e\u308c\u3069\u306e\u7a0b\u5ea6\u304b\u8abf\u3079\u3066\u307f\u307e\u3057\u3087\u3046\u3002 \uff08\u4f8b: titanic_age[\"age\"].describe() \u3092\u7528\u3044\u3066\u78ba\u8a8d\u3067\u304d\u307e\u3059\uff09</p> </li> <li> <p>(LC2) \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3067\u30d3\u30f3\u306e\u6570\u3084\u5e45\u3092\u5909\u66f4\u3059\u308b\u3068\u3001\u5206\u5e03\u306e\u89e3\u91c8\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u304c\u3042\u308b\u304b\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002 \uff08\u4f8b: \u30d3\u30f3\u306e\u6570\u3092\u5897\u3084\u3059\u3068\u7d30\u90e8\u304c\u898b\u3048\u3084\u3059\u304f\u306a\u308b\u4e00\u65b9\u3001\u30ce\u30a4\u30ba\u304c\u76ee\u7acb\u3064\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\uff09</p> </li> <li> <p>(LC3) \u6027\u5225\uff08sex\uff09\u3054\u3068\u306b\u5e74\u9f62\u306e\u5206\u5e03\u304c\u7570\u306a\u308b\u7406\u7531\u306b\u3064\u3044\u3066\u3001\u30c7\u30fc\u30bf\u3084\u6b74\u53f2\u7684\u80cc\u666f\u304b\u3089\u8003\u5bdf\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p> </li> </ul>"},{"location":"lectures/SIWS/02-visualization/#_13","title":"\u307e\u3068\u3081","text":"<p>\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306f\u30011\u3064\u306e\u6570\u5024\u5909\u6570\u306e\u5206\u5e03\uff08\u6700\u5c0f\u5024\u3001\u6700\u5927\u5024\u3001\u4e2d\u5fc3\u3001\u6563\u3089\u3070\u308a\u3001\u983b\u51fa\u5024\u306a\u3069\uff09\u3092\u8996\u899a\u5316\u3059\u308b\u305f\u3081\u306e\u6709\u52b9\u306a\u30c4\u30fc\u30eb\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001seaborn \u306e titanic \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e age \u5909\u6570\u3092\u4f8b\u306b\u3001\u57fa\u672c\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u4f5c\u6210\u3001\u30d3\u30f3\u306e\u8abf\u6574\u65b9\u6cd5\u3001\u3055\u3089\u306b\u30d5\u30a1\u30bb\u30c3\u30c8\u3092\u7528\u3044\u305f\u30b0\u30eb\u30fc\u30d7\u5225\u6bd4\u8f03\u306e\u65b9\u6cd5\u3092\u793a\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u624b\u6cd5\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u3084\u6f5c\u5728\u7684\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u76f4\u611f\u7684\u306b\u7406\u89e3\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#5ng4-boxplots","title":"5NG#4: \u7bb1\u3072\u3052\u56f3 (Boxplots)","text":"<p>\u7bb1\u3072\u3052\u56f3\u306f\u3001\u6570\u5024\u5909\u6570\u306e\u4e94\u6570\u8981\u7d04\uff08\u6700\u5c0f\u5024\u3001\u7b2c1\u56db\u5206\u4f4d\u6570\u3001\u4e2d\u592e\u5024\u3001\u7b2c3\u56db\u5206\u4f4d\u6570\u3001\u6700\u5927\u5024\uff09\u306b\u57fa\u3065\u3044\u3066\u3001\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u8996\u899a\u5316\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001seaborn \u306e tips \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3001\u98f2\u98df\u4ee3\uff08<code>total_bill</code>\uff09\u306e\u5206\u5e03\u3092\u4f8b\u306b\u7bb1\u3072\u3052\u56f3\u306e\u4f5c\u6210\u65b9\u6cd5\u3068\u89e3\u91c8\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#1_2","title":"1. \u7279\u5b9a\u30b0\u30eb\u30fc\u30d7\u306b\u304a\u3051\u308b\u4e94\u6570\u8981\u7d04\u306e\u78ba\u8a8d","text":"<p>\u307e\u305a\u3001tips \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u30c7\u30a3\u30ca\u30fc\u30bf\u30a4\u30e0\uff08<code>time</code> \u304c <code>\"Dinner\"</code>\uff09\u306e\u30c7\u30fc\u30bf\u306b\u6ce8\u76ee\u3057\u3001\u305d\u306e <code>total_bill</code> \u306e\u4e94\u6570\u8981\u7d04\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002</p> <p><pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# tips \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\ntips = sns.load_dataset(\"tips\")\n\n# \u30c7\u30a3\u30ca\u30fc\u30bf\u30a4\u30e0\u306e\u30c7\u30fc\u30bf\u306b\u7d5e\u308b\ndinner_tips = tips[tips[\"time\"] == \"Dinner\"]\n\n# \u30c7\u30a3\u30ca\u30fc\u30bf\u30a4\u30e0\u306e total_bill \u306b\u5bfe\u3059\u308b\u4e94\u6570\u8981\u7d04\u306e\u8a08\u7b97\nn_dinner = dinner_tips.shape[0]\nmin_dinner = dinner_tips[\"total_bill\"].min()\nmax_dinner = dinner_tips[\"total_bill\"].max()\nquartiles = dinner_tips[\"total_bill\"].quantile([0.25, 0.5, 0.75])\nfive_number_summary = pd.DataFrame({\n    \"total_bill\": [min_dinner, quartiles.iloc[0], quartiles.iloc[1], quartiles.iloc[2], max_dinner]\n}, index=[\"Min\", \"25%\", \"50%\", \"75%\", \"Max\"])\n\nprint(\"\u30c7\u30a3\u30ca\u30fc\u30bf\u30a4\u30e0\u306e total_bill \u306e\u4e94\u6570\u8981\u7d04:\")\nprint(five_number_summary)\n</code></pre> \u3053\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u30c7\u30a3\u30ca\u30fc\u30bf\u30a4\u30e0\u306b\u304a\u3051\u308b total_bill \u306e\u6700\u5c0f\u5024\u3001\u7b2c\u4e00\u56db\u5206\u4f4d\u6570\u3001\u4e2d\u592e\u5024\u3001\u7b2c\u4e09\u56db\u5206\u4f4d\u6570\u3001\u6700\u5927\u5024\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_14","title":"\u7bb1\u3072\u3052\u56f3\u4f5c\u6210\u306e\u6bb5\u968e\u7684\u306a\u69cb\u7bc9","text":"<p>\u4ee5\u4e0b\u3067\u306f\u3001\u30c7\u30a3\u30ca\u30fc\u30bf\u30a4\u30e0\u306e total_bill \u306e\u89b3\u6e2c\u5024\u3092\u5bfe\u8c61\u306b\u30013\u7a2e\u985e\u306e\u30d7\u30ed\u30c3\u30c8\u3092\u6bb5\u968e\u7684\u306b\u4f5c\u6210\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>(a) \u30b8\u30c3\u30bf\u30fc\u3092\u52a0\u3048\u305f\u30d7\u30ed\u30c3\u30c8\u3068\u4e94\u6570\u8981\u7d04\u306e\u5024\u306e\u8868\u793a \u307e\u305a\u3001\u5404\u89b3\u6e2c\u5024\u3092\u30b8\u30c3\u30bf\u30fc\uff08\u308f\u305a\u304b\u306a\u6a2a\u65b9\u5411\u306e\u305a\u308c\uff09\u3092\u4ed8\u4e0e\u3057\u3066\u30d7\u30ed\u30c3\u30c8\u3057\u3001\u3055\u3089\u306b\u4e94\u6570\u8981\u7d04\u306e\u5404\u5024\u3092\u8d64\u306e\u7834\u7dda\u3067\u793a\u3057\u307e\u3059\u3002</p> <pre><code># \u30b5\u30d6\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210\uff08\u6a2a\u306b3\u3064\u4e26\u3079\u308b\uff09\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n\n# \u30d7\u30ed\u30c3\u30c8\u306e\u57fa\u672c\u8a2d\u5b9a\uff1ax \u8ef8\u306f \"Dinner\" \u306e\u56fa\u5b9a\u5024\nx_value = [\"Dinner\"] * n_dinner\n\n# (a) \u30b8\u30c3\u30bf\u30fc\u3092\u52a0\u3048\u305f\u70b9\u306e\u30d7\u30ed\u30c3\u30c8\nsns.stripplot(x=dinner_tips[\"time\"], y=dinner_tips[\"total_bill\"],\n              jitter=True, ax=axes[0], color=\"gray\", alpha=0.3)\naxes[0].set_title(\"\u30b8\u30c3\u30bf\u30fc\u4ed8\u304d\u70b9\u3068\\n\u4e94\u6570\u8981\u7d04\u306e\u7834\u7dda\")\naxes[0].set_xlabel(\"\")\naxes[0].set_ylabel(\"Total Bill\")\n\n# \u4e94\u6570\u8981\u7d04\u306e\u5404\u5024\u3092\u7834\u7dda\u3067\u8ffd\u52a0\nfor value in five_number_summary[\"total_bill\"]:\n    axes[0].axhline(y=value, color='red', linestyle='--', linewidth=1.0)\n\n# (b) \u7bb1\u3072\u3052\u56f3\u3068\u30b8\u30c3\u30bf\u30fc\u4ed8\u304d\u70b9\u3001\u3055\u3089\u306b\u7834\u7dda\u306b\u3088\u308b\u4e94\u6570\u8981\u7d04\u306e\u8868\u793a\nsns.boxplot(x=\"time\", y=\"total_bill\", data=dinner_tips, ax=axes[1],\n            showcaps=True, boxprops={'facecolor':'None'})\nsns.stripplot(x=\"time\", y=\"total_bill\", data=dinner_tips,\n              jitter=True, ax=axes[1], color=\"gray\", alpha=0.3)\nfor value in five_number_summary[\"total_bill\"]:\n    axes[1].axhline(y=value, color='red', linestyle='--', linewidth=1.0)\naxes[1].set_title(\"\u7bb1\u3072\u3052\u56f3 + \u30b8\u30c3\u30bf\u30fc\u4ed8\u304d\u70b9\")\naxes[1].set_xlabel(\"\")\naxes[1].set_ylabel(\"\")\n\n# (c) \u7bb1\u3072\u3052\u56f3\u306e\u307f\uff08\u4f59\u8a08\u306a\u70b9\u3084\u7834\u7dda\u306f\u9664\u53bb\uff09\nsns.boxplot(x=\"time\", y=\"total_bill\", data=dinner_tips, ax=axes[2])\naxes[2].set_title(\"\u7bb1\u3072\u3052\u56f3\u306e\u307f\")\naxes[2].set_xlabel(\"\")\naxes[2].set_ylabel(\"\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <ul> <li>\u5de6\u306e\u30d7\u30ed\u30c3\u30c8\u3067\u306f\u3001\u30c7\u30a3\u30ca\u30fc\u30bf\u30a4\u30e0\u306e\u5404 total_bill \u306e\u89b3\u6e2c\u5024\u304c\u30b8\u30c3\u30bf\u30fc\u4ed8\u304d\u306e\u70b9\u3068\u3057\u3066\u8868\u793a\u3055\u308c\u3001\u8d64\u3044\u7834\u7dda\u3067\u4e94\u6570\u8981\u7d04\u306e\u4f4d\u7f6e\u304c\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002</li> <li>\u4e2d\u592e\u306e\u30d7\u30ed\u30c3\u30c8\u3067\u306f\u3001\u7bb1\u3072\u3052\u56f3\u304c\u63cf\u753b\u3055\u308c\u308b\u3068\u540c\u6642\u306b\u3001\u30b8\u30c3\u30bf\u30fc\u4ed8\u304d\u306e\u70b9\u3068\u7834\u7dda\u3082\u91cd\u306d\u3089\u308c\u3066\u3044\u307e\u3059\u3002</li> <li>\u53f3\u306e\u30d7\u30ed\u30c3\u30c8\u306f\u3001\u7d14\u7c8b\u306a\u7bb1\u3072\u3052\u56f3\u306e\u307f\u3067\u3001\u4f59\u8a08\u306a\u8981\u7d20\u304c\u9664\u53bb\u3055\u308c\u3066\u3044\u307e\u3059\u3002</li> </ul> <p>\u7bb1\u3072\u3052\u56f3\u306f\u3001\u7bb1\u306e\u4e0a\u4e0b\u304c\u305d\u308c\u305e\u308c\u7b2c\u4e00\u56db\u5206\u4f4d\u6570\uff0825%\uff09\u3068\u7b2c\u4e09\u56db\u5206\u4f4d\u6570\uff0875%\uff09\u3092\u793a\u3057\u3001\u7bb1\u5185\u306e\u592a\u3044\u7dda\u304c\u4e2d\u592e\u5024\uff0850%\uff09\u3092\u793a\u3057\u307e\u3059\u3002\u7bb1\u306e\u9ad8\u3055\u306f\u56db\u5206\u4f4d\u7bc4\u56f2 (IQR) \u3092\u8868\u3057\u3001\u4e0a\u4e0b\u306e\u300c\u3072\u3052\u300d\u306f\u901a\u5e38\u3001\u7bb1\u304b\u3089 1.5 \u00d7 IQR \u306e\u7bc4\u56f2\u5185\u306e\u6700\u5c0f\u5024\u30fb\u6700\u5927\u5024\u307e\u3067\u4f38\u3073\u3001\u7bc4\u56f2\u5916\u306e\u89b3\u6e2c\u5024\u306f\u500b\u5225\u306e\u70b9\uff08\u30a2\u30a6\u30c8\u30e9\u30a4\u30e4\u30fc\uff09\u3068\u3057\u3066\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_15","title":"\u8907\u6570\u30b0\u30eb\u30fc\u30d7\u306b\u304a\u3051\u308b\u7bb1\u3072\u3052\u56f3\u306e\u4f5c\u6210","text":"<p>\u6b21\u306b\u3001tips \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5168\u4f53\u3092\u7528\u3044\u3066\u3001\u66dc\u65e5\uff08day\uff09\u3054\u3068\u306b total_bill \u306e\u5206\u5e03\u3092\u6bd4\u8f03\u3059\u308b\u305f\u3081\u306e \u30b5\u30a4\u30c9\u30d0\u30a4\u30b5\u30a4\u30c9\u7bb1\u3072\u3052\u56f3 \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 tips \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e day \u5909\u6570\u306f\u65e2\u306b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\uff08\"Thur\", \"Fri\", \"Sat\", \"Sun\"\uff09\u3068\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001x \u8ef8\u306b\u76f4\u63a5\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <pre><code>plt.figure(figsize=(8, 6))\nsns.boxplot(x=\"day\", y=\"total_bill\", data=tips,\n            palette=\"pastel\", showcaps=True, \n            boxprops={'edgecolor':'black'},\n            whiskerprops={'color':'black'},\n            capprops={'color':'black'},\n            flierprops={'markerfacecolor':'red', 'marker':'o', 'markersize':5})\nplt.title(\"Tips\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff1a\u66dc\u65e5\u3054\u3068\u306e Total Bill \u306e\u7bb1\u3072\u3052\u56f3\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Total Bill\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u30d7\u30ed\u30c3\u30c8\u3067\u306f\u3001\u5404\u66dc\u65e5\u3054\u3068\u306b total_bill \u306e\u7bb1\u3072\u3052\u56f3\u304c\u63cf\u753b\u3055\u308c\u3001</p> <ul> <li>\u7bb1\u306f\u7b2c\u4e00\u56db\u5206\u4f4d\u6570\u3001\u4e2d\u592e\u5024\u3001\u7b2c\u4e09\u56db\u5206\u4f4d\u6570\u3092\u793a\u3057\u3001</li> <li>\u3072\u3052\u306f 1.5\u00d7IQR \u306e\u7bc4\u56f2\u5185\u306e\u5024\u3092\u8868\u3057\u3001</li> <li>\u305d\u306e\u7bc4\u56f2\u5916\u306e\u5024\u306f\u30a2\u30a6\u30c8\u30e9\u30a4\u30e4\u30fc\u3068\u3057\u3066\u8d64\u3044\u70b9\u3067\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002</li> </ul> <p>\u66dc\u65e5\u3054\u3068\u306e\u7bb1\u3072\u3052\u56f3\u3092\u4e26\u3079\u308b\u3053\u3068\u3067\u3001\u5404\u30b0\u30eb\u30fc\u30d7\u9593\u306e\u5206\u5e03\u306e\u9055\u3044\u3084\u3070\u3089\u3064\u304d\u3001\u3055\u3089\u306b\u306f\u5916\u308c\u5024\u306e\u6709\u7121\u3092\u7c21\u5358\u306b\u6bd4\u8f03\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_16","title":"\u30c1\u30a7\u30c3\u30af\u9805\u76ee","text":"<ul> <li>(LC1) \u30c7\u30a3\u30ca\u30fc\u30bf\u30a4\u30e0\u306e total_bill \u306e\u4e94\u6570\u8981\u7d04\u3092\u78ba\u8a8d\u3057\u305f\u3068\u304d\u3001\u5404\u5024\uff08\u6700\u5c0f\u5024\u3001\u7b2c\u4e00\u56db\u5206\u4f4d\u6570\u3001\u4e2d\u592e\u5024\u3001\u7b2c\u4e09\u56db\u5206\u4f4d\u6570\u3001\u6700\u5927\u5024\uff09\u306f\u3069\u306e\u3088\u3046\u306a\u6570\u5024\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u304b\uff1f</li> <li>(LC2) \u7bb1\u3072\u3052\u56f3\u306b\u304a\u3051\u308b\u7bb1\u306e\u9ad8\u3055\uff08IQR\uff09\u304c\u5927\u304d\u3044\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306a\u60c5\u5831\u304c\u5f97\u3089\u308c\u307e\u3059\u304b\uff1f</li> <li>(LC3) \u30a2\u30a6\u30c8\u30e9\u30a4\u30e4\u30fc\uff08\u7bb1\u3072\u3052\u56f3\u4e0a\u306e\u70b9\uff09\u304c\u793a\u3059\u610f\u5473\u306f\u4f55\u3067\u3057\u3087\u3046\u304b\uff1f\u307e\u305f\u3001\u30a2\u30a6\u30c8\u30e9\u30a4\u30e4\u30fc\u304c\u591a\u3044\u5834\u5408\u3001\u3069\u306e\u3088\u3046\u306a\u89e3\u91c8\u304c\u53ef\u80fd\u3067\u3057\u3087\u3046\u304b\uff1f</li> <li>(LC4) \u66dc\u65e5\u3054\u3068\u306e\u7bb1\u3072\u3052\u56f3\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u3067\u3001\u3069\u306e\u66dc\u65e5\u3067\u98f2\u98df\u4ee3\u306e\u3070\u3089\u3064\u304d\u304c\u5927\u304d\u3044\u304b\u3001\u307e\u305f\u4e2d\u592e\u5024\u306b\u9055\u3044\u304c\u3042\u308b\u304b\u3092\u3069\u306e\u3088\u3046\u306b\u8aad\u307f\u53d6\u308c\u307e\u3059\u304b\uff1f</li> </ul>"},{"location":"lectures/SIWS/02-visualization/#_17","title":"\u307e\u3068\u3081","text":"<p>\u7bb1\u3072\u3052\u56f3\u306f\u3001\u5358\u4e00\u306e\u6570\u5024\u5909\u6570\u306e\u5206\u5e03\u3092\u8996\u899a\u5316\u3059\u308b\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u3067\u3042\u308a\u3001</p> <ul> <li>\u4e2d\u592e\u5024 \u3084 \u56db\u5206\u4f4d\u6570 \u3092\u76f4\u611f\u7684\u306b\u628a\u63e1\u3067\u304d\u3001</li> <li>\u56db\u5206\u4f4d\u7bc4\u56f2\uff08IQR\uff09 \u306b\u3088\u308b\u6563\u3089\u3070\u308a\u306e\u5ea6\u5408\u3044\u3092\u793a\u3057\u3001</li> <li>\u30a2\u30a6\u30c8\u30e9\u30a4\u30e4\u30fc\uff08\u5916\u308c\u5024\uff09\u3092\u5bb9\u6613\u306b\u7279\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> </ul> <p>\u307e\u305f\u3001\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\uff08\u4f8b\uff1a\u66dc\u65e5\uff09\u306b\u5bfe\u3057\u3066\u30b5\u30a4\u30c9\u30d0\u30a4\u30b5\u30a4\u30c9\u3067\u7bb1\u3072\u3052\u56f3\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u3067\u3001\u30b0\u30eb\u30fc\u30d7\u9593\u306e\u5206\u5e03\u306e\u9055\u3044\u3084\u3070\u3089\u3064\u304d\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002tips \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f8b\u306b\u3001\u30c7\u30a3\u30ca\u30fc\u30bf\u30a4\u30e0\u306e total_bill \u3084\u66dc\u65e5\u3054\u3068\u306e total_bill \u3092\u901a\u3058\u3066\u3001\u7bb1\u3072\u3052\u56f3\u306e\u4f5c\u6210\u3068\u305d\u306e\u89e3\u91c8\u306e\u57fa\u672c\u3092\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#5ng5-barplots","title":"5NG#5: \u68d2\u30b0\u30e9\u30d5 (Barplots)","text":"<p>\u68d2\u30b0\u30e9\u30d5\u306f\u3001\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\uff08\u30ec\u30d9\u30eb\uff09\u306e\u51fa\u73fe\u983b\u5ea6\uff08\u30ab\u30a6\u30f3\u30c8\uff09\u3092\u8996\u899a\u5316\u3059\u308b\u305f\u3081\u306e\u57fa\u672c\u7684\u306a\u30b0\u30e9\u30d5\u3067\u3059\u3002 \u6570\u5024\u5909\u6570\u306e\u5206\u5e03\u3092\u53ef\u8996\u5316\u3059\u308b\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3084\u7bb1\u3072\u3052\u56f3\u3068\u306f\u7570\u306a\u308a\u3001\u68d2\u30b0\u30e9\u30d5\u306f\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u983b\u5ea6\u3084\u5272\u5408\u3092\u793a\u3059\u306e\u306b\u9069\u3057\u3066\u3044\u307e\u3059\u3002 \u307e\u305f\u3001\u30c7\u30fc\u30bf\u304c\u65e2\u306b\u300c\u4e8b\u524d\u96c6\u8a08\u6e08\u307f\u300d\u306e\u5834\u5408\u3068\u3001\u5404\u89b3\u6e2c\u5024\u304c\u500b\u5225\u306b\u8a18\u9332\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u3067\u3001\u30b0\u30e9\u30d5\u4f5c\u6210\u306e\u624b\u6cd5\u304c\u7570\u306a\u308a\u307e\u3059\u3002</p> <p>\u672c\u7bc0\u3067\u306f\u3001Gapminder \u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3066\u3001\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\uff08\u3053\u3053\u3067\u306f <code>continent</code>\uff1a\u5927\u9678\uff09\u306e\u51fa\u73fe\u983b\u5ea6\u3092\u68d2\u30b0\u30e9\u30d5\u3067\u53ef\u8996\u5316\u3059\u308b\u65b9\u6cd5\u3092\u793a\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#1_3","title":"1. \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f","text":"<p>\u307e\u305a\u3001Gapminder \u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092 URL \u304b\u3089\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002</p> <pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Gapminder\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\nurl = \"https://raw.githubusercontent.com/resbaz/r-novice-gapminder-files/master/data/gapminder-FiveYearData.csv\"\ngapminder = pd.read_csv(url)\ngapminder.head()\n</code></pre> <p>\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u306f\u3001<code>country</code>\uff08\u56fd\uff09\u3001<code>continent</code>\uff08\u5927\u9678\uff09\u3001<code>year</code>\uff08\u5e74\uff09\u3001<code>lifeExp</code>\uff08\u5e73\u5747\u5bff\u547d\uff09\u3001<code>pop</code>\uff08\u4eba\u53e3\uff09\u3001<code>gdpPercap</code>\uff081\u4eba\u5f53\u305f\u308aGDP\uff09\u306a\u3069\u306e\u5909\u6570\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_18","title":"\u672a\u96c6\u8a08\u30c7\u30fc\u30bf\u304b\u3089\u68d2\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3059\u308b","text":"<p>Gapminder \u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u5404\u89b3\u6e2c\u5024\uff08\u4f8b\u3048\u3070\u3001\u5404\u56fd\u306e\u5404\u5e74\u306e\u8a18\u9332\uff09\u304c\u500b\u5225\u306b\u8a18\u9332\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570 <code>continent</code> \u306e\u983b\u5ea6\u306f\u307e\u3060\u300c\u96c6\u8a08\u300d\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002 \u3053\u306e\u5834\u5408\u3001Seaborn \u306e <code>countplot()</code> \u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u81ea\u52d5\u7684\u306b\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u51fa\u73fe\u56de\u6570\u3092\u8a08\u7b97\u3057\u3066\u68d2\u30b0\u30e9\u30d5\u3092\u63cf\u753b\u3057\u3066\u304f\u308c\u307e\u3059\u3002</p> <pre><code>plt.figure(figsize=(8,6))\nsns.countplot(data=gapminder, x=\"continent\", palette=\"pastel\", edgecolor=\"black\")\nplt.title(\"Gapminder\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff1a\u5404\u5927\u9678\u306e\u30ec\u30b3\u30fc\u30c9\u6570\")\nplt.xlabel(\"\u5927\u9678\")\nplt.ylabel(\"\u30ab\u30a6\u30f3\u30c8\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u30b0\u30e9\u30d5\u306f\u3001\u5404\u5927\u9678\u306b\u8a72\u5f53\u3059\u308b\u30ec\u30b3\u30fc\u30c9\u6570\uff08\uff1d\u305d\u306e\u5927\u9678\u306b\u5c5e\u3059\u308b\u56fd\u306e\u6570\u00d7\u89b3\u6e2c\u5e74\u6570\uff09\u304c\u3069\u306e\u7a0b\u5ea6\u304b\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u68d2\u540c\u58eb\u306e\u9593\u306b\u306f\u9069\u5ea6\u306a\u9699\u9593\u304c\u3042\u308a\u3001\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u983b\u5ea6\u3092\u6bd4\u8f03\u3057\u3084\u3059\u304f\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_19","title":"\u4e8b\u524d\u96c6\u8a08\u6e08\u307f\u30c7\u30fc\u30bf\u304b\u3089\u68d2\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3059\u308b","text":"<p>\u4e00\u65b9\u3001\u30c7\u30fc\u30bf\u304c\u300c\u4e8b\u524d\u96c6\u8a08\u6e08\u307f\u300d\u3067\u3042\u308b\u5834\u5408\u3001\u3064\u307e\u308a\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u30ab\u30a6\u30f3\u30c8\u304c\u65e2\u306b\u8a08\u7b97\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001Seaborn \u306e <code>barplot()</code> \u3092\u4f7f\u7528\u3057\u3066\u68d2\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <p>\u307e\u305a\u3001continent \u3054\u3068\u306e\u30ec\u30b3\u30fc\u30c9\u6570\u3092\u96c6\u8a08\u3057\u3066\u307f\u307e\u3059\u3002</p> <pre><code># \u5927\u9678\u3054\u3068\u306e\u30ec\u30b3\u30fc\u30c9\u6570\u3092\u8a08\u7b97\ncontinent_counts = gapminder.groupby(\"continent\").size().reset_index(name=\"count\")\nprint(continent_counts)\n</code></pre> <p>\u6b21\u306b\u3001\u4e8b\u524d\u96c6\u8a08\u6e08\u307f\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u68d2\u30b0\u30e9\u30d5\u3092\u63cf\u753b\u3057\u307e\u3059\u3002</p> <pre><code>plt.figure(figsize=(8,6))\nsns.barplot(data=continent_counts, x=\"continent\", y=\"count\", palette=\"pastel\", edgecolor=\"black\")\nplt.title(\"Gapminder\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff1a\u5927\u9678\u3054\u3068\u306e\u30ec\u30b3\u30fc\u30c9\u6570\uff08\u4e8b\u524d\u96c6\u8a08\u6e08\u307f\uff09\")\nplt.xlabel(\"\u5927\u9678\")\nplt.ylabel(\"\u30ab\u30a6\u30f3\u30c8\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u65b9\u6cd5\u3067\u306f\u3001\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u30ab\u30a6\u30f3\u30c8\u304c\u65e2\u306b <code>count</code> \u5909\u6570\u306b\u8a18\u9332\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001<code>barplot()</code> \u306e <code>y</code> \u8ef8\u306b\u305d\u306e\u5909\u6570\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002 \u7d50\u679c\u3068\u3057\u3066\u3001\u672a\u96c6\u8a08\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f <code>countplot()</code> \u3068\u540c\u3058\u30b0\u30e9\u30d5\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#4","title":"4. \u8ffd\u52a0\u4f8b\uff1a\u7279\u5b9a\u306e\u5e74\u306b\u304a\u3051\u308b\u68d2\u30b0\u30e9\u30d5","text":"<p>Gapminder \u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u8907\u6570\u5e74\u306e\u30c7\u30fc\u30bf\u3092\u542b\u3080\u305f\u3081\u3001\u4f8b\u3048\u3070\u7279\u5b9a\u306e\u5e74\uff08\u4f8b\uff1a2007\u5e74\uff09\u306e\u30c7\u30fc\u30bf\u306b\u7d5e\u3063\u3066\u3001\u5404\u5927\u9678\u306e\u300c\u56fd\u6570\u300d\u3092\u53ef\u8996\u5316\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u30022007\u5e74\u306e\u30c7\u30fc\u30bf\u306f\u5404\u56fd1\u30ec\u30b3\u30fc\u30c9\u3067\u8868\u3055\u308c\u308b\u306e\u3067\u3001\u5404\u5927\u9678\u306b\u304a\u3051\u308b\u56fd\u6570\u3092\u793a\u3059\u68d2\u30b0\u30e9\u30d5\u306b\u306a\u308a\u307e\u3059\u3002</p> <pre><code># 2007\u5e74\u306e\u30c7\u30fc\u30bf\u306b\u7d5e\u308b\ngapminder_2007 = gapminder[gapminder[\"year\"] == 2007]\n\nplt.figure(figsize=(8,6))\nsns.countplot(data=gapminder_2007, x=\"continent\", palette=\"pastel\", edgecolor=\"black\")\nplt.title(\"Gapminder\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff082007\u5e74\uff09\uff1a\u5404\u5927\u9678\u306e\u56fd\u6570\")\nplt.xlabel(\"\u5927\u9678\")\nplt.ylabel(\"\u56fd\u6570\")\nplt.show()\n</code></pre> <p>\u3053\u306e\u30b0\u30e9\u30d5\u3067\u306f\u3001\u5404\u5927\u9678\u3054\u3068\u306e\u56fd\u6570\u304c\u6bd4\u8f03\u3057\u3084\u3059\u304f\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_20","title":"\u30c1\u30a7\u30c3\u30af\u9805\u76ee","text":"<ul> <li>(LC1) \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306f\u6570\u5024\u5909\u6570\u306e\u9023\u7d9a\u7684\u306a\u5206\u5e03\u3092\u53ef\u8996\u5316\u3059\u308b\u306e\u306b\u9069\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u306a\u305c\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306b\u306f\u9069\u3057\u3066\u3044\u306a\u3044\u306e\u3067\u3057\u3087\u3046\u304b\uff1f</li> <li>(LC2) \u672a\u96c6\u8a08\u306e\u30c7\u30fc\u30bf\u304b\u3089\u68d2\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u3068\u3001\u4e8b\u524d\u96c6\u8a08\u6e08\u307f\u306e\u30c7\u30fc\u30bf\u304b\u3089\u4f5c\u6210\u3059\u308b\u5834\u5408\u306e\u9055\u3044\u306f\u4f55\u3067\u3059\u304b\uff1f</li> <li>(LC3) Gapminder \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e <code>continent</code> \u5909\u6570\u306b\u304a\u3044\u3066\u3001\u3069\u306e\u5927\u9678\u306e\u30ec\u30b3\u30fc\u30c9\u6570\uff08\u307e\u305f\u306f\u56fd\u6570\uff09\u304c\u591a\u3044\u304b\u3001\u307e\u305f\u305d\u306e\u7406\u7531\u306b\u3064\u3044\u3066\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002</li> </ul>"},{"location":"lectures/SIWS/02-visualization/#_21","title":"\u307e\u3068\u3081","text":"<p>\u68d2\u30b0\u30e9\u30d5\u306f\u3001\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u51fa\u73fe\u983b\u5ea6\u3092\u76f4\u611f\u7684\u306b\u793a\u3059\u30b0\u30e9\u30d5\u3067\u3059\u3002</p> <ul> <li>\u672a\u96c6\u8a08\u30c7\u30fc\u30bf\u306e\u5834\u5408\u306f\u3001Seaborn \u306e countplot() \u3092\u7528\u3044\u308b\u3053\u3068\u3067\u81ea\u52d5\u7684\u306b\u983b\u5ea6\u3092\u8a08\u7b97\u30fb\u8868\u793a\u3067\u304d\u307e\u3059\u3002</li> <li>\u4e8b\u524d\u96c6\u8a08\u6e08\u307f\u30c7\u30fc\u30bf\u306e\u5834\u5408\u306f\u3001barplot() \u3092\u7528\u3044\u3066\u3001\u3042\u3089\u304b\u3058\u3081\u8a08\u7b97\u3055\u308c\u305f\u30ab\u30a6\u30f3\u30c8\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u307e\u3059\u3002</li> </ul> <p>Gapminder \u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u305f\u4f8b\u3067\u306f\u3001continent \u306e\u5404\u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u983b\u5ea6\u3084\u3001\u7279\u5b9a\u306e\u5e74\u306b\u304a\u3051\u308b\u56fd\u6570\u306a\u3069\u3001\u3055\u307e\u3056\u307e\u306a\u89d2\u5ea6\u304b\u3089\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u5206\u5e03\u3092\u8996\u899a\u5316\u3059\u308b\u65b9\u6cd5\u3092\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30c7\u30fc\u30bf\u5168\u4f53\u306e\u50be\u5411\u3084\u3001\u30b0\u30eb\u30fc\u30d7\u9593\u306e\u6bd4\u8f03\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_22","title":"\u30c7\u30fc\u30bf\u53ef\u8996\u5316\u306e\u307e\u3068\u3081","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u4ee5\u4e0b\u306e 5 \u3064\u306e\u4ee3\u8868\u7684\u306a\u30b0\u30e9\u30d5\uff085NG\uff09\u3092\u901a\u3058\u3066\u3001\u3055\u307e\u3056\u307e\u306a\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3084\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u3092\u8996\u899a\u5316\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u30b0\u30e9\u30d5\u3092\u99c6\u4f7f\u3059\u308b\u3053\u3068\u3067\u3001\u3042\u3089\u3086\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u7279\u5fb4\u3092\u76f4\u611f\u7684\u306b\u628a\u63e1\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u3055\u3089\u306b\u591a\u304f\u306e\u5909\u6570\u3092\u5404\u30d7\u30ed\u30c3\u30c8\u306e\u7f8e\u7684\u5c5e\u6027\uff08\u8272\u3001\u5f62\u3001\u5927\u304d\u3055\u306a\u3069\uff09\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u3001\u8996\u899a\u7684\u8868\u73fe\u306e\u53ef\u80fd\u6027\u306f\u7121\u9650\u306b\u5e83\u304c\u308a\u307e\u3059\u3002</p> \u30b0\u30e9\u30d5\u306e\u7a2e\u985e \u8868\u793a\u5185\u5bb9 \u30b8\u30aa\u30e1\u30c8\u30ea\u30c3\u30af\u30aa\u30d6\u30b8\u30a7\u30af\u30c8 \u5099\u8003 \u6563\u5e03\u56f3 2\u3064\u306e\u6570\u5024\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027 <code>plt.scatter()</code> \u307e\u305f\u306f <code>sns.scatterplot()</code> \u6298\u308c\u7dda\u30b0\u30e9\u30d5 2\u3064\u306e\u6570\u5024\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027 <code>plt.plot()</code> \u307e\u305f\u306f <code>sns.lineplot()</code> x\u8ef8\u306b\u9806\u5e8f\u304c\u3042\u308b\u5834\u5408\uff08\u4f8b\uff1a\u6642\u9593\uff09\u306b\u4f7f\u7528 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0 1\u3064\u306e\u6570\u5024\u5909\u6570\u306e\u5206\u5e03 <code>plt.hist()</code> \u307e\u305f\u306f <code>sns.histplot()</code> \u30d5\u30a1\u30bb\u30c3\u30c8\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306f\u3001\u5225\u306e\u5909\u6570\u306e\u5024\u3067\u5206\u5272\u3057\u305f\u6570\u5024\u5909\u6570\u306e\u5206\u5e03\u3092\u8868\u793a\u3059\u308b\u306e\u306b\u7528\u3044\u3089\u308c\u308b \u7bb1\u3072\u3052\u56f3 1\u3064\u306e\u6570\u5024\u5909\u6570\u306e\u5206\u5e03\u3092\u3001\u5225\u306e\u5909\u6570\u3067\u5206\u5272\u3057\u3066\u8868\u793a <code>plt.boxplot()</code> \u307e\u305f\u306f <code>sns.boxplot()</code> \u68d2\u30b0\u30e9\u30d5 1\u3064\u306e\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u5206\u5e03 \u672a\u96c6\u8a08\u30c7\u30fc\u30bf\u306e\u5834\u5408\uff1a<code>sns.countplot()</code>\u4e8b\u524d\u96c6\u8a08\u6e08\u307f\u30c7\u30fc\u30bf\u306e\u5834\u5408\uff1a<code>sns.barplot()</code> \u7a4d\u307f\u4e0a\u3052\u3001\u4e26\u5217\u3001\u30d5\u30a1\u30bb\u30c3\u30c8\u68d2\u30b0\u30e9\u30d5\u3092\u7528\u3044\u308b\u3068\u30012\u3064\u306e\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u540c\u6642\u5206\u5e03\u3082\u8868\u73fe\u53ef\u80fd"},{"location":"lectures/SIWS/02-visualization/#_23","title":"\u95a2\u6570\u5f15\u6570\u306e\u6307\u5b9a","text":"<p>Python \u306e\u591a\u304f\u306e\u95a2\u6570\u306f\u3001\u30ad\u30fc\u30ef\u30fc\u30c9\u5f15\u6570\u3092\u4f7f\u7528\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001Seaborn \u306e\u30d7\u30ed\u30c3\u30c8\u95a2\u6570\u3067\u306f\u3001\u5f15\u6570\u306e\u9806\u5e8f\u306b\u4f9d\u5b58\u305b\u305a\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5f15\u6570\u540d\u3092\u5165\u308c\u66ff\u3048\u3066\u3082\u540c\u3058\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <pre><code>import seaborn as sns\n\n# \u30bb\u30b0\u30e1\u30f3\u30c8 1: data \u3068 x \u3092\u660e\u793a\u7684\u306b\u6307\u5b9a\nsns.countplot(data=gapminder, x=\"continent\")\n\n# \u30bb\u30b0\u30e1\u30f3\u30c8 2: \u5f15\u6570\u306e\u9806\u5e8f\u3092\u5165\u308c\u66ff\u3048\u3066\u3082\u3001\u30ad\u30fc\u30ef\u30fc\u30c9\u5f15\u6570\u306a\u306e\u3067\u540c\u3058\u7d50\u679c\u306b\nsns.countplot(x=\"continent\", data=gapminder)\n</code></pre> <p>\u4e0a\u8a18\u306e\u4f8b\u306e\u3088\u3046\u306b\u3001Python \u3067\u306f\u30ad\u30fc\u30ef\u30fc\u30c9\u5f15\u6570\u306e\u9806\u5e8f\u306f\u4efb\u610f\u3067\u3042\u308a\u3001\u30b3\u30fc\u30c9\u306e\u8aad\u307f\u3084\u3059\u3055\u3092\u91cd\u8996\u3057\u3066\u597d\u307f\u306e\u30b9\u30bf\u30a4\u30eb\u3067\u8a18\u8ff0\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_24","title":"\u8ffd\u52a0\u306e\u30ea\u30bd\u30fc\u30b9","text":"<p>\u30c7\u30fc\u30bf\u53ef\u8996\u5316\u306e\u529b\u3092\u3055\u3089\u306b\u5f15\u304d\u51fa\u3059\u305f\u3081\u3001\u4ee5\u4e0b\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li> <p>Seaborn \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8     Seaborn \u306e\u4f7f\u3044\u65b9\u3084\u8c4a\u5bcc\u306a\u30d7\u30ed\u30c3\u30c8\u4f8b\u304c\u63b2\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> </li> <li> <p>Matplotlib \u30c1\u30fc\u30c8\u30b7\u30fc\u30c8     Matplotlib \u306e\u4e3b\u8981\u306a\u95a2\u6570\u3084\u8a2d\u5b9a\u65b9\u6cd5\u3092\u7c21\u6f54\u306b\u307e\u3068\u3081\u305f\u8cc7\u6599\u3067\u3059\u3002</p> </li> </ul> <p>\u3053\u308c\u3089\u306e\u8cc7\u6599\u306f\u3001\u57fa\u672c\u7684\u306a\u30b0\u30e9\u30d5\u4f5c\u6210\u306e\u77e5\u8b58\u3092\u8d85\u3048\u3066\u3001\u3055\u3089\u306b\u9ad8\u5ea6\u306a\u8996\u899a\u5316\u6280\u6cd5\u3092\u7fd2\u5f97\u3059\u308b\u969b\u306b\u5927\u3044\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/02-visualization/#_25","title":"\u6b21\u306e\u8a71\u984c","text":"<p>\u3053\u308c\u307e\u3067\u3001\u6563\u5e03\u56f3\u3001\u6298\u308c\u7dda\u30b0\u30e9\u30d5\u3001\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3001\u7bb1\u3072\u3052\u56f3\u3001\u68d2\u30b0\u30e9\u30d5\u3068\u3044\u3046 5 \u3064\u306e\u57fa\u672c\u7684\u306a\u30b0\u30e9\u30d5\u3092\u7528\u3044\u3066\u3001\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3084\u5909\u6570\u9593\u306e\u95a2\u4fc2\u6027\u3092\u8996\u899a\u5316\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3057\u305f\u3002 \u6b21\u306e\u7ae0\u3067\u306f\u3001\u30c7\u30fc\u30bf\u524d\u51e6\u7406\u30fb\u6574\u5f62\uff08Data Wrangling\uff09 \u306b\u7126\u70b9\u3092\u5f53\u3066\u3001Pandas \u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u306e\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3084\u5909\u63db\u65b9\u6cd5\u3092\u8a73\u3057\u304f\u89e3\u8aac\u3057\u307e\u3059\u3002 \u305f\u3068\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30b3\u30fc\u30c9\u3067\u3001\u30c7\u30fc\u30bf\u306e\u90e8\u5206\u96c6\u5408\u3092\u4f5c\u6210\u3057\u3001\u53ef\u8996\u5316\u306b\u6d3b\u7528\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002</p> <pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# \u4f8b 1: flights \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089 Alaska Airlines \u306e\u30c7\u30fc\u30bf\u306e\u307f\u3092\u62bd\u51fa\nalaska_flights = flights[flights[\"carrier\"] == \"AS\"]\nsns.scatterplot(data=alaska_flights, x=\"dep_delay\", y=\"arr_delay\")\nplt.title(\"Alaska Airlines \u306e\u51fa\u767a\u9045\u5ef6\u3068\u5230\u7740\u9045\u5ef6\u306e\u95a2\u4fc2\")\nplt.show()\n\n# \u4f8b 2: weather \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089 Newark \u7a7a\u6e2f (EWR) \u306e 1 \u6708\u4e0a\u65ec\u306e\u30c7\u30fc\u30bf\u306e\u307f\u3092\u62bd\u51fa\nearly_january_weather = weather[(weather[\"origin\"] == \"EWR\") &amp; (weather[\"month\"] == 1) &amp; (weather[\"day\"] &lt;= 15)]\nsns.lineplot(data=early_january_weather, x=\"time_hour\", y=\"temp\")\nplt.title(\"Newark \u7a7a\u6e2f 1 \u6708\u4e0a\u65ec\u306e\u6e29\u5ea6\u5909\u5316\")\nplt.show()\n</code></pre> <p>\u4e0a\u8a18\u306e\u4f8b\u306f\u3001Pandas \u3092\u7528\u3044\u305f\u30c7\u30fc\u30bf\u306e\u62bd\u51fa\u3068\u3001Seaborn \u3092\u7528\u3044\u305f\u53ef\u8996\u5316\u306e\u57fa\u672c\u7684\u306a\u9023\u643a\u4f8b\u3067\u3059\u3002\u6b21\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u6574\u5f62\u6280\u6cd5\u3092\u3088\u308a\u8a73\u7d30\u306b\u5b66\u3073\u3001\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u5206\u6790\u3084\u53ef\u8996\u5316\u306e\u305f\u3081\u306e\u4e0b\u5730\u3092\u4f5c\u3063\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>\u3053\u308c\u3067\u672c\u7ae0\u306e\u307e\u3068\u3081\u3068\u3001\u4eca\u5f8c\u306e\u5c55\u958b\u306b\u3064\u3044\u3066\u306e\u6982\u8981\u3092\u7d42\u308f\u308a\u307e\u3059\u3002 \u6b21\u306e\u7ae0\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3068\u5909\u63db\u3092\u901a\u3058\u3066\u3001\u3088\u308a\u52b9\u679c\u7684\u306a\u30c7\u30fc\u30bf\u5206\u6790\u306e\u57fa\u76e4\u3092\u69cb\u7bc9\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff01</p>"},{"location":"lectures/SIWS/03-wrangling/","title":"Python \u306b\u3088\u308b \u30c7\u30fc\u30bf\u6574\u5f62\uff08Data Wrangling\uff09","text":""},{"location":"lectures/SIWS/03-wrangling/#_1","title":"\u306f\u3058\u3081\u306b","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u6700\u3082\u91cd\u8981\u306a\u524d\u51e6\u7406\u30fb\u5909\u63db\u4f5c\u696d\u3067\u3042\u308b \u30c7\u30fc\u30bf\u6574\u5f62\uff08\u30c7\u30fc\u30bf\u30fb\u30ef\u30ea\u30f3\u30b0\u30ea\u30f3\u30b0\uff09 \u306e\u57fa\u672c\u7684\u306a\u64cd\u4f5c\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002 \u5177\u4f53\u7684\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u5185\u5bb9\u3092\u53d6\u308a\u6271\u3044\u307e\u3059\u3002</p> <ul> <li> <p>\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068\u78ba\u8a8d   \u5b9f\u30c7\u30fc\u30bf\uff08Students Performance in Exams\uff09\u3092\u8aad\u307f\u8fbc\u307f\u3001\u30c7\u30fc\u30bf\u306e\u5185\u5bb9\u3084\u69cb\u9020\u3092\u628a\u63e1\u3059\u308b\u65b9\u6cd5\u3002</p> </li> <li> <p>\u884c\u306e\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3068\u5217\u306e\u9078\u629e   \u5fc5\u8981\u306a\u30c7\u30fc\u30bf\u306e\u307f\u3092\u62bd\u51fa\u3059\u308b\u305f\u3081\u306e\u6761\u4ef6\u6307\u5b9a\u3084\u3001\u89e3\u6790\u306b\u5fc5\u8981\u306a\u5217\u3060\u3051\u3092\u53d6\u308a\u51fa\u3059\u65b9\u6cd5\u3002</p> </li> <li> <p>\u65b0\u3057\u3044\u5909\u6570\u306e\u4f5c\u6210   \u65e2\u5b58\u306e\u5909\u6570\u304b\u3089\u65b0\u305f\u306a\u6307\u6a19\uff08\u4f8b\uff1a\u7dcf\u5f97\u70b9\u3001\u5e73\u5747\u70b9\uff09\u3092\u8a08\u7b97\u3057\u3001\u30c7\u30fc\u30bf\u306b\u8ffd\u52a0\u3059\u308b\u65b9\u6cd5\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u306e\u4e26\u3079\u66ff\u3048   \u7279\u5b9a\u306e\u57fa\u6e96\uff08\u4f8b\uff1a\u5e73\u5747\u70b9\u306e\u964d\u9806\uff09\u306b\u57fa\u3065\u3044\u3066\u30c7\u30fc\u30bf\u3092\u30bd\u30fc\u30c8\u3059\u308b\u65b9\u6cd5\u3002</p> </li> <li> <p>\u30b0\u30eb\u30fc\u30d7\u5316\u3068\u96c6\u7d04   \u30ab\u30c6\u30b4\u30ea\u3054\u3068\u306b\u30c7\u30fc\u30bf\u3092\u307e\u3068\u3081\u3001\u5e73\u5747\u5024\u306a\u3069\u306e\u7d71\u8a08\u91cf\u3092\u7b97\u51fa\u3059\u308b\u65b9\u6cd5\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u306e\u7d50\u5408\u3068\u518d\u69cb\u7bc9\uff08\u30d4\u30dc\u30c3\u30c8\u64cd\u4f5c\uff09   \u8907\u6570\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7d50\u5408\u3057\u305f\u308a\u3001\u30c7\u30fc\u30bf\u306e\u5f62\u5f0f\u3092\u5909\u63db\uff08\u30ef\u30a4\u30c9\u5f62\u5f0f\u21c4\u30ed\u30f3\u30b0\u5f62\u5f0f\uff09\u3059\u308b\u65b9\u6cd5\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316   \u30c7\u30fc\u30bf\u306e\u50be\u5411\u3084\u5206\u5e03\u3092\u8996\u899a\u7684\u306b\u628a\u63e1\u3059\u308b\u305f\u3081\u306e\u30b0\u30e9\u30d5\u4f5c\u6210\u624b\u6cd5\u3002</p> </li> </ul> <p>\u3053\u308c\u3089\u306e\u64cd\u4f5c\u306f\u3001\u30c7\u30fc\u30bf\u306e\u30af\u30ec\u30f3\u30b8\u30f3\u30b0\u3001\u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u3001\u5206\u6790\u306e\u524d\u51e6\u7406\u306a\u3069\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u304a\u3044\u3066\u4e0d\u53ef\u6b20\u306a\u30b9\u30c6\u30c3\u30d7\u3067\u3059\u3002 \u6b63\u78ba\u306a\u30c7\u30fc\u30bf\u6574\u5f62\u306f\u3001\u5f8c\u7d9a\u306e\u30e2\u30c7\u30eb\u69cb\u7bc9\u3084\u5206\u6790\u306e\u7cbe\u5ea6\u5411\u4e0a\u306b\u76f4\u7d50\u3059\u308b\u305f\u3081\u3001\u672c\u7ae0\u3067\u5b66\u3076\u5185\u5bb9\u306f\u975e\u5e38\u306b\u91cd\u8981\u3067\u3059\u3002</p>"},{"location":"lectures/SIWS/03-wrangling/#1","title":"1. \u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8","text":"<p>\u307e\u305a\u3001\u6570\u5024\u8a08\u7b97\u3001\u30c7\u30fc\u30bf\u64cd\u4f5c\u3001\u30b0\u30e9\u30d5\u63cf\u753b\u306e\u305f\u3081\u306b\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/03-wrangling/#_2","title":"\u8aac\u660e","text":"<ul> <li>numpy: \u6570\u5024\u8a08\u7b97\u30e9\u30a4\u30d6\u30e9\u30ea\uff08\u4eca\u56de\u306f\u88dc\u52a9\u7684\u306b\u5229\u7528\uff09\u3002</li> <li>pandas: \u30c7\u30fc\u30bf\u64cd\u4f5c\u3084\u89e3\u6790\u306e\u305f\u3081\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3001DataFrame \u3092\u7528\u3044\u3066\u8868\u5f62\u5f0f\u30c7\u30fc\u30bf\u3092\u6271\u3044\u307e\u3059\u3002</li> <li>matplotlib \u3068 seaborn: \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002<code>seaborn</code> \u306f <code>matplotlib</code> \u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u304a\u308a\u3001\u898b\u3084\u3059\u3044\u30b0\u30e9\u30d5\u3092\u7c21\u5358\u306b\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# \u30b0\u30e9\u30d5\u306e\u30b9\u30bf\u30a4\u30eb\u8a2d\u5b9a\uff08\u898b\u305f\u76ee\u3092\u6574\u3048\u308b\u305f\u3081\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\nsns.set(style=\"whitegrid\")\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#2","title":"2. \u5b9f\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068\u78ba\u8a8d","text":"<p>\u4eca\u56de\u306f\u3001Kaggle \u306a\u3069\u3067\u516c\u958b\u3055\u308c\u3066\u3044\u308b\u300cStudents Performance in Exams\u300d\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5217\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\uff1a - <code>gender</code> - <code>race/ethnicity</code> - <code>parental level of education</code> - <code>lunch</code> - <code>test preparation course</code> - <code>math score</code> - <code>reading score</code> - <code>writing score</code></p> <p>\u30c7\u30fc\u30bf\u306f\u3001GitHub \u4e0a\u306b\u516c\u958b\u3055\u308c\u3066\u3044\u308b CSV \u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002</p> <pre><code># \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e URL\uff08\u203b\u5b9f\u969b\u306e\u5229\u7528\u6642\u306f\u30c7\u30fc\u30bf\u306e\u51fa\u6240\u30fb\u30e9\u30a4\u30bb\u30f3\u30b9\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\uff09\ndata_url = \"https://raw.githubusercontent.com/selva86/datasets/master/StudentsPerformance.csv\"\n\n# CSV \u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\ndf = pd.read_csv(data_url)\n\n# \u30c7\u30fc\u30bf\u306e\u5148\u982d\u90e8\u5206\u3092\u78ba\u8a8d\nprint(\"=== \u30aa\u30ea\u30b8\u30ca\u30eb\u30c7\u30fc\u30bf ===\")\nprint(df.head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#3","title":"3. \u30c7\u30fc\u30bf\u306e\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0","text":"<p>\u7279\u5b9a\u306e\u6761\u4ef6\u306b\u5408\u81f4\u3059\u308b\u884c\u306e\u307f\u3092\u62bd\u51fa\u3059\u308b\u64cd\u4f5c\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u4f8b\u3068\u3057\u3066\u3001\u300c\u6570\u5b66\u306e\u30b9\u30b3\u30a2\u304c 70 \u70b9\u4ee5\u4e0a\u300d\u306e\u751f\u5f92\u306e\u307f\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002 pandas \u3067\u306f\u3001\u30d6\u30fc\u30eb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u7528\u3044\u3066\u6761\u4ef6\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002</p> <pre><code># \u6570\u5b66\u306e\u30b9\u30b3\u30a2\u304c70\u4ee5\u4e0a\u306e\u751f\u5f92\u306e\u307f\u62bd\u51fa\ndf_filtered = df[df['math score'] &gt;= 70]\nprint(\"\\n=== \u6570\u5b66\u306e\u30b9\u30b3\u30a2\u304c70\u4ee5\u4e0a\u306e\u30c7\u30fc\u30bf ===\")\nprint(df_filtered.head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#4","title":"4. \u5fc5\u8981\u306a\u5217\u306e\u9078\u629e","text":"<p>\u89e3\u6790\u306b\u5fc5\u8981\u306a\u5217\u3060\u3051\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001<code>gender</code>\u3001<code>math score</code>\u3001<code>reading score</code>\u3001<code>writing score</code> \u306e4\u5217\u306e\u307f\u3092\u9078\u629e\u3057\u307e\u3059\u3002</p> <pre><code># \u5fc5\u8981\u306a\u5217\u3060\u3051\u3092\u62bd\u51fa\ndf_selected = df[['gender', 'math score', 'reading score', 'writing score']]\nprint(\"\\n=== \u9078\u629e\u3057\u305f\u5217 (gender, math score, reading score, writing score) ===\")\nprint(df_selected.head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#5-mutate","title":"5. \u65b0\u3057\u3044\u5909\u6570\u306e\u4f5c\u6210\uff08mutate \u306b\u76f8\u5f53\uff09","text":"<p>R \u306e <code>mutate()</code> \u3068\u540c\u69d8\u306b\u3001\u65b0\u305f\u306a\u5217\u3092\u8ffd\u52a0\u3057\u3066\u8a08\u7b97\u7d50\u679c\u3092\u4fdd\u5b58\u3057\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u5404\u751f\u5f92\u306e\u7dcf\u5f97\u70b9 (<code>total_score</code>) \u3068\u5e73\u5747\u70b9 (<code>mean_score</code>) \u3092\u8a08\u7b97\u3057\u3066\u8ffd\u52a0\u3057\u307e\u3059\u3002</p> <pre><code># \u65b0\u3057\u3044\u5217\u306e\u4f5c\u6210\uff1a\u7dcf\u5f97\u70b9\u3068\u5e73\u5747\u70b9\u3092\u8ffd\u52a0\ndf = df.assign(\n    total_score = df['math score'] + df['reading score'] + df['writing score'],\n    mean_score = df[['math score', 'reading score', 'writing score']].mean(axis=1)\n)\nprint(\"\\n=== \u65b0\u3057\u3044\u5909\u6570 (total_score, mean_score) \u3092\u8ffd\u52a0\u3057\u305f\u30c7\u30fc\u30bf ===\")\nprint(df.head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#_3","title":"\u8a73\u7d30\u8aac\u660e","text":"<ul> <li><code>df.assign(...)</code> \u306f\u3001\u5143\u306e DataFrame \u306b\u5bfe\u3057\u3066\u65b0\u3057\u3044\u5217\u3092\u8ffd\u52a0\u3057\u3001\u7d50\u679c\u306e DataFrame \u3092\u8fd4\u3057\u307e\u3059\u3002  </li> <li><code>mean(axis=1)</code> \u306f\u3001\u5404\u884c\uff08axis=1\uff09\u3054\u3068\u306b\u5e73\u5747\u3092\u8a08\u7b97\u3059\u308b\u6307\u5b9a\u3067\u3059\u3002</li> </ul>"},{"location":"lectures/SIWS/03-wrangling/#6-arrange","title":"6. \u884c\u306e\u4e26\u3079\u66ff\u3048\uff08arrange \u306b\u76f8\u5f53\uff09","text":"<p>\u30c7\u30fc\u30bf\u3092\u7279\u5b9a\u306e\u5217\u306b\u57fa\u3065\u3044\u3066\u4e26\u3079\u66ff\u3048\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001<code>mean_score</code>\uff08\u5e73\u5747\u70b9\uff09\u306e\u964d\u9806\u306b\u4e26\u3079\u66ff\u3048\u3092\u884c\u3044\u307e\u3059\u3002</p> <pre><code># \u5e73\u5747\u70b9\u306e\u964d\u9806\u3067\u4e26\u3079\u66ff\u3048\ndf_sorted = df.sort_values(by='mean_score', ascending=False)\nprint(\"\\n=== \u5e73\u5747\u70b9\u3067\u964d\u9806\u30bd\u30fc\u30c8\u3057\u305f\u30c7\u30fc\u30bf ===\")\nprint(df_sorted.head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#7-group_by-summarise","title":"7. \u30b0\u30eb\u30fc\u30d7\u5316\u3068\u8981\u7d04\uff08group_by &amp; summarise \u306b\u76f8\u5f53\uff09","text":""},{"location":"lectures/SIWS/03-wrangling/#a","title":"(a) \u6027\u5225\u3054\u3068\u306e\u6570\u5b66\u306e\u5e73\u5747\u30b9\u30b3\u30a2","text":"<p>\u6027\u5225\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5316\u3057\u3001\u5404\u30b0\u30eb\u30fc\u30d7\u5185\u306e\u300cmath score\u300d\u306e\u5e73\u5747\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002</p> <pre><code># \u6027\u5225\u3054\u3068\u306e\u6570\u5b66\u306e\u5e73\u5747\u30b9\u30b3\u30a2\nmean_math_by_gender = df.groupby('gender')['math score'].mean().reset_index()\nprint(\"\\n=== \u6027\u5225\u3054\u3068\u306e\u6570\u5b66\u306e\u5e73\u5747\u30b9\u30b3\u30a2 ===\")\nprint(mean_math_by_gender)\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#b","title":"(b) \u30c6\u30b9\u30c8\u6e96\u5099\u30b3\u30fc\u30b9\u5225\u306b\u6570\u5b66\u3001\u8aad\u89e3\u3001\u4f5c\u6587\u306e\u5e73\u5747\u30b9\u30b3\u30a2\u3092\u96c6\u8a08","text":"<p>\u751f\u5f92\u304c\u30c6\u30b9\u30c8\u6e96\u5099\u30b3\u30fc\u30b9\u3092\u53d7\u8b1b\u3057\u305f\u304b\u3069\u3046\u304b\u3067\u3001\u5404\u79d1\u76ee\u306e\u5e73\u5747\u30b9\u30b3\u30a2\u3092\u96c6\u8a08\u3057\u307e\u3059\u3002</p> <pre><code># \u30c6\u30b9\u30c8\u6e96\u5099\u30b3\u30fc\u30b9\u3054\u3068\u306b\u6570\u5b66\u3001\u8aad\u89e3\u3001\u4f5c\u6587\u306e\u5e73\u5747\u30b9\u30b3\u30a2\u3092\u96c6\u8a08\nmean_scores_by_prep = df.groupby('test preparation course').agg({\n    'math score': 'mean',\n    'reading score': 'mean',\n    'writing score': 'mean'\n}).reset_index()\nprint(\"\\n=== \u30c6\u30b9\u30c8\u6e96\u5099\u30b3\u30fc\u30b9\u3054\u3068\u306e\u5404\u79d1\u76ee\u306e\u5e73\u5747\u30b9\u30b3\u30a2 ===\")\nprint(mean_scores_by_prep)\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#_4","title":"\u8a73\u7d30\u8aac\u660e","text":"<ul> <li><code>groupby()</code> \u306b\u3088\u308a\u3001\u6307\u5b9a\u3057\u305f\u5217\uff08\u3053\u3053\u3067\u306f <code>gender</code> \u3084 <code>test preparation course</code>\uff09\u3067\u30c7\u30fc\u30bf\u3092\u30b0\u30eb\u30fc\u30d7\u5316\u3057\u307e\u3059\u3002  </li> <li><code>agg()</code> \u3092\u4f7f\u7528\u3057\u3066\u3001\u5404\u30b0\u30eb\u30fc\u30d7\u306b\u5bfe\u3057\u3066\u8907\u6570\u306e\u96c6\u7d04\u95a2\u6570\uff08\u4f8b\uff1a<code>mean</code>\uff09\u3092\u9069\u7528\u3067\u304d\u307e\u3059\u3002  </li> <li><code>reset_index()</code> \u306b\u3088\u308a\u3001\u30b0\u30eb\u30fc\u30d7\u5316\u5f8c\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u901a\u5e38\u306e\u5217\u306b\u623b\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"lectures/SIWS/03-wrangling/#8-join","title":"8. \u30c7\u30fc\u30bf\u306e\u7d50\u5408\uff08Join\uff09","text":"<p>\u8907\u6570\u306e DataFrame \u3092\u5171\u901a\u306e\u30ad\u30fc\u3067\u7d50\u5408\u3059\u308b\u64cd\u4f5c\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u5143\u306e\u30c7\u30fc\u30bf\u306b\u300c\u89aa\u306e\u5b66\u6b74\u300d\u3092\u6570\u5024\u5316\u3057\u305f\u60c5\u5831\u3092\u4ed8\u52a0\u3059\u308b\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/03-wrangling/#a-dataframe","title":"(a) \u8ffd\u52a0\u60c5\u5831\u306e DataFrame \u4f5c\u6210","text":"<p>\u89aa\u306e\u5b66\u6b74\u306f\u6587\u5b57\u5217\u3067\u3059\u304c\u3001\u3053\u3053\u3067\u306f\u4fbf\u5b9c\u4e0a\u3001\u5404\u30ec\u30d9\u30eb\u306b\u6570\u5024\u306e\u300c\u30e9\u30f3\u30af\u300d\u3092\u5272\u308a\u5f53\u3066\u305f DataFrame \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 \u4f8b\u3068\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u30e9\u30f3\u30af\u4ed8\u3051\u3092\u884c\u3044\u307e\u3059\uff1a - <code>some high school</code>: 1 - <code>high school</code>: 2 - <code>some college</code>: 3 - <code>associate's degree</code>: 4 - <code>bachelor's degree</code>: 5 - <code>master's degree</code>: 6  </p> <p>\u203b\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3067\u306f\u3001\u5024\u306e\u7a2e\u985e\u3084\u8868\u8a18\u304c\u7570\u306a\u308b\u5834\u5408\u304c\u3042\u308b\u305f\u3081\u3001\u9069\u5b9c\u8abf\u6574\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code># \u89aa\u306e\u5b66\u6b74\u3068\u305d\u306e\u30e9\u30f3\u30af\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u60c5\u5831\nparent_ed_levels = pd.DataFrame({\n    'parental level of education': [\n        'some high school', 'high school', 'some college', \n        \"associate's degree\", \"bachelor's degree\", \"master's degree\"\n    ],\n    'edu_rank': [1, 2, 3, 4, 5, 6]\n})\n\n# \u4f5c\u6210\u3057\u305f\u30de\u30c3\u30d4\u30f3\u30b0\u60c5\u5831\u3092\u3001\u5143\u306e\u30c7\u30fc\u30bf\u3068\u300c\u89aa\u306e\u5b66\u6b74\u300d\u3092\u30ad\u30fc\u306b\u5de6\u7d50\u5408\uff08left join\uff09\ndf_joined = pd.merge(df, parent_ed_levels, on='parental level of education', how='left')\nprint(\"\\n=== \u89aa\u306e\u5b66\u6b74\u306e\u30e9\u30f3\u30af\u60c5\u5831\u3092\u7d50\u5408\u3057\u305f\u30c7\u30fc\u30bf ===\")\nprint(df_joined[['parental level of education', 'edu_rank']].drop_duplicates())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#_5","title":"\u8a73\u7d30\u8aac\u660e","text":"<ul> <li><code>pd.merge()</code> \u306f\u30012 \u3064\u306e DataFrame \u3092\u5171\u901a\u306e\u30ad\u30fc\uff08\u3053\u3053\u3067\u306f <code>parental level of education</code>\uff09\u3067\u7d50\u5408\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\u3067\u3059\u3002  </li> <li><code>how='left'</code> \u3068\u3059\u308b\u3053\u3068\u3067\u3001\u5de6\u5074\uff08\u5143\u306e\u30c7\u30fc\u30bf\uff09\u306e\u5168\u884c\u3092\u4fdd\u6301\u3057\u3001\u53f3\u5074\u306e DataFrame \u304b\u3089\u5bfe\u5fdc\u3059\u308b\u60c5\u5831\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"lectures/SIWS/03-wrangling/#9-wide-long","title":"9. \u30c7\u30fc\u30bf\u306e\u518d\u69cb\u7bc9\uff08\u30d4\u30dc\u30c3\u30c8\u64cd\u4f5c\uff1awide \u21c4 long\uff09","text":""},{"location":"lectures/SIWS/03-wrangling/#a_1","title":"(a) \u30ef\u30a4\u30c9\u5f62\u5f0f\u304b\u3089\u30ed\u30f3\u30b0\u5f62\u5f0f\u3078\u306e\u5909\u63db","text":"<p>3 \u79d1\u76ee\uff08\u6570\u5b66\u3001\u8aad\u89e3\u3001\u4f5c\u6587\uff09\u306e\u30b9\u30b3\u30a2\u304c\u500b\u5225\u306e\u5217\u3068\u3057\u3066\u5b58\u5728\u3059\u308b\u30c7\u30fc\u30bf\u3092\u3001 <code>pd.melt()</code> \u3092\u7528\u3044\u3066\u300csubject\u300d\u3068\u300cscore\u300d\u306e 2 \u5217\u304b\u3089\u306a\u308b\u30ed\u30f3\u30b0\u5f62\u5f0f\u306b\u5909\u63db\u3057\u307e\u3059\u3002</p> <pre><code># \u6570\u5b66\u3001\u8aad\u89e3\u3001\u4f5c\u6587\u306e\u30b9\u30b3\u30a2\u3092\u30ed\u30f3\u30b0\u5f62\u5f0f\u306b\u5909\u63db\ndf_long = pd.melt(df,\n                  id_vars=['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'],\n                  value_vars=['math score', 'reading score', 'writing score'],\n                  var_name='subject',\n                  value_name='score')\nprint(\"\\n=== \u30ed\u30f3\u30b0\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf ===\")\nprint(df_long.head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#b_1","title":"(b) \u30ed\u30f3\u30b0\u5f62\u5f0f\u304b\u3089\u30ef\u30a4\u30c9\u5f62\u5f0f\u3078\u306e\u518d\u5909\u63db","text":"<p>\u5148\u307b\u3069\u306e\u30ed\u30f3\u30b0\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3092\u3001<code>pivot_table()</code> \u3092\u7528\u3044\u3066\u5143\u306e\u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u623b\u3057\u307e\u3059\u3002</p> <pre><code># \u30ed\u30f3\u30b0\u5f62\u5f0f\u304b\u3089\u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u5909\u63db\ndf_wide = df_long.pivot_table(index=['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'],\n                              columns='subject',\n                              values='score').reset_index()\nprint(\"\\n=== \u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u623b\u3057\u305f\u30c7\u30fc\u30bf ===\")\nprint(df_wide.head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#_6","title":"\u8a73\u7d30\u8aac\u660e","text":"<ul> <li><code>pd.melt()</code> \u306f\u3001\u8907\u6570\u306e\u5217\u3092 1 \u3064\u306e\u300c\u5024\u300d\u5217\u306b\u307e\u3068\u3081\u3001\u5bfe\u5fdc\u3059\u308b\u300c\u5909\u6570\u540d\u300d\u3092\u793a\u3059\u5217\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002  </li> <li><code>pivot_table()</code> \u3092\u7528\u3044\u308b\u3068\u3001\u6307\u5b9a\u3057\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u5217\u540d\u3092\u5143\u306b\u30c7\u30fc\u30bf\u3092\u518d\u69cb\u7bc9\u3057\u3001\u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u623b\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> </ul>"},{"location":"lectures/SIWS/03-wrangling/#10","title":"10. \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316","text":"<p>\u3053\u3053\u3067\u306f\u3001\u6574\u5f62\u3057\u305f\u30c7\u30fc\u30bf\u3092\u57fa\u306b\u53ef\u8996\u5316\u3092\u884c\u3044\u3001\u30c7\u30fc\u30bf\u306e\u50be\u5411\u3084\u5206\u5e03\u3092\u628a\u63e1\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/03-wrangling/#a_2","title":"(a) \u6563\u5e03\u56f3\uff1a\u6570\u5b66\u30b9\u30b3\u30a2\u3068\u8aad\u89e3\u30b9\u30b3\u30a2\u306e\u95a2\u4fc2","text":"<p>\u6027\u5225\u3054\u3068\u306b\u8272\u5206\u3051\u3057\u3066\u3001\u6570\u5b66\u30b9\u30b3\u30a2\u3068\u8aad\u89e3\u30b9\u30b3\u30a2\u306e\u95a2\u4fc2\u3092\u6563\u5e03\u56f3\u3067\u8868\u793a\u3057\u307e\u3059\u3002</p> <pre><code>plt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='math score', y='reading score', hue='gender', s=100)\nplt.title('\u6570\u5b66\u30b9\u30b3\u30a2\u3068\u8aad\u89e3\u30b9\u30b3\u30a2\u306e\u95a2\u4fc2')\nplt.xlabel('\u6570\u5b66\u30b9\u30b3\u30a2')\nplt.ylabel('\u8aad\u89e3\u30b9\u30b3\u30a2')\nplt.show()\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#b_2","title":"(b) \u30dc\u30c3\u30af\u30b9\u30d7\u30ed\u30c3\u30c8\uff1a\u6027\u5225\u3054\u3068\u306e\u6570\u5b66\u30b9\u30b3\u30a2\u306e\u5206\u5e03","text":"<p>\u5404\u6027\u5225\u306b\u304a\u3051\u308b\u6570\u5b66\u30b9\u30b3\u30a2\u306e\u5206\u5e03\uff08\u4e2d\u592e\u5024\u3001\u56db\u5206\u4f4d\u7bc4\u56f2\u3001\u5916\u308c\u5024\u306a\u3069\uff09\u3092\u30dc\u30c3\u30af\u30b9\u30d7\u30ed\u30c3\u30c8\u3067\u53ef\u8996\u5316\u3057\u307e\u3059\u3002</p> <pre><code>plt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x='gender', y='math score')\nplt.title('\u6027\u5225\u3054\u3068\u306e\u6570\u5b66\u30b9\u30b3\u30a2\u306e\u5206\u5e03')\nplt.xlabel('\u6027\u5225')\nplt.ylabel('\u6570\u5b66\u30b9\u30b3\u30a2')\nplt.show()\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#_7","title":"\u8a73\u7d30\u8aac\u660e","text":"<ul> <li><code>sns.scatterplot()</code> \u306f\u3001\u6563\u5e03\u56f3\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e\u95a2\u6570\u3067\u3059\u3002<code>hue</code> \u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3088\u308a\u6027\u5225\u3054\u3068\u306b\u8272\u5206\u3051\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002  </li> <li><code>sns.boxplot()</code> \u306f\u3001\u30ab\u30c6\u30b4\u30ea\u5225\u306e\u30c7\u30fc\u30bf\u5206\u5e03\uff08\u4e2d\u592e\u5024\u3001\u56db\u5206\u4f4d\u7bc4\u56f2\u3001\u5916\u308c\u5024\u306a\u3069\uff09\u3092\u8996\u899a\u7684\u306b\u628a\u63e1\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"lectures/SIWS/03-wrangling/#_8","title":"\u307e\u3068\u3081","text":"<p>\u672c\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u306f\u3001\u5b9f\u969b\u306e\u300cStudents Performance in Exams\u300d\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3001Python \u306b\u3088\u308b\u30c7\u30fc\u30bf\u6574\u5f62\u3068\u64cd\u4f5c\u306e\u57fa\u672c\u7684\u306a\u624b\u6cd5\u3092\u4ee5\u4e0b\u306e\u6d41\u308c\u3067\u89e3\u8aac\u3057\u307e\u3057\u305f\u3002</p> <ul> <li> <p>\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u30fb\u78ba\u8a8d   \u2192 CSV \u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u3001<code>head()</code> \u3067\u5148\u982d\u90e8\u5206\u3092\u8868\u793a\u3002</p> </li> <li> <p>\u884c\u306e\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0   \u2192 \u4f8b\u3068\u3057\u3066\u3001\u6570\u5b66\u30b9\u30b3\u30a2\u304c 70 \u70b9\u4ee5\u4e0a\u306e\u751f\u5f92\u3092\u62bd\u51fa\u3002</p> </li> <li> <p>\u5217\u306e\u9078\u629e   \u2192 \u89e3\u6790\u306b\u5fc5\u8981\u306a\u5217\uff08\u4f8b\uff1a<code>gender</code>, <code>math score</code>, <code>reading score</code>, <code>writing score</code>\uff09\u3092\u9078\u629e\u3002</p> </li> <li> <p>\u65b0\u3057\u3044\u5909\u6570\u306e\u4f5c\u6210   \u2192 <code>assign()</code> \u3092\u7528\u3044\u3001\u7dcf\u5f97\u70b9 (<code>total_score</code>) \u3084\u5e73\u5747\u70b9 (<code>mean_score</code>) \u3092\u8a08\u7b97\u3057\u3066\u8ffd\u52a0\u3002</p> </li> <li> <p>\u4e26\u3079\u66ff\u3048   \u2192 <code>sort_values()</code> \u306b\u3088\u308a\u3001\u5e73\u5747\u70b9\u306e\u964d\u9806\u3067\u4e26\u3079\u66ff\u3048\u3002</p> </li> <li> <p>\u30b0\u30eb\u30fc\u30d7\u5316\u3068\u8981\u7d04   \u2192 <code>groupby()</code> \u3068 <code>agg()</code> \u3092\u4f7f\u3044\u3001\u6027\u5225\u3084\u30c6\u30b9\u30c8\u6e96\u5099\u30b3\u30fc\u30b9\u3054\u3068\u306b\u5404\u79d1\u76ee\u306e\u5e73\u5747\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u306e\u7d50\u5408   \u2192 \u5225\u9014\u4f5c\u6210\u3057\u305f\u300c\u89aa\u306e\u5b66\u6b74\u30e9\u30f3\u30af\u300d\u60c5\u5831\u306e DataFrame \u3068\u7d50\u5408\u3057\u3001\u4ed8\u52a0\u60c5\u5831\u3092\u8ffd\u52a0\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u306e\u518d\u69cb\u7bc9\uff08\u30d4\u30dc\u30c3\u30c8\u64cd\u4f5c\uff09   \u2192 <code>melt()</code> \u3067\u30ef\u30a4\u30c9\u5f62\u5f0f\u306e\u30b9\u30b3\u30a2\u5217\u3092\u30ed\u30f3\u30b0\u5f62\u5f0f\u306b\u5909\u63db\u3057\u3001<code>pivot_table()</code> \u3067\u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u623b\u3059\u64cd\u4f5c\u3092\u5b9f\u65bd\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316   \u2192 seaborn \u3068 matplotlib \u3092\u7528\u3044\u3066\u3001\u6563\u5e03\u56f3\u3084\u30dc\u30c3\u30af\u30b9\u30d7\u30ed\u30c3\u30c8\u3067\u30c7\u30fc\u30bf\u306e\u50be\u5411\u3084\u5206\u5e03\u3092\u8996\u899a\u5316\u3002</p> </li> </ul> <p>\u3053\u308c\u3089\u306e\u57fa\u672c\u64cd\u4f5c\u306f\u3001\u30c7\u30fc\u30bf\u30af\u30ec\u30f3\u30b8\u30f3\u30b0\u3001\u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u3001\u305d\u3057\u3066\u6b63\u78ba\u306a\u5206\u6790\u7d50\u679c\u3092\u5f97\u308b\u305f\u3081\u306e\u524d\u51e6\u7406\u3068\u3057\u3066\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u73fe\u5834\u3067\u5fc5\u9808\u306e\u30b9\u30ad\u30eb\u3067\u3059\u3002 \u672c\u7ae0\u3067\u5b66\u3093\u3060\u624b\u6cd5\u3092\u8eab\u306b\u3064\u3051\u308b\u3053\u3068\u3067\u3001\u5b9f\u969b\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u304a\u3051\u308b\u30c7\u30fc\u30bf\u89e3\u6790\u3084\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u7cbe\u5ea6\u5411\u4e0a\u306b\u3064\u306a\u304c\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/03-wrangling/#_9","title":"\u6f14\u7fd2\u554f\u984c\uff08\u30c7\u30fc\u30bf\u6574\u5f62\uff09","text":"<p>\u3053\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u306f\u3001\u3053\u308c\u307e\u3067\u5b66\u3093\u3060\u5185\u5bb9\u3092\u5fa9\u7fd2\u3059\u308b\u305f\u3081\u306e\u6f14\u7fd2\u554f\u984c\u3067\u3059\u3002\u305f\u3060\u3057\u3001\u3053\u308c\u3089\u306e\u30b3\u30fc\u30c9\u3092\u899a\u3048\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002 \u3053\u306e\u30b3\u30fc\u30c9\u306f\u300c\u3053\u3046\u3044\u3046\u610f\u5473\u304b\u300d\u3068\u7406\u89e3\u3067\u304d\u308c\u3070\u554f\u984c\u306f\u306a\u3044\u3067\u3059\u3002 \u307e\u305a\u306f\u3001\u5b9f\u969b\u306b\u30b3\u30fc\u30c9\u3092\u52d5\u304b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\u3084\u30b0\u30e9\u30d5\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u305d\u306e\u5f8c\u3067\u3001\u305d\u308c\u305e\u308c\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u3069\u306e\u3088\u3046\u306a\u610f\u5473\u306a\u306e\u304b\u3092\u5b66\u3073\u76f4\u3057\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"lectures/SIWS/03-wrangling/#1_1","title":"\u6f14\u7fd2 1: \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068\u57fa\u672c\u78ba\u8a8d","text":""},{"location":"lectures/SIWS/03-wrangling/#_10","title":"\u554f\u984c","text":"<ol> <li>\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e URL <code>https://raw.githubusercontent.com/selva86/datasets/master/StudentsPerformance.csv</code>    \u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u3001DataFrame \u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002  </li> <li>\u8aad\u307f\u8fbc\u3093\u3060\u30c7\u30fc\u30bf\u306e\u5148\u982d 10 \u884c\u3092\u8868\u793a\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/SIWS/03-wrangling/#_11","title":"\u89e3\u7b54\u4f8b","text":"<pre><code>import pandas as pd\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e URL\ndata_url = \"https://raw.githubusercontent.com/selva86/datasets/master/StudentsPerformance.csv\"\n\n# CSV \u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\ndf = pd.read_csv(data_url)\n\n# \u30c7\u30fc\u30bf\u306e\u5148\u982d 10 \u884c\u3092\u8868\u793a\nprint(\"\u30c7\u30fc\u30bf\u306e\u5148\u982d10\u884c:\")\nprint(df.head(10))\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#2_1","title":"\u6f14\u7fd2 2: \u30c7\u30fc\u30bf\u306e\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3068\u5217\u9078\u629e","text":""},{"location":"lectures/SIWS/03-wrangling/#_12","title":"\u554f\u984c","text":"<ol> <li>\u300cmath score\u300d\u304c 80 \u70b9\u4ee5\u4e0a\u306e\u751f\u5f92\u306e\u307f\u3092\u62bd\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002  </li> <li>\u62bd\u51fa\u3057\u305f\u30c7\u30fc\u30bf\u304b\u3089\u3001<code>gender</code>\u3001<code>math score</code>\u3001<code>reading score</code>\u3001<code>writing score</code> \u306e 4 \u5217\u3060\u3051\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/SIWS/03-wrangling/#_13","title":"\u89e3\u7b54\u4f8b","text":"<pre><code># math score\u304c80\u70b9\u4ee5\u4e0a\u306e\u751f\u5f92\u3092\u62bd\u51fa\ndf_filtered = df[df['math score'] &gt;= 80]\nprint(\"math score\u304c80\u4ee5\u4e0a\u306e\u30c7\u30fc\u30bf:\")\nprint(df_filtered.head())\n\n# \u5fc5\u8981\u306a\u5217\u306e\u307f\u3092\u9078\u629e\ndf_selected = df_filtered[['gender', 'math score', 'reading score', 'writing score']]\nprint(\"\u9078\u629e\u3057\u305f\u5217:\")\nprint(df_selected.head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#3_1","title":"\u6f14\u7fd2 3: \u65b0\u3057\u3044\u5909\u6570\u306e\u4f5c\u6210","text":""},{"location":"lectures/SIWS/03-wrangling/#_14","title":"\u554f\u984c","text":"<ol> <li>\u300creading-writing average\u300d\u3068\u3044\u3046\u5217\u3092\u8ffd\u52a0\u3057\u3001<code>reading score</code> \u3068 <code>writing score</code> \u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u3066\u304f\u3060\u3055\u3044\u3002  </li> <li>\u5404\u751f\u5f92\u306e\u7dcf\u5f97\u70b9 (<code>total_score</code>: <code>math score</code> + <code>reading score</code> + <code>writing score</code>) \u3092\u8a08\u7b97\u3057\u3001\u5217\u3068\u3057\u3066\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/SIWS/03-wrangling/#_15","title":"\u89e3\u7b54\u4f8b","text":"<pre><code># \u65b0\u3057\u3044\u5217\u306e\u4f5c\u6210\uff1areading-writing average \u3068 total_score\ndf['reading-writing average'] = df[['reading score', 'writing score']].mean(axis=1)\ndf['total_score'] = df['math score'] + df['reading score'] + df['writing score']\n\nprint(\"\u65b0\u3057\u3044\u5217\u3092\u8ffd\u52a0\u3057\u305f\u30c7\u30fc\u30bf:\")\nprint(df[['reading-writing average', 'total_score']].head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#4_1","title":"\u6f14\u7fd2 4: \u4e26\u3079\u66ff\u3048","text":""},{"location":"lectures/SIWS/03-wrangling/#_16","title":"\u554f\u984c","text":"<p>\u7dcf\u5f97\u70b9 (<code>total_score</code>) \u306b\u57fa\u3065\u3044\u3066\u30c7\u30fc\u30bf\u3092\u964d\u9806\u306b\u4e26\u3079\u66ff\u3048\u3001\u4e0a\u4f4d 5 \u4ef6\u306e\u30c7\u30fc\u30bf\u3092\u8868\u793a\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/SIWS/03-wrangling/#_17","title":"\u89e3\u7b54\u4f8b","text":"<pre><code># total_score \u306b\u57fa\u3065\u3044\u3066\u964d\u9806\u306b\u4e26\u3079\u66ff\u3048\ndf_sorted = df.sort_values(by='total_score', ascending=False)\nprint(\"\u7dcf\u5f97\u70b9\u3067\u964d\u9806\u306b\u4e26\u3079\u66ff\u3048\u305f\u4e0a\u4f4d5\u4ef6:\")\nprint(df_sorted.head(5))\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#5","title":"\u6f14\u7fd2 5: \u30b0\u30eb\u30fc\u30d7\u5316\u3068\u8981\u7d04","text":""},{"location":"lectures/SIWS/03-wrangling/#_18","title":"\u554f\u984c","text":"<ol> <li>\u6027\u5225\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5316\u3057\u3001<code>math score</code> \u306e\u5e73\u5747\u5024\u3092\u8a08\u7b97\u3057\u3066\u8868\u793a\u3057\u3066\u304f\u3060\u3055\u3044\u3002  </li> <li><code>test preparation course</code> \u5225\u306b\u3001<code>math score</code>\u3001<code>reading score</code>\u3001<code>writing score</code> \u306e\u5404\u5e73\u5747\u5024\u3092\u96c6\u8a08\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/SIWS/03-wrangling/#_19","title":"\u89e3\u7b54\u4f8b","text":"<pre><code># \u6027\u5225\u3054\u3068\u306e math score \u306e\u5e73\u5747\u3092\u8a08\u7b97\nmean_math_by_gender = df.groupby('gender')['math score'].mean().reset_index()\nprint(\"\u6027\u5225\u3054\u3068\u306e math score \u306e\u5e73\u5747:\")\nprint(mean_math_by_gender)\n\n# test preparation course \u5225\u306b\u5404\u79d1\u76ee\u306e\u5e73\u5747\u5024\u3092\u96c6\u8a08\nmean_scores_by_prep = df.groupby('test preparation course').agg({\n    'math score': 'mean',\n    'reading score': 'mean',\n    'writing score': 'mean'\n}).reset_index()\nprint(\"test preparation course \u5225\u306e\u5404\u79d1\u76ee\u306e\u5e73\u5747:\")\nprint(mean_scores_by_prep)\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#6-join","title":"\u6f14\u7fd2 6: \u30c7\u30fc\u30bf\u306e\u7d50\u5408\uff08Join\uff09","text":""},{"location":"lectures/SIWS/03-wrangling/#_20","title":"\u554f\u984c","text":"<ol> <li>\u300cparental level of education\u300d\u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u30eb\u30fc\u30eb\u3067\u6570\u5024\u306e\u30e9\u30f3\u30af\u3092\u4ed8\u4e0e\u3059\u308b DataFrame \u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002  </li> <li><code>some high school</code>: 1  </li> <li><code>high school</code>: 2  </li> <li><code>some college</code>: 3  </li> <li><code>associate's degree</code>: 4  </li> <li><code>bachelor's degree</code>: 5  </li> <li><code>master's degree</code>: 6  </li> <li>\u4f5c\u6210\u3057\u305f DataFrame \u3092\u3001\u5143\u306e\u30c7\u30fc\u30bf\u3068 <code>parental level of education</code> \u3092\u30ad\u30fc\u306b\u5de6\u7d50\u5408\u3057\u3001<code>parental level of education</code> \u3068 <code>edu_rank</code> \u306e\u30e6\u30cb\u30fc\u30af\u306a\u7d44\u307f\u5408\u308f\u305b\u3092\u8868\u793a\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/SIWS/03-wrangling/#_21","title":"\u89e3\u7b54\u4f8b","text":"<pre><code># \u89aa\u306e\u5b66\u6b74\u30e9\u30f3\u30af\u306e DataFrame \u3092\u4f5c\u6210\nparent_ed_levels = pd.DataFrame({\n    'parental level of education': [\n        'some high school',\n        'high school',\n        'some college',\n        \"associate's degree\",\n        \"bachelor's degree\",\n        \"master's degree\"\n    ],\n    'edu_rank': [1, 2, 3, 4, 5, 6]\n})\n\n# \u5143\u306e\u30c7\u30fc\u30bf\u3068\u5de6\u7d50\u5408\ndf_joined = pd.merge(df, parent_ed_levels, on='parental level of education', how='left')\nprint(\"\u89aa\u306e\u5b66\u6b74\u3068edu_rank\u306e\u30e6\u30cb\u30fc\u30af\u306a\u7d44\u307f\u5408\u308f\u305b:\")\nprint(df_joined[['parental level of education', 'edu_rank']].drop_duplicates())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#7","title":"\u6f14\u7fd2 7: \u30d4\u30dc\u30c3\u30c8\u64cd\u4f5c\uff08\u30ef\u30a4\u30c9\u21c4\u30ed\u30f3\u30b0\u5909\u63db\uff09","text":""},{"location":"lectures/SIWS/03-wrangling/#_22","title":"\u554f\u984c","text":"<ol> <li><code>pd.melt()</code> \u3092\u7528\u3044\u3066\u3001<code>math score</code>\u3001<code>reading score</code>\u3001<code>writing score</code> \u306e\u5404\u5217\u3092\u300csubject\u300d\u3068\u300cscore\u300d\u306e 2 \u5217\u306b\u5909\u63db\u3057\u3066\u304f\u3060\u3055\u3044\u3002    \u203b <code>id_vars</code> \u3068\u3057\u3066 <code>gender</code>\u3001<code>race/ethnicity</code>\u3001<code>parental level of education</code>\u3001<code>lunch</code>\u3001<code>test preparation course</code> \u3092\u6307\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002  </li> <li>\u5148\u307b\u3069\u306e\u30ed\u30f3\u30b0\u5f62\u5f0f\u30c7\u30fc\u30bf\u3092 <code>pivot_table()</code> \u3092\u4f7f\u3063\u3066\u3001\u5143\u306e\u30ef\u30a4\u30c9\u5f62\u5f0f\uff08\u5404\u79d1\u76ee\u304c\u5225\u306e\u5217\uff09\u306b\u623b\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/SIWS/03-wrangling/#_23","title":"\u89e3\u7b54\u4f8b","text":"<pre><code># \u30ef\u30a4\u30c9\u5f62\u5f0f\u304b\u3089\u30ed\u30f3\u30b0\u5f62\u5f0f\u306b\u5909\u63db\ndf_long = pd.melt(df,\n                  id_vars=['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'],\n                  value_vars=['math score', 'reading score', 'writing score'],\n                  var_name='subject',\n                  value_name='score')\nprint(\"\u30ed\u30f3\u30b0\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf:\")\nprint(df_long.head())\n\n# \u30ed\u30f3\u30b0\u5f62\u5f0f\u304b\u3089\u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u518d\u5909\u63db\ndf_wide = df_long.pivot_table(index=['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'],\n                              columns='subject',\n                              values='score').reset_index()\nprint(\"\u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u623b\u3057\u305f\u30c7\u30fc\u30bf:\")\nprint(df_wide.head())\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#8","title":"\u6f14\u7fd2 8: \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316","text":""},{"location":"lectures/SIWS/03-wrangling/#_24","title":"\u554f\u984c","text":"<ol> <li>\u6027\u5225\u3054\u3068\u306b\u8272\u5206\u3051\u3057\u305f\u6563\u5e03\u56f3\u3092\u4f5c\u6210\u3057\u3001\u300cmath score\u300d\u3068\u300creading score\u300d\u306e\u95a2\u4fc2\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002  </li> <li><code>lunch</code> \u5225\u306b\u300cwriting score\u300d\u306e\u5206\u5e03\u3092\u793a\u3059\u30dc\u30c3\u30af\u30b9\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ol>"},{"location":"lectures/SIWS/03-wrangling/#_25","title":"\u89e3\u7b54\u4f8b","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# \u6563\u5e03\u56f3\u306e\u4f5c\u6210: math score vs reading score (\u6027\u5225\u3067\u8272\u5206\u3051)\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='math score', y='reading score', hue='gender', s=100)\nplt.title('Math Score vs Reading Score by Gender')\nplt.xlabel('Math Score')\nplt.ylabel('Reading Score')\nplt.show()\n\n# \u30dc\u30c3\u30af\u30b9\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210: lunch \u5225\u306e writing score \u5206\u5e03\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x='lunch', y='writing score')\nplt.title('Writing Score Distribution by Lunch')\nplt.xlabel('Lunch')\nplt.ylabel('Writing Score')\nplt.show()\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#9","title":"\u6f14\u7fd2 9: \u5fdc\u7528\u554f\u984c","text":""},{"location":"lectures/SIWS/03-wrangling/#9-1","title":"\u554f\u984c 9-1: \u8907\u5408\u7684\u306a\u30b0\u30eb\u30fc\u30d7\u5316\u3068\u53ef\u8996\u5316","text":"<p>\u30ed\u30f3\u30b0\u5f62\u5f0f\u306b\u5909\u63db\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u5404\u79d1\u76ee\u3054\u3068\u306b\u300cparental level of education\u300d\u5225\u306e\u5e73\u5747\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\u3057\u3001\u68d2\u30b0\u30e9\u30d5\u3067\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/SIWS/03-wrangling/#9-1_1","title":"\u89e3\u7b54\u4f8b 9-1","text":"<pre><code># \u30ed\u30f3\u30b0\u5f62\u5f0f (df_long) \u3092\u7528\u3044\u3066\u3001parental level of education \u3068 subject \u5225\u306e\u5e73\u5747\u30b9\u30b3\u30a2\u3092\u8a08\u7b97\ngrouped = df_long.groupby(['parental level of education', 'subject'])['score'].mean().reset_index()\nprint(\"parental level of education \u3068 subject \u5225\u306e\u5e73\u5747\u30b9\u30b3\u30a2:\")\nprint(grouped)\n\n# \u68d2\u30b0\u30e9\u30d5\u306e\u4f5c\u6210\nplt.figure(figsize=(10, 6))\nsns.barplot(data=grouped, x='parental level of education', y='score', hue='subject')\nplt.title('Average Score by Parental Level of Education and Subject')\nplt.xlabel('Parental Level of Education')\nplt.ylabel('Average Score')\nplt.xticks(rotation=45)\nplt.show()\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#9-2","title":"\u554f\u984c 9-2: \u4eee\u8aac\u691c\u8a3c\u306e\u305f\u3081\u306e\u524d\u51e6\u7406","text":"<p>\u3053\u3053\u3067\u306f\u3001test preparation course \u304c total_score \u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u691c\u8a3c\u3059\u308b\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002 - <code>test preparation course</code> \u5225\u306b total_score \u306e\u5206\u5e03\u3092\u30dc\u30c3\u30af\u30b9\u30d7\u30ed\u30c3\u30c8\u3067\u53ef\u8996\u5316\u3057\u3001 - \u5404\u30b0\u30eb\u30fc\u30d7\u306e\u5e73\u5747 total_score \u3092\u7b97\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"lectures/SIWS/03-wrangling/#9-2_1","title":"\u89e3\u7b54\u4f8b 9-2","text":"<pre><code># \u30dc\u30c3\u30af\u30b9\u30d7\u30ed\u30c3\u30c8\u306b\u3088\u308b\u53ef\u8996\u5316: test preparation course \u5225\u306e total_score \u5206\u5e03\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x='test preparation course', y='total_score')\nplt.title('Total Score by Test Preparation Course')\nplt.xlabel('Test Preparation Course')\nplt.ylabel('Total Score')\nplt.show()\n\n# test preparation course \u5225\u306e\u5e73\u5747 total_score \u3092\u8a08\u7b97\nmean_total_by_prep = df.groupby('test preparation course')['total_score'].mean().reset_index()\nprint(\"Test Preparation Course \u5225\u306e\u5e73\u5747 Total Score:\")\nprint(mean_total_by_prep)\n</code></pre>"},{"location":"lectures/SIWS/03-wrangling/#whats-to-come","title":"What's to come?","text":"<p>So far in this book, we've explored, visualized, and wrangled data saved in data frames. These data frames were saved in a spreadsheet-like format: in a rectangular shape with a certain number of rows corresponding to observations and a certain number of columns corresponding to variables describing these observations. </p> <p>We'll see in the upcoming Chapter \\@ref(tidy) that there are actually two ways to represent data in spreadsheet-type rectangular format: (1) \"wide\" format and (2) \"tall/narrow\" format. The tall/narrow format is also known as \"tidy\" format in R user circles. While the distinction between \"tidy\" and non-\"tidy\" formatted data is subtle, it has immense implications for our data science work. This is because almost all the packages used in this book, including the <code>ggplot2</code> package for data visualization and the <code>dplyr</code> package for data wrangling, all assume that all data frames are in \"tidy\" format. </p> <p>Furthermore, up until now we've only explored, visualized, and wrangled data saved within R packages. But what if you want to analyze data that you have saved in a Microsoft Excel, a Google Sheets, or a \"Comma-Separated Values\" (CSV) file? In Section \\@ref(csv), we'll show you how to import this data into R using the <code>readr</code> package. </p>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/","title":"Data Importing and \"Tidy\" Data","text":""},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#_1","title":"\u306f\u3058\u3081\u306b","text":"<p>\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u304a\u3044\u3066\u3001\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\uff08\u8868\u5f62\u5f0f\u30c7\u30fc\u30bf\uff09\u306f\u3001\u5404\u884c\u304c\u89b3\u6e2c\uff08\u4f8b\uff1a\u500b\u3005\u306e\u98db\u884c\u6a5f\u306e\u30d5\u30e9\u30a4\u30c8\u3001\u56fd\u3054\u3068\u306e\u7d71\u8a08\u306a\u3069\uff09\u3001\u5404\u5217\u304c\u89b3\u6e2c\u306e\u5909\u6570\uff08\u4f8b\uff1a\u5e74\u9f62\u3001\u30b9\u30b3\u30a2\u3001\u65e5\u4ed8\u306a\u3069\uff09\u3092\u8868\u3059\u3082\u306e\u3067\u3059\u3002\u672c\u7ae0\u3067\u306f\u3001R \u7248\u300cData Science with tidyverse\u300d\u3067\u6271\u308f\u308c\u305f\u5185\u5bb9\u3092\u3082\u3068\u306b\u3001 \u30fb\u30c7\u30fc\u30bf\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\u65b9\u6cd5 \u30fb\u300c\u6574\u7136\u3068\u3057\u305f\uff08tidy\uff09\u300d\u30c7\u30fc\u30bf\u306e\u6982\u5ff5\u3068\u305d\u306e\u91cd\u8981\u6027 \u30fb\u30ef\u30a4\u30c9\u5f62\u5f0f\u3068\u30ed\u30f3\u30b0\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u5909\u63db \u30fb\u6574\u7136\u306a\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u53ef\u8996\u5316 \u306a\u3069\u3092\u3001Python \u306e\u4e3b\u8981\u30e9\u30a4\u30d6\u30e9\u30ea\uff08numpy, pandas, matplotlib, seaborn\uff09\u3092\u4f7f\u3063\u3066\u5b9f\u6f14\u3057\u307e\u3059\u3002</p> <p>\u300ctidy\uff08\u6574\u7136\u3068\u3057\u305f\uff09\u30c7\u30fc\u30bf\u300d\u3068\u306f\u3001\u4ee5\u4e0b\u306e3\u539f\u5247\u306b\u57fa\u3065\u304f\u30c7\u30fc\u30bf\u69cb\u9020\u306e\u3053\u3068\u3067\u3059\uff1a</p> <ol> <li> <p>\u5404\u5909\u6570\u306f 1 \u5217\u306b\u683c\u7d0d\u3055\u308c\u308b    \u4f8b\uff1a\u8eab\u9577\u3001\u4f53\u91cd\u3001\u5e74\u9f62\u306a\u3069\u306f\u305d\u308c\u305e\u308c\u72ec\u7acb\u3057\u305f\u5217\u306b\u914d\u7f6e\u3059\u308b\u3002</p> </li> <li> <p>\u5404\u89b3\u6e2c\u306f 1 \u884c\u306b\u683c\u7d0d\u3055\u308c\u308b    \u4f8b\uff1a1 \u4eba\u306e\u88ab\u9a13\u8005\u306e\u5168\u60c5\u5831\u304c 1 \u884c\u306b\u307e\u3068\u307e\u3063\u3066\u3044\u308b\u3002</p> </li> <li> <p>\u5404\u89b3\u6e2c\u5358\u4f4d\u306f\u5225\u306e\u30c6\u30fc\u30d6\u30eb\u306b\u307e\u3068\u3081\u308b    \u4f8b\uff1a\u5b66\u751f\u30c7\u30fc\u30bf\u3068\u6559\u5e2b\u30c7\u30fc\u30bf\u304c\u6df7\u5728\u305b\u305a\u3001\u305d\u308c\u305e\u308c\u500b\u5225\u306e\u30c6\u30fc\u30d6\u30eb\u306b\u4fdd\u5b58\u3055\u308c\u308b\u3002</p> </li> </ol> <p>\u3053\u308c\u3089\u306e\u539f\u5247\u306b\u3088\u308a\u3001\u30c7\u30fc\u30bf\u306e\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3001\u5909\u5f62\u3001\u96c6\u7d04\u3001\u7d50\u5408\u3001\u53ef\u8996\u5316\u306a\u3069\u306e\u64cd\u4f5c\u304c\u52b9\u7387\u3088\u304f\u884c\u3048\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 \u307e\u305f\u3001\u5f8c\u7d9a\u306e\u89e3\u6790\uff08\u56de\u5e30\u5206\u6790\u3084\u7d71\u8a08\u7684\u63a8\u8ad6\u306a\u3069\uff09\u306b\u304a\u3044\u3066\u3082\u3001\u3053\u306e\u6574\u7136\u3068\u3057\u305f\u30c7\u30fc\u30bf\u5f62\u5f0f\u306f\u5fc5\u9808\u3067\u3059\u3002</p>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#1","title":"1. \u30c7\u30fc\u30bf\u306e\u30a4\u30f3\u30dd\u30fc\u30c8","text":"<p>\u3053\u3053\u3067\u306f\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3092\u81ea\u5206\u306e\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3084\u30aa\u30f3\u30e9\u30a4\u30f3\u4e0a\u304b\u3089\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002 \u30b9\u30d7\u30ec\u30c3\u30c9\u30b7\u30fc\u30c8\u306e\u30c7\u30fc\u30bf\u306f\u3001\u4e3b\u306b\u4ee5\u4e0b\u306e\u5f62\u5f0f\u3067\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002</p> <ul> <li> <p>CSV\uff08Comma Separated Values\uff09\u30d5\u30a1\u30a4\u30eb   \u5404\u884c\u304c\u89b3\u6e2c\uff081\u884c\uff1d1\u89b3\u6e2c\uff09\u3001\u5404\u5024\u306f\u30ab\u30f3\u30de\u3067\u533a\u5207\u3089\u308c\u3001\u6700\u521d\u306e\u884c\u304c\u30d8\u30c3\u30c0\u30fc\u306e\u5834\u5408\u304c\u591a\u3044\u3002</p> </li> <li> <p>Excel\uff08.xlsx\uff09\u30d5\u30a1\u30a4\u30eb   Microsoft Excel \u306e\u5f62\u5f0f\u3067\u3001CSV \u3088\u308a\u3082\u591a\u304f\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\uff08\u30d5\u30a9\u30f3\u30c8\u3001\u30bb\u30eb\u306e\u8272\u3001\u5217\u5e45\u3001\u6570\u5f0f\u306a\u3069\uff09\u3092\u542b\u3080\u3002</p> </li> <li> <p>Google Sheets   \u30af\u30e9\u30a6\u30c9\u4e0a\u306e\u30b9\u30d7\u30ec\u30c3\u30c9\u30b7\u30fc\u30c8\u3002CSV \u3084 Excel \u5f62\u5f0f\u3067\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u53ef\u80fd\u3002</p> </li> </ul>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#11-csv","title":"1.1 CSV \u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30f3\u30dd\u30fc\u30c8","text":"<p>\u3053\u3053\u3067\u306f\u3001\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u4e0a\u306b\u3042\u308b CSV \u30d5\u30a1\u30a4\u30eb <code>dem_score.csv</code>\uff08https://moderndive.com/data/dem_score.csv\uff09\u3092 pandas \u306e <code>read_csv()</code> \u95a2\u6570\u3092\u7528\u3044\u3066\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002</p> <pre><code>import pandas as pd\n\n# CSV \u30d5\u30a1\u30a4\u30eb\u3092\u30aa\u30f3\u30e9\u30a4\u30f3\u304b\u3089\u8aad\u307f\u8fbc\u3080\ndem_score = pd.read_csv(\"https://moderndive.com/data/dem_score.csv\")\nprint(\"\u3010dem_score\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3011\")\nprint(dem_score.head())\n</code></pre> <p>\u3053\u306e\u30c7\u30fc\u30bf\u3067\u306f\u3001\u5404\u56fd\u306e\u6c11\u4e3b\u4e3b\u7fa9\u5ea6\uff08democracy_score\uff09\u304c\u6e2c\u5b9a\u3055\u308c\u3066\u304a\u308a\u3001\u5024\u304c -10 \u3067\u9ad8\u5ea6\u306a\u72ec\u88c1\u56fd\u5bb6\u300110 \u3067\u9ad8\u5ea6\u306a\u6c11\u4e3b\u56fd\u5bb6\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#12-excel","title":"1.2 Excel \u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30f3\u30dd\u30fc\u30c8","text":"<p>Excel \u30d5\u30a1\u30a4\u30eb\u306e\u5834\u5408\u306f\u3001<code>read_excel()</code> \u95a2\u6570\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u203b Python \u3067\u306f RStudio \u306e\u3088\u3046\u306a GUI \u3092\u7528\u3044\u305f\u30a4\u30f3\u30dd\u30fc\u30c8\u306f\u6a19\u6e96\u3067\u306f\u63d0\u4f9b\u3055\u308c\u307e\u305b\u3093\u304c\u3001\u30d5\u30a1\u30a4\u30eb\u30a8\u30af\u30b9\u30d7\u30ed\u30fc\u30e9\u30fc\u7b49\u3067\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u5f8c\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002</p> <pre><code># \u4e8b\u524d\u306b \"dem_score.xlsx\" \u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u4f5c\u696d\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3057\u3066\u304a\u304f\ndem_score_excel = pd.read_excel(\"dem_score.xlsx\")\nprint(\"\u3010dem_score (Excel) \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3011\")\nprint(dem_score_excel.head())\n</code></pre>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#2-tidy-data","title":"2. \"Tidy\" Data\uff08\u6574\u7136\u3068\u3057\u305f\u30c7\u30fc\u30bf\uff09","text":"<p>\u3053\u3053\u304b\u3089\u306f\u3001\u6574\u7136\u3068\u3057\u305f\uff08tidy\uff09\u30c7\u30fc\u30bf\u306e\u6982\u5ff5\u3068\u3001\u30ef\u30a4\u30c9\u5f62\u5f0f\u21c4\u30ed\u30f3\u30b0\u5f62\u5f0f\u3078\u306e\u5909\u63db\u306e\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#21","title":"2.1 \u30ef\u30a4\u30c9\u5f62\u5f0f\u3068\u30ed\u30f3\u30b0\u5f62\u5f0f\u306e\u4f8b","text":""},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#_2","title":"\u30ef\u30a4\u30c9\u5f62\u5f0f\u306e\u4f8b","text":"<p>\u6b21\u306e\u4f8b\u306f\u3001\u5404\u56fd\u3054\u3068\u306b 1999 \u5e74\u3068 2000 \u5e74\u306e\u300c\u75be\u75c5\u767a\u751f\u4ef6\u6570\uff08cases\uff09\u300d\u304c\u8a18\u9332\u3055\u308c\u305f\u30ef\u30a4\u30c9\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3067\u3059\uff08\u5404\u5e74\u304c\u500b\u5225\u306e\u5217\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff09\u3002</p> <pre><code># \u30ef\u30a4\u30c9\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u4f8b\ndf_wide = pd.DataFrame({\n    'country': ['Afghanistan', 'Brazil', 'China'],\n    '1999': [745, 37737, 212258],\n    '2000': [2666, 80488, 213766]\n})\nprint(\"\u3010\u30ef\u30a4\u30c9\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3011\")\nprint(df_wide)\n</code></pre>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#_3","title":"\u30ef\u30a4\u30c9\u5f62\u5f0f\u304b\u3089\u30ed\u30f3\u30b0\u5f62\u5f0f\u3078\u306e\u5909\u63db","text":"<p>pandas \u306e <code>melt()</code> \u95a2\u6570\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u30ef\u30a4\u30c9\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3092\u30ed\u30f3\u30b0\u5f62\u5f0f\uff08\u6574\u7136\u3068\u3057\u305f\u30c7\u30fc\u30bf\uff09\u306b\u5909\u63db\u3067\u304d\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001<code>country</code> \u5217\u306f\u305d\u306e\u307e\u307e\u6b8b\u3057\u3001\u6b8b\u308a\u306e\u5404\u5217\uff08\u5e74\uff09\u3092 1 \u3064\u306e\u5909\u6570 <code>year</code> \u3068\u3057\u3066\u3001\u5bfe\u5fdc\u3059\u308b\u5024\u3092 <code>cases</code> \u5217\u306b\u307e\u3068\u3081\u307e\u3059\u3002</p> <pre><code># \u30ef\u30a4\u30c9\u5f62\u5f0f\u304b\u3089\u30ed\u30f3\u30b0\u5f62\u5f0f\u3078\u5909\u63db\ndf_long = pd.melt(df_wide, id_vars=['country'], var_name='year', value_name='cases')\nprint(\"\\n\u3010\u30ed\u30f3\u30b0\u5f62\u5f0f\u306b\u5909\u63db\u3057\u305f\u30c7\u30fc\u30bf\u3011\")\nprint(df_long)\n</code></pre>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#_4","title":"\u30ed\u30f3\u30b0\u5f62\u5f0f\u304b\u3089\u30ef\u30a4\u30c9\u5f62\u5f0f\u3078\u306e\u518d\u5909\u63db","text":"<p>\u9006\u306b\u3001<code>pivot()</code> \u3084 <code>pivot_table()</code> \u95a2\u6570\u3092\u4f7f\u3063\u3066\u3001\u30ed\u30f3\u30b0\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3092\u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u623b\u3059\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002</p> <pre><code># \u30ed\u30f3\u30b0\u5f62\u5f0f\u304b\u3089\u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u518d\u5909\u63db\ndf_wide_again = df_long.pivot(index='country', columns='year', values='cases').reset_index()\nprint(\"\\n\u3010\u30ed\u30f3\u30b0\u5f62\u5f0f\u304b\u3089\u518d\u69cb\u7bc9\u3057\u305f\u30ef\u30a4\u30c9\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3011\")\nprint(df_wide_again)\n</code></pre>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#3","title":"3. \u6574\u7136\u30c7\u30fc\u30bf\u306e\u5229\u70b9\u3092\u6d3b\u304b\u3057\u305f\u53ef\u8996\u5316","text":"<p>\u6574\u7136\u3068\u3057\u305f\u30c7\u30fc\u30bf\u5f62\u5f0f\u306f\u3001\u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316\u3084\u64cd\u4f5c\u3092\u975e\u5e38\u306b\u52b9\u7387\u3088\u304f\u884c\u3046\u305f\u3081\u306e\u524d\u63d0\u6761\u4ef6\u3068\u306a\u308a\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u4f8b\u3067\u306f\u3001\u6574\u5f62\u524d\u306e\u30c7\u30fc\u30bf <code>drinks_smaller</code>\uff08\u30ef\u30a4\u30c9\u5f62\u5f0f\uff09\u3092\u3001\u6574\u7136\u306a\u30ed\u30f3\u30b0\u5f62\u5f0f\u306b\u5909\u63db\u3057\u3066\u304b\u3089\u3001\u56fd\u3054\u3068\u306b\u5404\u7a2e\u30a2\u30eb\u30b3\u30fc\u30eb\u306e\u6442\u53d6\u91cf\u3092\u30b5\u30a4\u30c9\u30d0\u30a4\u30b5\u30a4\u30c9\u306e\u68d2\u30b0\u30e9\u30d5\u3067\u6bd4\u8f03\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#31-drinks","title":"3.1 \u4f8b\uff1adrinks \u30c7\u30fc\u30bf\u306e\u6574\u5f62\u3068\u68d2\u30b0\u30e9\u30d5\u306e\u4f5c\u6210","text":"<p>\u307e\u305a\u3001\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3068\u3057\u3066\u300cdrinks\u300d\u30c7\u30fc\u30bf\uff08\u30a2\u30eb\u30b3\u30fc\u30eb\u6442\u53d6\u91cf\u306e\u30c7\u30fc\u30bf\uff09\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u30a2\u30e1\u30ea\u30ab\u3001China\u3001Italy\u3001Saudi Arabia \u306e 4 \u30ab\u56fd\u306e\u30c7\u30fc\u30bf\u3068\u3057\u3066\u3001\u30d3\u30fc\u30eb\u3001\u30b9\u30d4\u30ea\u30c3\u30c4\u3001\u30ef\u30a4\u30f3\u306e\u6442\u53d6\u91cf\u3092\u7528\u3044\u307e\u3059\u3002</p> <pre><code>import pandas as pd\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\uff1adrinks_smaller\uff08\u30ef\u30a4\u30c9\u5f62\u5f0f\uff09\ndrinks_smaller = pd.DataFrame({\n    'country': ['USA', 'China', 'Italy', 'Saudi Arabia'],\n    'beer_servings': [245, 89, 212, 15],\n    'spirit_servings': [150, 110, 98, 5],\n    'wine_servings': [50, 20, 300, 0]\n})\n\n# \u5217\u540d\u306e\u5909\u66f4\uff08rename\uff09\ndrinks_smaller = drinks_smaller.rename(columns={\n    'beer_servings': 'beer',\n    'spirit_servings': 'spirit',\n    'wine_servings': 'wine'\n})\nprint(\"\u3010drinks_smaller (\u30ef\u30a4\u30c9\u5f62\u5f0f)\u3011\")\nprint(drinks_smaller)\n</code></pre> <p>\u6b21\u306b\u3001pandas \u306e <code>melt()</code> \u95a2\u6570\u3092\u7528\u3044\u3066\u3001\u30ef\u30a4\u30c9\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3092\u30ed\u30f3\u30b0\u5f62\u5f0f\uff08\u6574\u7136\u30c7\u30fc\u30bf\uff09\u306b\u5909\u63db\u3057\u307e\u3059\u3002</p> <pre><code># \u30ef\u30a4\u30c9\u5f62\u5f0f\u304b\u3089\u30ed\u30f3\u30b0\u5f62\u5f0f\u3078\u5909\u63db\ndrinks_smaller_tidy = pd.melt(drinks_smaller, id_vars=['country'],\n                              var_name='type', value_name='servings')\nprint(\"\\n\u3010drinks_smaller_tidy (\u6574\u7136\u306a\u30ed\u30f3\u30b0\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf)\u3011\")\nprint(drinks_smaller_tidy)\n</code></pre> <p>\u3053\u306e\u6574\u7136\u306a\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001seaborn \u306e <code>barplot()</code> \u3092\u4f7f\u3044\u30b5\u30a4\u30c9\u30d0\u30a4\u30b5\u30a4\u30c9\u306e\u68d2\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(8, 4))\nsns.barplot(data=drinks_smaller_tidy, x='country', y='servings', hue='type', dodge=True)\nplt.xlabel(\"\u56fd\")\nplt.ylabel(\"\u30b5\u30fc\u30d3\u30f3\u30b0\u6570\")\nplt.title(\"4 \u30ab\u56fd\u306b\u304a\u3051\u308b\u30a2\u30eb\u30b3\u30fc\u30eb\u6442\u53d6\u91cf\u306e\u6bd4\u8f03\")\nplt.show()\n</code></pre>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#4-tidy-data","title":"4. \"Tidy\" Data \u306e\u5b9a\u7fa9\u3068\u4f8b","text":"<p>\u6574\u7136\u3068\u3057\u305f\u30c7\u30fc\u30bf\uff08tidy data\uff09\u3068\u306f\u3001\u4ee5\u4e0b\u306e\u30eb\u30fc\u30eb\u306b\u5f93\u3063\u3066\u69cb\u9020\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\u3067\u3059\uff1a</p> <ol> <li>\u5404\u5909\u6570\u306f 1 \u5217\u306b </li> <li>\u5404\u89b3\u6e2c\u306f 1 \u884c\u306b </li> <li>\u5404\u89b3\u6e2c\u5358\u4f4d\u306f 1 \u30c6\u30fc\u30d6\u30eb\u306b</li> </ol> <p>\u4f8b\u3048\u3070\u3001\u4ee5\u4e0b\u306f\u682a\u4fa1\u30c7\u30fc\u30bf\u306e\u30ef\u30a4\u30c9\u5f62\u5f0f\uff08\u975e tidy\uff09\u306e\u4f8b\u3067\u3059\u3002</p> <pre><code>import pandas as pd\n\nstocks = pd.DataFrame({\n    'Date': pd.date_range(\"2009-01-01\", periods=5),\n    'Boeing stock price': [\"$173.55\", \"$172.61\", \"$173.86\", \"$170.77\", \"$174.29\"],\n    'Amazon stock price': [\"$174.90\", \"$171.42\", \"$171.58\", \"$173.89\", \"$170.16\"],\n    'Google stock price': [\"$174.34\", \"$170.04\", \"$173.65\", \"$174.87\", \"$172.19\"]\n})\nprint(\"\u3010\u682a\u4fa1\u30c7\u30fc\u30bf\uff08\u975e tidy\uff09\u3011\")\nprint(stocks.head(2))\n</code></pre> <p>\u3053\u306e\u30c7\u30fc\u30bf\u306f\u3001\u5404\u9298\u67c4\u3054\u3068\u306b\u5217\u304c\u5206\u304b\u308c\u3066\u3044\u308b\u305f\u3081\u975e tidy \u3067\u3059\u3002 \u3053\u308c\u3092\u3001\u5404\u89b3\u6e2c\u5358\u4f4d\uff08Date\uff09\u3054\u3068\u306b\u300cStock Name\u300d\u3068\u300cStock Price\u300d\u3068\u3044\u3046 2 \u5217\u306b\u5909\u63db\u3059\u308b\u3068 tidy \u306b\u306a\u308a\u307e\u3059\u3002</p> <pre><code># \u30ef\u30a4\u30c9\u5f62\u5f0f\u304b\u3089\u30ed\u30f3\u30b0\u5f62\u5f0f\uff08tidy \u5f62\u5f0f\uff09\u3078\u5909\u63db\nstocks_tidy = stocks.melt(id_vars=['Date'], \n                          var_name='Stock Name', \n                          value_name='Stock Price')\nprint(\"\\n\u3010\u682a\u4fa1\u30c7\u30fc\u30bf\uff08tidy \u5f62\u5f0f\uff09\u3011\")\nprint(stocks_tidy.head(6))\n</code></pre> <p>\u306a\u304a\u3001\u540c\u69d8\u306b\u4ed6\u306e\u30c7\u30fc\u30bf\uff08\u4f8b\uff1a\u6c17\u8c61\u60c5\u5831\u3084\u822a\u7a7a\u5b89\u5168\u60c5\u5831\u306a\u3069\uff09\u3082\u3001\u6574\u7136\u306a\u30c7\u30fc\u30bf\u5f62\u5f0f\u306b\u3059\u308b\u3053\u3068\u3067\u3001pandas \u3084 seaborn\u3001matplotlib \u3067\u306e\u89e3\u6790\u30fb\u53ef\u8996\u5316\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#5-case-study-democracy-in-guatemala","title":"5. Case Study: Democracy in Guatemala","text":"<p>\u3053\u3053\u3067\u306f\u3001\u5148\u307b\u3069\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u305f <code>dem_score</code> \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3001Guatemala\uff08\u30b0\u30a2\u30c6\u30de\u30e9\uff09\u306e\u6c11\u4e3b\u4e3b\u7fa9\u30b9\u30b3\u30a2\u306e\u6642\u7cfb\u5217\u5909\u5316\u3092\u30d7\u30ed\u30c3\u30c8\u3059\u308b\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002 \u3053\u306e\u30c7\u30fc\u30bf\u306f\u30011952 \u5e74\u304b\u3089 1992 \u5e74\u307e\u3067\u306e\u5404\u5e74\u306e\u30b9\u30b3\u30a2\u304c\u30ef\u30a4\u30c9\u5f62\u5f0f\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u307e\u305a\u6574\u7136\u306a\uff08\u30ed\u30f3\u30b0\u5f62\u5f0f\uff09\u30c7\u30fc\u30bf\u306b\u5909\u63db\u3057\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#51","title":"5.1 \u30b0\u30a2\u30c6\u30de\u30e9\u306e\u30c7\u30fc\u30bf\u62bd\u51fa","text":"<pre><code># dem_score \u306f\u65e2\u306b\u4e0a\u8a18\u3067\u8aad\u307f\u8fbc\u3093\u3067\u3042\u308b\u3082\u306e\u3068\u3059\u308b\n# Guatemala \u306e\u307f\u62bd\u51fa\nguat_dem = dem_score[dem_score['country'] == \"Guatemala\"]\nprint(\"\u3010Guatemala \u306e\u30c7\u30fc\u30bf (\u30ef\u30a4\u30c9\u5f62\u5f0f)\u3011\")\nprint(guat_dem)\n</code></pre>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#52","title":"5.2 \u30ef\u30a4\u30c9\u5f62\u5f0f\u304b\u3089\u30ed\u30f3\u30b0\u5f62\u5f0f\u3078\u306e\u5909\u63db","text":"<p>\u3053\u3053\u3067\u306f\u3001<code>pd.melt()</code> \u3092\u7528\u3044\u3066\u3001\u5217\u540d\uff08\u5404\u5e74\uff09\u3092 <code>year</code> \u5909\u6570\u3001\u5024\u3092 <code>democracy_score</code> \u5909\u6570\u3068\u3057\u3066\u62bd\u51fa\u3057\u307e\u3059\u3002 \u203b pandas \u3067\u306f\u3001\u5217\u540d\u306f\u6587\u5b57\u5217\u3068\u3057\u3066\u6271\u308f\u308c\u308b\u305f\u3081\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u578b\u5909\u63db\u3057\u307e\u3059\u3002</p> <pre><code># country \u5217\u306f\u305d\u306e\u307e\u307e\u3067\u3001\u6b8b\u308a\u306e\u5217\uff08\u5e74\uff09\u3092 melt \u3059\u308b\nguat_dem_tidy = pd.melt(guat_dem, id_vars=['country'],\n                        var_name='year', value_name='democracy_score')\n# year \u5217\u3092\u6574\u6570\u578b\u306b\u5909\u63db\nguat_dem_tidy['year'] = guat_dem_tidy['year'].astype(int)\nprint(\"\\n\u3010Guatemala \u306e\u30c7\u30fc\u30bf (tidy \u5f62\u5f0f)\u3011\")\nprint(guat_dem_tidy)\n</code></pre>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#53","title":"5.3 \u6642\u7cfb\u5217\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210","text":"<p>seaborn \u307e\u305f\u306f matplotlib \u306e <code>plot</code> \u6a5f\u80fd\u3092\u4f7f\u3063\u3066\u30011952 \u5e74\u304b\u3089 1992 \u5e74\u307e\u3067\u306e\u6c11\u4e3b\u4e3b\u7fa9\u30b9\u30b3\u30a2\u306e\u5909\u5316\u3092\u7dda\u30b0\u30e9\u30d5\u3067\u63cf\u304d\u307e\u3059\u3002</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 4))\nplt.plot(guat_dem_tidy['year'], guat_dem_tidy['democracy_score'], marker='o', linestyle='-')\nplt.xlabel(\"Year\")\nplt.ylabel(\"Democracy Score\")\nplt.title(\"Guatemala \u306b\u304a\u3051\u308b 1952\uff5e1992 \u5e74\u306e\u6c11\u4e3b\u4e3b\u7fa9\u30b9\u30b3\u30a2\u306e\u5909\u5316\")\nplt.grid(True)\nplt.show()\n</code></pre> <p>\u203b \u3082\u3057 <code>year</code> \u5217\u306e\u578b\u5909\u63db\u3092\u884c\u308f\u306a\u304b\u3063\u305f\u5834\u5408\u3001\u30d7\u30ed\u30c3\u30c8\u6642\u306b\u6b63\u3057\u3044\u9806\u5e8f\u3067\u4e26\u3070\u306a\u3044\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#6-python-tidy-data","title":"6. Python \u306b\u304a\u3051\u308b \"Tidy\" Data \u51e6\u7406\u306e\u307e\u3068\u3081","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u5185\u5bb9\u3092\u5b66\u3073\u307e\u3057\u305f\uff1a</p> <ul> <li>\u30c7\u30fc\u30bf\u306e\u30a4\u30f3\u30dd\u30fc\u30c8 </li> <li> <p>pandas \u306e <code>read_csv()</code> \u3084 <code>read_excel()</code> \u3092\u7528\u3044\u305f CSV\uff0fExcel \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f</p> </li> <li> <p>\u6574\u7136\u3068\u3057\u305f\uff08tidy\uff09\u30c7\u30fc\u30bf\u306e\u6982\u5ff5 </p> </li> <li> <p>\u5404\u5909\u6570\u306f 1 \u5217\u3001\u5404\u89b3\u6e2c\u306f 1 \u884c\u3001\u5404\u89b3\u6e2c\u5358\u4f4d\u306f 1 \u30c6\u30fc\u30d6\u30eb\u306b\u307e\u3068\u3081\u308b\u3068\u3044\u3046\u30eb\u30fc\u30eb</p> </li> <li> <p>\u30c7\u30fc\u30bf\u306e\u5909\u5f62\u64cd\u4f5c </p> </li> <li>\u30ef\u30a4\u30c9\u5f62\u5f0f\u304b\u3089\u30ed\u30f3\u30b0\u5f62\u5f0f\u3078\u306e\u5909\u63db\uff1a<code>pd.melt()</code> \u3092\u4f7f\u7528  </li> <li> <p>\u30ed\u30f3\u30b0\u5f62\u5f0f\u304b\u3089\u30ef\u30a4\u30c9\u5f62\u5f0f\u3078\u306e\u5909\u63db\uff1a<code>pivot()</code> / <code>pivot_table()</code> \u3092\u4f7f\u7528</p> </li> <li> <p>\u6574\u7136\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u53ef\u8996\u5316 </p> </li> <li>seaborn \u3084 matplotlib \u3092\u7528\u3044\u305f\u68d2\u30b0\u30e9\u30d5\u3084\u7dda\u30b0\u30e9\u30d5\u306e\u4f5c\u6210</li> </ul> <p>\u6574\u7136\u306a\u30c7\u30fc\u30bf\u5f62\u5f0f\u306b\u3088\u308a\u3001Python \u306e\u30c7\u30fc\u30bf\u64cd\u4f5c\u30e9\u30a4\u30d6\u30e9\u30ea\u9593\u3067\u4e00\u8cab\u3057\u305f\u5165\u529b\u30fb\u51fa\u529b\u304c\u53ef\u80fd\u3068\u306a\u308a\u3001\u89e3\u6790\u30fb\u53ef\u8996\u5316\u306e\u30d7\u30ed\u30bb\u30b9\u304c\u5927\u5e45\u306b\u7c21\u7565\u5316\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#7","title":"7. \u8ffd\u52a0\u30ea\u30bd\u30fc\u30b9\u3068\u4eca\u5f8c\u306e\u5c55\u958b","text":"<p>Python \u3067\u306f\u3001pandas\u3001matplotlib\u3001seaborn \u306b\u52a0\u3048\u3066\u3001numpy \u306a\u3069\u3082\u30c7\u30fc\u30bf\u89e3\u6790\u3067\u983b\u7e41\u306b\u5229\u7528\u3055\u308c\u307e\u3059\u3002 R \u306e tidyverse \u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u3088\u3046\u306b\u3001Python \u306b\u306f\u4e00\u9023\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u304c\u9023\u643a\u3057\u3066\u52d5\u4f5c\u3059\u308b\u305f\u3081\u3001\u30c7\u30fc\u30bf\u306e\u6574\u5f62\u3001\u89e3\u6790\u3001\u53ef\u8996\u5316\u306e\u5404\u5de5\u7a0b\u304c\u30b9\u30e0\u30fc\u30ba\u306b\u9023\u643a\u3057\u307e\u3059\u3002</p> <p>\u307e\u305f\u3001\u4ee5\u4e0b\u306e\u30ea\u30bd\u30fc\u30b9\u3082\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\uff1a</p> <ul> <li>pandas \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8</li> <li>seaborn \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8</li> <li>matplotlib \u30c1\u30fc\u30c8\u30b7\u30fc\u30c8</li> <li>Python Data Science Handbook</li> </ul>"},{"location":"lectures/SIWS/04-data-import-and-tidy-data/#_5","title":"\u4eca\u5f8c\u306e\u5c55\u958b","text":"<p>\u304a\u3081\u3067\u3068\u3046\u3054\u3056\u3044\u307e\u3059\uff01 \u3053\u308c\u3067\u300cPython \u306b\u3088\u308b\u30c7\u30fc\u30bf\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\u3068\u6574\u7136\u3068\u3057\u305f\u30c7\u30fc\u30bf\u300d\u306e\u5185\u5bb9\u3092\u5b66\u7fd2\u3057\u307e\u3057\u305f\u3002 \u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u3068\u3057\u3066\u3001\u3053\u308c\u307e\u3067\u306e\u524d\u51e6\u7406\u30fb\u53ef\u8996\u5316\u306e\u30b9\u30ad\u30eb\u3092\u6d3b\u304b\u3057\u3066\u3001\u56de\u5e30\u5206\u6790\u3084\u7d71\u8a08\u7684\u63a8\u8ad6\u306a\u3069\u3001\u3055\u3089\u306b\u9ad8\u5ea6\u306a\u30c7\u30fc\u30bf\u30e2\u30c7\u30ea\u30f3\u30b0\u306b\u9032\u3093\u3067\u3044\u304d\u307e\u3059\u3002</p> <p>```</p>"},{"location":"lectures/SP/","title":"Index","text":""},{"location":"lectures/SP/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> \\[ f(x) = \\int_{-\\infty}^\\infty     \\hat f(\\xi)\\,e^{2 \\pi i \\xi x}     \\,d\\xi \\] <p>inline tex codes \\(E=mc^2\\).</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]"},{"location":"research/","title":"Index","text":""},{"location":"research/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> \\[ f(x) = \\int_{-\\infty}^\\infty     \\hat f(\\xi)\\,e^{2 \\pi i \\xi x}     \\,d\\xi \\] <p>inline tex codes \\(E=mc^2\\).</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]"},{"location":"research/note/causal-data-repository/","title":"What is this project","text":"<p>\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u306f\u3001\u7d71\u8a08\u7684\u56e0\u679c\u63a8\u8ad6\u3067\u7528\u3044\u3089\u308c\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u3064\u3044\u3066\u3001\u305d\u306e\u5185\u5bb9\u3001\u524d\u51e6\u7406\u3001\u8ad6\u6587\u3067\u306e\u4f7f\u7528\u306e\u3055\u308c\u65b9\u306b\u3064\u3044\u3066\u307e\u3068\u3081\u3066\u3044\u307e\u3059\u3002\u73fe\u72b6\u3001raw \u30c7\u30fc\u30bf\u3092\u63d0\u4f9b\u3057\u3066\u3044\u308b\u3051\u3069\u3001\u305d\u306e\u5f8c\u306e\u8ad6\u6587\u306b\u57fa\u3065\u3044\u305f\u52a0\u5de5\u65b9\u6cd5\u304c\u4e00\u7dd2\u306b\u8f09\u3063\u3066\u3044\u306a\u3044\u30b5\u30a4\u30c8\u306a\u3069\u304c\u591a\u304f\u3001\u7d50\u5c40\u4f7f\u7528\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u3044\u306a\u3069\u306e\u554f\u984c\u304c\u591a\u304f\u898b\u53d7\u3051\u3089\u308c\u307e\u3059\u3002\u3053\u306e\u30da\u30fc\u30b8\u306f\u3001\u305d\u306e\u554f\u984c\u306b\u5bfe\u51e6\u3057\u3001\u3055\u307e\u3056\u307e\u306a\u4eba\u304cBenchmark\u30c7\u30fc\u30bf\u3092\u5229\u7528\u53ef\u80fd\u306a\u3088\u3046\u306b\u6574\u5099\u3059\u308b\u3053\u3068\u30921\u3064\u306e\u76ee\u7684\u3068\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/","title":"\u7b2c1\u7ae0 \u306f\u3058\u3081\u306b","text":""},{"location":"research/note/causal-mediation-analysis/#11","title":"1.1 \u7814\u7a76\u80cc\u666f","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\uff08Causal Mediation Analysis\uff09\u306f\u3001\u3042\u308b\u4ecb\u5165\u3084\u66dd\u9732\u304c\u7d50\u679c\u306b\u53ca\u307c\u3059\u7dcf\u52b9\u679c\u3092\u3001\u4ecb\u5728\u5909\u6570\u3092\u901a\u3058\u305f\u9593\u63a5\u52b9\u679c\u3068\u76f4\u63a5\u306e\u76f4\u63a5\u52b9\u679c\u306b\u5206\u89e3\u3057\u3001\u305d\u306e\u56e0\u679c\u30e1\u30ab\u30cb\u30ba\u30e0\u3092\u89e3\u660e\u3059\u308b\u305f\u3081\u306e\u624b\u6cd5\u3067\u3059\u3002 \u521d\u671f\u306e\u5a92\u4ecb\u5206\u6790\u306f\u3001Baron &amp; Kenny (1986) \u306b\u4ee3\u8868\u3055\u308c\u308b\u56de\u5e30\u5206\u6790\u306b\u57fa\u3065\u304f\u4f1d\u7d71\u7684\u624b\u6cd5\u3068\u3057\u3066\u767a\u5c55\u3057\u307e\u3057\u305f\u304c\u3001\u3053\u306e\u624b\u6cd5\u306f\u56e0\u679c\u63a8\u8ad6\u306e\u53b3\u5bc6\u306a\u67a0\u7d44\u307f\u3092\u6b20\u3044\u3066\u3044\u307e\u3057\u305f\u3002</p> <p>1990\u5e74\u4ee3\u521d\u982d\u306b\u306f\u3001\u53cd\u4e8b\u5b9f\u7684\uff08\u30dd\u30c6\u30f3\u30b7\u30e3\u30eb\u30a2\u30a6\u30c8\u30ab\u30e0\uff09\u67a0\u7d44\u307f\u304c\u5c0e\u5165\u3055\u308c\u3001Robins\u3084Greenland\u3089\u306b\u3088\u3063\u3066\u56e0\u679c\u52b9\u679c\u306e\u5b9a\u5f0f\u5316\u304c\u9032\u3081\u3089\u308c\u307e\u3057\u305f\u3002\u7279\u306b\u3001Judea Pearl (2001) \u306b\u3088\u308b\u7814\u7a76\u3067\u306f\u3001Directed Acyclic Graphs (DAGs) \u3092\u7528\u3044\u3066\u56e0\u679c\u69cb\u9020\u3092\u660e\u793a\u5316\u3057\u3001\u81ea\u7136\u76f4\u63a5\u52b9\u679c (\\(NDE\\)) \u3068\u81ea\u7136\u9593\u63a5\u52b9\u679c (\\(NIE\\)) \u3068\u3044\u3063\u305f\u6982\u5ff5\u3092\u5c0e\u5165\u3059\u308b\u3053\u3068\u3067\u3001\u7dcf\u52b9\u679c\u306e\u5206\u89e3\u304c\u7406\u8ad6\u7684\u306b\u78ba\u7acb\u3055\u308c\u307e\u3057\u305f\u3002</p> <p>\u8fd1\u5e74\u3067\u306f\u3001Imai, Keele, Tingley \u3089\u306b\u3088\u308b\u7d71\u8a08\u7684\u63a8\u5b9a\u624b\u6cd5\u306e\u767a\u5c55\uff08\u4f8b: Imai et al., 2010\uff09\u304c\u3042\u308a\u3001\u5b9f\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u304f\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u9069\u7528\u304c\u98db\u8e8d\u7684\u306b\u9032\u307f\u307e\u3057\u305f\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3084\u611f\u5ea6\u5206\u6790\u306a\u3069\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u3001\u3088\u308a\u5b9f\u8df5\u7684\u304b\u3064\u9811\u5065\u306a\u63a8\u5b9a\u65b9\u6cd5\u304c\u958b\u767a\u3055\u308c\u3001\u4ecb\u5728\u5909\u6570\u3068\u4ea4\u7d61\u56e0\u5b50\u306e\u8907\u96d1\u306a\u95a2\u4fc2\u6027\u3092\u89e3\u660e\u3059\u308b\u305f\u3081\u306e\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p> <p>\u3055\u3089\u306b\u3001\u3053\u308c\u3089\u306e\u7406\u8ad6\u7684\u30fb\u65b9\u6cd5\u8ad6\u7684\u9032\u5c55\u306b\u3088\u308a\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306f\u533b\u5b66\u3001\u75ab\u5b66\u3001\u5fc3\u7406\u5b66\u3001\u7d4c\u6e08\u5b66\u3001\u793e\u4f1a\u79d1\u5b66\u306a\u3069\u3001\u5e45\u5e83\u3044\u5206\u91ce\u306b\u304a\u3044\u3066\u3001\u4ecb\u5165\u306e\u30e1\u30ab\u30cb\u30ba\u30e0\u89e3\u660e\u3084\u653f\u7b56\u8a55\u4fa1\u3001\u81e8\u5e8a\u8a66\u9a13\u306e\u89e3\u6790\u306b\u5fdc\u7528\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#12","title":"1.2 \u7814\u7a76\u76ee\u7684","text":"<p>\u672c\u7a3f\u306e\u76ee\u7684\u306f\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u63d0\u6848\u8ad6\u6587\u306b\u7aef\u3092\u767a\u3059\u308b\u7406\u8ad6\u7684\u80cc\u666f\u304b\u3089\u3001\u6700\u65b0\u306e\u767a\u5c55\u307e\u3067\u306e\u7d4c\u7def\u3092\u5305\u62ec\u7684\u306b\u6574\u7406\u3057\u3001\u4ee5\u4e0b\u306e\u70b9\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u3053\u3068\u306b\u3042\u308a\u307e\u3059\u3002</p> <ul> <li> <p>\u767a\u5c55\u7d4c\u7def\u306e\u6574\u7406\uff1a   \u521d\u671f\u306e\u4f1d\u7d71\u7684\u306a\u5a92\u4ecb\u5206\u6790\u624b\u6cd5\u304b\u3089\u3001\u53cd\u4e8b\u5b9f\u7684\u67a0\u7d44\u307f\u304a\u3088\u3073DAGs\u3092\u7528\u3044\u305f\u56e0\u679c\u63a8\u8ad6\u3001\u3055\u3089\u306b\u6700\u65b0\u306e\u63a8\u5b9a\u624b\u6cd5\u306b\u81f3\u308b\u307e\u3067\u306e\u7406\u8ad6\u7684\u767a\u5c55\u3068\u3001\u305d\u306e\u80cc\u666f\u306b\u3042\u308b\u57fa\u672c\u6982\u5ff5\u3092\u660e\u793a\u3059\u308b\u3002</p> </li> <li> <p>\u5fdc\u7528\u5206\u91ce\u306e\u691c\u8a3c\uff1a   \u533b\u5b66\u3001\u75ab\u5b66\u3001\u5fc3\u7406\u5b66\u3001\u7d4c\u6e08\u5b66\u306a\u3069\u5404\u5206\u91ce\u306b\u304a\u3051\u308b\u5177\u4f53\u7684\u306a\u9069\u7528\u4f8b\u3092\u901a\u3058\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u304c\u3069\u306e\u3088\u3046\u306b\u5b9f\u8a3c\u7814\u7a76\u3084\u653f\u7b56\u8a55\u4fa1\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u691c\u8a3c\u3059\u308b\u3002</p> </li> <li> <p>\u4eca\u5f8c\u306e\u7814\u7a76\u8ab2\u984c\u306e\u63d0\u793a\uff1a   \u73fe\u5728\u306e\u65b9\u6cd5\u8ad6\u306e\u4eee\u5b9a\u3084\u9650\u754c\u3001\u7279\u306b\u4ecb\u5728\u5909\u6570\u3068\u4ea4\u7d61\u56e0\u5b50\u306e\u53d6\u308a\u6271\u3044\u306b\u95a2\u3059\u308b\u8ab2\u984c\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3057\u3001\u4eca\u5f8c\u306e\u7814\u7a76\u65b9\u5411\u6027\u3084\u624b\u6cd5\u306e\u6539\u826f\u306b\u5411\u3051\u305f\u793a\u5506\u3092\u63d0\u4f9b\u3059\u308b\u3002</p> </li> </ul> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u672c\u7a3f\u3067\u306f\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u57fa\u790e\u7406\u8ad6\u3068\u6700\u65b0\u306e\u65b9\u6cd5\u8ad6\u3092\u6574\u7406\u30fb\u89e3\u8aac\u3059\u308b\u3053\u3068\u3067\u3001\u7814\u7a76\u8005\u304c\u8907\u96d1\u306a\u56e0\u679c\u6a5f\u5e8f\u3092\u3088\u308a\u660e\u78ba\u306b\u6349\u3048\u3001\u5b9f\u8df5\u7684\u306a\u5fdc\u7528\u306b\u7e4b\u3052\u308b\u305f\u3081\u306e\u7406\u8ad6\u7684\u30fb\u5b9f\u8df5\u7684\u67a0\u7d44\u307f\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#2","title":"\u7b2c2\u7ae0 \u56e0\u679c\u63a8\u8ad6\u306e\u57fa\u790e","text":""},{"location":"research/note/causal-mediation-analysis/#21","title":"2.1 \u56e0\u679c\u52b9\u679c\u306e\u5b9a\u7fa9","text":"<p>\u56e0\u679c\u63a8\u8ad6\u306e\u4e2d\u5fc3\u7684\u306a\u76ee\u7684\u306f\u3001\u51e6\u7f6e\uff08\u307e\u305f\u306f\u4ecb\u5165\uff09\u3068\u7d50\u679c\u3068\u306e\u9593\u306b\u5b58\u5728\u3059\u308b\u56e0\u679c\u95a2\u4fc2\u3092\u5b9a\u91cf\u5316\u3059\u308b\u3053\u3068\u306b\u3042\u308a\u307e\u3059\u3002 \u4f1d\u7d71\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3068\u3057\u3066\u3001Rubin\u306e\u6f5c\u5728\u7d50\u679c\uff08Potential Outcome\uff09\u67a0\u7d44\u307f\u304c\u63a1\u7528\u3055\u308c\u3001\u5404\u500b\u4f53\\(i\\)\u306b\u5bfe\u3057\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002</p> <ul> <li>\\(Y_i(1)\\)\uff1a\u500b\u4f53\\(i\\)\u304c\u51e6\u7f6e\u3092\u53d7\u3051\u305f\u5834\u5408\u306e\u6f5c\u5728\u7d50\u679c  </li> <li>\\(Y_i(0)\\)\uff1a\u500b\u4f53\\(i\\)\u304c\u51e6\u7f6e\u3092\u53d7\u3051\u306a\u304b\u3063\u305f\u5834\u5408\u306e\u6f5c\u5728\u7d50\u679c</li> </ul> <p>\u500b\u3005\u306e\u56e0\u679c\u52b9\u679c\u306f\u3001 $$ \\tau_i = Y_i(1) - Y_i(0) $$ \u3068\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u304c\u3001\u5b9f\u969b\u306b\u306f\u3069\u3061\u3089\u304b\u4e00\u65b9\u306e\u7d50\u679c\u3057\u304b\u89b3\u5bdf\u3067\u304d\u306a\u3044\u305f\u3081\u3001\u57fa\u672c\u7684\u306a\u8b58\u5225\u554f\u984c\uff08Fundamental Problem of Causal Inference\uff09\u304c\u751f\u3058\u307e\u3059\u3002  </p> <p>\u305d\u306e\u305f\u3081\u3001\u96c6\u56e3\u5168\u4f53\u3067\u306e\u56e0\u679c\u52b9\u679c\u3092\u8a55\u4fa1\u3059\u308b\u969b\u306b\u306f\u3001\u5e73\u5747\u56e0\u679c\u52b9\u679c\uff08Average Causal Effect: \\(ACE\\)\uff09\u304c\u7528\u3044\u3089\u308c\u3001\u3053\u308c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\u3002 $$ ACE = E[Y(1)] - E[Y(0)] $$ \u3053\u306e\u5b9a\u7fa9\u306b\u57fa\u3065\u304d\u3001\u7121\u4f5c\u70ba\u5316\u5b9f\u9a13\u3084\u9069\u5207\u306a\u5171\u5909\u91cf\u8abf\u6574\u3092\u901a\u3058\u3066\u3001\u56e0\u679c\u52b9\u679c\u306e\u63a8\u5b9a\u304c\u8a66\u307f\u3089\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#22-counterfactual-framework","title":"2.2 \u53cd\u4e8b\u5b9f\u7684\u67a0\u7d44\u307f\uff08Counterfactual Framework\uff09\u306e\u6982\u8981","text":"<p>\u53cd\u4e8b\u5b9f\u7684\u67a0\u7d44\u307f\u306f\u3001\u5404\u500b\u4f53\u306b\u5bfe\u3057\u3066\u5b9f\u969b\u306b\u306f\u89b3\u5bdf\u3055\u308c\u306a\u3044\u53cd\u4e8b\u5b9f\u7684\u306a\u30b7\u30ca\u30ea\u30aa\u3092\u8003\u616e\u3059\u308b\u3053\u3068\u3067\u3001\u56e0\u679c\u52b9\u679c\u3092\u5b9a\u5f0f\u5316\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002 \u3059\u306a\u308f\u3061\u3001\u5404\u500b\u4f53\\(i\\)\u306b\u3064\u3044\u3066\u3001\u5b9f\u969b\u306b\u53d7\u3051\u305f\u51e6\u7f6e\u3068\u306f\u7570\u306a\u308b\u51e6\u7f6e\u3092\u53d7\u3051\u305f\u5834\u5408\u306e\u6f5c\u5728\u7d50\u679c\uff08\u53cd\u4e8b\u5b9f\u7684\u7d50\u679c\uff09\u3092\u8003\u3048\u3001\u305d\u306e\u5dee\u7570\u3092\u56e0\u679c\u52b9\u679c\u3068\u3057\u3066\u6349\u3048\u307e\u3059\u3002</p> <p>\u3053\u306e\u67a0\u7d44\u307f\u306e\u57fa\u76e4\u3068\u306a\u308b\u4e3b\u306a\u4eee\u5b9a\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> <ul> <li> <p>\u4e00\u8cab\u6027\uff08Consistency\uff09   \u500b\u4f53\u304c\u5b9f\u969b\u306b\u53d7\u3051\u305f\u51e6\u7f6e\u306b\u5bfe\u5fdc\u3059\u308b\u6f5c\u5728\u7d50\u679c\u304c\u3001\u89b3\u5bdf\u3055\u308c\u305f\u7d50\u679c\u3068\u4e00\u81f4\u3059\u308b\u3068\u3044\u3046\u4eee\u5b9a\u3067\u3059\u3002   \u4f8b\u3048\u3070\u3001\u500b\u4f53\\(i\\)\u304c\u51e6\u7f6e\\(t\\)\u3092\u53d7\u3051\u305f\u5834\u5408\u3001\u89b3\u5bdf\u3055\u308c\u308b\u7d50\u679c\u306f\\(Y_i = Y_i(t)\\)\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>Stable Unit Treatment Value Assumption (SUTVA)   \u3042\u308b\u500b\u4f53\u3078\u306e\u51e6\u7f6e\u304c\u4ed6\u306e\u500b\u4f53\u306e\u7d50\u679c\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u306a\u3044\uff08\u5e72\u6e09\u304c\u5b58\u5728\u3057\u306a\u3044\uff09\u3068\u3044\u3046\u4eee\u5b9a\u3068\u3001\u51e6\u7f6e\u306e\u5b9a\u7fa9\u304c\u4e00\u610f\u3067\u3042\u308b\u3068\u3044\u3046\u524d\u63d0\u3067\u3059\u3002</p> </li> <li> <p>\u7121\u4ea4\u7d61\u6027\uff08Ignorability\u307e\u305f\u306fExchangeability\uff09   \u89b3\u5bdf\u53ef\u80fd\u306a\u5171\u5909\u91cf\u3092\u6761\u4ef6\u306b\u3001\u51e6\u7f6e\u306e\u5272\u308a\u5f53\u3066\u3068\u53cd\u4e8b\u5b9f\u7684\u7d50\u679c\u304c\u72ec\u7acb\u3067\u3042\u308b\u3068\u3044\u3046\u4eee\u5b9a\u3067\u3059\u3002   \u3053\u306e\u4eee\u5b9a\u306b\u3088\u308a\u3001\u4ea4\u7d61\u56e0\u5b50\u306e\u5f71\u97ff\u3092\u7d71\u8a08\u7684\u306b\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u3059\u308b\u3053\u3068\u3067\u3001\u56e0\u679c\u52b9\u679c\u306e\u63a8\u5b9a\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> </ul> <p>\u3053\u308c\u3089\u306e\u524d\u63d0\u306e\u4e0b\u3001\u53cd\u4e8b\u5b9f\u7684\u67a0\u7d44\u307f\u306f\u56e0\u679c\u52b9\u679c\u306e\u660e\u793a\u7684\u306a\u5b9a\u5f0f\u5316\u3092\u5b9f\u73fe\u3057\u3001\u4ecb\u5728\u52b9\u679c\u306e\u5206\u89e3\u306a\u3069\u3001\u3088\u308a\u8907\u96d1\u306a\u56e0\u679c\u30e1\u30ab\u30cb\u30ba\u30e0\u306e\u89e3\u6790\u306b\u5fdc\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u307e\u305f\u3001DAGs\uff08\u6709\u5411\u975e\u5de1\u56de\u30b0\u30e9\u30d5\uff09\u3068\u306e\u7d44\u307f\u5408\u308f\u305b\u306b\u3088\u308a\u3001\u56e0\u679c\u69cb\u9020\u306e\u53ef\u8996\u5316\u3084\u8b58\u5225\u6226\u7565\u306e\u691c\u8a0e\u306b\u3082\u5927\u304d\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#3","title":"\u7b2c3\u7ae0 \u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u7406\u8ad6\u7684\u67a0\u7d44\u307f","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u3067\u306f\u3001\u51e6\u7f6e\u304c\u7d50\u679c\u306b\u53ca\u307c\u3059\u7dcf\u52b9\u679c\u3092\u3001\u76f4\u63a5\u52b9\u679c\u3068\u9593\u63a5\u52b9\u679c\u306b\u5206\u89e3\u3059\u308b\u3053\u3068\u3067\u3001\u4ecb\u5728\u6a5f\u5e8f\uff08\u30e1\u30ab\u30cb\u30ba\u30e0\uff09\u3092\u89e3\u660e\u3059\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u6f5c\u5728\u7d50\u679c\u67a0\u7d44\u307f\u3092\u7528\u3044\u3066\u3001\u4ecb\u5728\u52b9\u679c\u306e\u5b9a\u7fa9\u3068\u305d\u306e\u5206\u89e3\u65b9\u6cd5\u306b\u3064\u3044\u3066\u7406\u8ad6\u7684\u306b\u6574\u7406\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#31-mediation-effect","title":"3.1 \u4ecb\u5728\u52b9\u679c\uff08Mediation Effect\uff09\u306e\u5b9a\u7fa9","text":""},{"location":"research/note/causal-mediation-analysis/#311","title":"3.1.1 \u6f5c\u5728\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u306e\u5c0e\u5165","text":"<p>\u307e\u305a\u3001\u5404\u500b\u4f53\u306b\u3064\u3044\u3066\u51e6\u7f6e\u3001\u5a92\u4ecb\u5909\u6570\u3001\u7d50\u679c\u306b\u5bfe\u3059\u308b\u6f5c\u5728\u7d50\u679c\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 - \u51e6\u7f6e\u3092 \\(T\\)\u3001\u5a92\u4ecb\u5909\u6570\u3092 \\(M\\)\u3001\u7d50\u679c\u3092 \\(Y\\) \u3068\u3057\u307e\u3059\u3002 - \\(M(t)\\) \u306f\u3001\u51e6\u7f6e \\(T=t\\) \u3092\u53d7\u3051\u305f\u5834\u5408\u306b\u89b3\u5bdf\u3055\u308c\u308b\u5a92\u4ecb\u5909\u6570\u306e\u6f5c\u5728\u5024\u3067\u3059\u3002 - \\(Y(t, m)\\) \u306f\u3001\u51e6\u7f6e\u304c \\(t\\) \u3067\u5a92\u4ecb\u5909\u6570\u304c \\(m\\) \u306b\u56fa\u5b9a\u3055\u308c\u305f\u5834\u5408\u306e\u7d50\u679c\u306e\u6f5c\u5728\u5024\u3067\u3059\u3002</p> <p>\u3053\u306e\u8a2d\u5b9a\u306b\u3088\u308a\u3001\u5b9f\u969b\u306e\u89b3\u5bdf\u5024\u306f\u4ee5\u4e0b\u306e\u4e00\u8cab\u6027\u306e\u4eee\u5b9a\uff08Consistency\uff09\u3092\u6e80\u305f\u3057\u307e\u3059\u3002 $$ \\text{\u5b9f\u969b\u306b } T=t \\text{ \u3092\u53d7\u3051\u305f\u500b\u4f53\u3067\u306f } M = M(t) \\quad \\text{\u304b\u3064} \\quad Y = Y(t, M(t)). $$</p>"},{"location":"research/note/causal-mediation-analysis/#312","title":"3.1.2 \u7dcf\u52b9\u679c\u3068\u305d\u306e\u5206\u89e3","text":"<p>\u51e6\u7f6e \\(T\\) \u306e\u7dcf\u52b9\u679c\uff08Total Effect, \\(TE\\)\uff09\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002 $$ TE = E\\left[Y(1, M(1))\\right] - E\\left[Y(0, M(0))\\right]. $$ \u3053\u3053\u3067\u3001\\(E[\\cdot]\\) \u306f\u671f\u5f85\u5024\u3092\u8868\u3057\u307e\u3059\u3002 \u7dcf\u52b9\u679c\u3092\u3001\u4ecb\u5728\u5909\u6570\u3092\u7d4c\u7531\u3057\u306a\u3044\u76f4\u63a5\u7684\u306a\u52b9\u679c\u3068\u3001\u5a92\u4ecb\u5909\u6570\u3092\u901a\u3058\u305f\u9593\u63a5\u7684\u306a\u52b9\u679c\u306b\u5206\u89e3\u3059\u308b\u3053\u3068\u304c\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u4e2d\u5fc3\u7684\u306a\u76ee\u7684\u3067\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#32","title":"3.2 \u76f4\u63a5\u52b9\u679c\u3068\u9593\u63a5\u52b9\u679c","text":"<p>\u7dcf\u52b9\u679c\u306e\u5206\u89e3\u306b\u306f\u3001\u4ee5\u4e0b\u306e2\u7a2e\u985e\u306e\u52b9\u679c\u304c\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#321-natural-direct-effect-nde","title":"3.2.1 \u81ea\u7136\u76f4\u63a5\u52b9\u679c\uff08Natural Direct Effect: \\(NDE\\)\uff09","text":"<p>\u81ea\u7136\u76f4\u63a5\u52b9\u679c\u306f\u3001\u5a92\u4ecb\u5909\u6570\u3092\u51e6\u7f6e \\(T=0\\) \u306e\u4e0b\u3067\u89b3\u5bdf\u3055\u308c\u308b\u5024\u306b\u56fa\u5b9a\u3057\u305f\u5834\u5408\u306b\u3001\u51e6\u7f6e\u306e\u5909\u5316\u304c\u7d50\u679c\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u8868\u3057\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001 $$ NDE = E\\left[Y(1, M(0))\\right] - E\\left[Y(0, M(0))\\right]. $$ \u3053\u306e\u5b9a\u7fa9\u306f\u3001\u5a92\u4ecb\u5909\u6570\u306e\u5024\u3092\u51e6\u7f6e\u306b\u4f9d\u5b58\u3057\u306a\u3044\u5f62\u3067\u56fa\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u51e6\u7f6e\u306e\u76f4\u63a5\u7684\u306a\u5f71\u97ff\u3092\u62bd\u51fa\u3059\u308b\u3053\u3068\u3092\u8a66\u307f\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#322-natural-indirect-effect-nie","title":"3.2.2 \u81ea\u7136\u9593\u63a5\u52b9\u679c\uff08Natural Indirect Effect: \\(NIE\\)\uff09","text":"<p>\u81ea\u7136\u9593\u63a5\u52b9\u679c\u306f\u3001\u51e6\u7f6e\u3092\u56fa\u5b9a\u3057\u305f\u72b6\u614b\u3067\u3001\u5a92\u4ecb\u5909\u6570\u304c\u51e6\u7f6e\u306e\u5909\u5316\u306b\u3088\u308a\u3069\u306e\u3088\u3046\u306b\u5909\u52d5\u3057\u3001\u305d\u306e\u5909\u52d5\u304c\u7d50\u679c\u306b\u53ca\u307c\u3059\u5f71\u97ff\u3092\u6349\u3048\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u51e6\u7f6e \\(T=1\\) \u3092\u56fa\u5b9a\u3057\u305f\u5834\u5408\u3001\u5a92\u4ecb\u5909\u6570\u304c \\(M(0)\\) \u304b\u3089 \\(M(1)\\) \u306b\u5909\u5316\u3059\u308b\u3053\u3068\u306b\u3088\u308b\u52b9\u679c\u306f\u3001 $$ NIE = E\\left[Y(1, M(1))\\right] - E\\left[Y(1, M(0))\\right]. $$ \u3053\u306e\u3088\u3046\u306b\u3001\u51e6\u7f6e\u81ea\u4f53\u306f\u5909\u3048\u305a\u306b\u5a92\u4ecb\u5909\u6570\u306e\u5909\u5316\u3060\u3051\u304c\u7d50\u679c\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#33","title":"3.3 \u7dcf\u52b9\u679c\u306e\u5206\u89e3","text":"<p>\u4e0a\u8a18\u306e\u81ea\u7136\u76f4\u63a5\u52b9\u679c\u3068\u81ea\u7136\u9593\u63a5\u52b9\u679c\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u7dcf\u52b9\u679c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5206\u89e3\u3055\u308c\u307e\u3059\u3002 $$ TE = E\\left[Y(1, M(1))\\right] - E\\left[Y(0, M(0))\\right] = NDE + NIE. $$</p> <p>\u3053\u306e\u5206\u89e3\u306f\u3001\u4ee5\u4e0b\u306e\u70b9\u306b\u6ce8\u610f\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> <ul> <li> <p>\u4ea4\u4e92\u4f5c\u7528\u306e\u5b58\u5728\uff1a   \u51e6\u7f6e\u3068\u5a92\u4ecb\u5909\u6570\u306e\u9593\u306b\u76f8\u4e92\u4f5c\u7528\uff08Interaction\uff09\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u76f4\u63a5\u52b9\u679c\u3068\u9593\u63a5\u52b9\u679c\u306e\u5b9a\u7fa9\u3084\u89e3\u91c8\u304c\u8907\u96d1\u5316\u3057\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u52b9\u679c\u306e\u540c\u6642\u63a8\u5b9a\u3084\u76f8\u4e92\u4f5c\u7528\u9805\u3092\u542b\u3080\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\u304c\u5fc5\u8981\u3068\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u8b58\u5225\u6761\u4ef6\uff1a   \u4e0a\u8a18\u306e\u52b9\u679c\u3092\u6b63\u78ba\u306b\u8b58\u5225\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u51e6\u7f6e\u3068\u5a92\u4ecb\u5909\u6570\u3001\u3055\u3089\u306b\u306f\u5a92\u4ecb\u5909\u6570\u3068\u7d50\u679c\u9593\u306e\u4ea4\u7d61\u56e0\u5b50\u304c\u9069\u5207\u306b\u5236\u5fa1\u3055\u308c\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u7121\u4ea4\u7d61\u6027\uff08Ignorability\uff09\u3084SUTVA\u306a\u3069\u3001\u524d\u7ae0\u3067\u8ff0\u3079\u305f\u57fa\u672c\u7684\u4eee\u5b9a\u306e\u53b3\u5bc6\u306a\u691c\u8a3c\u304c\u4e0d\u53ef\u6b20\u3067\u3059\u3002</p> </li> <li> <p>\u5b9f\u8a3c\u7684\u9069\u7528\u306e\u9650\u754c\uff1a   \u73fe\u5b9f\u306e\u30c7\u30fc\u30bf\u306b\u304a\u3044\u3066\u306f\u3001\u5a92\u4ecb\u5909\u6570\u306b\u5bfe\u3059\u308b\u4ecb\u5165\u304c\u4e0d\u53ef\u80fd\u3067\u3042\u3063\u305f\u308a\u3001\u4ea4\u7d61\u56e0\u5b50\u306e\u5168\u3066\u3092\u89b3\u5bdf\u3067\u304d\u306a\u304b\u3063\u305f\u308a\u3059\u308b\u305f\u3081\u3001\u4e0a\u8a18\u306e\u5206\u89e3\u304c\u7406\u8ad6\u4e0a\u306e\u3082\u306e\u306b\u7559\u307e\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u611f\u5ea6\u5206\u6790\u306a\u3069\u3092\u901a\u3058\u305f\u4eee\u5b9a\u306e\u691c\u8a3c\u304c\u5b9f\u8a3c\u7814\u7a76\u306b\u304a\u3044\u3066\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002</p> </li> </ul> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u7b2c3\u7ae0\u3067\u306f\u3001\u51e6\u7f6e\u304c\u7d50\u679c\u306b\u4e0e\u3048\u308b\u7dcf\u52b9\u679c\u3092\u3001\u5a92\u4ecb\u5909\u6570\u3092\u7d4c\u7531\u3059\u308b\u52b9\u679c\uff08\u9593\u63a5\u52b9\u679c\uff09\u3068\u305d\u3046\u3067\u306a\u3044\u52b9\u679c\uff08\u76f4\u63a5\u52b9\u679c\uff09\u306b\u5206\u89e3\u3059\u308b\u305f\u3081\u306e\u7406\u8ad6\u7684\u67a0\u7d44\u307f\u3092\u69cb\u7bc9\u3057\u307e\u3057\u305f\u3002\u6b21\u7ae0\u4ee5\u964d\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u52b9\u679c\u3092\u5b9f\u969b\u306b\u63a8\u5b9a\u3059\u308b\u305f\u3081\u306e\u5177\u4f53\u7684\u306a\u65b9\u6cd5\u8ad6\u3084\u3001\u8b58\u5225\u306e\u305f\u3081\u306e\u8ffd\u52a0\u7684\u306a\u4eee\u5b9a\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3092\u6df1\u3081\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#4","title":"\u7b2c4\u7ae0 \u8b58\u5225\u6761\u4ef6\u3068\u4eee\u5b9a","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u81ea\u7136\u76f4\u63a5\u52b9\u679c\u3068\u81ea\u7136\u9593\u63a5\u52b9\u679c\u3092\u6b63\u78ba\u306b\u63a8\u5b9a\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u3044\u304f\u3064\u304b\u306e\u8b58\u5225\u6761\u4ef6\u3068\u4eee\u5b9a\u304c\u4e0d\u53ef\u6b20\u3067\u3059\u3002\u672c\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u57fa\u672c\u7684\u306a\u8b58\u5225\u4eee\u5b9a\u3001\u4ecb\u5728\u5909\u6570\u3068\u4ea4\u7d61\u56e0\u5b50\u306e\u53d6\u308a\u6271\u3044\u3001\u305d\u3057\u3066\u4eee\u5b9a\u306e\u691c\u8a3c\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8a73\u8ff0\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#41","title":"4.1 \u8b58\u5225\u306e\u305f\u3081\u306e\u57fa\u672c\u4eee\u5b9a","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u3092\u5b9f\u65bd\u3059\u308b\u4e0a\u3067\u3001\u4ee5\u4e0b\u306e\u4eee\u5b9a\u304c\u7406\u8ad6\u7684\u57fa\u76e4\u3068\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u304c\u6210\u7acb\u3059\u308b\u5834\u5408\u306b\u9650\u308a\u3001\u4ecb\u5728\u52b9\u679c\u306e\u8b58\u5225\u3068\u63a8\u5b9a\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002</p> <ul> <li> <p>\u4e00\u8cab\u6027\uff08Consistency\uff09   \u5404\u500b\u4f53\u306b\u5bfe\u3057\u3066\u3001\u5b9f\u969b\u306b\u53d7\u3051\u305f\u51e6\u7f6e\u3084\u5a92\u4ecb\u5909\u6570\u306e\u5024\u306b\u5bfe\u5fdc\u3059\u308b\u6f5c\u5728\u7d50\u679c\u304c\u89b3\u5bdf\u3055\u308c\u308b\u3068\u3044\u3046\u4eee\u5b9a\u3067\u3059\u3002   \u4f8b\u3048\u3070\u3001\u500b\u4f53\u304c\u51e6\u7f6e \\(T=t\\) \u3092\u53d7\u3051\u305f\u5834\u5408\u3001\u89b3\u5bdf\u3055\u308c\u308b\u7d50\u679c\u306f \\(Y=Y(t, M(t))\\) \u3068\u4e00\u81f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>Stable Unit Treatment Value Assumption (SUTVA)   \u5404\u500b\u4f53\u3078\u306e\u4ecb\u5165\u304c\u72ec\u7acb\u306b\u4f5c\u7528\u3057\u3001\u4ed6\u306e\u500b\u4f53\u306e\u4ecb\u5165\u304c\u305d\u306e\u7d50\u679c\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u306a\u3044\u3068\u3044\u3046\u4eee\u5b9a\u3067\u3059\u3002\u307e\u305f\u3001\u51e6\u7f6e\u3084\u5a92\u4ecb\u5909\u6570\u306e\u5b9a\u7fa9\u304c\u4e00\u610f\u3067\u3042\u308b\u3053\u3068\u3082\u542b\u307e\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u7121\u4ea4\u7d61\u6027\uff08Sequential Ignorability\uff09   \u8b58\u5225\u306e\u305f\u3081\u306b\u6700\u3082\u91cd\u8981\u306a\u4eee\u5b9a\u306e\u4e00\u3064\u3067\u3059\u3002\u4e8c\u6bb5\u968e\u306e\u7121\u4ea4\u7d61\u6027\u304c\u8981\u6c42\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u51e6\u7f6e\u3068\u5a92\u4ecb\u5909\u6570\u30fb\u7d50\u679c\u306e\u7121\u4ea4\u7d61\u6027      \u89b3\u5bdf\u53ef\u80fd\u306a\u5171\u5909\u91cf \\(X\\) \u3092\u6761\u4ef6\u306b\u3001\u51e6\u7f6e \\(T\\) \u304c\u5a92\u4ecb\u5909\u6570 \\(M\\) \u304a\u3088\u3073\u7d50\u679c \\(Y\\) \u306e\u6f5c\u5728\u5024\u306b\u5bfe\u3057\u3066\u72ec\u7acb\u3067\u3042\u308b\u3053\u3068\u3001\u3059\u306a\u308f\u3061      $$      {Y(t, m), M(t)} \\perp T \\mid X      $$      \u304c\u6210\u7acb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u5a92\u4ecb\u5909\u6570\u3068\u7d50\u679c\u306e\u7121\u4ea4\u7d61\u6027      \u51e6\u7f6e \\(T=t\\) \u3068\u5171\u5909\u91cf \\(X\\) \u3092\u6761\u4ef6\u306b\u3001\u5a92\u4ecb\u5909\u6570 \\(M\\) \u3068\u7d50\u679c\u306e\u6f5c\u5728\u5024 \\(Y(t, m)\\) \u304c\u72ec\u7acb\u3067\u3042\u308b\u3001\u3059\u306a\u308f\u3061      $$      Y(t, m) \\perp M \\mid T=t, X      $$      \u304c\u6210\u7acb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002      \u3053\u306e\u4eee\u5b9a\u306b\u3088\u308a\u3001\u51e6\u7f6e\u3068\u5a92\u4ecb\u5909\u6570\u3001\u3055\u3089\u306b\u5a92\u4ecb\u5909\u6570\u3068\u7d50\u679c\u306e\u9593\u306b\u5b58\u5728\u3059\u308b\u4ea4\u7d61\u56e0\u5b50\u306e\u5f71\u97ff\u304c\u7d71\u5236\u3055\u308c\u3001\u4ecb\u5728\u52b9\u679c\u306e\u8b58\u5225\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/causal-mediation-analysis/#42","title":"4.2 \u4ecb\u5728\u5909\u6570\u3068\u4ea4\u7d61\u56e0\u5b50\u306e\u53d6\u308a\u6271\u3044","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u5a92\u4ecb\u5909\u6570\u306f\u56e0\u679c\u7d4c\u8def\u306e\u4e2d\u6838\u3092\u306a\u3059\u3068\u540c\u6642\u306b\u3001\u7d50\u679c\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u4ea4\u7d61\u56e0\u5b50\u3068\u3057\u3066\u3082\u50cd\u304f\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u70b9\u306b\u7559\u610f\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> <ul> <li> <p>\u5a92\u4ecb\u5909\u6570\u3068\u7d50\u679c\u9593\u306e\u4ea4\u7d61\u56e0\u5b50   \u5a92\u4ecb\u5909\u6570 \\(M\\) \u3068\u7d50\u679c \\(Y\\) \u306e\u9593\u306b\u306f\u3001\u5171\u901a\u306e\u4ea4\u7d61\u56e0\u5b50\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u56e0\u5b50\u304c\u9069\u5207\u306b\u8abf\u6574\u3055\u308c\u306a\u3044\u3068\u3001\u4ecb\u5728\u52b9\u679c\u306e\u63a8\u5b9a\u304c\u504f\u308b\u6050\u308c\u304c\u3042\u308a\u307e\u3059\u3002\u89b3\u5bdf\u7814\u7a76\u3067\u306f\u3001\u5171\u5909\u91cf \\(X\\) \u306b\u3088\u308b\u8abf\u6574\u3084\u3001\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u611f\u5ea6\u5206\u6790\u3092\u901a\u3058\u3066\u3053\u308c\u3089\u306e\u5f71\u97ff\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u51e6\u7f6e\u3068\u5a92\u4ecb\u5909\u6570\u9593\u306e\u4ea4\u7d61   \u51e6\u7f6e \\(T\\) \u3068\u5a92\u4ecb\u5909\u6570 \\(M\\) \u306e\u9593\u306b\u3082\u4ea4\u7d61\u56e0\u5b50\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u3001\u3053\u308c\u3089\u306e\u56e0\u5b50\u306e\u8abf\u6574\u304c\u91cd\u8981\u3068\u306a\u308a\u307e\u3059\u3002\u7121\u4f5c\u70ba\u5316\u5b9f\u9a13\u3067\u306f\u3053\u306e\u554f\u984c\u306f\u7de9\u548c\u3055\u308c\u307e\u3059\u304c\u3001\u89b3\u5bdf\u30c7\u30fc\u30bf\u3067\u306f\u7d71\u8a08\u7684\u624b\u6cd5\uff08\u4f8b\uff1a\u50be\u5411\u30b9\u30b3\u30a2\u306e\u5229\u7528\u306a\u3069\uff09\u306b\u3088\u3063\u3066\u88dc\u6b63\u304c\u8a66\u307f\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u6642\u9593\u7684\u9806\u5e8f\u306e\u660e\u78ba\u5316   \u51e6\u7f6e\u3001\u5a92\u4ecb\u5909\u6570\u3001\u7d50\u679c\u306e\u9593\u306b\u306f\u660e\u78ba\u306a\u6642\u9593\u7684\u9806\u5e8f\u304c\u5fc5\u8981\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u56e0\u679c\u7d4c\u8def\u304c\u660e\u78ba\u306b\u306a\u308a\u3001\u9006\u56e0\u679c\u95a2\u4fc2\u3084\u6f5c\u5728\u7684\u306a\u4ea4\u7d61\u306e\u30ea\u30b9\u30af\u3092\u4f4e\u6e1b\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</p> </li> </ul>"},{"location":"research/note/causal-mediation-analysis/#43","title":"4.3 \u4eee\u5b9a\u306e\u691c\u8a3c\u65b9\u6cd5","text":"<p>\u7406\u8ad6\u4e0a\u6210\u7acb\u3059\u308b\u3053\u308c\u3089\u306e\u4eee\u5b9a\u304c\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u306b\u304a\u3044\u3066\u3082\u59a5\u5f53\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3059\u308b\u3053\u3068\u306f\u975e\u5e38\u306b\u91cd\u8981\u3067\u3059\u3002\u4ee5\u4e0b\u306b\u3001\u4e3b\u8981\u306a\u691c\u8a3c\u624b\u6cd5\u3092\u793a\u3057\u307e\u3059\u3002</p> <ul> <li> <p>\u611f\u5ea6\u5206\u6790   \u7121\u4ea4\u7d61\u6027\u4eee\u5b9a\u304c\u90e8\u5206\u7684\u306b\u7834\u3089\u308c\u305f\u5834\u5408\u306b\u3001\u63a8\u5b9a\u7d50\u679c\u304c\u3069\u306e\u7a0b\u5ea6\u5f71\u97ff\u3092\u53d7\u3051\u308b\u304b\u3092\u8a55\u4fa1\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u611f\u5ea6\u5206\u6790\u306b\u3088\u308a\u3001\u4ea4\u7d61\u56e0\u5b50\u306e\u5f71\u97ff\u3084\u4eee\u5b9a\u9055\u53cd\u306e\u9811\u5065\u6027\u3092\u5b9a\u91cf\u5316\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u30e2\u30c7\u30eb\u8a3a\u65ad\u3068\u9069\u5408\u5ea6\u8a55\u4fa1   \u56de\u5e30\u30e2\u30c7\u30eb\u3084\u69cb\u9020\u65b9\u7a0b\u5f0f\u30e2\u30c7\u30ea\u30f3\u30b0\uff08SEM\uff09\u3092\u7528\u3044\u308b\u5834\u5408\u3001\u6b8b\u5dee\u89e3\u6790\u3084\u9069\u5408\u5ea6\u6307\u6a19\uff08\u4f8b\uff1aAIC\u3001BIC\u3001RMSEA\u306a\u3069\uff09\u3092\u7528\u3044\u3066\u3001\u30e2\u30c7\u30eb\u306e\u4eee\u5b9a\u304c\u30c7\u30fc\u30bf\u306b\u9069\u5408\u3057\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u5b9f\u9a13\u7684\u4ecb\u5165\u307e\u305f\u306f\u88dc\u5b8c\u7684\u7814\u7a76\u30c7\u30b6\u30a4\u30f3   \u53ef\u80fd\u3067\u3042\u308c\u3070\u3001\u5b9f\u9a13\u7684\u4ecb\u5165\u3084\u8a08\u753b\u7814\u7a76\u3092\u4f75\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u7121\u4ea4\u7d61\u6027\u306e\u4eee\u5b9a\u306e\u59a5\u5f53\u6027\u3092\u76f4\u63a5\u691c\u8a3c\u3059\u308b\u65b9\u6cd5\u3082\u3042\u308a\u307e\u3059\u3002\u7279\u306b\u3001\u5a92\u4ecb\u5909\u6570\u306e\u64cd\u4f5c\u304c\u53ef\u80fd\u306a\u72b6\u6cc1\u4e0b\u3067\u306f\u3001\u3088\u308a\u53b3\u5bc6\u306a\u56e0\u679c\u52b9\u679c\u306e\u691c\u8a3c\u304c\u53ef\u80fd\u3067\u3059\u3002</p> </li> <li> <p>\u5916\u90e8\u60c5\u5831\u306e\u6d3b\u7528   \u65e2\u5b58\u306e\u6587\u732e\u3084\u7406\u8ad6\u7684\u77e5\u898b\u3001\u5c02\u9580\u5bb6\u306e\u610f\u898b\u3092\u53d6\u308a\u5165\u308c\u308b\u3053\u3068\u3067\u3001\u4ea4\u7d61\u56e0\u5b50\u306e\u7279\u5b9a\u3084\u30e2\u30c7\u30eb\u4fee\u6b63\u3092\u884c\u3044\u3001\u8b58\u5225\u4eee\u5b9a\u306e\u73fe\u5b9f\u6027\u3084\u9650\u754c\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p> </li> </ul> <p>\u3053\u308c\u3089\u306e\u691c\u8a3c\u65b9\u6cd5\u306f\u3001\u5358\u72ec\u3067\u306f\u306a\u304f\u3001\u8907\u6570\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306b\u304a\u3051\u308b\u8b58\u5225\u4eee\u5b9a\u306e\u4fe1\u983c\u6027\u3092\u9ad8\u3081\u308b\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u307e\u3059\u3002 \u307e\u305f\u3001\u5b9f\u969b\u306e\u89e3\u6790\u306b\u304a\u3044\u3066\u306f\u3001\u3053\u308c\u3089\u306e\u4eee\u5b9a\u304c\u5b8c\u5168\u306b\u6210\u7acb\u3059\u308b\u3053\u3068\u306f\u7a00\u3067\u3042\u308b\u305f\u3081\u3001\u4eee\u5b9a\u9055\u53cd\u306e\u53ef\u80fd\u6027\u3068\u305d\u306e\u5f71\u97ff\u306b\u3064\u3044\u3066\u5341\u5206\u306b\u8b70\u8ad6\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#5","title":"\u7b2c5\u7ae0 \u63a8\u5b9a\u65b9\u6cd5","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306b\u304a\u3044\u3066\u3001\u8b58\u5225\u3055\u308c\u305f\u76f4\u63a5\u52b9\u679c\u3084\u9593\u63a5\u52b9\u679c\u3092\u5b9a\u91cf\u5316\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u9069\u5207\u306a\u63a8\u5b9a\u65b9\u6cd5\u304c\u4e0d\u53ef\u6b20\u3067\u3059\u3002 \u672c\u7ae0\u3067\u306f\u3001\u57fa\u672c\u7684\u306a\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u7528\u3044\u305f\u63a8\u5b9a\u624b\u6cd5\u304b\u3089\u3001\u50be\u5411\u30b9\u30b3\u30a2\u3084\u69cb\u9020\u65b9\u7a0b\u5f0f\u30e2\u30c7\u30ea\u30f3\u30b0\uff08SEM\uff09\u3068\u3044\u3063\u305f\u67d4\u8edf\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3001\u3055\u3089\u306b\u306f\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306b\u3088\u308b\u4fe1\u983c\u533a\u9593\u306e\u69cb\u7bc9\u65b9\u6cd5\u306b\u3064\u3044\u3066\u89e3\u8aac\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#51","title":"5.1 \u56de\u5e30\u30e2\u30c7\u30eb\u3092\u7528\u3044\u305f\u63a8\u5b9a","text":"<p>\u56de\u5e30\u30e2\u30c7\u30eb\u306f\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u63a8\u5b9a\u306b\u304a\u3044\u3066\u6700\u3082\u57fa\u672c\u7684\u304b\u3064\u5e83\u304f\u7528\u3044\u3089\u308c\u308b\u624b\u6cd5\u3067\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u7dda\u5f62\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u4f8b\u306b\u3001\u51e6\u7f6e\u3001\u5a92\u4ecb\u5909\u6570\u3001\u7d50\u679c\u306e\u95a2\u4fc2\u3092\u5b9a\u5f0f\u5316\u3057\u3001\u81ea\u7136\u76f4\u63a5\u52b9\u679c\uff08\\(NDE\\)\uff09\u3068\u81ea\u7136\u9593\u63a5\u52b9\u679c\uff08\\(NIE\\)\uff09\u306e\u63a8\u5b9a\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#511","title":"5.1.1 \u30e2\u30c7\u30eb\u306e\u5b9a\u5f0f\u5316","text":"<p>\u307e\u305a\u3001\u4ee5\u4e0b\u306e2\u6bb5\u968e\u306e\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002</p> <ul> <li> <p>\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\uff1a   $$   M = \\alpha_0 + \\alpha_1 T + \\alpha_2 X + \\epsilon_M,   $$   \u3053\u3053\u3067\u3001\\(T\\) \u306f\u51e6\u7f6e\u3001\\(X\\) \u306f\u4ea4\u7d61\u56e0\u5b50\uff08\u5171\u5909\u91cf\uff09\u3001\\(\\epsilon_M\\) \u306f\u5a92\u4ecb\u5909\u6570\u306e\u8aa4\u5dee\u9805\u3067\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u30e2\u30c7\u30eb\uff1a   $$   Y = \\beta_0 + \\beta_1 T + \\beta_2 M + \\beta_3 X + \\epsilon_Y,   $$   \u3053\u3053\u3067\u3001\\(\\epsilon_Y\\) \u306f\u7d50\u679c\u306e\u8aa4\u5dee\u9805\u3092\u8868\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/causal-mediation-analysis/#512","title":"5.1.2 \u52b9\u679c\u306e\u63a8\u5b9a","text":"<p>\u4e0a\u8a18\u306e\u30e2\u30c7\u30eb\u3092\u7528\u3044\u305f\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u52b9\u679c\u3092\u5206\u89e3\u3057\u3066\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</p> <ul> <li> <p>\u76f4\u63a5\u52b9\u679c\uff08\\(NDE\\)\uff09   \u5a92\u4ecb\u5909\u6570\u306e\u5024\u3092\u56fa\u5b9a\uff08\u901a\u5e38\u306f\u57fa\u6e96\u3068\u306a\u308b\u6c34\u6e96 \\(M(0)\\)\uff09\u3057\u305f\u5834\u5408\u306e\u3001\u51e6\u7f6e\u306e\u5909\u5316\u306b\u3088\u308b\u7d50\u679c\u306e\u5909\u5316\u3068\u3057\u3066\u3001\\(NDE\\) \u306f \\(\\beta_1\\) \u306e\u9805\u306b\u5bfe\u5fdc\u3059\u308b\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u9593\u63a5\u52b9\u679c\uff08\\(NIE\\)\uff09   \u51e6\u7f6e\u306e\u5909\u5316\u304c\u5a92\u4ecb\u5909\u6570\u306b\u53ca\u307c\u3059\u5f71\u97ff \\(\\alpha_1\\) \u3068\u3001\u5a92\u4ecb\u5909\u6570\u304c\u7d50\u679c\u306b\u53ca\u307c\u3059\u5f71\u97ff \\(\\beta_2\\) \u306e\u7a4d \\(\\alpha_1\\beta_2\\) \u304c\u3001\u9593\u63a5\u52b9\u679c\u306e\u63a8\u5b9a\u5024\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> </ul> <p>\u3053\u306e\u624b\u6cd5\u306b\u3088\u308a\u3001\u51e6\u7f6e\u306e\u7dcf\u52b9\u679c\u306f $$ TE = NDE + NIE = \\beta_1 + \\alpha_1\\beta_2, $$ \u3068\u5206\u89e3\u3057\u3066\u8a55\u4fa1\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 \u305f\u3060\u3057\u3001\u5404\u30e2\u30c7\u30eb\u306e\u524d\u63d0\u6761\u4ef6\uff08\u7dda\u5f62\u6027\u3001\u8aa4\u5dee\u9805\u306e\u72ec\u7acb\u6027\u30fb\u6b63\u898f\u6027\u306a\u3069\uff09\u306e\u691c\u8a3c\u3084\u3001\u5171\u5909\u91cf \\(X\\) \u306b\u3088\u308b\u8abf\u6574\u304c\u9069\u5207\u306b\u884c\u308f\u308c\u308b\u3053\u3068\u304c\u524d\u63d0\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#52","title":"5.2 \u50be\u5411\u30b9\u30b3\u30a2\u3084\u69cb\u9020\u65b9\u7a0b\u5f0f\u30e2\u30c7\u30ea\u30f3\u30b0\u3092\u7528\u3044\u305f\u63a8\u5b9a","text":"<p>\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u4eee\u5b9a\u304c\u53b3\u3057\u304f\u306a\u308b\u5834\u5408\u3084\u3001\u3088\u308a\u67d4\u8edf\u306a\u30e2\u30c7\u30ea\u30f3\u30b0\u304c\u8981\u6c42\u3055\u308c\u308b\u5834\u5408\u306b\u306f\u3001\u50be\u5411\u30b9\u30b3\u30a2\u6cd5\u3084\u69cb\u9020\u65b9\u7a0b\u5f0f\u30e2\u30c7\u30ea\u30f3\u30b0\uff08SEM\uff09\u306e\u6d3b\u7528\u304c\u691c\u8a0e\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#521","title":"5.2.1 \u50be\u5411\u30b9\u30b3\u30a2\u6cd5","text":"<p>\u50be\u5411\u30b9\u30b3\u30a2\u306f\u3001\u51e6\u7f6e\u306e\u5272\u308a\u5f53\u3066\u306b\u95a2\u3059\u308b\u4ea4\u7d61\u56e0\u5b50\u306e\u5f71\u97ff\u3092\u7d71\u5236\u3059\u308b\u305f\u3081\u306b\u7528\u3044\u3089\u308c\u308b\u624b\u6cd5\u3067\u3059\u3002 \u5177\u4f53\u7684\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u51e6\u7f6e\u306e\u78ba\u7387\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002 $$ e(X) = P(T=1 \\mid X). $$ \u63a8\u5b9a\u3055\u308c\u305f\u50be\u5411\u30b9\u30b3\u30a2\u3092\u7528\u3044\u3066\u3001\u4ee5\u4e0b\u306e\u65b9\u6cd5\u304c\u9069\u7528\u3055\u308c\u307e\u3059\u3002</p> <ul> <li> <p>\u30de\u30c3\u30c1\u30f3\u30b0\uff1a   \u540c\u69d8\u306e\u50be\u5411\u30b9\u30b3\u30a2\u3092\u6301\u3064\u51e6\u7f6e\u7fa4\u3068\u5bfe\u7167\u7fa4\u306e\u500b\u4f53\u3092\u30de\u30c3\u30c1\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u3001\u4ea4\u7d61\u56e0\u5b50\u306e\u30d0\u30e9\u30f3\u30b9\u3092\u53d6\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u30b9\u30c8\u30e9\u30c6\u30a3\u30d5\u30a3\u30b1\u30fc\u30b7\u30e7\u30f3\u3084\u91cd\u307f\u4ed8\u3051\uff1a   \u50be\u5411\u30b9\u30b3\u30a2\u306b\u57fa\u3065\u3044\u305f\u5c64\u5225\u89e3\u6790\u3084\u9006\u78ba\u7387\u91cd\u307f\u4ed8\u3051\u3092\u884c\u3044\u3001\u51e6\u7f6e\u5272\u308a\u5f53\u3066\u306e\u4e0d\u5747\u8861\u3092\u88dc\u6b63\u3057\u307e\u3059\u3002</p> </li> </ul> <p>\u3053\u308c\u306b\u3088\u308a\u3001\u5f93\u6765\u306e\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u5fc5\u8981\u3068\u3055\u308c\u308b\u53b3\u5bc6\u306a\u7dda\u5f62\u6027\u3084\u5206\u5e03\u306e\u4eee\u5b9a\u306b\u983c\u3089\u305a\u3001\u3088\u308a\u67d4\u8edf\u306b\u56e0\u679c\u52b9\u679c\u306e\u63a8\u5b9a\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#522-sem","title":"5.2.2 \u69cb\u9020\u65b9\u7a0b\u5f0f\u30e2\u30c7\u30ea\u30f3\u30b0\uff08SEM\uff09","text":"<p>SEM\u306f\u3001\u8907\u6570\u306e\u5909\u6570\u9593\u306e\u56e0\u679c\u95a2\u4fc2\u3092\u540c\u6642\u306b\u63a8\u5b9a\u3067\u304d\u308b\u5305\u62ec\u7684\u306a\u624b\u6cd5\u3067\u3059\u3002 SEM\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u51e6\u7f6e\u3001\u5a92\u4ecb\u5909\u6570\u3001\u7d50\u679c\u306e\u4e09\u8005\u9593\u306e\u56e0\u679c\u69cb\u9020\u3092\u30d1\u30b9\u89e3\u6790\u3068\u3057\u3066\u660e\u793a\u7684\u306b\u30e2\u30c7\u30eb\u5316\u3067\u304d\u307e\u3059\u3002 \u5177\u4f53\u7684\u306b\u306f\u3001\u5404\u5909\u6570\u9593\u306e\u76f4\u63a5\u7684\u30fb\u9593\u63a5\u7684\u306a\u30d1\u30b9\u3092\u8a2d\u5b9a\u3057\u3001\u52b9\u679c\u306e\u5927\u304d\u3055\u3092\u540c\u6642\u306b\u63a8\u5b9a\u3057\u307e\u3059\u3002 \u3053\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u3001\u8907\u96d1\u306a\u4ecb\u5728\u30e1\u30ab\u30cb\u30ba\u30e0\u3092\u8a73\u7d30\u306b\u89e3\u660e\u3059\u308b\u5834\u5408\u306b\u6709\u7528\u3067\u3042\u308a\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u7406\u8ad6\u7684\u67a0\u7d44\u307f\u3092\u7d71\u8a08\u30e2\u30c7\u30eb\u306b\u843d\u3068\u3057\u8fbc\u3080\u4e0a\u3067\u5f37\u529b\u306a\u624b\u6bb5\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#53","title":"5.3 \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306b\u3088\u308b\u4fe1\u983c\u533a\u9593\u306e\u63a8\u5b9a","text":"<p>\u63a8\u5b9a\u3055\u308c\u305f\u76f4\u63a5\u52b9\u679c\u3084\u9593\u63a5\u52b9\u679c\u306e\u4fe1\u983c\u6027\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306b\u3001\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u304c\u5e83\u304f\u63a1\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306f\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u5b9f\u65bd\u3055\u308c\u307e\u3059\u3002</p> <ol> <li> <p>\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\uff1a    \u5143\u306e\u30c7\u30fc\u30bf\u304b\u3089\u518d\u6a19\u672c\u62bd\u51fa\uff08\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\uff09\u3092 \\(B\\) \u56de\u884c\u3044\u3001\u305d\u308c\u305e\u308c\u306e\u30b5\u30f3\u30d7\u30eb\u306b\u3064\u3044\u3066 \\(NDE\\) \u3084 \\(NIE\\) \u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u52b9\u679c\u306e\u5206\u5e03\u63a8\u5b9a\uff1a    \u5f97\u3089\u308c\u305f \\(B\\) \u500b\u306e\u52b9\u679c\u63a8\u5b9a\u5024\u304b\u3089\u3001\u305d\u306e\u5206\u5e03\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u4fe1\u983c\u533a\u9593\u306e\u69cb\u7bc9\uff1a    \u30d1\u30fc\u30bb\u30f3\u30bf\u30a4\u30eb\u6cd5\u3084BCa\u6cd5\uff08Bias-Corrected and accelerated method\uff09\u306a\u3069\u3092\u7528\u3044\u3001\u52b9\u679c\u306e\u4fe1\u983c\u533a\u9593\u3092\u6c42\u3081\u307e\u3059\u3002</p> </li> </ol> <p>\u3053\u306e\u65b9\u6cd5\u306f\u3001\u7406\u8ad6\u7684\u306a\u5206\u5e03\u306b\u4f9d\u5b58\u305b\u305a\u3001\u975e\u6b63\u898f\u6027\u3084\u5c0f\u6a19\u672c\u30b5\u30a4\u30ba\u306e\u72b6\u6cc1\u4e0b\u3067\u3082\u9811\u5065\u306a\u4fe1\u983c\u533a\u9593\u306e\u63a8\u5b9a\u3092\u53ef\u80fd\u306b\u3057\u307e\u3059\u3002 \u305d\u306e\u305f\u3081\u3001\u5b9f\u8a3c\u7814\u7a76\u306b\u304a\u3044\u3066\u3082\u3001\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306f\u56e0\u679c\u5a92\u4ecb\u52b9\u679c\u306e\u4e0d\u78ba\u5b9f\u6027\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u6a19\u6e96\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3068\u3057\u3066\u5e83\u304f\u5229\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#6","title":"\u7b2c6\u7ae0 \u611f\u5ea6\u5206\u6790","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u3067\u306f\u3001\u57fa\u672c\u7684\u306a\u8b58\u5225\u4eee\u5b9a\uff08\u7279\u306b\u7121\u4ea4\u7d61\u6027\u3084Sequential Ignorability\uff09\u306e\u6210\u7acb\u304c\u6975\u3081\u3066\u91cd\u8981\u3067\u3059\u304c\u3001\u73fe\u5b9f\u306e\u30c7\u30fc\u30bf\u3067\u306f\u3053\u308c\u3089\u306e\u4eee\u5b9a\u304c\u5b8c\u5168\u306b\u6e80\u305f\u3055\u308c\u308b\u3053\u3068\u306f\u7a00\u3067\u3059\u3002\u305d\u3053\u3067\u3001\u4eee\u5b9a\u9055\u53cd\u304c\u63a8\u5b9a\u7d50\u679c\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306b\u3001\u611f\u5ea6\u5206\u6790\u304c\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002\u672c\u7ae0\u3067\u306f\u3001\u611f\u5ea6\u5206\u6790\u306e\u5fc5\u8981\u6027\u3068\u76ee\u7684\u3001\u5177\u4f53\u7684\u306a\u624b\u6cd5\u3084\u5b9f\u88c5\u4f8b\u306b\u3064\u3044\u3066\u89e3\u8aac\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#61","title":"6.1 \u611f\u5ea6\u5206\u6790\u306e\u5fc5\u8981\u6027\u3068\u76ee\u7684","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306b\u304a\u3051\u308b\u63a8\u5b9a\u306f\u3001\u4ee5\u4e0b\u306e\u8b58\u5225\u4eee\u5b9a\u306b\u5927\u304d\u304f\u4f9d\u5b58\u3057\u3066\u3044\u307e\u3059\u3002</p> <ul> <li>\u4e00\u8cab\u6027\uff08Consistency\uff09</li> <li>SUTVA</li> <li>\u7121\u4ea4\u7d61\u6027\uff08Sequential Ignorability\uff09</li> </ul> <p>\u7279\u306b\u3001\u5a92\u4ecb\u5909\u6570\u3068\u7d50\u679c\u9593\u306e\u7121\u4ea4\u7d61\u6027\u306f\u3001\u89b3\u5bdf\u3067\u304d\u306a\u3044\u4ea4\u7d61\u56e0\u5b50\u306e\u5b58\u5728\u306b\u3088\u308a\u5bb9\u6613\u306b\u7834\u3089\u308c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3082\u3057\u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u56e0\u5b50\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u76f4\u63a5\u52b9\u679c\uff08\\(NDE\\)\uff09\u3084\u9593\u63a5\u52b9\u679c\uff08\\(NIE\\)\uff09\u306e\u63a8\u5b9a\u5024\u306f\u30d0\u30a4\u30a2\u30b9\u3092\u53d7\u3051\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u304f\u306a\u308a\u307e\u3059\u3002</p> <p>\u611f\u5ea6\u5206\u6790\u306e\u76ee\u7684\u306f\u3001\u4ee5\u4e0b\u306e\u70b9\u306b\u3042\u308a\u307e\u3059\u3002</p> <ul> <li> <p>\u4eee\u5b9a\u9055\u53cd\u306e\u5f71\u97ff\u306e\u5b9a\u91cf\u5316\uff1a   \u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u56e0\u5b50\u304c\u5b58\u5728\u3057\u305f\u5834\u5408\u306b\u3001\u63a8\u5b9a\u3055\u308c\u305f\u52b9\u679c\u304c\u3069\u306e\u7a0b\u5ea6\u5909\u52d5\u3059\u308b\u304b\u3092\u8a55\u4fa1\u3059\u308b\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u9811\u5065\u6027\u306e\u691c\u8a3c\uff1a   \u4eee\u5b9a\u9055\u53cd\u304c\u8efd\u5fae\u306a\u5834\u5408\u3001\u63a8\u5b9a\u7d50\u679c\u304c\u3069\u306e\u7a0b\u5ea6\u9811\u5065\u3067\u3042\u308b\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3067\u3001\u7d50\u8ad6\u306e\u4fe1\u983c\u6027\u3092\u9ad8\u3081\u308b\u3002</p> </li> <li> <p>\u7814\u7a76\u8005\u3078\u306e\u793a\u5506\uff1a   \u672a\u89b3\u6e2c\u4ea4\u7d61\u56e0\u5b50\u306e\u5f71\u97ff\u3092\u660e\u793a\u3059\u308b\u3053\u3068\u3067\u3001\u5c06\u6765\u7684\u306a\u30c7\u30fc\u30bf\u53ce\u96c6\u3084\u7814\u7a76\u30c7\u30b6\u30a4\u30f3\u306e\u6539\u5584\u70b9\u3092\u793a\u5506\u3059\u308b\u3002</p> </li> </ul>"},{"location":"research/note/causal-mediation-analysis/#62","title":"6.2 \u611f\u5ea6\u5206\u6790\u306e\u624b\u6cd5\u3068\u5b9f\u88c5\u4f8b","text":"<p>\u611f\u5ea6\u5206\u6790\u306b\u306f\u3055\u307e\u3056\u307e\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u3053\u3053\u3067\u306f\u4e00\u822c\u7684\u306a\u624b\u6cd5\u3068\u305d\u306e\u5b9f\u88c5\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#621","title":"6.2.1 \u30d1\u30e9\u30e1\u30c8\u30ea\u30c3\u30af\u611f\u5ea6\u5206\u6790","text":"<p>Imai et al. (2010) \u306b\u4ee3\u8868\u3055\u308c\u308b\u65b9\u6cd5\u3067\u306f\u3001\u5a92\u4ecb\u5909\u6570\u3068\u7d50\u679c\u306e\u30e2\u30c7\u30eb\u306e\u6b8b\u5dee\u9593\u306e\u76f8\u95a2\uff08\u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u3092\u53cd\u6620\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\\(\\rho\\)\uff09\u3092\u611f\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066\u5c0e\u5165\u3057\u307e\u3059\u3002 \u3053\u306e\u5834\u5408\u3001\u7d50\u679c\u30e2\u30c7\u30eb\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u518d\u5b9a\u5f0f\u5316\u3055\u308c\u308b\u3053\u3068\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u3002</p> \\[ Y = \\beta_0 + \\beta_1 T + \\beta_2 M + \\beta_3 X + \\epsilon_Y, \\] <p>\u305f\u3060\u3057\u3001\\(\\epsilon_Y\\) \u306b\u306f\u3001\u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u56e0\u5b50\u306b\u3088\u308b\u5f71\u97ff\u304c\u542b\u307e\u308c\u3066\u304a\u308a\u3001\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u306e\u6b8b\u5dee \\(\\epsilon_M\\) \u3068\u306e\u76f8\u95a2\u304c \\(\\rho\\) \u3068\u3057\u3066\u8868\u73fe\u3055\u308c\u307e\u3059\u3002 \u3053\u306e\u76f8\u95a2\\(\\rho\\)\u30920\u304b\u3089\u3042\u308b\u7bc4\u56f2\uff08\u4f8b\uff1a\\(-0.5 \\le \\rho \\le 0.5\\)\uff09\u3067\u5909\u5316\u3055\u305b\u306a\u304c\u3089\u3001\u81ea\u7136\u76f4\u63a5\u52b9\u679c\u3084\u81ea\u7136\u9593\u63a5\u52b9\u679c\u306e\u63a8\u5b9a\u5024\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u304b\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002 \u5177\u4f53\u7684\u306b\u306f\u3001\u611f\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf\\(\\rho\\)\u306b\u5fdc\u3058\u305f\u30d0\u30a4\u30a2\u30b9\u88dc\u6b63\u5f8c\u306e\u52b9\u679c\u3092\u8a08\u7b97\u3057\u3001\u52b9\u679c\u63a8\u5b9a\u306e\u4fe1\u983c\u533a\u9593\u304c\u3069\u306e\u7a0b\u5ea6\u30b7\u30d5\u30c8\u3059\u308b\u304b\u3092\u30b0\u30e9\u30d5\u7b49\u3067\u53ef\u8996\u5316\u3059\u308b\u65b9\u6cd5\u304c\u4e00\u822c\u7684\u3067\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#622","title":"6.2.2 \u5b9f\u88c5\u4f8b","text":"<p>\u4ee5\u4e0b\u306f\u3001R\u306a\u3069\u306e\u7d71\u8a08\u89e3\u6790\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306b\u304a\u3051\u308b\u611f\u5ea6\u5206\u6790\u306e\u5b9f\u88c5\u4f8b\u306e\u6d41\u308c\u3067\u3059\u3002</p> <ol> <li> <p>\u57fa\u672c\u30e2\u30c7\u30eb\u306e\u63a8\u5b9a\uff1a    \u307e\u305a\u3001\u524d\u7ae0\u307e\u3067\u3067\u793a\u3057\u305f\u56de\u5e30\u30e2\u30c7\u30eb\u3084SEM\u3092\u7528\u3044\u3066\u3001\\(NDE\\)\u304a\u3088\u3073\\(NIE\\)\u306e\u521d\u671f\u63a8\u5b9a\u5024\u3092\u6c42\u3081\u307e\u3059\u3002</p> </li> <li> <p>\u611f\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8a2d\u5b9a\uff1a    \u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u56e0\u5b50\u306e\u5f71\u97ff\u3092\u8868\u3059\u76f8\u95a2\\(\\rho\\)\u3092\u3001\u4e8b\u524d\u306e\u77e5\u898b\u3084\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u57fa\u3065\u3044\u3066\u8907\u6570\u306e\u5024\uff08\u4f8b\uff1a\\(-0.3, -0.2, \\dots, 0.3\\)\uff09\u3067\u8a2d\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u30d0\u30a4\u30a2\u30b9\u88dc\u6b63\uff1a    \u5404\\(\\rho\\)\u306b\u5bfe\u3057\u3066\u3001\u63a8\u5b9a\u3055\u308c\u305f\u52b9\u679c\u306b\u5bfe\u3059\u308b\u30d0\u30a4\u30a2\u30b9\u88dc\u6b63\u3092\u884c\u3044\u307e\u3059\u3002\u6570\u5f0f\u7684\u306b\u306f\u3001\u88dc\u6b63\u9805\u3092\u52b9\u679c\u63a8\u5b9a\u306b\u52a0\u5473\u3059\u308b\u5f62\u3067\u518d\u8a08\u7b97\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u53ef\u8996\u5316\uff1a \\(\\rho\\)\u306e\u5909\u5316\u306b\u4f34\u3046\\(NDE\\)\u3001\\(NIE\\)\u304a\u3088\u3073\u7dcf\u52b9\u679c\uff08\\(TE\\)\uff09\u306e\u63a8\u5b9a\u5024\u306e\u63a8\u79fb\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3001\u4eee\u5b9a\u9055\u53cd\u306e\u5f71\u97ff\u3092\u8996\u899a\u7684\u306b\u8a55\u4fa1\u3057\u307e\u3059\u3002</p> </li> </ol>"},{"location":"research/note/causal-mediation-analysis/#623","title":"6.2.3 \u89e3\u91c8\u3068\u7559\u610f\u70b9","text":"<ul> <li> <p>\u7bc4\u56f2\u306e\u9078\u5b9a\uff1a   \u611f\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf\\(\\rho\\)\u306e\u9078\u5b9a\u306f\u3001\u7814\u7a76\u5206\u91ce\u306e\u65e2\u5b58\u306e\u77e5\u898b\u3084\u30d1\u30a4\u30ed\u30c3\u30c8\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u3044\u3066\u884c\u3046\u3053\u3068\u304c\u671b\u307e\u3057\u3044\u3067\u3059\u3002   \u6975\u7aef\u306a\u5024\u3092\u4eee\u5b9a\u3059\u308b\u3068\u73fe\u5b9f\u7684\u306a\u72b6\u6cc1\u304b\u3089\u9038\u8131\u3059\u308b\u305f\u3081\u3001\u59a5\u5f53\u306a\u7bc4\u56f2\u3092\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u9811\u5065\u6027\uff1a   \u63a8\u5b9a\u3055\u308c\u305f\u52b9\u679c\u304c\u3001\\(\\rho\\)\u306e\u5909\u52d5\u306b\u5bfe\u3057\u3066\u5927\u304d\u304f\u5909\u52d5\u3059\u308b\u5834\u5408\u306f\u3001\u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u56e0\u5b50\u306e\u5f71\u97ff\u304c\u5927\u304d\u3044\u3053\u3068\u3092\u793a\u5506\u3057\u307e\u3059\u3002   \u9006\u306b\u3001\\(\\rho\\)\u306e\u5909\u52d5\u306b\u5bfe\u3057\u3066\u52b9\u679c\u63a8\u5b9a\u304c\u6bd4\u8f03\u7684\u5b89\u5b9a\u3057\u3066\u3044\u308c\u3070\u3001\u5206\u6790\u7d50\u679c\u306f\u9811\u5065\u3067\u3042\u308b\u3068\u8a55\u4fa1\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u5b9f\u8a3c\u7814\u7a76\u3078\u306e\u5fdc\u7528\uff1a   \u611f\u5ea6\u5206\u6790\u306e\u7d50\u679c\u306f\u3001\u7814\u7a76\u8ad6\u6587\u3084\u5831\u544a\u66f8\u306b\u304a\u3044\u3066\u3001\u8b58\u5225\u4eee\u5b9a\u306b\u5bfe\u3059\u308b\u61f8\u5ff5\u70b9\u3092\u660e\u793a\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u30a8\u30d3\u30c7\u30f3\u30b9\u3068\u306a\u308a\u307e\u3059\u3002   \u307e\u305f\u3001\u5c06\u6765\u7684\u306a\u7814\u7a76\u30c7\u30b6\u30a4\u30f3\u306e\u6539\u5584\u3084\u8ffd\u52a0\u30c7\u30fc\u30bf\u53ce\u96c6\u306e\u5fc5\u8981\u6027\u3092\u8b70\u8ad6\u3059\u308b\u969b\u306e\u53c2\u8003\u306b\u3082\u306a\u308a\u307e\u3059\u3002</p> </li> </ul> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u611f\u5ea6\u5206\u6790\u306f\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u63a8\u5b9a\u7d50\u679c\u306e\u4fe1\u983c\u6027\u3092\u88dc\u5b8c\u3059\u308b\u305f\u3081\u306e\u5f37\u529b\u306a\u624b\u6cd5\u3067\u3059\u3002 \u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u56e0\u5b50\u306e\u5f71\u97ff\u3092\u5b9a\u91cf\u7684\u306b\u8a55\u4fa1\u3059\u308b\u3053\u3068\u3067\u3001\u7406\u8ad6\u7684\u4eee\u5b9a\u306e\u9650\u754c\u3092\u8a8d\u8b58\u3057\u3001\u3088\u308a\u5805\u7262\u306a\u56e0\u679c\u63a8\u8ad6\u306b\u5411\u3051\u305f\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#7","title":"\u7b2c7\u7ae0 \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3068\u5b9f\u8a3c\u7814\u7a76","text":""},{"location":"research/note/causal-mediation-analysis/#71","title":"7.1 \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30b9\u30bf\u30c7\u30a3\u306e\u8a73\u7d30","text":"<p>\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30b9\u30bf\u30c7\u30a3\u306f\u3001\u65e2\u77e5\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3084\u8a2d\u5b9a\u306b\u57fa\u3065\u3044\u3066\u4eee\u60f3\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u5404\u63a8\u5b9a\u624b\u6cd5\uff08\u76f4\u63a5\u52b9\u679c\u304a\u3088\u3073\u9593\u63a5\u52b9\u679c\u306e\u63a8\u5b9a\uff09\u304c\u3069\u306e\u7a0b\u5ea6\u6b63\u78ba\u306b\u6a5f\u80fd\u3059\u308b\u304b\u3001\u307e\u305f\u8b58\u5225\u4eee\u5b9a\u306e\u9055\u53cd\u304c\u63a8\u5b9a\u7d50\u679c\u306b\u3069\u306e\u3088\u3046\u306a\u5f71\u97ff\u3092\u53ca\u307c\u3059\u304b\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u624b\u6cd5\u3067\u3059\u3002\u672c\u7bc0\u3067\u306f\u3001\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u624b\u9806\u3068\u305d\u306e\u8a55\u4fa1\u65b9\u6cd5\u306b\u3064\u3044\u3066\u3001\u4ee5\u4e0b\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u8a73\u7d30\u306b\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#711","title":"7.1.1 \u30c7\u30fc\u30bf\u751f\u6210\u30d7\u30ed\u30bb\u30b9\u306e\u8a2d\u5b9a","text":"<p>\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u6f5c\u5728\u7684\u56e0\u679c\u30e2\u30c7\u30eb\u306b\u57fa\u3065\u3044\u3066\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u307e\u3059\u3002</p> <ul> <li> <p>\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb   \u51e6\u7f6e \\(T\\) \u3068\u5171\u5909\u91cf \\(X\\) \u304c\u5a92\u4ecb\u5909\u6570 \\(M\\) \u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u3001\u6b21\u306e\u7dda\u5f62\u30e2\u30c7\u30eb\u3067\u8868\u73fe\u3057\u307e\u3059\u3002   $$   M = \\alpha_0 + \\alpha_1 T + \\alpha_2 X + \\epsilon_M,   $$   \u3053\u3053\u3067\u3001\\(\\epsilon_M\\) \u306f\u5e73\u57470\u306e\u8aa4\u5dee\u9805\u3067\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u30e2\u30c7\u30eb   \u51e6\u7f6e \\(T\\)\u3001\u5a92\u4ecb\u5909\u6570 \\(M\\)\u3001\u304a\u3088\u3073\u5171\u5909\u91cf \\(X\\) \u304c\u7d50\u679c \\(Y\\) \u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u3001\u4ee5\u4e0b\u306e\u30e2\u30c7\u30eb\u3067\u8868\u73fe\u3057\u307e\u3059\u3002   $$   Y = \\beta_0 + \\beta_1 T + \\beta_2 M + \\beta_3 X + \\epsilon_Y,   $$   \u3053\u3053\u3067\u3001\\(\\epsilon_Y\\) \u3082\u5e73\u57470\u306e\u8aa4\u5dee\u9805\u3067\u3059\u3002</p> </li> </ul> <p>\u3053\u306e\u8a2d\u5b9a\u306b\u3088\u308a\u3001\u81ea\u7136\u9593\u63a5\u52b9\u679c\uff08\\(NIE\\)\uff09 \u306f \\(\\alpha_1 \\times \\beta_2\\) \u3068\u3057\u3066\u63a8\u5b9a\u3067\u304d\u3001\u81ea\u7136\u76f4\u63a5\u52b9\u679c\uff08\\(NDE\\)\uff09 \u306f \\(\\beta_1\\) \u3068\u3057\u3066\u89e3\u91c8\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#712","title":"7.1.2 \u4eee\u5b9a\u9055\u53cd\u30b7\u30ca\u30ea\u30aa\u306e\u69cb\u7bc9\u3068\u63a8\u5b9a\u624b\u6cd5\u306e\u8a55\u4fa1","text":"<p>\u5b9f\u969b\u306e\u89e3\u6790\u3067\u306f\u3001\u7121\u4ea4\u7d61\u6027\u3084\u4e00\u8cab\u6027\u306a\u3069\u306e\u8b58\u5225\u4eee\u5b9a\u304c\u90e8\u5206\u7684\u306b\u7834\u3089\u308c\u308b\u5834\u5408\u306e\u5f71\u97ff\u3082\u8003\u616e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3067\u306f\u3001\u4f8b\u3048\u3070\u4ee5\u4e0b\u306e\u30b7\u30ca\u30ea\u30aa\u3092\u691c\u8a0e\u3067\u304d\u307e\u3059\u3002</p> <ul> <li> <p>\u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u56e0\u5b50\u306e\u5c0e\u5165   \u5a92\u4ecb\u5909\u6570 \\(M\\) \u3068\u7d50\u679c \\(Y\\) \u306e\u9593\u306b\u672a\u89b3\u6e2c\u306e\u5909\u6570\u3092\u52a0\u3048\u308b\u3053\u3068\u3067\u3001\u7121\u4ea4\u7d61\u6027\u4eee\u5b9a\u306e\u9055\u53cd\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30c8\u3059\u308b\u3002</p> </li> <li> <p>\u8aa4\u5dee\u9805\u306e\u76f8\u95a2 \\(\\epsilon_M\\) \u3068 \\(\\epsilon_Y\\) \u306e\u9593\u306b\u76f8\u95a2\u3092\u6301\u305f\u305b\u308b\u3053\u3068\u3067\u3001\u611f\u5ea6\u5206\u6790\u306e\u5bfe\u8c61\u3068\u306a\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u4f8b\uff1a\u76f8\u95a2\u4fc2\u6570 \\(\\rho\\)\uff09\u306e\u5f71\u97ff\u3092\u8a55\u4fa1\u3059\u308b\u3002</p> </li> </ul> <p>\u3053\u308c\u3089\u306e\u30b7\u30ca\u30ea\u30aa\u306b\u5bfe\u3057\u3066\u3001\u5404\u63a8\u5b9a\u624b\u6cd5\uff08\u56de\u5e30\u30e2\u30c7\u30eb\u3001SEM\u3001\u50be\u5411\u30b9\u30b3\u30a2\u6cd5\u306a\u3069\uff09\u3092\u7528\u3044\u3066\u52b9\u679c\u306e\u63a8\u5b9a\u3092\u884c\u3044\u3001\u771f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u5024\u3068\u306e\u6bd4\u8f03\u3084\u30d0\u30a4\u30a2\u30b9\u3001\u5206\u6563\u3001\u30ab\u30d0\u30ec\u30c3\u30b8\u7387\u306a\u3069\u306e\u8a55\u4fa1\u6307\u6a19\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u3001\u624b\u6cd5\u306e\u9811\u5065\u6027\u3084\u9650\u754c\u3092\u660e\u3089\u304b\u306b\u3057\u307e\u3059\u3002</p> <p>\u4ee5\u4e0b\u306b\u3001Python \u3092\u7528\u3044\u305f\u30b7\u30f3\u30d7\u30eb\u306a\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u30c7\u30e2\u3092\u793a\u3057\u307e\u3059\u3002\u3053\u306e\u30c7\u30e2\u3067\u306f\u3001\u51e6\u7f6e \\(T\\)\u3001\u5171\u5909\u91cf \\(X\\)\u3001\u5a92\u4ecb\u5909\u6570 \\(M\\)\u3001\u304a\u3088\u3073\u7d50\u679c \\(Y\\) \u306e\u95a2\u4fc2\u3092\u4e0a\u8a18\u306e\u30e2\u30c7\u30eb\u306b\u57fa\u3065\u3044\u3066\u30c7\u30fc\u30bf\u751f\u6210\u3057\u3001\u56de\u5e30\u30e2\u30c7\u30eb\u306b\u3088\u308a\u81ea\u7136\u9593\u63a5\u52b9\u679c\uff08\\(\\alpha_1 \\times \\beta_2\\)\uff09\u304a\u3088\u3073\u81ea\u7136\u76f4\u63a5\u52b9\u679c\uff08\\(\\beta_1\\)\uff09\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#713-python","title":"7.1.3 Python\u306b\u3088\u308b\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30e2","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306f\u30011000\u30b5\u30f3\u30d7\u30eb\u306e\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3001\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u3068\u7d50\u679c\u30e2\u30c7\u30eb\u3092OLS\u56de\u5e30\u3067\u63a8\u5b9a\u3059\u308b\u4f8b\u3067\u3059\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# \u518d\u73fe\u6027\u306e\u305f\u3081\u4e71\u6570\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\nnp.random.seed(42)\n\n# \u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\nn = 1000\n\n# \u5171\u5909\u91cf X \u3092\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u304b\u3089\u751f\u6210\nX = np.random.normal(0, 1, n)\n\n# \u51e6\u7f6e T \u306f\u4e8c\u9805\u5206\u5e03\uff08\u6210\u529f\u78ba\u73870.5\uff09\u304b\u3089\u751f\u6210\uff080\u307e\u305f\u306f1\uff09\nT = np.random.binomial(1, 0.5, n)\n\n# \u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\nalpha0 = 0.5\nalpha1 = 1.0  # \u51e6\u7f6e\u306e\u5a92\u4ecb\u5909\u6570\u3078\u306e\u5f71\u97ff\nalpha2 = 0.5  # \u5171\u5909\u91cf X \u306e\u5f71\u97ff\n\n# \u8aa4\u5dee\u9805 \u03b5_M \u306f\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u304b\u3089\u751f\u6210\nerror_M = np.random.normal(0, 1, n)\n# \u5a92\u4ecb\u5909\u6570 M \u306e\u751f\u6210\nM = alpha0 + alpha1 * T + alpha2 * X + error_M\n\n# \u7d50\u679c\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\nbeta0 = 0.5\nbeta1 = 0.5  # \u51e6\u7f6e\u306e\u76f4\u63a5\u52b9\u679c\uff08\u81ea\u7136\u76f4\u63a5\u52b9\u679c\uff09\nbeta2 = 1.5  # \u5a92\u4ecb\u5909\u6570\u306e\u52b9\u679c\uff08\u9593\u63a5\u52b9\u679c\u306e\u4e00\u90e8\uff09\nbeta3 = 0.5  # \u5171\u5909\u91cf X \u306e\u5f71\u97ff\n\n# \u8aa4\u5dee\u9805 \u03b5_Y \u306f\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u304b\u3089\u751f\u6210\nerror_Y = np.random.normal(0, 1, n)\n# \u7d50\u679c Y \u306e\u751f\u6210\nY = beta0 + beta1 * T + beta2 * M + beta3 * X + error_Y\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u307e\u3068\u3081\u308b\ndata = pd.DataFrame({'Y': Y, 'T': T, 'M': M, 'X': X})\n\n# \u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u306e\u63a8\u5b9a\uff1a M ~ T + X\nmediator_model = smf.ols('M ~ T + X', data=data).fit()\nprint(\"\u3010\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u306e\u7d50\u679c\u3011\")\nprint(mediator_model.summary())\n\n# \u7d50\u679c\u30e2\u30c7\u30eb\u306e\u63a8\u5b9a\uff1a Y ~ T + M + X\noutcome_model = smf.ols('Y ~ T + M + X', data=data).fit()\nprint(\"\\n\u3010\u7d50\u679c\u30e2\u30c7\u30eb\u306e\u7d50\u679c\u3011\")\nprint(outcome_model.summary())\n\n# \u63a8\u5b9a\u3055\u308c\u305f\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u304b\u3089 T \u306e\u4fc2\u6570\uff08alpha1\u306e\u63a8\u5b9a\u5024\uff09\u3092\u53d6\u5f97\nalpha1_est = mediator_model.params['T']\n# \u63a8\u5b9a\u3055\u308c\u305f\u7d50\u679c\u30e2\u30c7\u30eb\u304b\u3089 M \u306e\u4fc2\u6570\uff08beta2\u306e\u63a8\u5b9a\u5024\uff09\u3092\u53d6\u5f97\nbeta2_est = outcome_model.params['M']\n\n# \u9593\u63a5\u52b9\u679c\u306e\u63a8\u5b9a\u5024\uff1a alpha1_est * beta2_est\nindirect_effect = alpha1_est * beta2_est\n\n# \u76f4\u63a5\u52b9\u679c\u306e\u63a8\u5b9a\u5024\uff1a \u7d50\u679c\u30e2\u30c7\u30eb\u306e T \u306e\u4fc2\u6570\uff08beta1\u306e\u63a8\u5b9a\u5024\uff09\ndirect_effect = outcome_model.params['T']\n\nprint(\"\\n\u3010\u63a8\u5b9a\u3055\u308c\u305f\u52b9\u679c\u3011\")\nprint(\"\u9593\u63a5\u52b9\u679c (alpha1 * beta2): {:.4f}\".format(indirect_effect))\nprint(\"\u76f4\u63a5\u52b9\u679c (beta1): {:.4f}\".format(direct_effect))\n</code></pre>"},{"location":"research/note/causal-mediation-analysis/#714","title":"7.1.4 \u30c7\u30e2\u30b3\u30fc\u30c9\u306e\u89e3\u8aac","text":"<ol> <li>\u30c7\u30fc\u30bf\u751f\u6210: </li> <li>\u5171\u5909\u91cf \\(X\\) \u3092\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u304b\u3089\u751f\u6210\u3057\u3001\u51e6\u7f6e \\(T\\) \u306f\u4e8c\u9805\u5206\u5e03\uff08\u6210\u529f\u78ba\u73870.5\uff09\u304b\u3089\u751f\u6210\u3057\u3066\u3044\u307e\u3059\u3002  </li> <li>\u5a92\u4ecb\u5909\u6570 \\(M\\) \u306f\u3001\u8a2d\u5b9a\u3057\u305f\u30d1\u30e9\u30e1\u30fc\u30bf (\\(\\alpha_0\\), \\(\\alpha_1\\), \\(\\alpha_2\\)) \u3068\u4e71\u6570\u8aa4\u5dee \\(\\epsilon_M\\) \u3092\u7528\u3044\u3066\u751f\u6210\u3057\u3066\u3044\u307e\u3059\u3002  </li> <li> <p>\u7d50\u679c \\(Y\\) \u306f\u3001\u51e6\u7f6e \\(T\\)\u3001\u5a92\u4ecb\u5909\u6570 \\(M\\)\u3001\u304a\u3088\u3073\u5171\u5909\u91cf \\(X\\) \u306e\u7dda\u5f62\u7d50\u5408\u3068\u4e71\u6570\u8aa4\u5dee \\(\\epsilon_Y\\) \u306b\u3088\u308a\u751f\u6210\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u30e2\u30c7\u30eb\u306e\u63a8\u5b9a: </p> </li> <li><code>statsmodels</code> \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3057\u3066\u3001\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\uff08\\(M \\sim T + X\\)\uff09\u3068\u7d50\u679c\u30e2\u30c7\u30eb\uff08\\(Y \\sim T + M + X\\)\uff09\u3092OLS\u56de\u5e30\u3067\u63a8\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002  </li> <li> <p>\u5404\u30e2\u30c7\u30eb\u306e\u63a8\u5b9a\u7d50\u679c\u306e\u6982\u8981\uff08\u4fc2\u6570\u3001\u6a19\u6e96\u8aa4\u5dee\u3001p\u5024\u306a\u3069\uff09\u3092\u51fa\u529b\u3057\u3001\u63a8\u5b9a\u306e\u6b63\u78ba\u6027\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u52b9\u679c\u306e\u5206\u89e3: </p> </li> <li>\u63a8\u5b9a\u3055\u308c\u305f\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u306e \\(T\\) \u306e\u4fc2\u6570\uff08\\(\\hat{\\alpha}_1\\)\uff09\u3068\u3001\u7d50\u679c\u30e2\u30c7\u30eb\u306e \\(M\\) \u306e\u4fc2\u6570\uff08\\(\\hat{\\beta}_2\\)\uff09\u306e\u7a4d\u3092\u7528\u3044\u3066\u3001\u9593\u63a5\u52b9\u679c\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002  </li> <li>\u7d50\u679c\u30e2\u30c7\u30eb\u306e \\(T\\) \u306e\u4fc2\u6570\u304c\u76f4\u63a5\u52b9\u679c\u306e\u63a8\u5b9a\u5024\u3068\u306a\u308a\u307e\u3059\u3002</li> </ol> <p>\u3053\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30e2\u306f\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u57fa\u672c\u7684\u306a\u8003\u3048\u65b9\u3068\u3001\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u7528\u3044\u305f\u52b9\u679c\u306e\u5206\u89e3\u65b9\u6cd5\u3092\u76f4\u611f\u7684\u306b\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u51fa\u767a\u70b9\u3068\u306a\u308a\u307e\u3059\u3002\u5b9f\u969b\u306e\u7814\u7a76\u3067\u306f\u3001\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u591a\u6570\u56de\u7e70\u308a\u8fd4\u3057\u3001\u63a8\u5b9a\u306e\u30d0\u30a4\u30a2\u30b9\u3084\u5206\u6563\u3001\u611f\u5ea6\u5206\u6790\u306a\u3069\u3092\u8a73\u7d30\u306b\u8a55\u4fa1\u3059\u308b\u3053\u3068\u3067\u3001\u5404\u624b\u6cd5\u306e\u9811\u5065\u6027\u3092\u691c\u8a3c\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#72","title":"7.2 \u611f\u5ea6\u5206\u6790\u306e\u30c7\u30e2","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u3067\u306f\u3001\u5a92\u4ecb\u5909\u6570\u3068\u7d50\u679c\u306e\u9593\u306e\u672a\u89b3\u6e2c\u4ea4\u7d61\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u8b58\u5225\u4eee\u5b9a\uff08\u7279\u306b\u7121\u4ea4\u7d61\u6027\uff09\u304c\u7834\u3089\u308c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u3053\u3067\u306f\u3001\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u3068\u7d50\u679c\u30e2\u30c7\u30eb\u306e\u8aa4\u5dee\u9805\u9593\u306b\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u306e\u611f\u5ea6\u5206\u6790\u306e\u30c7\u30e2\u3092\u793a\u3057\u307e\u3059\u3002 \u3053\u306e\u76f8\u95a2\uff08\u611f\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\rho\\)\uff09\u304c\u7570\u306a\u308b\u72b6\u6cc1\u4e0b\u3067\u3001\u9593\u63a5\u52b9\u679c\uff08\\(NIE\\)\uff09\u3068\u76f4\u63a5\u52b9\u679c\uff08\\(NDE\\)\uff09\u306e\u63a8\u5b9a\u5024\u304c\u3069\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u304b\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#_1","title":"\u30c7\u30e2\u306e\u6982\u8981","text":"<ol> <li>\u30c7\u30fc\u30bf\u751f\u6210:    \u5171\u5909\u91cf \\(X\\)\u3001\u51e6\u7f6e \\(T\\) \u3092\u751f\u6210\u3057\u3001\u5a92\u4ecb\u5909\u6570 \\(M\\) \u3068\u7d50\u679c \\(Y\\) \u3092\u4ee5\u4e0b\u306e\u30e2\u30c7\u30eb\u3067\u751f\u6210\u3057\u307e\u3059\u3002  </li> <li>\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb:      $$      M = \\alpha_0 + \\alpha_1 T + \\alpha_2 X + \\epsilon_M      $$</li> <li>\u7d50\u679c\u30e2\u30c7\u30eb:      $$      Y = \\beta_0 + \\beta_1 T + \\beta_2 M + \\beta_3 X + \\epsilon_Y      $$</li> </ol> <p>\u3053\u3053\u3067\u3001\\(\\epsilon_M\\) \u3068 \\(\\epsilon_Y\\) \u306f\u3001\u5e73\u57470\u3001\u5206\u65631 \u306e\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3044\u307e\u3059\u304c\u3001    \u3053\u308c\u30892\u3064\u306e\u8aa4\u5dee\u9805\u306f\u3001\u76f8\u95a2\u4fc2\u6570 \\(\\rho\\) \u3092\u6301\u3064\u3088\u3046\u306b\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u304b\u3089\u540c\u6642\u306b\u751f\u6210\u3057\u307e\u3059\u3002</p> <ol> <li> <p>\u611f\u5ea6\u5206\u6790: \\(\\rho\\) \u306e\u5024\u3092 \\(-0.5\\) \u304b\u3089 \\(0.5\\) \u307e\u3067\u5909\u5316\u3055\u305b\u3001\u5404\u5024\u306b\u5bfe\u3057\u3066\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3001OLS\u56de\u5e30\u306b\u3088\u308a    \u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u304a\u3088\u3073\u7d50\u679c\u30e2\u30c7\u30eb\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002    \u305d\u306e\u5f8c\u3001\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b \\(T\\) \u306e\u4fc2\u6570\uff08\\(\\hat{\\alpha}_1\\)\uff09\u3068\u7d50\u679c\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b \\(M\\) \u306e\u4fc2\u6570\uff08\\(\\hat{\\beta}_2\\)\uff09\u306e\u7a4d\u3092\u7528\u3044\u3066\u9593\u63a5\u52b9\u679c\u3092\u8a08\u7b97\u3057\u3001\u307e\u305f\u7d50\u679c\u30e2\u30c7\u30eb\u306e \\(T\\) \u306e\u4fc2\u6570\u3092\u76f4\u63a5\u52b9\u679c\u3068\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u53ef\u8996\u5316: \\(\\rho\\) \u306e\u5024\u306b\u5bfe\u3059\u308b\u63a8\u5b9a\u3055\u308c\u305f\u9593\u63a5\u52b9\u679c\u304a\u3088\u3073\u76f4\u63a5\u52b9\u679c\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3001    \u771f\u306e\u52b9\u679c\uff08\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u8a2d\u5b9a\u3067\u306e \\(\\alpha_1 \\times \\beta_2\\) \u304a\u3088\u3073 \\(\\beta_1\\)\uff09\u3068\u6bd4\u8f03\u3057\u307e\u3059\u3002</p> </li> </ol>"},{"location":"research/note/causal-mediation-analysis/#python","title":"Python \u30b3\u30fc\u30c9\u4f8b","text":"<p>\u4ee5\u4e0b\u306f\u3001\u4e0a\u8a18\u306e\u624b\u9806\u306b\u57fa\u3065\u3044\u305f\u611f\u5ea6\u5206\u6790\u306e\u30c7\u30e2\u30b3\u30fc\u30c9\u3067\u3059\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\n# \u518d\u73fe\u6027\u306e\u305f\u3081\u4e71\u6570\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\nnp.random.seed(42)\n\n# \u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\nn = 1000\n\n# \u5171\u5909\u91cf X \u3068\u51e6\u7f6e T \u306e\u751f\u6210\nX = np.random.normal(0, 1, n)\nT = np.random.binomial(1, 0.5, n)\n\n# \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u771f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\nalpha0, alpha1, alpha2 = 0.5, 1.0, 0.5\nbeta0, beta1, beta2, beta3 = 0.5, 0.5, 1.5, 0.5\n\n# \u611f\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf (\u30a8\u30e9\u30fc\u9805\u306e\u76f8\u95a2) \u306e\u7bc4\u56f2\nrho_values = np.arange(-0.5, 0.6, 0.1)\nindirect_effect_estimates = []\ndirect_effect_estimates = []\n\n# \u5404 rho \u5024\u306b\u5bfe\u3059\u308b\u63a8\u5b9a\u3092\u5b9f\u65bd\nfor rho in rho_values:\n    # \u5171\u5206\u6563\u884c\u5217\u306e\u8a2d\u5b9a\uff1a\u5206\u65631, \u76f8\u95a2 rho\n    cov_matrix = np.array([[1, rho],\n                           [rho, 1]])\n\n    # \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u304b\u3089\u8aa4\u5dee\u9805 (\u03b5_M, \u03b5_Y) \u3092\u751f\u6210\n    errors = np.random.multivariate_normal(mean=[0, 0], cov=cov_matrix, size=n)\n    error_M = errors[:, 0]\n    error_Y = errors[:, 1]\n\n    # \u5a92\u4ecb\u5909\u6570 M \u306e\u751f\u6210\n    M = alpha0 + alpha1 * T + alpha2 * X + error_M\n\n    # \u7d50\u679c Y \u306e\u751f\u6210\n    Y = beta0 + beta1 * T + beta2 * M + beta3 * X + error_Y\n\n    # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210\n    data = pd.DataFrame({'Y': Y, 'T': T, 'M': M, 'X': X})\n\n    # \u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u306e\u63a8\u5b9a: M ~ T + X\n    mediator_model = smf.ols('M ~ T + X', data=data).fit()\n\n    # \u7d50\u679c\u30e2\u30c7\u30eb\u306e\u63a8\u5b9a: Y ~ T + M + X\n    outcome_model = smf.ols('Y ~ T + M + X', data=data).fit()\n\n    # \u5404\u30e2\u30c7\u30eb\u304b\u3089\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u63a8\u5b9a\u5024\u3092\u62bd\u51fa\n    alpha1_est = mediator_model.params['T']\n    beta2_est = outcome_model.params['M']\n\n    # \u9593\u63a5\u52b9\u679c\u3068\u76f4\u63a5\u52b9\u679c\u306e\u8a08\u7b97\n    indirect_effect = alpha1_est * beta2_est\n    direct_effect = outcome_model.params['T']\n\n    indirect_effect_estimates.append(indirect_effect)\n    direct_effect_estimates.append(direct_effect)\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(8, 6))\nplt.plot(rho_values, indirect_effect_estimates, marker='o', label='\u63a8\u5b9a\u9593\u63a5\u52b9\u679c')\nplt.plot(rho_values, direct_effect_estimates, marker='x', label='\u63a8\u5b9a\u76f4\u63a5\u52b9\u679c')\nplt.axhline(alpha1 * beta2, color='gray', linestyle='--', label='\u771f\u306e\u9593\u63a5\u52b9\u679c')\nplt.axhline(beta1, color='black', linestyle='--', label='\u771f\u306e\u76f4\u63a5\u52b9\u679c')\nplt.xlabel('\u8aa4\u5dee\u9805\u9593\u306e\u76f8\u95a2 (rho)')\nplt.ylabel('\u52b9\u679c\u306e\u63a8\u5b9a\u5024')\nplt.title('\u611f\u5ea6\u5206\u6790: \u8aa4\u5dee\u9805\u76f8\u95a2\u306b\u3088\u308b\u52b9\u679c\u63a8\u5b9a\u306e\u5909\u52d5')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"research/note/causal-mediation-analysis/#_2","title":"\u30b3\u30fc\u30c9\u306e\u89e3\u8aac","text":"<ul> <li> <p>\u8aa4\u5dee\u9805\u306e\u751f\u6210: <code>np.random.multivariate_normal</code> \u3092\u7528\u3044\u3066\u3001\u8aa4\u5dee\u9805 \\(\\epsilon_M\\) \u3068 \\(\\epsilon_Y\\) \u306e\u9593\u306b\u76f8\u95a2 \\(\\rho\\) \u3092\u6301\u305f\u305b\u305f\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u3068\u7d50\u679c\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u306e\u5f71\u97ff\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30c8\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u30e2\u30c7\u30eb\u306e\u63a8\u5b9a:   \u5404 \\(\\rho\\) \u306e\u5024\u306b\u3064\u3044\u3066\u3001OLS\u56de\u5e30\u3067\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb (<code>M ~ T + X</code>) \u3068\u7d50\u679c\u30e2\u30c7\u30eb (<code>Y ~ T + M + X</code>) \u3092\u63a8\u5b9a\u3057\u3001\u5a92\u4ecb\u5909\u6570\u30e2\u30c7\u30eb\u304b\u3089 \\(T\\) \u306e\u4fc2\u6570\uff08\\(\\hat{\\alpha}_1\\)\uff09\u3068\u7d50\u679c\u30e2\u30c7\u30eb\u304b\u3089 \\(M\\) \u306e\u4fc2\u6570\uff08\\(\\hat{\\beta}_2\\)\uff09\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u52b9\u679c\u306e\u8a08\u7b97:   \u63a8\u5b9a\u3055\u308c\u305f\u9593\u63a5\u52b9\u679c\u306f \\(\\hat{\\alpha}_1 \\times \\hat{\\beta}_2\\) \u3068\u3057\u3066\u8a08\u7b97\u3057\u3001\u7d50\u679c\u30e2\u30c7\u30eb\u306e \\(T\\) \u306e\u4fc2\u6570\u304c\u76f4\u63a5\u52b9\u679c\u306e\u63a8\u5b9a\u5024\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u53ef\u8996\u5316:   \u30eb\u30fc\u30d7\u5185\u3067\u5f97\u3089\u308c\u305f\u5404 \\(\\rho\\) \u5024\u306b\u5bfe\u3059\u308b\u9593\u63a5\u52b9\u679c\u304a\u3088\u3073\u76f4\u63a5\u52b9\u679c\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3001\u771f\u306e\u5024\uff08\u8a2d\u5b9a\u3057\u305f \\(\\alpha_1 \\times \\beta_2\\) \u304a\u3088\u3073 \\(\\beta_1\\)\uff09\u3068\u6bd4\u8f03\u3057\u3066\u3044\u307e\u3059\u3002</p> </li> </ul> <p>\u3053\u306e\u611f\u5ea6\u5206\u6790\u306e\u30c7\u30e2\u306f\u3001\u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u306b\u8d77\u56e0\u3059\u308b\u8aa4\u5dee\u9805\u9593\u306e\u76f8\u95a2\u304c\u3001\u5a92\u4ecb\u5206\u6790\u306e\u52b9\u679c\u63a8\u5b9a\u306b\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u304b\u3092\u76f4\u611f\u7684\u306b\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u4e00\u4f8b\u3067\u3059\u3002 \u5b9f\u969b\u306e\u5fdc\u7528\u3067\u306f\u3001\u3088\u308a\u591a\u304f\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u53cd\u5fa9\u3084\u4ed6\u306e\u611f\u5ea6\u5206\u6790\u624b\u6cd5\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u63a8\u5b9a\u306e\u9811\u5065\u6027\u3092\u691c\u8a3c\u3059\u308b\u3053\u3068\u304c\u6c42\u3081\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#8","title":"\u7b2c8\u7ae0 \u8b70\u8ad6\u3068\u4eca\u5f8c\u306e\u8ab2\u984c","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u3053\u308c\u307e\u3067\u306b\u793a\u3057\u305f\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u7406\u8ad6\u7684\u67a0\u7d44\u307f\u3001\u63a8\u5b9a\u624b\u6cd5\u3001\u611f\u5ea6\u5206\u6790\u3001\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u53ca\u3073\u5b9f\u8a3c\u7814\u7a76\u306e\u7d50\u679c\u3092\u8e0f\u307e\u3048\u3001\u5206\u6790\u7d50\u679c\u306e\u89e3\u91c8\u3068\u305d\u306e\u9650\u754c\u3001\u4e26\u3073\u306b\u4eca\u5f8c\u306e\u65b9\u6cd5\u8ad6\u7684\u767a\u5c55\u3068\u7814\u7a76\u5c55\u671b\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#81","title":"8.1 \u5206\u6790\u7d50\u679c\u306e\u89e3\u91c8\u3068\u9650\u754c","text":""},{"location":"research/note/causal-mediation-analysis/#811","title":"8.1.1 \u52b9\u679c\u306e\u89e3\u91c8\u3068\u305d\u306e\u610f\u7fa9","text":"<p>\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306b\u3088\u308a\u3001\u51e6\u7f6e\u304c\u7d50\u679c\u306b\u4e0e\u3048\u308b\u7dcf\u52b9\u679c\u3092\u76f4\u63a5\u52b9\u679c\uff08\\(NDE\\)\uff09\u3068\u9593\u63a5\u52b9\u679c\uff08\\(NIE\\)\uff09\u306b\u5206\u89e3\u3059\u308b\u3053\u3068\u3067\u3001\u4ecb\u5165\u306e\u30e1\u30ab\u30cb\u30ba\u30e0\u3092\u8a73\u7d30\u306b\u89e3\u660e\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3057\u305f\u3002 \u3053\u306e\u52b9\u679c\u306e\u5206\u89e3\u306f\u3001\u653f\u7b56\u8a55\u4fa1\u3084\u81e8\u5e8a\u8a66\u9a13\u3001\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u306b\u304a\u3044\u3066\u3001\u3069\u306e\u30d7\u30ed\u30bb\u30b9\u304c\u4e3b\u8981\u306a\u5f71\u97ff\u3092\u62c5\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u660e\u78ba\u306b\u793a\u3059\u305f\u3081\u306e\u6709\u7528\u306a\u624b\u6cd5\u3068\u3057\u3066\u6ce8\u76ee\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#812","title":"8.1.2 \u9650\u754c\u3068\u4eee\u5b9a\u306e\u53b3\u5bc6\u6027","text":"<p>\u4e00\u65b9\u3067\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u63a8\u5b9a\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u9650\u754c\u3084\u4eee\u5b9a\u306b\u5f37\u304f\u4f9d\u5b58\u3057\u3066\u3044\u307e\u3059\u3002</p> <ul> <li> <p>\u8b58\u5225\u4eee\u5b9a\u306e\u6210\u7acb:   \u7121\u4ea4\u7d61\u6027\u3084SUTVA\u3001Sequential Ignorability\u306a\u3069\u306e\u4eee\u5b9a\u304c\u73fe\u5b9f\u306e\u30c7\u30fc\u30bf\u306b\u304a\u3044\u3066\u53b3\u5bc6\u306b\u6210\u7acb\u3059\u308b\u3053\u3068\u306f\u96e3\u3057\u3044\u5834\u5408\u304c\u591a\u304f\u3001\u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u56e0\u5b50\u304c\u5b58\u5728\u3059\u308b\u3068\u52b9\u679c\u63a8\u5b9a\u306b\u30d0\u30a4\u30a2\u30b9\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u30e2\u30c7\u30eb\u306e\u4eee\u5b9a\u3068\u67d4\u8edf\u6027:   \u56de\u5e30\u30e2\u30c7\u30eb\u3084SEM\u306a\u3069\u306e\u63a8\u5b9a\u624b\u6cd5\u306f\u3001\u7dda\u5f62\u6027\u3084\u6b63\u898f\u6027\u306a\u3069\u306e\u7d71\u8a08\u7684\u4eee\u5b9a\u306b\u4f9d\u5b58\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u4eee\u5b9a\u304c\u90e8\u5206\u7684\u306b\u7834\u3089\u308c\u308b\u5834\u5408\u3001\u52b9\u679c\u306e\u89e3\u91c8\u306b\u4e0d\u78ba\u5b9f\u6027\u304c\u4f34\u3044\u307e\u3059\u3002</p> </li> <li> <p>\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u3068\u7d71\u8a08\u7684\u691c\u51fa\u529b:   \u5c0f\u6a19\u672c\u306e\u5834\u5408\u3001\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306b\u3088\u308b\u4fe1\u983c\u533a\u9593\u306e\u63a8\u5b9a\u3084\u611f\u5ea6\u5206\u6790\u306e\u7d50\u679c\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u3001\u5341\u5206\u306a\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u304c\u6c42\u3081\u3089\u308c\u307e\u3059\u3002</p> </li> </ul> <p>\u3053\u308c\u3089\u306e\u9650\u754c\u3092\u8a8d\u8b58\u3057\u305f\u4e0a\u3067\u3001\u611f\u5ea6\u5206\u6790\u3084\u5916\u90e8\u60c5\u5831\u306e\u6d3b\u7528\u306b\u3088\u308a\u3001\u4eee\u5b9a\u9055\u53cd\u306e\u5f71\u97ff\u3092\u8a55\u4fa1\u30fb\u88dc\u6b63\u3059\u308b\u8a66\u307f\u304c\u4e0d\u53ef\u6b20\u3067\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#82","title":"8.2 \u65b9\u6cd5\u8ad6\u306e\u767a\u5c55\u3068\u4eca\u5f8c\u306e\u7814\u7a76\u5c55\u671b","text":""},{"location":"research/note/causal-mediation-analysis/#821","title":"8.2.1 \u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3078\u306e\u5bfe\u5fdc","text":"<p>\u73fe\u4ee3\u306e\u5fdc\u7528\u7814\u7a76\u3067\u306f\u3001\u907a\u4f1d\u60c5\u5831\u3084\u30d3\u30c3\u30b0\u30c7\u30fc\u30bf\u89e3\u6790\u306e\u3088\u3046\u306b\u9ad8\u6b21\u5143\u304b\u3064\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u69cb\u9020\u304c\u4e00\u822c\u7684\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002 \u3053\u308c\u306b\u5bfe\u3057\u3066\u3001\u5f93\u6765\u306e\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u624b\u6cd5\u306f\u3001\u5909\u6570\u9078\u629e\u3084\u6b21\u5143\u524a\u6e1b\u3001\u6a5f\u68b0\u5b66\u7fd2\u624b\u6cd5\u3068\u306e\u7d71\u5408\u306a\u3069\u3001\u65b0\u305f\u306a\u7d71\u8a08\u7684\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u6c42\u3081\u3089\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#822","title":"8.2.2 \u975e\u7dda\u5f62\u6027\u3068\u76f8\u4e92\u4f5c\u7528\u306e\u53d6\u308a\u6271\u3044","text":"<p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3067\u306f\u3001\u51e6\u7f6e\u3001\u5a92\u4ecb\u5909\u6570\u3001\u7d50\u679c\u306e\u95a2\u4fc2\u304c\u5fc5\u305a\u3057\u3082\u7dda\u5f62\u3067\u306a\u3044\u5834\u5408\u3084\u3001\u8907\u6570\u306e\u4ea4\u4e92\u4f5c\u7528\u304c\u5b58\u5728\u3059\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3067\u3059\u3002 \u3053\u308c\u306b\u5bfe\u5fdc\u3059\u308b\u305f\u3081\u3001\u975e\u7dda\u5f62\u30e2\u30c7\u30eb\u3084\u534a\u30d1\u30e9\u30e1\u30c8\u30ea\u30c3\u30af\u30fb\u975e\u30d1\u30e9\u30e1\u30c8\u30ea\u30c3\u30af\u624b\u6cd5\u306e\u958b\u767a\u304c\u9032\u3080\u3053\u3068\u304c\u671f\u5f85\u3055\u308c\u307e\u3059\u3002 \u7279\u306b\u3001\u6a5f\u68b0\u5b66\u7fd2\u6280\u8853\uff08\u4f8b\uff1a\u30d6\u30fc\u30b9\u30c6\u30a3\u30f3\u30b0\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306a\u3069\uff09\u3092\u53d6\u308a\u5165\u308c\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u67d4\u8edf\u306a\u56e0\u679c\u63a8\u8ad6\u304c\u53ef\u80fd\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#823","title":"8.2.3 \u52d5\u7684\u56e0\u679c\u5a92\u4ecb\u5206\u6790","text":"<p>\u5f93\u6765\u306e\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306f\u3001\u9759\u7684\u306a\u4ecb\u5728\u52b9\u679c\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u6642\u9593\u306e\u7d4c\u904e\u306b\u4f34\u3046\u52b9\u679c\u306e\u5909\u52d5\u3084\u52d5\u7684\u306a\u4ecb\u5728\u30e1\u30ab\u30cb\u30ba\u30e0\u306e\u89e3\u660e\u304c\u6c42\u3081\u3089\u308c\u3066\u3044\u307e\u3059\u3002 \u30d1\u30cd\u30eb\u30c7\u30fc\u30bf\u3084\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u52d5\u7684\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u624b\u6cd5\u306e\u78ba\u7acb\u306f\u3001\u4eca\u5f8c\u306e\u7814\u7a76\u8ab2\u984c\u3068\u3057\u3066\u91cd\u8981\u306a\u30c6\u30fc\u30de\u3068\u306a\u308b\u3067\u3057\u3087\u3046\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#824","title":"8.2.4 \u8b58\u5225\u6226\u7565\u306e\u62e1\u5145","text":"<p>\u65e2\u5b58\u306e\u8b58\u5225\u4eee\u5b9a\u306e\u53b3\u683c\u6027\u306b\u4f9d\u5b58\u3057\u306a\u3044\u3001\u65b0\u305f\u306a\u8b58\u5225\u6226\u7565\u306e\u6a21\u7d22\u3082\u5fc5\u8981\u3067\u3059\u3002 \u4f8b\u3048\u3070\u3001\u81ea\u7136\u5b9f\u9a13\u3084\u30a4\u30f3\u30b9\u30c8\u30eb\u30e1\u30f3\u30bf\u30eb\u5909\u6570\u3092\u6d3b\u7528\u3057\u305f\u30a2\u30d7\u30ed\u30fc\u30c1\u3001\u307e\u305f\u306f\u30e9\u30f3\u30c0\u30e0\u5316\u5b9f\u9a13\u3068\u89b3\u5bdf\u7814\u7a76\u3092\u7d71\u5408\u3059\u308b\u30cf\u30a4\u30d6\u30ea\u30c3\u30c9\u30fb\u30c7\u30b6\u30a4\u30f3\u304c\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u7cbe\u5ea6\u5411\u4e0a\u306b\u5bc4\u4e0e\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#83","title":"8.3 \u7d50\u8ad6\u3068\u4eca\u5f8c\u306e\u65b9\u5411\u6027","text":"<p>\u672c\u7a3f\u3067\u8b70\u8ad6\u3057\u305f\u3088\u3046\u306b\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306f\u4ecb\u5165\u306e\u30e1\u30ab\u30cb\u30ba\u30e0\u89e3\u660e\u306b\u304a\u3044\u3066\u6975\u3081\u3066\u6709\u7528\u306a\u30c4\u30fc\u30eb\u3067\u3042\u308b\u4e00\u65b9\u3001\u305d\u306e\u63a8\u5b9a\u306b\u306f\u53b3\u5bc6\u306a\u4eee\u5b9a\u3084\u9069\u5207\u306a\u30e2\u30c7\u30eb\u8a2d\u5b9a\u304c\u4e0d\u53ef\u6b20\u3067\u3059\u3002 \u4eca\u5f8c\u306f\u3001\u30c7\u30fc\u30bf\u306e\u591a\u69d8\u5316\u306b\u4f34\u3046\u9ad8\u6b21\u5143\u30fb\u975e\u7dda\u5f62\u554f\u984c\u3084\u52d5\u7684\u306a\u52b9\u679c\u306e\u8a55\u4fa1\u3001\u305d\u3057\u3066\u65b0\u305f\u306a\u8b58\u5225\u6226\u7565\u306e\u958b\u767a\u304c\u9032\u3080\u3053\u3068\u3067\u3001\u3088\u308a\u5805\u7262\u304b\u3064\u5b9f\u8df5\u7684\u306a\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u5b9f\u73fe\u304c\u671f\u5f85\u3055\u308c\u307e\u3059\u3002 \u6700\u7d42\u7684\u306b\u306f\u3001\u3053\u308c\u3089\u306e\u767a\u5c55\u304c\u5404\u5fdc\u7528\u5206\u91ce\u306b\u304a\u3051\u308b\u653f\u7b56\u8a55\u4fa1\u3084\u4ecb\u5165\u52b9\u679c\u306e\u6b63\u78ba\u306a\u89e3\u660e\u3001\u3072\u3044\u3066\u306f\u793e\u4f1a\u3084\u533b\u7642\u73fe\u5834\u3067\u306e\u610f\u601d\u6c7a\u5b9a\u306e\u8cea\u5411\u4e0a\u306b\u5927\u304d\u304f\u5bc4\u4e0e\u3059\u308b\u3053\u3068\u304c\u671f\u5f85\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#9","title":"\u7b2c9\u7ae0 \u7d50\u8ad6","text":""},{"location":"research/note/causal-mediation-analysis/#91","title":"9.1 \u672c\u7a3f\u306e\u307e\u3068\u3081","text":"<p>\u672c\u7a3f\u3067\u306f\u3001\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u7406\u8ad6\u7684\u57fa\u76e4\u304b\u3089\u6700\u65b0\u306e\u63a8\u5b9a\u624b\u6cd5\u3001\u3055\u3089\u306b\u306f\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3084\u5b9f\u8a3c\u7814\u7a76\u306b\u81f3\u308b\u307e\u3067\u3001\u5e45\u5e83\u3044\u8996\u70b9\u3067\u89e3\u8aac\u3092\u884c\u3044\u307e\u3057\u305f\u3002 \u307e\u305a\u3001\u56e0\u679c\u63a8\u8ad6\u306e\u57fa\u672c\u6982\u5ff5\u304a\u3088\u3073\u53cd\u4e8b\u5b9f\u7684\u67a0\u7d44\u307f\u3092\u5c0e\u5165\u3057\u3001\u51e6\u7f6e\u304c\u7d50\u679c\u306b\u53ca\u307c\u3059\u7dcf\u52b9\u679c\u3092\u3001\u5a92\u4ecb\u5909\u6570\u3092\u4ecb\u3059\u308b\u76f4\u63a5\u52b9\u679c\u3068\u9593\u63a5\u52b9\u679c\u306b\u5206\u89e3\u3059\u308b\u65b9\u6cd5\u3092\u660e\u3089\u304b\u306b\u3057\u307e\u3057\u305f\u3002 \u3055\u3089\u306b\u3001\u8b58\u5225\u4eee\u5b9a\uff08\u7121\u4ea4\u7d61\u6027\u3001SUTVA\u3001Sequential Ignorability\uff09\u3084\u3001\u3053\u308c\u3089\u306e\u4eee\u5b9a\u306e\u53b3\u5bc6\u306a\u691c\u8a3c\u306e\u91cd\u8981\u6027\u3092\u8b70\u8ad6\u3059\u308b\u3068\u3068\u3082\u306b\u3001\u56de\u5e30\u30e2\u30c7\u30eb\u3001\u50be\u5411\u30b9\u30b3\u30a2\u6cd5\u3001\u69cb\u9020\u65b9\u7a0b\u5f0f\u30e2\u30c7\u30ea\u30f3\u30b0\uff08SEM\uff09\u306a\u3069\u3001\u591a\u69d8\u306a\u63a8\u5b9a\u624b\u6cd5\u306e\u9069\u7528\u4f8b\u3092\u793a\u3057\u307e\u3057\u305f\u3002 \u52a0\u3048\u3066\u3001\u672a\u89b3\u6e2c\u306e\u4ea4\u7d61\u56e0\u5b50\u306e\u5f71\u97ff\u3092\u8a55\u4fa1\u3059\u308b\u611f\u5ea6\u5206\u6790\u306e\u5b9f\u65bd\u65b9\u6cd5\u3084\u3001\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u304a\u3088\u3073\u5b9f\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u30b1\u30fc\u30b9\u30b9\u30bf\u30c7\u30a3\u306b\u3088\u3063\u3066\u3001\u7406\u8ad6\u3068\u5b9f\u8df5\u306e\u4e21\u9762\u304b\u3089\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u6709\u7528\u6027\u3068\u9650\u754c\u306b\u3064\u3044\u3066\u691c\u8a3c\u3057\u307e\u3057\u305f\u3002</p>"},{"location":"research/note/causal-mediation-analysis/#92","title":"9.2 \u7814\u7a76\u3078\u306e\u793a\u5506","text":"<p>\u672c\u7a3f\u306e\u5185\u5bb9\u3068\u8b70\u8ad6\u3092\u8e0f\u307e\u3048\u3001\u4ee5\u4e0b\u306e\u70b9\u304c\u4eca\u5f8c\u306e\u7814\u7a76\u8ab2\u984c\u304a\u3088\u3073\u5fdc\u7528\u306b\u304a\u3044\u3066\u91cd\u8981\u3067\u3042\u308b\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002</p> <ul> <li> <p>\u8b58\u5225\u4eee\u5b9a\u306e\u691c\u8a3c\u3068\u88dc\u6b63:   \u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306f\u3001\u7121\u4ea4\u7d61\u6027\u306a\u3069\u306e\u53b3\u683c\u306a\u4eee\u5b9a\u306b\u4f9d\u5b58\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u672a\u89b3\u6e2c\u4ea4\u7d61\u56e0\u5b50\u3078\u306e\u5bfe\u51e6\u3084\u611f\u5ea6\u5206\u6790\u306e\u3055\u3089\u306a\u308b\u7cbe\u7dfb\u5316\u304c\u6c42\u3081\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u9ad8\u6b21\u5143\u30fb\u975e\u7dda\u5f62\u30c7\u30fc\u30bf\u3078\u306e\u5bfe\u5fdc:   \u73fe\u5b9f\u306e\u30c7\u30fc\u30bf\u3067\u306f\u3001\u5f93\u6765\u306e\u7dda\u5f62\u30e2\u30c7\u30eb\u306e\u524d\u63d0\u304c\u6210\u308a\u7acb\u305f\u306a\u3044\u5834\u5408\u3082\u591a\u304f\u3001\u6a5f\u68b0\u5b66\u7fd2\u6280\u8853\u3084\u975e\u30d1\u30e9\u30e1\u30c8\u30ea\u30c3\u30af\u624b\u6cd5\u3068\u306e\u878d\u5408\u306b\u3088\u308a\u3001\u67d4\u8edf\u304b\u3064\u9ad8\u7cbe\u5ea6\u306a\u56e0\u679c\u63a8\u8ad6\u304c\u53ef\u80fd\u306b\u306a\u308b\u3068\u671f\u5f85\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u52d5\u7684\u56e0\u679c\u52b9\u679c\u306e\u89e3\u660e:   \u6642\u7cfb\u5217\u3084\u30d1\u30cd\u30eb\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u52d5\u7684\u306a\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u65b9\u6cd5\u8ad6\u306e\u767a\u5c55\u306b\u3088\u308a\u3001\u6642\u9593\u7684\u5909\u52d5\u3092\u8003\u616e\u3057\u305f\u4ecb\u5728\u30e1\u30ab\u30cb\u30ba\u30e0\u306e\u89e3\u660e\u304c\u4eca\u5f8c\u306e\u7814\u7a76\u306e\u5927\u304d\u306a\u8ab2\u984c\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u7d71\u5408\u7684\u7814\u7a76\u30c7\u30b6\u30a4\u30f3\u306e\u63a8\u9032:   \u89b3\u5bdf\u7814\u7a76\u3068\u5b9f\u9a13\u7814\u7a76\u306e\u30cf\u30a4\u30d6\u30ea\u30c3\u30c9\u30fb\u30c7\u30b6\u30a4\u30f3\u3084\u3001\u8907\u6570\u306e\u5206\u6790\u624b\u6cd5\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u7d71\u5408\u7684\u30a2\u30d7\u30ed\u30fc\u30c1\u306b\u3088\u308a\u3001\u56e0\u679c\u52b9\u679c\u306e\u63a8\u5b9a\u7cbe\u5ea6\u3068\u89e3\u91c8\u306e\u4fe1\u983c\u6027\u304c\u5411\u4e0a\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> </ul> <p>\u672c\u7a3f\u3067\u793a\u3057\u305f\u7406\u8ad6\u7684\u67a0\u7d44\u307f\u304a\u3088\u3073\u5b9f\u8a3c\u7684\u691c\u8a3c\u306f\u3001\u4eca\u5f8c\u306e\u56e0\u679c\u5a92\u4ecb\u5206\u6790\u306e\u767a\u5c55\u306b\u5411\u3051\u305f\u91cd\u8981\u306a\u57fa\u76e4\u3068\u306a\u308b\u3068\u3068\u3082\u306b\u3001\u533b\u5b66\u3001\u793e\u4f1a\u79d1\u5b66\u3001\u7d4c\u6e08\u5b66\u306a\u3069\u3055\u307e\u3056\u307e\u306a\u5206\u91ce\u306b\u304a\u3051\u308b\u4ecb\u5165\u52b9\u679c\u306e\u89e3\u660e\u3068\u653f\u7b56\u8a55\u4fa1\u306b\u5927\u304d\u304f\u5bc4\u4e0e\u3059\u308b\u3053\u3068\u304c\u671f\u5f85\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/consistency-of-soft-decision-trees/","title":"Soft Regression Tree (SRT)","text":"<p>\u3053\u3053\u3067\u306f\u3001Soft Regression Trees \u306e\u7406\u8ad6\u3068\u3001\u305d\u306e\u62e1\u5f35\u306b\u3064\u3044\u3066\u307e\u3068\u3081\u308b\u3002\u307e\u305f\u3001Soft Regression Tree \u3067\u306f\u3001Hinton et al., (2017) \u306b\u3088\u3063\u3066\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u542b\u307e\u308c\u308b\u5909\u6570\u3092\u591a\u6b21\u5143\u306b\u62e1\u5f35\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u3063\u3066\u3044\u308b\u304c\u3001\u5b9f\u9a13\u3092\u884c\u3063\u305f\u3068\u3053\u308d\u3001\u5358\u4e00\u5909\u6570\u306b\u4f9d\u5b58\u3059\u308b\u6728\u306e\u307b\u3046\u304censemble \u3092\u884c\u3046\u5834\u5408\u306b\u306f\u6027\u80fd\u304c\u9ad8\u3044\u3053\u3068\u304c\u308f\u304b\u308b\u3002\u3053\u308c\u306f\u3001\u6728\u540c\u58eb\u306e\u76f8\u95a2\u304c\u5927\u304d\u304f\u306a\u308b\u305f\u3081\u3001ensemble \u3092\u3059\u308b\u3053\u3068\u3067\u6027\u80fd\u5411\u4e0a\u304c\u8d77\u3053\u3089\u306a\u3044\u3068\u3044\u3046\u3075\u3046\u306b\u8003\u3048\u308b\u306e\u304c\u59a5\u5f53\u3067\u3042\u308b\u3002</p> <p>\u306a\u306e\u3067\u3001\u5358\u4e00\u306eSRT\u306b\u57fa\u3065\u304f\u4e88\u6e2c\u3067\u306f\u3001sigmoid \u3092\u7528\u3044\u3066\u6ed1\u3089\u304b\u306b\u4e88\u6e2c\u3092\u884c\u3044\u3001\u4e00\u65b9\u3067boosting \u3092\u884c\u3046\u5834\u5408\u306b\u306fsigmoid \u95a2\u6570\u306b\u542b\u307e\u308c\u308b\u5909\u6570\u306f\u5168\u5909\u6570\u3067\u306f\u306a\u304f\u3001\u5909\u6570\u30bb\u30ec\u30af\u30bf\u30fc\u306a\u3069\u3092\u5b9f\u88c5\u3059\u308b\u3053\u3068\u3067\u3001\u758e\u306a\u6728\u3092\u4f5c\u308b\u3053\u3068\u304c\u5fc5\u8981\u3068\u306a\u308b\u3002\u3088\u3063\u3066\u65b9\u5411\u6027\u3068\u3057\u3066\u306f\u3001SoftBART\u306b\u8fd1\u3044\u65b9\u91dd\u3092\u53d6\u3089\u306a\u3044\u9650\u308a\u306f\u3001\u5b9f\u8cea\u7684\u306a\u7cbe\u5ea6\u304c\u4e0a\u6607\u3057\u306a\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u3066\u3044\u308b\u3002</p> <p>\u3064\u307e\u308a\u958b\u767a\u306e\u65b9\u91dd\u3068\u3057\u3066\u306f\u3001 - [\u512a\u5148\u9806\u4f4d\u9ad8] Global \u6b63\u5247\u5316\u3092\u542b\u3080\u3088\u3046\u306a\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u9593\u6570\u306b\u5168\u5909\u6570\u3092\u7528\u3044\u305f Soft regression trees - [\u512a\u5148\u9806\u4f4d\u9ad8] Global \u6b63\u5247\u5316\u3092\u542b\u3080\u3088\u3046\u306a\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u9593\u6570\u306b\u5358\u4e00\u5909\u6570\u306e\u307f\u7528\u3044\u305f Soft regression trees - [\u512a\u5148\u9806\u4f4d\u4e2d] \u5358\u4e00\u5909\u6570\u306b\u3088\u308b Soft regression trees \u306b\u3001ensemble \u6642\u306b\u5168\u4f53\u3092\u6b63\u5247\u5316\u3059\u308b\u3088\u3046\u306a SRT Boosting  - [\u512a\u5148\u9806\u4f4d\u4f4e] \u5206\u5c90\u30922\u5206\u6728\u3067\u306f\u306a\u304f\u3001\u591a\u5206\u6728\u306b\u5909\u66f4\u3059\u308b\uff08\u305f\u3060\u3057\u5206\u5272\u306e\u500b\u6570\u3092\u3001\u6c7a\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\uff09</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#1-soft-decision-regression-trees","title":"1. Soft Decision / Regression Trees","text":""},{"location":"research/note/consistency-of-soft-decision-trees/#11","title":"1.1 \u30e2\u30c7\u30eb\u69cb\u9020","text":"<p>Soft Regression Tree\u306f\u3001\\(d\\)\u306e\u6df1\u3055\u3092\u6301\u3064\u5b8c\u5168\u4e8c\u5206\u6728\u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3067\u304d\u307e\u3059\u3002\u3053\u306e\u6728\u306f\\(2^d - 1\\)\u500b\u306e\u5185\u90e8\u30ce\u30fc\u30c9\u3068\\(2^d\\)\u500b\u306e\u8449\u30ce\u30fc\u30c9\u3092\u6301\u3061\u307e\u3059\u3002\u5404\u5185\u90e8\u30ce\u30fc\u30c9\u306f\u3001\u5165\u529b\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x} \\in \\mathbb{R}^p\\) \u306e\u7dda\u5f62\u5909\u63db\u306b\u57fa\u3065\u304f\u5206\u5272\u95a2\u6570\u3092\u6301\u3061\u307e\u3059\uff1a</p> \\[s_j(\\mathbf{x}; \\mathbf{w}_j, b_j) = \\sigma\\left(\\frac{\\mathbf{w}_j^T \\mathbf{x} + b_j}{\\tau}\\right)\\] <p>\u3053\u3053\u3067\u3001 - \\(\\mathbf{w}_j \\in \\mathbb{R}^p\\) \u306f\u5185\u90e8\u30ce\u30fc\u30c9 \\(j\\) \u306e\u91cd\u307f\u30d9\u30af\u30c8\u30eb - \\(b_j \\in \\mathbb{R}\\) \u306f\u30d0\u30a4\u30a2\u30b9\u9805 - \\(\\sigma(\\cdot)\\) \u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) - \\(\\tau &gt; 0\\) \u306f\u6e29\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#12","title":"1.2 \u8449\u30ce\u30fc\u30c9\u3078\u306e\u7d4c\u8def\u78ba\u7387","text":"<p>\u5165\u529b \\(\\mathbf{x}\\) \u304c\u8449\u30ce\u30fc\u30c9 \\(l\\) \u306b\u5230\u9054\u3059\u308b\u78ba\u7387 \\(\\mu_l(\\mathbf{x})\\) \u306f\u3001\u6839\u30ce\u30fc\u30c9\u304b\u3089\u8449 \\(l\\) \u307e\u3067\u306e\u7d4c\u8def\u4e0a\u306e\u5404\u5206\u5c90\u78ba\u7387\u306e\u7a4d\u3068\u3057\u3066\u8a08\u7b97\u3055\u308c\u307e\u3059\uff1a</p> \\[\\mu_l(\\mathbf{x}) = \\prod_{j \\in \\mathcal{P}_l^L} s_j(\\mathbf{x}) \\prod_{j \\in \\mathcal{P}_l^R} (1 - s_j(\\mathbf{x}))\\] <p>\u3053\u3053\u3067\u3001 - \\(\\mathcal{P}_l^L\\) \u306f\u8449 \\(l\\) \u3078\u306e\u7d4c\u8def\u4e0a\u3067\u5de6\u306b\u5206\u5c90\u3059\u308b\u5185\u90e8\u30ce\u30fc\u30c9\u306e\u96c6\u5408 - \\(\\mathcal{P}_l^R\\) \u306f\u8449 \\(l\\) \u3078\u306e\u7d4c\u8def\u4e0a\u3067\u53f3\u306b\u5206\u5c90\u3059\u308b\u5185\u90e8\u30ce\u30fc\u30c9\u306e\u96c6\u5408</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#13","title":"1.3 \u4e88\u6e2c\u5024","text":"<p>\u5165\u529b \\(\\mathbf{x}\\) \u306b\u5bfe\u3059\u308b\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u5024 \\(f(\\mathbf{x})\\) \u306f\u3001\u5404\u8449\u30ce\u30fc\u30c9\u306e\u4e88\u6e2c\u5024 \\(v_l\\) \u3092\u7d4c\u8def\u78ba\u7387 \\(\\mu_l(\\mathbf{x})\\) \u3067\u91cd\u307f\u4ed8\u3051\u3057\u305f\u548c\u3068\u3057\u3066\u8a08\u7b97\u3055\u308c\u307e\u3059\uff1a</p> \\[f(\\mathbf{x}) = \\sum_{l=1}^{2^d} \\mu_l(\\mathbf{x}) v_l\\]"},{"location":"research/note/consistency-of-soft-decision-trees/#2","title":"2. \u640d\u5931\u95a2\u6570\u3068\u6b63\u5247\u5316\u9805","text":""},{"location":"research/note/consistency-of-soft-decision-trees/#21","title":"2.1 \u57fa\u672c\u640d\u5931\u95a2\u6570","text":"<p>\u8a13\u7df4\u30c7\u30fc\u30bf \\(\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n\\) \u306b\u5bfe\u3059\u308b\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\uff08MSE\uff09\u640d\u5931\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\uff1a</p> \\[\\mathcal{L}_{\\text{MSE}} = \\frac{1}{n} \\sum_{i=1}^n (f(\\mathbf{x}_i) - y_i)^2\\]"},{"location":"research/note/consistency-of-soft-decision-trees/#22","title":"2.2 \u5f93\u6765\u306e\u6b63\u5247\u5316\u9805","text":""},{"location":"research/note/consistency-of-soft-decision-trees/#221","title":"2.2.1 \u91cd\u307f\u6b63\u5247\u5316","text":"<p>\u500b\u3005\u306e\u30ce\u30fc\u30c9\u306e\u91cd\u307f\u306b\u5bfe\u3059\u308b\u4e00\u822c\u7684\u306a\u6b63\u5247\u5316\u9805\uff1a</p> \\[\\mathcal{R}_{\\text{weights}} = \\lambda_1 \\sum_{j=1}^{2^d-1} \\|\\mathbf{w}_j\\|_1 + \\lambda_2 \\sum_{j=1}^{2^d-1} \\|\\mathbf{w}_j\\|_2^2\\] <p>\u3053\u3053\u3067\u3001\\(\\|\\mathbf{w}_j\\|_1 = \\sum_{k=1}^p |w_{jk}|\\) \u306f \\(L_1\\) \u30ce\u30eb\u30e0\u3001\\(\\|\\mathbf{w}_j\\|_2^2 = \\sum_{k=1}^p w_{jk}^2\\) \u306f \\(L_2\\) \u30ce\u30eb\u30e0\u306e\u4e8c\u4e57\u3067\u3059\u3002</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#222","title":"2.2.2 \u69cb\u9020\u7684\u8907\u96d1\u3055\u30da\u30ca\u30eb\u30c6\u30a3","text":"<p>\u30a2\u30af\u30c6\u30a3\u30d6\u30ce\u30fc\u30c9\u6570\u306b\u57fa\u3065\u304f\u30da\u30ca\u30eb\u30c6\u30a3\uff1a</p> \\[\\mathcal{R}_{\\text{complexity}} = \\lambda_c \\sum_{j=1}^{2^d-1} \\mathbb{I}(\\|\\mathbf{w}_j\\|_2 &gt; \\epsilon) + \\lambda_l \\sum_{l=1}^{2^d} \\mathbb{I}(|v_l| &gt; \\epsilon)\\] <p>\u3053\u3053\u3067\u3001\\(\\mathbb{I}(\\cdot)\\) \u306f\u6307\u793a\u95a2\u6570\u3067\u3001\u6761\u4ef6\u304c\u771f\u306e\u5834\u5408\u306b1\u3001\u507d\u306e\u5834\u5408\u306b0\u3092\u8fd4\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#3","title":"3. \u30b0\u30ed\u30fc\u30d0\u30eb\u7279\u5fb4\u91cf\u6b63\u5247\u5316","text":""},{"location":"research/note/consistency-of-soft-decision-trees/#31","title":"3.1 \u7406\u8ad6\u7684\u5b9a\u5f0f\u5316","text":"<p>\u63d0\u6848\u3059\u308b\u30b0\u30ed\u30fc\u30d0\u30eb\u7279\u5fb4\u91cf\u6b63\u5247\u5316\u306f\u3001\u6728\u5168\u4f53\u3067\u306e\u5404\u7279\u5fb4\u91cf\u306e\u5f71\u97ff\u3092\u5236\u5fa1\u3057\u307e\u3059\u3002\u7279\u5fb4\u91cf \\(k\\) (\\(k = 1, 2, \\ldots, p\\)) \u306b\u5bfe\u3057\u3066\u3001\u3059\u3079\u3066\u306e\u5185\u90e8\u30ce\u30fc\u30c9\u306b\u304a\u3051\u308b\u91cd\u307f \\(w_{jk}\\) \u306e\u7d76\u5bfe\u5024\u306e\u5408\u8a08\u3092\u8003\u3048\u307e\u3059\uff1a</p> \\[g_k = \\sum_{j=1}^{2^d-1} |w_{jk}|\\] <p>\u3053\u308c\u306f\u3001\u7279\u5fb4\u91cf \\(k\\) \u306e\u30e2\u30c7\u30eb\u5168\u4f53\u3067\u306e\u4f7f\u7528\u983b\u5ea6\u3068\u91cd\u8981\u5ea6\u306e\u6307\u6a19\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#32-l_1","title":"3.2 \u30b0\u30ed\u30fc\u30d0\u30eb\u7279\u5fb4\u91cf \\(L_1\\) \u6b63\u5247\u5316","text":"<p>\u30b0\u30ed\u30fc\u30d0\u30eb\u7279\u5fb4\u91cf \\(L_1\\) \u6b63\u5247\u5316\u306f\u3001\u7279\u5fb4\u91cf\u5168\u4f53\u306e\u4f7f\u7528\u3092\u758e\u306b\u3059\u308b\u305f\u3081\u306b\u9069\u7528\u3055\u308c\u307e\u3059\uff1a</p> \\[\\mathcal{R}_{\\text{global-}L_1} = \\lambda_{\\text{global-}L_1} \\sum_{k=1}^p g_k = \\lambda_{\\text{global-}L_1} \\sum_{k=1}^p \\sum_{j=1}^{2^d-1} |w_{jk}|\\] <p>\u3053\u306e\u6b63\u5247\u5316\u9805\u306f\u3001\u3059\u3079\u3066\u306e\u5185\u90e8\u30ce\u30fc\u30c9\u306b\u308f\u305f\u308b\u7279\u5fb4\u91cf \\(k\\) \u306e\u91cd\u307f\u306e\u7d76\u5bfe\u5024\u306e\u5408\u8a08\u306b\u5bfe\u3057\u3066\u30da\u30ca\u30eb\u30c6\u30a3\u3092\u8ab2\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30e2\u30c7\u30eb\u5168\u4f53\u3067\u4e0d\u8981\u306a\u7279\u5fb4\u91cf\u306e\u91cd\u307f\u3092\u30bc\u30ed\u306b\u7e2e\u5c0f\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#33-l_2","title":"3.3 \u30b0\u30ed\u30fc\u30d0\u30eb\u7279\u5fb4\u91cf \\(L_2\\) \u6b63\u5247\u5316","text":"<p>\u30b0\u30ed\u30fc\u30d0\u30eb\u7279\u5fb4\u91cf \\(L_2\\) \u6b63\u5247\u5316\u306f\u3001\u7279\u5fb4\u91cf\u306e\u5f71\u97ff\u3092\u5747\u4e00\u306b\u6291\u5236\u3059\u308b\u305f\u3081\u306b\u9069\u7528\u3055\u308c\u307e\u3059\uff1a</p> \\[\\mathcal{R}_{\\text{global-}L_2} = \\lambda_{\\text{global-}L_2} \\sum_{k=1}^p \\sqrt{\\sum_{j=1}^{2^d-1} w_{jk}^2}\\] <p>\u3053\u308c\u306f\u5404\u7279\u5fb4\u91cf\u306b\u5bfe\u3059\u308b\u30b0\u30eb\u30fc\u30d7Lasso\u578b\u306e\u6b63\u5247\u5316\u3067\u3042\u308a\u3001\u4e0d\u8981\u306a\u7279\u5fb4\u91cf\u3092\u5b8c\u5168\u306b\u6392\u9664\u3057\u3064\u3064\u3001\u91cd\u8981\u306a\u7279\u5fb4\u91cf\u306e\u91cd\u307f\u306f\u4fdd\u6301\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#4","title":"4. \u6700\u7d42\u7684\u306a\u6700\u9069\u5316\u554f\u984c","text":"<p>\u3059\u3079\u3066\u306e\u640d\u5931\u9805\u3068\u6b63\u5247\u5316\u9805\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u6700\u7d42\u7684\u306a\u6700\u9069\u5316\u554f\u984c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff1a</p> \\[\\min_{\\mathbf{W}, \\mathbf{b}, \\mathbf{v}} \\mathcal{L}_{\\text{MSE}} + \\mathcal{R}_{\\text{weights}} + \\mathcal{R}_{\\text{complexity}} + \\mathcal{R}_{\\text{global-}L_1} + \\mathcal{R}_{\\text{global-}L_2}\\] <p>\u3053\u3053\u3067\u3001 - \\(\\mathbf{W} = \\{\\mathbf{w}_j\\}_{j=1}^{2^d-1}\\) \u306f\u3059\u3079\u3066\u306e\u5185\u90e8\u30ce\u30fc\u30c9\u306e\u91cd\u307f\u30d9\u30af\u30c8\u30eb - \\(\\mathbf{b} = \\{b_j\\}_{j=1}^{2^d-1}\\) \u306f\u3059\u3079\u3066\u306e\u5185\u90e8\u30ce\u30fc\u30c9\u306e\u30d0\u30a4\u30a2\u30b9\u9805 - \\(\\mathbf{v} = \\{v_l\\}_{l=1}^{2^d}\\) \u306f\u3059\u3079\u3066\u306e\u8449\u30ce\u30fc\u30c9\u306e\u4e88\u6e2c\u5024</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#5","title":"5. \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u7406\u8ad6\u7684\u89e3\u91c8","text":""},{"location":"research/note/consistency-of-soft-decision-trees/#51-tau","title":"5.1 \u6e29\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\tau\\)","text":"<p>\u6e29\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\tau\\) \u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306e\u50be\u304d\u3092\u5236\u5fa1\u3057\u307e\u3059\uff1a</p> \\[\\frac{\\partial s_j(\\mathbf{x})}{\\partial (\\mathbf{w}_j^T \\mathbf{x} + b_j)} = \\frac{1}{\\tau} s_j(\\mathbf{x})(1 - s_j(\\mathbf{x}))\\] <p>\\(\\tau \\to 0\\) \u306e\u3068\u304d\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306f\u30b9\u30c6\u30c3\u30d7\u95a2\u6570\u306b\u8fd1\u3065\u304d\u3001\u78ba\u5b9a\u7684\u306a\u6c7a\u5b9a\u5883\u754c\u3092\u5f62\u6210\u3057\u307e\u3059\u3002 \\(\\tau\\) \u304c\u5927\u304d\u3044\u3068\u304d\u3001\u5206\u5272\u306f\u3088\u308a\u6ed1\u3089\u304b\u3067\u78ba\u7387\u7684\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#52","title":"5.2 \u6b63\u5247\u5316\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u52b9\u679c","text":"<p>\u5404\u6b63\u5247\u5316\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u6700\u9069\u306a\u91cd\u307f \\(\\mathbf{w}_j^*\\) \u306b\u4e0e\u3048\u308b\u5f71\u97ff\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u7279\u5fb4\u3065\u3051\u3089\u308c\u307e\u3059\uff1a</p> <ul> <li>\\(\\lambda_1\\)\uff08\u500b\u5225 \\(L_1\\) \u6b63\u5247\u5316\uff09: \u5404\u30ce\u30fc\u30c9\u5185\u3067\u4e00\u90e8\u306e\u7279\u5fb4\u91cf\u306e\u91cd\u307f\u3092\u30bc\u30ed\u306b\u3057\u307e\u3059</li> <li>\\(\\lambda_2\\)\uff08\u500b\u5225 \\(L_2\\) \u6b63\u5247\u5316\uff09: \u5404\u30ce\u30fc\u30c9\u306e\u3059\u3079\u3066\u306e\u91cd\u307f\u3092\u5747\u4e00\u306b\u7e2e\u5c0f\u3057\u307e\u3059</li> <li>\\(\\lambda_{\\text{global-}L_1}\\): \u30e2\u30c7\u30eb\u5168\u4f53\u3067\u4e00\u90e8\u306e\u7279\u5fb4\u91cf\u306e\u91cd\u307f\u3092\u30bc\u30ed\u306b\u3057\u307e\u3059</li> <li>\\(\\lambda_{\\text{global-}L_2}\\): \u30e2\u30c7\u30eb\u5168\u4f53\u3067\u7279\u5fb4\u91cf\u306e\u5f71\u97ff\u3092\u5747\u4e00\u306b\u6291\u5236\u3057\u307e\u3059</li> </ul> <p>\u7279\u306b\u3001\u30b0\u30ed\u30fc\u30d0\u30eb \\(L_1\\) \u6b63\u5247\u5316\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\lambda_{\\text{global-}L_1}\\) \u306f\u3001\u4ee5\u4e0b\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u5834\u5408\u306b\u7279\u5fb4\u91cf \\(k\\) \u3092\u30e2\u30c7\u30eb\u5168\u4f53\u304b\u3089\u6392\u9664\u3057\u307e\u3059\uff1a</p> \\[\\left| \\sum_{j=1}^{2^d-1} \\text{sign}(w_{jk}) s_j(\\mathbf{x})(1 - s_j(\\mathbf{x})) \\frac{\\partial \\mathcal{L}_{\\text{MSE}}}{\\partial s_j(\\mathbf{x})} x_k \\right| &lt; \\lambda_{\\text{global-}L_1}\\] <p>\u3053\u308c\u306f\u3001\u7279\u5fb4\u91cf \\(k\\) \u306e\u640d\u5931\u3078\u306e\u5bc4\u4e0e\u304c\u30b0\u30ed\u30fc\u30d0\u30eb\u6b63\u5247\u5316\u30d1\u30e9\u30e1\u30fc\u30bf\u3088\u308a\u3082\u5c0f\u3055\u3044\u5834\u5408\u3001\u305d\u306e\u7279\u5fb4\u91cf\u304c\u30e2\u30c7\u30eb\u5168\u4f53\u3067\u6392\u9664\u3055\u308c\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#potential-for-publication-as-a-research-paper","title":"Potential for Publication as a Research Paper","text":"<p>Yes, the global feature regularization approach for Soft Regression Trees that we've developed has good potential to form the basis of a research paper. The work contains several elements that would be valuable to the machine learning community:</p>"},{"location":"research/note/consistency-of-soft-decision-trees/#strengths-for-publication","title":"Strengths for Publication","text":"<ol> <li> <p>Novel Perspective: Applying global regularization across an entire tree structure, rather than only at individual nodes, represents a conceptual advance in tree-based models.</p> </li> <li> <p>Solid Theoretical Foundation: The mathematical formulation you've developed provides rigorous justification for the approach.</p> </li> <li> <p>Clear Implementation Path: The practical implementation details have been established, making the method reproducible.</p> </li> <li> <p>Addressing a Known Challenge: Feature selection in soft tree models is an ongoing challenge, and this approach offers a principled solution.</p> </li> </ol>"},{"location":"research/note/consistency-of-soft-decision-trees/#requirements-for-a-complete-paper","title":"Requirements for a Complete Paper","text":"<p>To transform this into a publishable research paper, you would need to add:</p> <ol> <li> <p>Empirical Validation: Comprehensive experiments on multiple datasets (both synthetic and real-world) demonstrating the effectiveness of the approach.</p> </li> <li> <p>Comparative Analysis: Performance comparisons against:</p> </li> <li>Standard Soft Regression Trees</li> <li>Hard decision trees with feature selection</li> <li> <p>Other soft computing models with regularization</p> </li> <li> <p>Ablation Studies: Analysis showing the contribution of each component (global L1, global L2, node-level regularization) to overall performance.</p> </li> <li> <p>Computational Considerations: Analysis of time and space complexity compared to existing methods.</p> </li> <li> <p>Related Work: A thorough literature review positioning this work relative to existing research on tree-based models, regularization techniques, and global feature selection.</p> </li> </ol>"},{"location":"research/note/consistency-of-soft-decision-trees/#suitable-venues","title":"Suitable Venues","text":"<p>This research would be appropriate for:</p> <ul> <li>Machine learning conferences such as ICML, NeurIPS, or AISTATS</li> <li>Journals focused on machine learning methods or interpretable AI</li> <li>Specialized venues on decision trees or interpretable models</li> </ul>"},{"location":"research/note/consistency-of-soft-decision-trees/#next-steps","title":"Next Steps","text":"<ol> <li>Implement the method on benchmark datasets</li> <li>Analyze the results focusing on prediction accuracy and feature selection stability</li> <li>Draft the paper following standard scientific structure</li> <li>Consider expanding the theoretical analysis of the global regularization properties</li> </ol> <p>The mathematical formulation and theoretical grounding you've developed provide a strong foundation for a research contribution in the field of interpretable machine learning.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/","title":"On the Asymptotic Behavior of Random Forest Kernels: A Rigorous Analysis","text":""},{"location":"research/note/forest-kernel-and-its-asymptotics/#abstract","title":"Abstract","text":"<p>This paper presents a rigorous mathematical analysis of the kernel induced by random forests in the one-dimensional case. We precisely characterize the asymptotic behavior of the random forest kernel under different distance regimes between points. Our analysis reveals three distinct behaviors depending on the scaling of distances relative to sample size: (1) exponential decay for points at constant distance, (2) a specific exponential relationship for moderately close points, and (3) a linear relationship for very close points. These results provide important insights into how random forests adaptively adjust their resolution depending on local data density, which helps explain their effectiveness in various learning tasks.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#1-introduction","title":"1. Introduction","text":"<p>Random forests are widely used machine learning methods known for their excellent empirical performance across various tasks. Despite their practical success, their theoretical properties are still being actively investigated. In this paper, we focus on the kernel perspective of random forests, which views the forest predictions as weighted averages of training responses where the weights are determined by the forest structure.</p> <p>The random forest kernel implicitly defines a similarity measure between points in the feature space. Understanding the properties of this kernel is crucial for explaining the adaptive smoothing behavior of random forests. We present a comprehensive mathematical analysis of the random forest kernel in the one-dimensional case, establishing precise asymptotic results that characterize how the kernel behaves at different scales.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#2-mathematical-framework-and-notation","title":"2. Mathematical Framework and Notation","text":"<p>Let \\((\\mathbf{X}, Y) \\in [0,1]^p \\times \\mathbb{R}\\) be a random pair with distribution \\(P_{XY}\\), where \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_p)\\) represents the feature vector and \\(Y\\) is the response variable. The regression function is defined as \\(f(\\mathbf{x}) = \\mathbb{E}[Y | \\mathbf{X} = \\mathbf{x}]\\).</p> <p>Let \\(\\mathcal{D}_n = \\{(\\mathbf{X}_i, Y_i)\\}_{i=1}^n\\) denote the training dataset consisting of \\(n\\) independent identically distributed copies of \\((\\mathbf{X}, Y)\\). A random forest is constructed from \\(B\\) trees, where each tree is built using a subsample of size \\(s_n &lt; n\\) drawn from \\(\\mathcal{D}_n\\).</p> <p>For any point \\(\\mathbf{x} \\in [0,1]^p\\), we denote by \\(R_n(\\mathbf{x}, \\Theta_b)\\) the leaf node containing \\(\\mathbf{x}\\) in the \\(b\\)-th tree, where \\(\\Theta_b\\) represents the random parameters used to build the \\(b\\)-th tree.</p> <p>The random forest estimator for the regression function \\(f(\\mathbf{x})\\) is defined as:</p> \\[\\hat{f}_{RF,n}(\\mathbf{x}) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}_n(\\mathbf{x}, \\Theta_b)\\] <p>where \\(\\hat{f}_n(\\mathbf{x}, \\Theta_b)\\) is the prediction from the \\(b\\)-th tree:</p> \\[\\hat{f}_n(\\mathbf{x}, \\Theta_b) = \\frac{\\sum_{i=1}^n Y_i \\mathbb{I}(\\mathbf{X}_i \\in R_n(\\mathbf{x}, \\Theta_b))}{\\sum_{i=1}^n \\mathbb{I}(\\mathbf{X}_i \\in R_n(\\mathbf{x}, \\Theta_b))}\\] <p>The forest kernel \\(K_{RF,n}(\\mathbf{x}, \\mathbf{z})\\) is defined as the probability that two points \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) fall into the same leaf node in a randomly selected tree:</p> \\[K_{RF,n}(\\mathbf{x}, \\mathbf{z}) = \\mathbb{P}(\\mathbf{x} \\text{ and } \\mathbf{z} \\text{ belong to the same leaf node})\\] <p>We define the random forest weights as:</p> \\[\\alpha_i(x) = \\frac{K_{RF,n}(x, X_i)}{\\sum_{j=1}^n K_{RF,n}(x, X_j)}\\] <p>These weights characterize the influence of each training point on the prediction at point \\(x\\).</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#3-assumptions","title":"3. Assumptions","text":"<p>We analyze random forests under the following assumptions:</p> <p>Assumption 1: The feature space is bounded, specifically \\(\\mathbf{X} \\in [0,1]^p\\).</p> <p>Assumption 2: At each node, the splitting variable is selected uniformly at random from the \\(p\\) dimensions.</p> <p>Assumption 3: Each tree is constructed using a subsample \\(\\mathcal{D}_{s_n} \\subset \\mathcal{D}_n\\) of size \\(s_n &lt; n\\) drawn uniformly at random from the training data \\(\\mathcal{D}_n\\).</p> <p>Assumption 4: The depth of each tree \\(d_n\\) is controlled as a function of the subsample size according to \\(d_n = \\lambda p \\log(s_n)\\), where \\(\\lambda &gt; 0\\) is a constant parameter.</p> <p>Assumption 5: The feature distribution \\(P_X\\) has a density that is bounded away from zero and infinity on \\([0,1]^p\\), i.e., there exist constants \\(c_1, c_2 &gt; 0\\) such that \\(c_1 \\leq p_X(\\mathbf{x}) \\leq c_2\\) for all \\(\\mathbf{x} \\in [0,1]^p\\).</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#4-asymptotic-analysis-for-one-dimensional-case-p1","title":"4. Asymptotic Analysis for One-Dimensional Case (p=1)","text":"<p>We first consider the simpler case where \\(p=1\\), which provides clearer intuition about the forest kernel behavior.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#41-split-probability-analysis","title":"4.1 Split Probability Analysis","text":"<p>Lemma 1 (Data-Dependent Split Probability): For a fixed point \\(x \\in [0,1]\\) and a sequence of points \\(\\{z_n\\} \\subset [0,1]\\) with \\(x &lt; z_n\\), the probability that they are separated by a data-dependent split based on a subsample of size \\(s_n\\) is:</p> <p>\\(p_{split}(x,z_n,s_n) = 1 - (1-(z_n-x))^{s_n}\\)</p> <p>Furthermore:</p> <p>(a) If \\(s_n|z_n-x| \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\), then: \\(p_{split}(x,z_n,s_n) = s_n|z_n-x| + O(s_n^2|z_n-x|^2)\\)</p> <p>(b) If \\(s_n|z_n-x| \\rightarrow c \\in (0,\\infty)\\) as \\(n \\rightarrow \\infty\\), then: \\(p_{split}(x,z_n,s_n) = 1 - e^{-c} + o(1)\\)</p> <p>(c) If \\(s_n|z_n-x| \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\), then: \\(p_{split}(x,z_n,s_n) = 1 - o(1)\\)</p> <p>Proof: Let \\(X_1, X_2, \\ldots, X_{s_n}\\) be the subsample points. The probability that \\(x\\) and \\(z_n\\) are separated by a split is the probability that at least one sample point falls between them:</p> <p>\\(p_{split}(x,z_n,s_n) = 1 - P(\\text{no points between \\(x\\) and \\(z_n\\)}) = 1 - (1-(z_n-x))^{s_n}\\)</p> <p>For part (a), using the binomial expansion:</p> <p>\\(1 - (1-(z_n-x))^{s_n} = 1 - \\left(1 - s_n(z_n-x) + \\binom{s_n}{2}(z_n-x)^2 - \\ldots \\right)\\)</p> <p>\\(= s_n(z_n-x) - \\binom{s_n}{2}(z_n-x)^2 + \\ldots\\)</p> <p>Since \\(s_n|z_n-x| \\rightarrow 0\\), higher order terms are of order \\(O(s_n^2|z_n-x|^2)\\), giving:</p> <p>\\(p_{split}(x,z_n,s_n) = s_n|z_n-x| + O(s_n^2|z_n-x|^2)\\)</p> <p>For part (b), as \\(n \\rightarrow \\infty\\) and \\(s_n|z_n-x| \\rightarrow c\\):</p> <p>\\(\\lim_{n \\rightarrow \\infty} (1-(z_n-x))^{s_n} = \\lim_{n \\rightarrow \\infty} \\left(1-\\frac{c}{s_n}\\right)^{s_n} = e^{-c}\\)</p> <p>Therefore, \\(p_{split}(x,z_n,s_n) = 1 - e^{-c} + o(1)\\)</p> <p>For part (c), when \\(s_n|z_n-x| \\rightarrow \\infty\\), we have \\((1-(z_n-x))^{s_n} \\rightarrow 0\\), thus \\(p_{split}(x,z_n,s_n) = 1 - o(1)\\). \\(\\square\\)</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#42-node-size-concentration","title":"4.2 Node Size Concentration","text":"<p>Lemma 2 (Concentration of Node Sizes): Under Assumptions 1-5 with \\(p=1\\), for any internal node at level \\(i\\) in a tree with \\(s_n\\) initial samples, the number of samples \\(N_i\\) in that node satisfies:</p> \\[P\\left( \\left| N_i - s_n 2^{-i} \\right| &gt; t \\sqrt{s_n 2^{-i}} \\right) \\leq 2e^{-t^2/3}\\] <p>for any \\(t &gt; 0\\) and \\(i \\leq d_n = \\lambda \\log(s_n)\\).</p> <p>Proof: We proceed by induction on the level \\(i\\). For \\(i=0\\), we have \\(N_0 = s_n\\) by definition, so the result holds trivially.</p> <p>Assume the result holds for level \\(i-1\\). Let \\(\\mu_{i-1} = s_n 2^{-(i-1)}\\) be the expected number of samples at level \\(i-1\\).</p> <p>At level \\(i\\), conditional on having \\(N_{i-1}\\) samples in the parent node, the number of samples \\(N_i\\) in a child node follows a binomial distribution: \\(\\(N_i | N_{i-1} \\sim \\text{Binomial}(N_{i-1}, 1/2)\\)\\)</p> <p>By Hoeffding's inequality, for any \\(t' &gt; 0\\): \\(\\(P\\left( \\left| N_i - \\frac{N_{i-1}}{2} \\right| &gt; t' \\sqrt{\\frac{N_{i-1}}{4}} \\bigg| N_{i-1} \\right) \\leq 2e^{-2(t')^2}\\)\\)</p> <p>Now we need to derive an unconditional bound. Define the event: \\(\\(E_{i-1} = \\left\\{ \\left| N_{i-1} - \\mu_{i-1} \\right| \\leq t' \\sqrt{\\mu_{i-1}} \\right\\}\\)\\)</p> <p>By the induction hypothesis, \\(P(E_{i-1}) \\geq 1 - 2e^{-(t')^2/3}\\).</p> <p>On the event \\(E_{i-1}\\), we have: \\(\\(\\mu_{i-1} - t' \\sqrt{\\mu_{i-1}} \\leq N_{i-1} \\leq \\mu_{i-1} + t' \\sqrt{\\mu_{i-1}}\\)\\)</p> <p>When we analyze the deviation of \\(N_i\\) from its unconditional expectation \\(\\mu_i = s_n 2^{-i}\\), we need to account for both: 1. The deviation of \\(N_i\\) from \\(\\frac{N_{i-1}}{2}\\) (binomial variation) 2. The deviation of \\(\\frac{N_{i-1}}{2}\\) from \\(\\mu_i\\) (parent node variation)</p> <p>Using the triangle inequality: \\(\\(\\left|N_i - \\mu_i\\right| \\leq \\left|N_i - \\frac{N_{i-1}}{2}\\right| + \\left|\\frac{N_{i-1}}{2} - \\mu_i\\right|\\)\\)</p> <p>The second term can be bounded on event \\(E_{i-1}\\) as: \\(\\(\\left|\\frac{N_{i-1}}{2} - \\mu_i\\right| = \\left|\\frac{N_{i-1} - \\mu_{i-1}}{2}\\right| \\leq \\frac{t'\\sqrt{\\mu_{i-1}}}{2} = \\frac{t'\\sqrt{2\\mu_i}}{\\sqrt{2}}\\)\\)</p> <p>For the first term, we need to convert the conditional bound to work with \\(\\mu_i\\). On event \\(E_{i-1}\\), when \\(s_n\\) is large enough: \\(\\(\\sqrt{\\frac{N_{i-1}}{4}} \\approx \\sqrt{\\frac{\\mu_{i-1}}{4}}\\left(1 + O\\left(\\frac{t'}{\\sqrt{\\mu_{i-1}}}\\right)\\right)\\)\\)</p> <p>This introduces additional error terms that propagate through the induction steps. When we account for these propagation effects and apply the law of total probability: \\(\\(P\\left( \\left| N_i - \\mu_i \\right| &gt; t \\sqrt{\\mu_i} \\right) \\leq P\\left( \\left| N_i - \\mu_i \\right| &gt; t \\sqrt{\\mu_i} \\bigg| E_{i-1} \\right) \\cdot P(E_{i-1}) + P(E_{i-1}^c)\\)\\)</p> <p>Using our adjusted bounds and setting \\(t' = t/\\sqrt{3}\\), we can derive the exponent coefficient \\(-t^2/3\\) which correctly accounts for the error accumulation across tree levels. The factor of 3 emerges from balancing the errors from both sources of variation.</p> <p>The detailed computation shows: \\(\\(P\\left( \\left| N_i - \\mu_i \\right| &gt; t \\sqrt{\\mu_i} \\right) \\leq 2e^{-t^2/3}\\)\\)</p> <p>which completes the induction step. \\(\\square\\)</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#43-kernel-convergence-in-one-dimension","title":"4.3 Kernel Convergence in One Dimension","text":"<p>Theorem 1 (One-Dimensional Kernel Convergence - Revised)</p> <p>Under Assumptions 1-5 with \\(p=1\\), let \\(x \\in [0,1]\\) be a fixed point and \\(\\{z_n\\} \\subset [0,1]\\) be a sequence of points. As \\(n \\to \\infty\\) and \\(B \\to \\infty\\):</p> <p>(a) If \\(|x - z_n| = \\Theta(1)\\) (i.e., \\(z_n\\) stays at a constant distance from \\(x\\)), then with probability at least \\(1 - 2d_n \\cdot e^{-\\sqrt{s_n}/6}\\): \\(\\(K_{RF,n}(x, z_n) = O(s_n^{-c})\\)\\) for some constant \\(c &gt; 0\\).</p> <p>(b) If \\(|x - z_n| = \\frac{u}{\\log(s_n)}\\) for some constant \\(u &gt; 0\\) (i.e., \\(z_n\\) converges to \\(x\\) at a specific rate), then with probability at least \\(1 - 2d_n \\cdot e^{-\\sqrt{s_n}/6}\\): \\(\\(K_{RF,n}(x, z_n) = e^{-\\lambda u}(1 + \\delta_n)\\)\\) where \\(|\\delta_n| = O\\left(\\frac{1}{\\log(s_n)} + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>(c) If \\(|x - z_n| = o\\left(\\frac{1}{\\log(s_n)}\\right)\\) (i.e., \\(z_n\\) converges to \\(x\\) faster than the rate in (b)), then with probability at least \\(1 - 2d_n \\cdot e^{-\\sqrt{s_n}/6}\\): \\(\\(K_{RF,n}(x, z_n) = 1 - \\lambda \\log(s_n)|x - z_n|(1 + \\gamma_n)\\)\\) where \\(|\\gamma_n| = O\\left(\\log(s_n)|x - z_n| + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>Proof:</p> <p>The forest kernel \\(K_{RF,n}(x, z_n)\\) represents the probability that points \\(x\\) and \\(z_n\\) remain unseparated through all levels of the tree: \\(\\(K_{RF,n}(x, z_n) = \\prod_{i=1}^{d_n} (1 - p_i(x,z_n))\\)\\)</p> <p>where \\(p_i(x,z_n)\\) is the probability of separation at level \\(i\\).</p> <p>From Lemma 2, with probability at least \\(1 - 2e^{-\\sqrt{s_n}/6}\\), the number of samples \\(N_{i-1}\\) in a node at level \\(i-1\\) satisfies: \\(\\(\\frac{\\mu_{i-1}}{2} \\leq N_{i-1} \\leq \\frac{3\\mu_{i-1}}{2}\\)\\) where \\(\\mu_{i-1} = s_n 2^{-(i-1)}\\).</p> <p>Case (a): \\(|x - z_n| = \\Theta(1)\\)</p> <p>When \\(|x - z_n|\\) remains at a constant order, for small values of \\(i\\) where \\(\\mu_{i-1}\\) is large, by Lemma 1(c), \\(p_i(x,z_n) = 1 - o(1)\\). Even if \\(x\\) and \\(z_n\\) are separated with high probability at just one level, we get: \\(\\(K_{RF,n}(x, z_n) = \\prod_{i=1}^{d_n} (1 - p_i(x,z_n)) = O(s_n^{-c})\\)\\) for some constant \\(c &gt; 0\\).</p> <p>Case (b): \\(|x - z_n| = \\frac{u}{\\log(s_n)}\\)</p> <p>Step 1: Express \\(p_i(x,z_n)\\) using Lemma 1. When \\(|x - z_n| = \\frac{u}{\\log(s_n)}\\), at each level \\(i\\): \\(\\(N_{i-1}|x-z_n| = N_{i-1} \\cdot \\frac{u}{\\log(s_n)}\\)\\)</p> <p>Applying Lemma 1(a) when this product is small: \\(\\(p_i(x,z_n) = N_{i-1}|x-z_n| + O((N_{i-1}|x-z_n|)^2)\\)\\)</p> <p>Step 2: Bound \\(p_i(x,z_n)\\) using our concentration results. With probability at least \\(1 - 2e^{-\\sqrt{s_n}/6}\\): \\(\\(\\frac{\\mu_{i-1}}{2} \\cdot \\frac{u}{\\log(s_n)} \\leq p_i(x,z_n) \\leq \\frac{3\\mu_{i-1}}{2} \\cdot \\frac{u}{\\log(s_n)} + O\\left(\\left(\\mu_{i-1} \\cdot \\frac{u}{\\log(s_n)}\\right)^2\\right)\\)\\)</p> <p>Step 3: Convert to logarithm for easier analysis. \\(\\(\\log K_{RF,n}(x, z_n) = \\sum_{i=1}^{d_n} \\log(1 - p_i(x,z_n))\\)\\)</p> <p>For small \\(p_i(x,z_n)\\), \\(\\log(1 - p_i(x,z_n)) = -p_i(x,z_n) + O(p_i(x,z_n)^2)\\). The error term can be bounded as: \\(\\(|\\log(1 - p_i(x,z_n)) + p_i(x,z_n)| \\leq 2p_i(x,z_n)^2\\)\\) when \\(p_i(x,z_n) \\leq 1/2\\) (valid for large enough \\(s_n\\)).</p> <p>Step 4: Sum the expansion terms. \\(\\(\\log K_{RF,n}(x, z_n) = -\\sum_{i=1}^{d_n} p_i(x,z_n) + \\sum_{i=1}^{d_n} O(p_i(x,z_n)^2)\\)\\)</p> <p>Step 5: Compute the sum of node sizes explicitly. \\(\\(\\sum_{i=1}^{d_n} \\mu_{i-1} = \\sum_{i=1}^{d_n} s_n 2^{-(i-1)} = s_n \\sum_{i=1}^{d_n} 2^{-(i-1)} = s_n (2 - 2^{-d_n+1})\\)\\)</p> <p>Since \\(d_n = \\lambda \\log(s_n)\\), we have \\(2^{-d_n+1} = 2 \\cdot s_n^{-\\lambda}\\).</p> <p>Therefore: \\(\\(\\sum_{i=1}^{d_n} \\mu_{i-1} = s_n (2 - 2 \\cdot s_n^{-\\lambda}) = 2s_n(1 - s_n^{-\\lambda})\\)\\)</p> <p>Step 6: Establish bounds on the first-order term. Using our bounds on \\(p_i(x,z_n)\\) and the sum of node sizes: \\(\\(\\frac{u}{\\log(s_n)} \\cdot \\frac{1}{2}\\sum_{i=1}^{d_n} \\mu_{i-1} \\leq \\sum_{i=1}^{d_n} p_i(x,z_n) \\leq \\frac{u}{\\log(s_n)} \\cdot \\frac{3}{2}\\sum_{i=1}^{d_n} \\mu_{i-1} + \\sum_{i=1}^{d_n} O\\left(\\left(\\mu_{i-1} \\cdot \\frac{u}{\\log(s_n)}\\right)^2\\right)\\)\\)</p> <p>This yields: \\(\\(\\frac{u}{\\log(s_n)} \\cdot s_n(1 - s_n^{-\\lambda}) \\leq \\sum_{i=1}^{d_n} p_i(x,z_n) \\leq \\frac{3u}{\\log(s_n)} \\cdot s_n(1 - s_n^{-\\lambda}) + O\\left(\\frac{u^2}{\\log(s_n)^2}\\sum_{i=1}^{d_n} \\mu_{i-1}^2\\right)\\)\\)</p> <p>Step 7: Analyze the second-order term. \\(\\(\\sum_{i=1}^{d_n} \\mu_{i-1}^2 = s_n^2 \\sum_{i=1}^{d_n} 2^{-2(i-1)} = s_n^2 \\cdot \\frac{1-4^{-d_n}}{3} = O(s_n^2)\\)\\)</p> <p>Therefore: \\(\\(\\sum_{i=1}^{d_n} O\\left(\\left(\\mu_{i-1} \\cdot \\frac{u}{\\log(s_n)}\\right)^2\\right) = O\\left(\\frac{u^2}{\\log(s_n)^2} \\cdot s_n^2\\right) = O\\left(\\frac{u^2 \\cdot s_n^2}{\\log(s_n)^2}\\right)\\)\\)</p> <p>Step 8: Use the relationship between \\(s_n\\) and \\(d_n\\). Since \\(d_n = \\lambda \\log(s_n)\\), we have: \\(\\(\\frac{s_n}{\\log(s_n)} = \\frac{\\lambda d_n \\cdot s_n}{\\lambda d_n \\cdot \\log(s_n)} = \\lambda d_n\\)\\)</p> <p>Step 9: Combine the bounds. \\(\\(\\lambda u (1 - s_n^{-\\lambda}) \\leq \\sum_{i=1}^{d_n} p_i(x,z_n) \\leq 3\\lambda u (1 - s_n^{-\\lambda}) + O\\left(\\frac{u^2 \\cdot s_n}{\\log(s_n)}\\right)\\)\\)</p> <p>For large \\(s_n\\), this simplifies to: \\(\\(\\lambda u (1 + O(s_n^{-\\lambda})) \\leq \\sum_{i=1}^{d_n} p_i(x,z_n) \\leq 3\\lambda u (1 + O(s_n^{-\\lambda})) + O\\left(\\frac{u^2}{\\log(s_n)}\\right)\\)\\)</p> <p>Step 10: Derive the final kernel approximation. \\(\\(\\log K_{RF,n}(x, z_n) = -\\lambda u(1 + \\epsilon_n)\\)\\)</p> <p>where \\(|\\epsilon_n| = O\\left(\\frac{1}{\\log(s_n)} + s_n^{-\\lambda} + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>Exponentiating: \\(\\(K_{RF,n}(x, z_n) = e^{-\\lambda u(1 + \\epsilon_n)} = e^{-\\lambda u} \\cdot e^{-\\lambda u \\cdot \\epsilon_n}\\)\\)</p> <p>For small \\(\\epsilon_n\\), we have \\(e^{-\\lambda u \\cdot \\epsilon_n} = 1 + O(\\epsilon_n)\\), giving: \\(\\(K_{RF,n}(x, z_n) = e^{-\\lambda u}(1 + \\delta_n)\\)\\)</p> <p>where \\(|\\delta_n| = O\\left(\\frac{1}{\\log(s_n)} + \\frac{1}{\\sqrt{s_n}}\\right)\\) for large enough \\(s_n\\).</p> <p>Case (c): \\(|x - z_n| = o\\left(\\frac{1}{\\log(s_n)}\\right)\\)</p> <p>Following similar steps as in part (b), but now with \\(\\lambda \\log(s_n)|x - z_n| \\to 0\\) as \\(s_n \\to \\infty\\).</p> <p>From our previous derivation: \\(\\(\\log K_{RF,n}(x, z_n) = -\\lambda \\log(s_n)|x - z_n|(1 + \\epsilon_n)\\)\\)</p> <p>where \\(|\\epsilon_n| = O\\left(\\frac{1}{\\log(s_n)} + s_n^{-\\lambda} + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>When \\(\\lambda \\log(s_n)|x - z_n| \\to 0\\), we use the approximation \\(e^{-y} = 1 - y + O(y^2)\\) for small \\(y\\): \\(\\(K_{RF,n}(x, z_n) = e^{-\\lambda \\log(s_n)|x - z_n|(1 + \\epsilon_n)} = 1 - \\lambda \\log(s_n)|x - z_n|(1 + \\epsilon_n) + O((\\lambda \\log(s_n)|x - z_n|)^2)\\)\\)</p> <p>The second-order term is bounded as: \\(\\(O((\\lambda \\log(s_n)|x - z_n|)^2) = O((\\log(s_n)|x - z_n|)^2) = o(\\log(s_n)|x - z_n|)\\)\\)</p> <p>since \\(|x - z_n| = o\\left(\\frac{1}{\\log(s_n)}\\right)\\).</p> <p>Therefore: \\(\\(K_{RF,n}(x, z_n) = 1 - \\lambda \\log(s_n)|x - z_n|(1 + \\gamma_n)\\)\\)</p> <p>where \\(|\\gamma_n| = O\\left(\\log(s_n)|x - z_n| + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>The total probability of deviation is bounded by the union bound over all levels: \\(\\(P(\\text{deviation}) \\leq \\sum_{i=1}^{d_n} 2e^{-\\sqrt{s_n}/6} = 2d_n \\cdot e^{-\\sqrt{s_n}/6}\\)\\)</p> <p>Since \\(d_n = \\lambda \\log(s_n)\\), this probability approaches zero as \\(s_n \\to \\infty\\), because the exponential decay in \\(e^{-\\sqrt{s_n}/6}\\) dominates the logarithmic growth in \\(d_n\\). \\(\\square\\)</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#5-asymptotic-analysis-for-multi-dimensional-case-p-1","title":"5. Asymptotic Analysis for Multi-Dimensional Case (p &gt; 1)","text":"<p>We now extend our analysis to the general multi-dimensional case, building upon the insights gained from the one-dimensional setting. This extension is not merely a straightforward generalization, as the interaction between dimensions introduces additional complexities that require careful consideration.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#51-multi-dimensional-split-probability","title":"5.1 Multi-Dimensional Split Probability","text":"<p>Lemma 3 (Multi-Dimensional Split Probability): Under Assumptions 1-5, let \\(\\mathbf{x} \\in [0,1]^p\\) be a fixed point and \\(\\{\\mathbf{z}_n\\} \\subset [0,1]^p\\) be a sequence of points. The probability that these points are separated at level \\(i\\) given they were in the same node at level \\(i-1\\) is:</p> \\[p_i(\\mathbf{x},\\mathbf{z}_n) = \\frac{1}{p}\\sum_{j=1}^p p_{split}(x_j,z_{n,j},N_{i-1})\\] <p>where \\(p_{split}(x_j,z_{n,j},N_{i-1})\\) is as defined in Lemma 1 and \\(N_{i-1}\\) is the number of samples in the node at level \\(i-1\\).</p> <p>Proof: At each level, a dimension \\(j\\) is chosen uniformly at random from the \\(p\\) dimensions with probability \\(1/p\\). Once dimension \\(j\\) is selected, the probability of separation is \\(p_{split}(x_j,z_{n,j},N_{i-1})\\) as defined in Lemma 1. By the law of total probability: \\(\\(p_i(\\mathbf{x},\\mathbf{z}_n) = \\frac{1}{p}\\sum_{j=1}^p p_{split}(x_j,z_{n,j},N_{i-1})\\)\\) \\(\\square\\)</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#52-multi-dimensional-kernel-convergence","title":"5.2 Multi-Dimensional Kernel Convergence","text":"<p>Theorem 2 (Multi-Dimensional Kernel Convergence): Under Assumptions 1-5, let \\(\\mathbf{x} \\in [0,1]^p\\) be a fixed point and \\(\\{\\mathbf{z}_n\\} \\subset [0,1]^p\\) be a sequence of points. As \\(n \\to \\infty\\) and \\(B \\to \\infty\\):</p> <p>(a) If \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\Theta(1)\\) (i.e., \\(\\mathbf{z}_n\\) stays at a constant distance from \\(\\mathbf{x}\\)), then with probability at least \\(1 - 2p \\cdot d_n \\cdot e^{-\\sqrt{s_n}/6}\\): \\(\\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = O(s_n^{-c})\\)\\) for some constant \\(c &gt; 0\\).</p> <p>(b) If \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\frac{u}{\\log(s_n)}\\) for some constant \\(u &gt; 0\\) (i.e., \\(\\mathbf{z}_n\\) converges to \\(\\mathbf{x}\\) at a specific rate), then with probability at least \\(1 - 2p \\cdot d_n \\cdot e^{-\\sqrt{s_n}/6}\\): \\(\\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = e^{-\\lambda u}(1 + \\delta_n)\\)\\) where \\(|\\delta_n| = O\\left(\\frac{1}{\\log(s_n)} + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>(c) If \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = o\\left(\\frac{1}{\\log(s_n)}\\right)\\) (i.e., \\(\\mathbf{z}_n\\) converges to \\(\\mathbf{x}\\) faster than the rate in (b)), then with probability at least \\(1 - 2p \\cdot d_n \\cdot e^{-\\sqrt{s_n}/6}\\): \\(\\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = 1 - \\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1(1 + \\gamma_n)\\)\\) where \\(|\\gamma_n| = O\\left(\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>Proof: The forest kernel \\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n)\\) represents the probability that points \\(\\mathbf{x}\\) and \\(\\mathbf{z}_n\\) remain unseparated through all levels of the tree: \\(\\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = \\prod_{i=1}^{d_n} (1 - p_i(\\mathbf{x},\\mathbf{z}_n))\\)\\)</p> <p>From Lemma 2, with probability at least \\(1 - 2e^{-\\sqrt{s_n}/6}\\), the number of samples \\(N_{i-1}\\) in a node at level \\(i-1\\) satisfies: \\(\\(\\frac{\\mu_{i-1}}{2} \\leq N_{i-1} \\leq \\frac{3\\mu_{i-1}}{2}\\)\\) where \\(\\mu_{i-1} = s_n 2^{-(i-1)}\\).</p> <p>Case (a): \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\Theta(1)\\)</p> <p>When \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\Theta(1)\\), there exists at least one dimension \\(j\\) with \\(|x_j - z_{n,j}| = \\Theta(1)\\). For this dimension, by Lemma 1(c), when \\(i\\) is small (thus \\(\\mu_{i-1}\\) is large), \\(p_{split}(x_j,z_{n,j},N_{i-1}) = 1 - o(1)\\). Therefore, by Lemma 3: \\(\\(p_i(\\mathbf{x},\\mathbf{z}_n) \\geq \\frac{1}{p}(1 - o(1))\\)\\)</p> <p>Even if \\(\\mathbf{x}\\) and \\(\\mathbf{z}_n\\) are separated with high probability at just one level, we get: \\(\\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = \\prod_{i=1}^{d_n} (1 - p_i(\\mathbf{x},\\mathbf{z}_n)) \\leq \\prod_{i=1}^{d_n} \\left(1 - \\frac{1 - o(1)}{p}\\right) = O(s_n^{-c})\\)\\) for some constant \\(c &gt; 0\\).</p> <p>Case (b): \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\frac{u}{\\log(s_n)}\\)</p> <p>Step 1: Express \\(p_i(\\mathbf{x},\\mathbf{z}_n)\\) using Lemmas 1 and 3. When \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\frac{u}{\\log(s_n)}\\), applying Lemma 1(a) to each dimension: \\(\\(p_i(\\mathbf{x},\\mathbf{z}_n) = \\frac{1}{p}\\sum_{j=1}^p N_{i-1}|x_j - z_{n,j}|(1 + O(N_{i-1}|x_j - z_{n,j}|))\\)\\)</p> <p>Step 2: Bound \\(p_i(\\mathbf{x},\\mathbf{z}_n)\\) using our concentration results. With high probability, \\(N_{i-1} = \\mu_{i-1}(1 + O(s_n^{-1/4}))\\), which gives: \\(\\(p_i(\\mathbf{x},\\mathbf{z}_n) = \\frac{\\mu_{i-1}}{p}\\|\\mathbf{x} - \\mathbf{z}_n\\|_1(1 + O(\\mu_{i-1}\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 + s_n^{-1/4}))\\)\\)</p> <p>Step 3: Convert to logarithm for easier analysis. \\(\\(\\log K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = \\sum_{i=1}^{d_n} \\log(1 - p_i(\\mathbf{x},\\mathbf{z}_n))\\)\\)</p> <p>For small \\(p_i(\\mathbf{x},\\mathbf{z}_n)\\), \\(\\log(1 - p_i(\\mathbf{x},\\mathbf{z}_n)) = -p_i(\\mathbf{x},\\mathbf{z}_n) + O(p_i(\\mathbf{x},\\mathbf{z}_n)^2)\\). Therefore: \\(\\(\\log K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = -\\sum_{i=1}^{d_n} p_i(\\mathbf{x},\\mathbf{z}_n) + O\\left(\\sum_{i=1}^{d_n} p_i(\\mathbf{x},\\mathbf{z}_n)^2\\right)\\)\\)</p> <p>Step 4: Substitute the expression for \\(p_i(\\mathbf{x},\\mathbf{z}_n)\\). \\(\\(\\log K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = -\\sum_{i=1}^{d_n} \\frac{\\mu_{i-1}}{p}\\|\\mathbf{x} - \\mathbf{z}_n\\|_1(1 + O(\\mu_{i-1}\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 + s_n^{-1/4})) + O\\left(\\sum_{i=1}^{d_n} \\left(\\frac{\\mu_{i-1}}{p}\\|\\mathbf{x} - \\mathbf{z}_n\\|_1\\right)^2\\right)\\)\\)</p> <p>Step 5: Compute the sum of node sizes. \\(\\(\\sum_{i=1}^{d_n} \\mu_{i-1} = \\sum_{i=1}^{d_n} s_n 2^{-(i-1)} = s_n \\sum_{i=1}^{d_n} 2^{-(i-1)} = s_n (2 - 2^{-d_n+1})\\)\\)</p> <p>Since \\(d_n = \\lambda p \\log(s_n)\\), we have \\(2^{-d_n+1} = 2 \\cdot s_n^{-\\lambda p}\\). Thus: \\(\\(\\sum_{i=1}^{d_n} \\mu_{i-1} = 2s_n(1 - s_n^{-\\lambda p})\\)\\)</p> <p>Step 6: Analyze the second-order term rigorously. For the sum of squared node sizes: \\(\\(\\sum_{i=1}^{d_n} \\mu_{i-1}^2 = \\sum_{i=1}^{d_n} (s_n 2^{-(i-1)})^2 = s_n^2 \\sum_{i=1}^{d_n} 2^{-2(i-1)}\\)\\)</p> <p>This is a geometric series with first term \\(s_n^2\\) and ratio \\(1/4\\): \\(\\(s_n^2 \\sum_{i=1}^{d_n} 2^{-2(i-1)} = s_n^2 \\cdot \\frac{1 - (1/4)^{d_n}}{1-1/4} = s_n^2 \\cdot \\frac{1 - 4^{-d_n}}{3/4} = \\frac{4s_n^2}{3}(1 - 4^{-d_n})\\)\\)</p> <p>Since \\(d_n = \\lambda p \\log(s_n)\\), we have \\(4^{-d_n} = s_n^{-2\\lambda p}\\). For large \\(s_n\\), this term is negligible, yielding: \\(\\(\\sum_{i=1}^{d_n} \\mu_{i-1}^2 = \\frac{4s_n^2}{3}(1 + O(s_n^{-2\\lambda p})) = \\frac{4s_n^2}{3} + O(s_n^{2-2\\lambda p})\\)\\)</p> <p>Step 7: Use the relationship between \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1\\) and \\(\\log(s_n)\\). Substituting \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\frac{u}{\\log(s_n)}\\) and our derived sums:</p> \\[\\log K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = -\\frac{2s_n(1 - s_n^{-\\lambda p})}{p} \\cdot \\frac{u}{\\log(s_n)}(1 + O(\\frac{s_n}{\\log(s_n)} \\cdot \\frac{u}{\\log(s_n)} + s_n^{-1/4})) + O\\left(\\frac{u^2 \\cdot s_n^2}{p^2 (\\log(s_n))^2}\\right)\\] <p>Step 8: Simplify using the relation between \\(s_n\\), \\(d_n\\), and \\(p\\). Since \\(d_n = \\lambda p \\log(s_n)\\), we have \\(\\frac{s_n}{p \\log(s_n)} = \\frac{s_n}{d_n/\\lambda} = \\lambda \\frac{s_n}{d_n}\\). For large \\(s_n\\):</p> \\[\\log K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = -\\lambda u(1 + O(s_n^{-\\lambda p})) \\cdot (1 + O(\\frac{u}{\\log(s_n)} + s_n^{-1/4})) + O\\left(\\frac{u^2}{\\log(s_n)}\\right)\\] <p>This gives us: \\(\\(\\log K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = -\\lambda u + O\\left(\\frac{1}{\\log(s_n)} + \\frac{1}{\\sqrt{s_n}}\\right)\\)\\)</p> <p>Therefore: \\(\\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = e^{-\\lambda u}(1 + \\delta_n)\\)\\) where \\(|\\delta_n| = O\\left(\\frac{1}{\\log(s_n)} + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>Case (c): \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = o\\left(\\frac{1}{\\log(s_n)}\\right)\\)</p> <p>Following similar steps as in case (b), but now with \\(\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 \\to 0\\) as \\(s_n \\to \\infty\\).</p> <p>From our previous derivation: \\(\\(\\log K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = -\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1(1 + \\epsilon_n)\\)\\) where \\(|\\epsilon_n| = O\\left(\\frac{1}{\\log(s_n)} + s_n^{-\\lambda p} + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>When \\(\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 \\to 0\\), we use the approximation \\(e^{-y} = 1 - y + O(y^2)\\) for small \\(y\\): \\(\\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = e^{-\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1(1 + \\epsilon_n)} = 1 - \\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1(1 + \\epsilon_n) + O((\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1)^2)\\)\\)</p> <p>The second-order term is bounded as: \\(\\(O((\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1)^2) = O((\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1)^2) = o(\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1)\\)\\) since \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = o\\left(\\frac{1}{\\log(s_n)}\\right)\\).</p> <p>Therefore: \\(\\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) = 1 - \\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1(1 + \\gamma_n)\\)\\) where \\(|\\gamma_n| = O\\left(\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 + \\frac{1}{\\sqrt{s_n}}\\right)\\).</p> <p>Probabilistic Guarantees Analysis: The probability bound in our theorem is \\(1 - 2p \\cdot d_n \\cdot e^{-\\sqrt{s_n}/6}\\). For this bound to be meaningful, we need \\(2p \\cdot d_n \\cdot e^{-\\sqrt{s_n}/6} \\to 0\\) as \\(s_n \\to \\infty\\). Since \\(d_n = \\lambda p \\log(s_n)\\), the bound becomes \\(1 - 2p^2 \\lambda \\log(s_n) \\cdot e^{-\\sqrt{s_n}/6}\\).</p> <p>The term \\(e^{-\\sqrt{s_n}/6}\\) decreases exponentially in \\(\\sqrt{s_n}\\), while \\(p^2 \\log(s_n)\\) grows only polynomially in \\(\\log(s_n)\\) and quadratically in \\(p\\). Therefore, for any fixed dimension \\(p\\), the probability bound approaches 1 as \\(s_n \\to \\infty\\). Even in high-dimensional settings where \\(p\\) is large (but still fixed), the exponential decay in \\(e^{-\\sqrt{s_n}/6}\\) dominates, provided \\(s_n\\) is sufficiently large relative to \\(p^2\\). Specifically, we need \\(\\sqrt{s_n} \\gg 6 \\log(p^2 \\log(s_n))\\) for the bound to be tight, which is satisfied when \\(s_n\\) grows faster than \\(\\log^2(p)\\). \\(\\square\\)</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#53-continuity-of-distance-regimes-and-unified-kernel-representation","title":"5.3 Continuity of Distance Regimes and Unified Kernel Representation","text":"<p>In the preceding analysis, we have categorized the behavior of random forest kernels into three distinct distance regimes. However, it is important to emphasize that these regimes do not have strict boundaries but rather represent a continuous spectrum of behaviors in the asymptotic setting as \\(n \\to \\infty\\).</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#531-continuous-transition-between-distance-regimes","title":"5.3.1 Continuous Transition Between Distance Regimes","text":"<p>The distance regimes we have defined: 1. Near distance: \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = o\\left(\\frac{1}{\\log(s_n)}\\right)\\) 2. Intermediate distance: \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\Theta\\left(\\frac{1}{\\log(s_n)}\\right)\\) 3. Far distance: \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\Theta(1)\\)</p> <p>represent mathematically convenient characterizations rather than discrete categories. In reality, the kernel function transitions smoothly across these regimes.</p> <p>Proposition 1 (Continuous Kernel Transition): Under Assumptions 1-5, the random forest kernel \\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n)\\) can be expressed in terms of a scaled distance \\(\\mathbf{u} = (\\mathbf{x} - \\mathbf{z}_n)\\log(s_n)\\) as:</p> <p>\\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}_n) \\approx \\begin{cases} 1 - \\lambda \\|\\mathbf{u}\\|_1 + O(\\|\\mathbf{u}\\|_1^2) &amp; \\text{if } \\|\\mathbf{u}\\|_1 \\to 0 \\\\ e^{-\\lambda \\|\\mathbf{u}\\|_1} + o(1) &amp; \\text{if } \\|\\mathbf{u}\\|_1 = \\Theta(1) \\\\ O(s_n^{-c}) &amp; \\text{if } \\|\\mathbf{u}\\|_1 \\to \\infty \\end{cases}\\)</p> <p>with probability at least \\(1 - O(p \\cdot d_n \\cdot e^{-\\sqrt{s_n}/6})\\).</p> <p>Proof: This follows directly from Theorem 2 by substituting \\(\\mathbf{u} = (\\mathbf{x} - \\mathbf{z}_n)\\log(s_n)\\) and considering the limiting behavior as \\(\\|\\mathbf{u}\\|_1\\) varies. For \\(\\|\\mathbf{u}\\|_1 \\to 0\\), we have \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = o\\left(\\frac{1}{\\log(s_n)}\\right)\\), and using the approximation \\(e^{-x} = 1 - x + O(x^2)\\) for small \\(x\\), we obtain the linear form. For \\(\\|\\mathbf{u}\\|_1 = \\Theta(1)\\), we have \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1 = \\Theta\\left(\\frac{1}{\\log(s_n)}\\right)\\), yielding the exponential form. \\(\\square\\)</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#532-unified-kernel-representation","title":"5.3.2 Unified Kernel Representation","text":"<p>The scaled distance \\(\\mathbf{u} = (\\mathbf{x} - \\mathbf{z}_n)\\log(s_n)\\) provides a unified framework for understanding the adaptive behavior of random forest kernels. This scaling is crucial in asymptotic analysis for the following reasons:</p> <ol> <li> <p>Preserving relative distances: As \\(n \\to \\infty\\), the feature space becomes increasingly dense with samples, causing nearest-neighbor distances to approach zero. The logarithmic scaling preserves the relative importance of distances in the asymptotic setting.</p> </li> <li> <p>Revealing adaptive bandwidth: The effective kernel bandwidth \\(h_n = \\Theta(1/\\log(s_n))\\) shrinks as sample size increases, but using scaled distances allows us to analyze the kernel shape independent of this shrinkage.</p> </li> <li> <p>Connecting theoretical regimes: The different functional forms across distance regimes can be understood as parts of a single, continuous kernel function when expressed in terms of scaled distances.</p> </li> </ol> <p>Corollary 1 (Limiting Kernel Function): As \\(n \\to \\infty\\), the random forest kernel approaches a limiting function of the scaled distance:</p> <p>\\(\\lim_{n \\to \\infty} K_{RF,n}(\\mathbf{x}, \\mathbf{x} + \\mathbf{u}/\\log(s_n)) = K_{\\infty}(\\mathbf{u})\\)</p> <p>where \\(K_{\\infty}(\\mathbf{u})\\) has the following properties: 1. For small \\(\\|\\mathbf{u}\\|_1\\): \\(K_{\\infty}(\\mathbf{u}) \\approx 1 - \\lambda \\|\\mathbf{u}\\|_1\\) 2. For moderate \\(\\|\\mathbf{u}\\|_1\\): \\(K_{\\infty}(\\mathbf{u}) \\approx e^{-\\lambda \\|\\mathbf{u}\\|_1}\\) 3. For large \\(\\|\\mathbf{u}\\|_1\\): \\(K_{\\infty}(\\mathbf{u}) \\approx 0\\)</p> <p>This limiting kernel function differs from traditional kernels (e.g., Gaussian, Laplace) in that it exhibits a linear decay near the origin rather than quadratic (Gaussian) or linear with constant slope (Laplace). This unique property contributes to the strong adaptive behavior of random forests.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#54-comparison-with-one-dimensional-results","title":"5.4 Comparison with One-Dimensional Results","text":"<p>The multi-dimensional results in Theorem 2 extend our one-dimensional findings in Theorem 1 in several important ways. Both theorems identify three distinct regimes of kernel behavior, characterized by the convergence rate of the sequence of points to the reference point. However, there are key differences and similarities worth highlighting:</p> <ol> <li> <p>Dimensionality Effect: In the multi-dimensional case, the \\(L_1\\) norm \\(\\|\\mathbf{x} - \\mathbf{z}_n\\|_1\\) replaces the absolute difference \\(|x - z_n|\\) in the one-dimensional case. This naturally captures the aggregated distance across all dimensions.</p> </li> <li> <p>Structural Similarity: Despite the dimensional difference, the asymptotic behaviors in all three regimes maintain the same functional form: exponential decay for distant points, specific exponential relationship for moderately close points, and linear relationship for very close points.</p> </li> <li> <p>Dimensional Scaling: The probability of separation at each level is averaged across dimensions, introducing a factor of \\(1/p\\) that reflects dimension-uniform split selection. This affects the constants in the convergence rates but not their asymptotic form.</p> </li> <li> <p>Error Propagation: The error terms in the multi-dimensional case account for potential imbalances across dimensions, but the overall convergence rates remain comparable to the one-dimensional case.</p> </li> <li> <p>Unified Representation: The scaled distance formulation introduced in Section 5.3 applies to both one-dimensional and multi-dimensional cases, showing that the fundamental adaptive behavior of random forest kernels is dimension-invariant when properly scaled.</p> </li> </ol> <p>These results demonstrate the consistency of random forest kernel behavior across different dimensionalities, strengthening our understanding of their adaptive properties.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#55-implications-for-adaptive-resolution-and-practice","title":"5.5 Implications for Adaptive Resolution and Practice","text":""},{"location":"research/note/forest-kernel-and-its-asymptotics/#551-practical-implications-of-continuous-distance-regimes","title":"5.5.1 Practical Implications of Continuous Distance Regimes","text":"<p>The continuous nature of distance regimes has important practical implications:</p> <ol> <li> <p>Smooth adaptation: Random forests smoothly adapt their prediction weights based on distance, without abrupt changes between \"included\" and \"excluded\" points. This property helps explain their robust performance across diverse datasets.</p> </li> <li> <p>Dimensional impact: In \\(p\\)-dimensional space, the effective number of points with non-negligible weights is approximately \\(N_{eff} \\approx n \\cdot (1/\\log(s_n))^p\\), which decreases with dimension \\(p\\) but not as rapidly as with fixed-bandwidth kernels. This gives random forests a relative advantage in moderately high-dimensional settings.</p> </li> <li> <p>Parameter tuning guidance: The tree depth parameter \\(\\lambda\\) directly affects the rate of weight decay with distance, providing a theoretical basis for tuning this parameter based on the desired level of locality in predictions. Specifically, larger values of \\(\\lambda\\) lead to more localized predictions.</p> </li> </ol>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#552-connection-to-adaptive-kernel-methods","title":"5.5.2 Connection to Adaptive Kernel Methods","text":"<p>The behavior of the random forest kernel bears striking similarities to adaptive kernel methods in nonparametric statistics (Scott, 2015; Wasserman, 2006). However, unlike traditional kernel methods that typically require explicit bandwidth selection, random forests implicitly adapt their resolution based on local data density. Our results formalize this connection, showing how:</p> <ul> <li>The effective bandwidth is automatically larger in sparse regions (regime (a))</li> <li>The bandwidth transitions smoothly in moderately dense regions (regime (b))</li> <li>Fine discrimination occurs in high-density regions (regime (c))</li> </ul> <p>This automatic adaptation explains many of the advantages of random forests over fixed-bandwidth methods, particularly in heterogeneous data settings where optimal bandwidth varies across the feature space.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#553-behavior-in-high-dimensional-settings","title":"5.5.3 Behavior in High-Dimensional Settings","text":"<p>In high-dimensional spaces, our results have particularly important implications. As dimensionality increases:</p> <ol> <li> <p>Sparsity Effects: The probability of points falling into the same leaf node decreases exponentially with dimension, a manifestation of the \"curse of dimensionality.\" However, the adaptive resolution property helps mitigate this challenge by adjusting the effective neighborhood size.</p> </li> <li> <p>Relevance for Feature Selection: When features vary in relevance, random forests with uniform dimension selection (Assumption 2) might be suboptimal. Our analysis suggests that modifications to the splitting rule to favor more informative dimensions could potentially improve performance in high dimensions.</p> </li> <li> <p>Robustness to Irrelevant Features: The exponential decay of kernel values for distant points (regime (a)) helps random forests remain robust to irrelevant features, as points that differ mainly in noise dimensions will still have small kernel values.</p> </li> </ol>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#554-sensitivity-to-model-assumptions","title":"5.5.4 Sensitivity to Model Assumptions","text":"<p>Our theoretical guarantees depend on several key assumptions. If these assumptions are violated, we expect the following effects:</p> <ul> <li> <p>Non-uniform Feature Selection (Assumption 2): If certain dimensions are selected with higher probability, the kernel will adapt more quickly along these dimensions. This would manifest as anisotropic behavior in the kernel, potentially beneficial when feature relevance varies.</p> </li> <li> <p>Non-uniform Feature Distributions (Assumption 5): When the feature distribution deviates from uniformity, the effective node sizes may vary significantly from our theoretical predictions. The kernel will adapt more finely in regions of high data density, further enhancing the adaptive resolution property.</p> </li> <li> <p>Different Tree Depths (Assumption 4): If trees are grown beyond depth \\(d_n = \\lambda p \\log(s_n)\\), the kernel will exhibit even sharper discrimination between close points, potentially leading to overfitting if noise is present.</p> </li> </ul> <p>These insights not only deepen our theoretical understanding of random forests but also provide practical guidance for their application and potential modification in various data settings.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#56-conclusion","title":"5.6 Conclusion","text":"<p>Our analysis of random forest kernels in the multi-dimensional case reveals that their adaptive resolution properties extend naturally from the one-dimensional setting. We have demonstrated that these kernels exhibit three distinct but continuously transitioning regimes of behavior based on the relative distance between points. By introducing the concept of scaled distance, we have provided a unified framework for understanding how random forests automatically adjust their smoothing bandwidth based on local data density.</p> <p>The limiting kernel function \\(K_{\\infty}(\\mathbf{u})\\) offers a novel characterization of random forest behavior that distinguishes it from traditional kernel methods. Its unique property of linear decay near the origin combined with exponential decay at moderate distances explains the strong adaptive behavior observed in practice. This theoretical characterization helps bridge the gap between random forests and adaptive kernel methods, providing new insights into why random forests perform well across diverse learning tasks and data structures.</p> <p>Our analysis of high-dimensional behavior and sensitivity to model assumptions provides valuable guidance for practitioners. The findings suggest specific directions for optimizing random forest performance in challenging settings, including potential modifications to feature selection strategies and tree depth control based on dimensional considerations.</p> <p>Future research directions might include extending these results to more general tree construction methods, exploring the implications for feature importance measures, and investigating the connection between kernel properties and generalization error in random forests. Additionally, the unified kernel representation could inform the development of new forest-inspired kernel methods that explicitly leverage the adaptive properties we have characterized.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#6-analysis-of-random-forest-weights-and-effective-neighborhood","title":"6. Analysis of Random Forest Weights and Effective Neighborhood","text":"<p>Having established the asymptotic behavior of the random forest kernel across different distance regimes, we now analyze the weights that random forests assign to training points when making predictions. These weights determine how the influence of training points varies with their distance from the query point, which is fundamental to understanding the adaptive nature of random forests. We also characterize the effective neighborhood size, providing precise bounds on the region of feature space that significantly influences predictions.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#61-asymptotic-behavior-of-weights","title":"6.1 Asymptotic Behavior of Weights","text":"<p>Recall that the random forest estimator can be expressed as a weighted average of training responses:</p> \\[\\hat{f}_{RF,n}(\\mathbf{x}) = \\sum_{i=1}^n \\alpha_i(\\mathbf{x})Y_i\\] <p>where the weights are defined as:</p> \\[\\alpha_i(\\mathbf{x}) = \\frac{K_{RF,n}(\\mathbf{x},\\mathbf{X}_i)}{\\sum_{j=1}^n K_{RF,n}(\\mathbf{x},\\mathbf{X}_j)}\\] <p>These weights determine how much influence each training point has on the prediction at query point \\(\\mathbf{x}\\). The following theorem characterizes their asymptotic behavior.</p> <p>Theorem 3 (Asymptotic Behavior of Random Forest Weights): Under Assumptions 1-5 and as \\(n,B \\to \\infty\\), the weights \\(\\alpha_i(\\mathbf{x})\\) in the random forest estimator satisfy with probability at least \\(1 - O(n \\cdot p \\cdot d_n \\cdot e^{-\\sqrt{s_n}/6})\\):</p> <p>(a) For \\(\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\Theta(1)\\) (distant points): \\(\\(\\alpha_i(\\mathbf{x}) = O(s_n^{-c})\\)\\) for some constant \\(c &gt; 0\\).</p> <p>(b) For \\(\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\frac{u_i}{\\log(s_n)}\\) with \\(u_i &gt; 0\\) (moderately close points): \\(\\(\\alpha_i(\\mathbf{x}) = \\frac{e^{-\\lambda u_i}(1 + O(\\frac{1}{\\log(s_n)} + \\frac{1}{\\sqrt{s_n}}))}{\\sum_{j: \\|\\mathbf{x} - \\mathbf{X}_j\\|_1 = \\Theta(1/\\log(s_n))} e^{-\\lambda u_j}(1 + O(\\frac{1}{\\log(s_n)} + \\frac{1}{\\sqrt{s_n}}))}\\)\\)</p> <p>(c) For \\(\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = o\\left(\\frac{1}{\\log(s_n)}\\right)\\) (very close points): \\(\\(\\alpha_i(\\mathbf{x}) \\approx \\frac{1 - \\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 + O((\\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1)^2)}{\\sum_{j: \\|\\mathbf{x} - \\mathbf{X}_j\\|_1 = O(1/\\log(s_n))} (1 - \\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_j\\|_1 + O((\\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_j\\|_1)^2))}\\)\\)</p> <p>Proof: The forest weights are defined as: \\(\\(\\alpha_i(\\mathbf{x}) = \\frac{K_{RF,n}(\\mathbf{x}, \\mathbf{X}_i)}{\\sum_{j=1}^n K_{RF,n}(\\mathbf{x}, \\mathbf{X}_j)}\\)\\)</p> <p>From Theorem 2, we know the asymptotic behavior of \\(K_{RF,n}(\\mathbf{x}, \\mathbf{X}_i)\\) across different distance regimes. To determine the weights, we need to analyze both the numerator (the kernel value for a specific point) and the denominator (the sum of kernel values across all training points).</p> <p>Under Assumption 5 (bounded density), the denominator \\(\\sum_{j=1}^n K_{RF,n}(\\mathbf{x}, \\mathbf{X}_j)\\) is dominated by points in the \\(\\Theta(1/\\log(s_n))\\) neighborhood of \\(\\mathbf{x}\\). With high probability, there are \\(\\Theta(n \\cdot (1/\\log(s_n))^p)\\) points in this neighborhood, each contributing substantially to the sum.</p> <p>For case (a), where \\(\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\Theta(1)\\), Theorem 2(a) gives us \\(K_{RF,n}(\\mathbf{x}, \\mathbf{X}_i) = O(s_n^{-c})\\). The denominator is \\(\\Theta(n \\cdot (1/\\log(s_n))^p)\\), reflecting the number of points with significant kernel values. Therefore: \\(\\(\\alpha_i(\\mathbf{x}) = \\frac{O(s_n^{-c})}{\\Theta(n \\cdot (1/\\log(s_n))^p)} = O(s_n^{-c})\\)\\)</p> <p>This shows that points at a constant distance from the query point have exponentially decreasing influence as the sample size increases.</p> <p>For case (b), where \\(\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\frac{u_i}{\\log(s_n)}\\), Theorem 2(b) gives us \\(K_{RF,n}(\\mathbf{x}, \\mathbf{X}_i) = e^{-\\lambda u_i}(1 + \\delta_i)\\) where \\(|\\delta_i| = O(\\frac{1}{\\log(s_n)} + \\frac{1}{\\sqrt{s_n}})\\). The denominator sum can be expressed as: \\(\\(\\sum_{j=1}^n K_{RF,n}(\\mathbf{x}, \\mathbf{X}_j) = \\sum_{j: \\|\\mathbf{x} - \\mathbf{X}_j\\|_1 = \\Theta(1/\\log(s_n))} e^{-\\lambda u_j}(1 + \\delta_j) + \\sum_{j: \\|\\mathbf{x} - \\mathbf{X}_j\\|_1 \\neq \\Theta(1/\\log(s_n))} K_{RF,n}(\\mathbf{x}, \\mathbf{X}_j)\\)\\)</p> <p>The second sum is negligible compared to the first, as points outside the \\(\\Theta(1/\\log(s_n))\\) neighborhood have exponentially smaller kernel values. Therefore: \\(\\(\\alpha_i(\\mathbf{x}) = \\frac{e^{-\\lambda u_i}(1 + \\delta_i)}{\\sum_{j: \\|\\mathbf{x} - \\mathbf{X}_j\\|_1 = \\Theta(1/\\log(s_n))} e^{-\\lambda u_j}(1 + \\delta_j)}\\)\\)</p> <p>For case (c), we use the linear approximation of the kernel function derived in Theorem 2(c). For points very close to \\(\\mathbf{x}\\), the kernel value is approximately \\(1 - \\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1(1 + \\gamma_i)\\) where \\(|\\gamma_i| = O(\\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 + \\frac{1}{\\sqrt{s_n}})\\). Substituting into the weight formula and simplifying: \\(\\(\\alpha_i(\\mathbf{x}) \\approx \\frac{1 - \\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 + O((\\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1)^2)}{\\sum_{j: \\|\\mathbf{x} - \\mathbf{X}_j\\|_1 = O(1/\\log(s_n))} (1 - \\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_j\\|_1 + O((\\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_j\\|_1)^2))}\\)\\)</p> <p>The probability bound \\(1 - O(n \\cdot p \\cdot d_n \\cdot e^{-\\sqrt{s_n}/6})\\) is derived by applying the union bound to the concentration results from Theorem 2 across all \\(n\\) training points. The term \\(e^{-\\sqrt{s_n}/6}\\) decreases exponentially with \\(\\sqrt{s_n}\\), while the factors \\(n\\), \\(p\\), and \\(d_n = \\lambda p \\log(s_n)\\) increase polynomially. For any fixed dimension \\(p\\), as \\(s_n\\) increases, the exponential decay dominates the polynomial growth, ensuring that the probability bound approaches 1. Specifically, when \\(s_n = \\Omega((\\log n)^2)\\), the probability bound becomes \\(1 - o(1)\\). \\(\\square\\)</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#611-implications-of-weight-distribution","title":"6.1.1 Implications of Weight Distribution","text":"<p>Theorem 3 reveals several important properties of random forest weights:</p> <ol> <li> <p>Exponential decay with distance: Points outside the \\(\\Theta(1/\\log(s_n))\\) neighborhood have negligible influence on predictions, effectively creating an adaptive soft boundary for relevant points.</p> </li> <li> <p>Relative importance within neighborhood: Within the effective neighborhood, the importance of training points decays exponentially with their scaled distance from the query point, with the parameter \\(\\lambda\\) controlling the rate of decay.</p> </li> <li> <p>Adaptive smoothing: For very close points, the weights vary almost linearly with distance, providing finer-grained discrimination in regions of high data density.</p> </li> <li> <p>Dimensional scaling: The number of points with non-negligible weights scales as \\(\\Theta(n \\cdot (1/\\log(s_n))^p)\\), which decreases with dimension \\(p\\) but less dramatically than with fixed-bandwidth kernels.</p> </li> </ol> <p>These properties demonstrate how random forests automatically adapt their prediction weights based on both local data density and the global sample size, without requiring explicit bandwidth selection.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#62-effective-neighborhood-size-and-boundary","title":"6.2 Effective Neighborhood Size and Boundary","text":"<p>A key question in understanding local adaptive methods is: how large is the neighborhood that effectively influences predictions? The following theorem provides a precise characterization of this effective neighborhood size.</p> <p>Theorem 4 (Expected Maximum Distance): Under Assumptions 1-5, as \\(n,B \\to \\infty\\), the expected maximum distance of points with non-negligible weights satisfies:</p> \\[\\mathbb{E}\\left[\\sup\\{\\|\\mathbf{X}_i - \\mathbf{x}\\|_2: \\alpha_i(\\mathbf{x}) &gt; \\varepsilon_n\\}\\right] = \\Theta\\left(\\frac{\\sqrt{p}}{\\log(s_n)}\\right)\\] <p>where \\(\\varepsilon_n = n^{-\\beta}\\) for any fixed \\(0 &lt; \\beta &lt; c\\) is a threshold that approaches zero more slowly than the smallest non-zero weight \\(O(s_n^{-c})\\).</p> <p>Proof: From Theorem 3, we know that points with \\(\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\Theta(1)\\) have weights \\(\\alpha_i(\\mathbf{x}) = O(s_n^{-c})\\). As \\(s_n \\to \\infty\\), these weights become effectively zero. However, there is no sharp boundary where weights suddenly become zero; instead, they decrease continuously with distance.</p> <p>To formalize the concept of an \"effective neighborhood,\" we consider points with weights exceeding a threshold \\(\\varepsilon_n = n^{-\\beta}\\) for some fixed \\(0 &lt; \\beta &lt; c\\). This specific form ensures that \\(\\varepsilon_n\\) approaches zero more slowly than \\(O(s_n^{-c})\\) as \\(n\\) increases, capturing points that have a non-negligible influence on predictions as the sample size grows.</p> <p>The effective neighborhood is primarily determined by the intermediate distance regime where \\(\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\Theta(1/\\log(s_n))\\). To find the boundary of this neighborhood, we need to determine the distance at which \\(\\alpha_i(\\mathbf{x}) = \\varepsilon_n\\).</p> <p>From Theorem 3(b) and using the unified kernel representation established in Section 5.3, we have:</p> \\[\\alpha_i(\\mathbf{x}) \\approx \\frac{e^{-\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1}}{\\sum_{j} e^{-\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_j\\|_1}}\\] <p>For a point at the boundary of the effective neighborhood, \\(\\alpha_i(\\mathbf{x}) = \\varepsilon_n\\), which implies:</p> \\[e^{-\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1} = \\varepsilon_n \\cdot \\sum_{j} e^{-\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_j\\|_1}\\] <p>Taking logarithms of both sides:</p> \\[-\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\log(\\varepsilon_n) + \\log\\left(\\sum_{j} e^{-\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_j\\|_1}\\right)\\] <p>Solving for \\(\\|\\mathbf{x} - \\mathbf{X}_i\\|_1\\):</p> \\[\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\frac{-\\log(\\varepsilon_n) - \\log\\left(\\sum_{j} e^{-\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_j\\|_1}\\right)}{\\lambda \\log(s_n)}\\] <p>Under Assumption 5 (bounded density), there are approximately \\(\\Theta(n \\cdot (1/\\log(s_n))^p)\\) points in the neighborhood of \\(\\mathbf{x}\\) with non-negligible kernel values. The sum in the denominator of the weights is dominated by points close to \\(\\mathbf{x}\\), giving:</p> \\[\\sum_{j} e^{-\\lambda \\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_j\\|_1} = \\Theta\\left(n \\cdot \\left(\\frac{1}{\\log(s_n)}\\right)^p\\right)\\] <p>Substituting this and \\(\\varepsilon_n = n^{-\\beta}\\) into our expression for \\(\\|\\mathbf{x} - \\mathbf{X}_i\\|_1\\):</p> \\[\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\frac{\\beta \\log(n) - \\log\\left(\\Theta\\left(n \\cdot \\left(\\frac{1}{\\log(s_n)}\\right)^p\\right)\\right)}{\\lambda \\log(s_n)}\\] \\[= \\frac{\\beta \\log(n) - \\log(n) - \\log\\left(\\Theta\\left(\\left(\\frac{1}{\\log(s_n)}\\right)^p\\right)\\right)}{\\lambda \\log(s_n)}\\] \\[= \\frac{(\\beta-1) \\log(n) + p \\log(\\log(s_n)) + O(1)}{\\lambda \\log(s_n)}\\] <p>For typical parameter settings where \\(s_n = \\Theta(n)\\), and since \\(\\beta &lt; 1\\) for our choice of threshold, the dominant term is:</p> \\[\\|\\mathbf{x} - \\mathbf{X}_i\\|_1 = \\Theta\\left(\\frac{1}{\\log(s_n)}\\right)\\] <p>Using the relationship between L1 and L2 norms in \\(\\mathbb{R}^p\\):</p> \\[\\|\\mathbf{x} - \\mathbf{X}_i\\|_2 \\leq \\|\\mathbf{x} - \\mathbf{X}_i\\|_1 \\leq \\sqrt{p} \\cdot \\|\\mathbf{x} - \\mathbf{X}_i\\|_2\\] <p>We obtain:</p> \\[\\mathbb{E}\\left[\\sup\\{\\|\\mathbf{X}_i - \\mathbf{x}\\|_2: \\alpha_i(\\mathbf{x}) &gt; \\varepsilon_n\\}\\right] = \\Theta\\left(\\frac{\\sqrt{p}}{\\log(s_n)}\\right)\\] <p>This shows that the effective neighborhood radius in Euclidean distance scales inversely with \\(\\log(s_n)\\) and proportionally to the square root of the dimension. \\(\\square\\)</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#63-expected-number-of-points-in-the-effective-neighborhood","title":"6.3 Expected Number of Points in the Effective Neighborhood","text":"<p>Given our characterization of the effective neighborhood size, we can now analyze the expected number of training points that significantly influence predictions.</p> <p>Corollary 1 (Expected Neighborhood Population): Under Assumptions 1-5, as \\(n,B \\to \\infty\\), the expected number of training points with non-negligible weights satisfies:</p> \\[\\mathbb{E}[|\\{i: \\alpha_i(\\mathbf{x}) &gt; \\varepsilon_n\\}|] = \\Theta\\left(n \\cdot \\left(\\frac{1}{\\log(s_n)}\\right)^p\\right)\\] <p>Proof: From Theorem 4, the effective neighborhood has a radius of \\(\\Theta(\\sqrt{p}/\\log(s_n))\\) in the L2 norm. To calculate the volume of this neighborhood, we need to consider the p-dimensional ball with this radius.</p> <p>The volume of a p-dimensional ball with radius \\(r\\) is \\(V_p(r) = \\frac{\\pi^{p/2}}{\\Gamma(p/2+1)}r^p\\), where \\(\\Gamma\\) is the gamma function. Substituting \\(r = \\Theta(\\sqrt{p}/\\log(s_n))\\):</p> \\[V_p\\left(\\Theta\\left(\\frac{\\sqrt{p}}{\\log(s_n)}\\right)\\right) = \\frac{\\pi^{p/2}}{\\Gamma(p/2+1)} \\cdot \\Theta\\left(\\frac{\\sqrt{p}}{\\log(s_n)}\\right)^p\\] <p>This simplifies to:</p> \\[V_p\\left(\\Theta\\left(\\frac{\\sqrt{p}}{\\log(s_n)}\\right)\\right) = \\Theta\\left(\\frac{p^{p/2}}{(\\log(s_n))^p} \\cdot \\frac{\\pi^{p/2}}{\\Gamma(p/2+1)}\\right)\\] <p>Using Stirling's approximation for the gamma function: \\(\\Gamma(p/2+1) \\approx \\sqrt{2\\pi} \\cdot (p/2)^{p/2} \\cdot e^{-p/2}\\), we find that the ratio \\(\\frac{\\pi^{p/2}}{\\Gamma(p/2+1)}\\) is \\(\\Theta(p^{-p/2})\\), which cancels the \\(p^{p/2}\\) term in the numerator. Thus:</p> \\[V_p\\left(\\Theta\\left(\\frac{\\sqrt{p}}{\\log(s_n)}\\right)\\right) = \\Theta\\left(\\left(\\frac{1}{\\log(s_n)}\\right)^p\\right)\\] <p>Under Assumption 5 (bounded density), the expected number of points in this volume is proportional to \\(n\\) times the volume, giving:</p> <p>\\(\\(\\mathbb{E}[|\\{i: \\alpha_i(\\mathbf{x}) &gt; \\varepsilon_n\\}|] = \\Theta\\left(n \\cdot \\left(\\frac{1}{\\log(s_n)}\\right)^p\\right)\\)\\) \\(\\square\\)</p> <p>This result reveals an important property of random forests: as the sample size increases, the absolute number of influential points grows, but their proportion relative to the total sample size decreases. Specifically, the proportion of influential points decreases as \\(\\Theta((\\log(s_n))^{-p})\\).</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#64-comparison-with-traditional-kernel-methods","title":"6.4 Comparison with Traditional Kernel Methods","text":"<p>The behavior of random forest weights reveals important differences from traditional kernel methods that help explain their adaptive properties. </p> <p>In traditional kernel regression with a fixed bandwidth \\(h\\), the weights typically take the form:</p> \\[\\alpha_i^{kernel}(\\mathbf{x}) = \\frac{K\\left(\\frac{\\|\\mathbf{x} - \\mathbf{X}_i\\|}{h}\\right)}{\\sum_{j=1}^n K\\left(\\frac{\\|\\mathbf{x} - \\mathbf{X}_j\\|}{h}\\right)}\\] <p>where \\(K(\\cdot)\\) is a kernel function such as the Gaussian or Epanechnikov kernel.</p> <p>For instance, the Gaussian kernel yields weights of the form:</p> \\[\\alpha_i^{Gauss}(\\mathbf{x}) = \\frac{\\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{X}_i\\|_2^2}{2h^2}\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{X}_j\\|_2^2}{2h^2}\\right)}\\] <p>When compared to random forest weights, several key differences emerge:</p> <ol> <li> <p>Adaptive bandwidth: Random forests implicitly use a bandwidth of \\(h_n = \\Theta(1/\\log(s_n))\\) that adapts to the sample size, automatically becoming more localized as more data becomes available.</p> </li> <li> <p>Shape adaptation: The random forest kernel exhibits different functional forms at different distance scales (linear for very close points, exponential for moderately close points), providing more nuanced adaptation than traditional kernels with a fixed functional form.</p> </li> <li> <p>Dimensional scaling: The effective neighborhood size in random forests scales as \\(O(\\sqrt{p}/\\log(s_n))\\), which is less sensitive to the curse of dimensionality than the typical \\(O(h)\\) scaling in fixed-bandwidth methods.</p> </li> <li> <p>Decay profile: For close points, the Gaussian kernel exhibits quadratic decay (\\(\\alpha_i^{Gauss}(\\mathbf{x}) \\approx 1 - \\Theta((\\log(s_n))^2\\|\\mathbf{x} - \\mathbf{X}_i\\|_2^2)\\)), whereas random forests show linear decay (\\(\\alpha_i(\\mathbf{x}) \\approx 1 - \\Theta(\\log(s_n)\\|\\mathbf{x} - \\mathbf{X}_i\\|_1)\\)).</p> </li> </ol> <p>These differences contribute to random forests' adaptive behavior and help explain their empirical success across diverse learning problems.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#65-practical-implications","title":"6.5 Practical Implications","text":"<p>The theoretical results on random forest weights and effective neighborhood size have several important implications for practitioners:</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#651-feature-space-coverage","title":"6.5.1 Feature Space Coverage","text":"<p>In high-dimensional spaces, the curse of dimensionality typically makes it difficult to achieve adequate coverage of the feature space. Our results show that random forests mitigate this problem through their adaptive neighborhood sizing. As dimensionality increases, the effective neighborhood expands just enough to include a sufficient number of training points.</p> <p>The expected number of influential points scales as \\(\\Theta(n \\cdot (1/\\log(s_n))^p)\\), which decreases exponentially with dimension \\(p\\). However, this decrease is milder than in fixed-bandwidth methods where the number of points in a neighborhood of radius \\(h\\) decreases as \\(\\Theta(n \\cdot h^p)\\). This explains why random forests often outperform traditional methods in moderate to high-dimensional settings.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#652-parameter-tuning-guidance","title":"6.5.2 Parameter Tuning Guidance","text":"<p>Our analysis provides theoretical guidance for tuning the key parameters of random forests:</p> <ol> <li> <p>Tree depth parameter \\(\\lambda\\): This parameter directly affects the rate of weight decay with distance. Larger values of \\(\\lambda\\) lead to more localized predictions with sharper transitions between distance regimes. In practice, this corresponds to growing deeper trees relative to the subsample size.</p> </li> <li> <p>Subsample size \\(s_n\\): The effective neighborhood size scales as \\(\\Theta(1/\\log(s_n))\\). Larger subsample sizes result in smaller effective neighborhoods and more localized predictions. This suggests that increasing the subsample size can help reduce bias in regions with sufficient data density, but may increase variance in sparse regions.</p> </li> <li> <p>Number of trees \\(B\\): While our asymptotic results assume \\(B \\to \\infty\\), in practice, a finite number of trees introduces additional variance. To ensure that the empirical weights are close to their theoretical values with high probability, the number of trees should increase with the desired precision of the weights.</p> </li> </ol>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#653-local-variable-importance","title":"6.5.3 Local Variable Importance","text":"<p>The characterization of random forest weights provides a foundation for developing more precise local variable importance measures. By understanding how training points influence predictions based on their distance from the query point, researchers can develop variable importance measures that reflect the local structure of the feature space more accurately.</p> <p>This understanding could lead to more interpretable models that can identify which variables are most important in different regions of the feature space, rather than relying solely on global importance measures.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#66-conclusion","title":"6.6 Conclusion","text":"<p>Our analysis of random forest weights and effective neighborhood size provides key insights into how random forests adaptively adjust their prediction influence based on sample size, dimensionality, and local data density. The weights exhibit a smooth transition from linear to exponential decay with distance, creating an effective soft boundary for relevant points.</p> <p>The effective neighborhood size scales as \\(\\Theta(\\sqrt{p}/\\log(s_n))\\) in Euclidean distance, demonstrating how random forests automatically adapt their resolution based on both sample size and dimensionality. This adaptive behavior helps explain the strong empirical performance of random forests across diverse learning tasks and data structures.</p> <p>Through comparison with traditional kernel methods, we have shown that random forests combine the benefits of linear and exponential weight decay in different distance regimes, providing a unique adaptive profile that mitigates the curse of dimensionality while maintaining local sensitivity.</p> <p>These results not only deepen our theoretical understanding of random forests but also provide practical guidance for their application and tuning in various settings. The connection between random forest weights and adaptive kernel methods opens new avenues for developing hybrid approaches that combine the strengths of both paradigms.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#7-comprehensive-simulation-studies","title":"7. Comprehensive Simulation Studies","text":"<p>In this section, we present a detailed empirical validation of our theoretical framework through extensive simulation studies. These simulations are specifically designed to verify the key properties of random forest kernels across different distance regimes, while also investigating the effect of sample size and subsampling strategies on kernel behavior.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#71-simulation-design","title":"7.1 Simulation Design","text":"<p>Our simulation studies employ a custom implementation of random forests that strictly adheres to the theoretical assumptions specified in Section 3. We focus on evaluating how closely the empirical kernel behavior aligns with theoretical predictions across various parameter configurations.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#711-experimental-setup","title":"7.1.1 Experimental Setup","text":"<p>The key components of our simulation design include:</p> <ol> <li> <p>Multiple Trials: For each parameter configuration, we conduct 10 independent trials to assess the variability in empirical kernel estimates.</p> </li> <li> <p>Varied Sample Sizes: We investigate sample sizes \\(n \\in \\{200, 500, 1000, 1500, 2000\\}\\) to evaluate convergence properties.</p> </li> <li> <p>Subsampling Strategies: We explore five different subsampling approaches:</p> </li> <li>\\(s_n = \\sqrt{n}\\) (traditional random forests)</li> <li>\\(s_n = n/3\\) (common in practice)</li> <li>\\(s_n = n^{0.8}\\) (moderate subsampling)</li> <li>\\(s_n = n^{0.9}\\) (Wager and Athey approach)</li> <li> <p>\\(s_n = n^{0.98}\\) (nearly full sampling)</p> </li> <li> <p>Fixed Parameters:</p> </li> <li>Dimension: \\(p = 2\\) (fixed for clarity of visualization)</li> <li>Number of trees: \\(B = 2000\\) (to ensure stability in kernel estimates)</li> <li> <p>Tree depth: \\(d_n = \\lambda p \\log(s_n)\\) as specified in Assumption 4</p> </li> <li> <p>Distance Regimes: We systematically generate test points across the three distance regimes:</p> </li> <li>Very close regime: \\(\\|\\mathbf{x} - \\mathbf{z}\\|_1 = o\\left(\\frac{1}{\\log(s_n)}\\right)\\)</li> <li>Intermediate regime: \\(\\|\\mathbf{x} - \\mathbf{z}\\|_1 = \\Theta\\left(\\frac{1}{\\log(s_n)}\\right)\\)</li> <li>Far regime: \\(\\|\\mathbf{x} - \\mathbf{z}\\|_1 = \\Theta(1)\\)</li> </ol> <p>For each configuration, we compute both empirical kernel values and their corresponding theoretical predictions, allowing direct comparison between theory and practice.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#712-implementation-details","title":"7.1.2 Implementation Details","text":"<p>We implemented a custom random forest algorithm that faithfully follows our theoretical framework:</p> <ol> <li> <p>Tree Building: Each tree is constructed using a random subsample of the training data, with splits chosen uniformly at random from available features (Assumption 2).</p> </li> <li> <p>Kernel Computation: The kernel value \\(K_{RF,n}(\\mathbf{x}, \\mathbf{z})\\) is calculated as the proportion of trees in which points \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) fall into the same leaf node.</p> </li> <li> <p>Theoretical Curves: For comprehensive comparison, we compute theoretical values for both the very close regime (\\(1 - \\lambda\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}\\|_1\\)) and the intermediate regime (\\(e^{-\\lambda\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}\\|_1}\\)) across the entire range of distances.</p> </li> </ol> <p>This implementation enables us to directly validate our theoretical characterization of the random forest kernel.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#72-empirical-validation-of-kernel-behavior","title":"7.2 Empirical Validation of Kernel Behavior","text":"<p>Our primary objective is to examine how closely the empirical kernel behavior aligns with our theoretical predictions across different distance regimes. Figure 1 illustrates this comparison for a sample size of \\(n = 1000\\) with the subsampling strategy \\(s_n = n^{0.9}\\).</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#721-regime-specific-behavior","title":"7.2.1 Regime-Specific Behavior","text":"<p>The empirical results strongly validate our theoretical predictions regarding the three distinct regimes of kernel behavior:</p> <ol> <li> <p>Very Close Regime: For points at distances significantly smaller than \\(1/\\log(s_n)\\), the kernel values exhibit a linear relationship with distance, closely following the theoretical prediction \\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}) \\approx 1 - \\lambda\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}\\|_1\\). This confirms Theorem 2(c).</p> </li> <li> <p>Intermediate Regime: For points at distances proportional to \\(1/\\log(s_n)\\), the kernel values demonstrate exponential decay, matching the theoretical prediction \\(K_{RF,n}(\\mathbf{x}, \\mathbf{z}) \\approx e^{-\\lambda\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}\\|_1}\\). This validates Theorem 2(b).</p> </li> <li> <p>Far Regime: For points at constant distances, the kernel values rapidly approach zero, consistent with the exponential decay predicted in Theorem 2(a).</p> </li> </ol> <p>The smooth transition between these regimes confirms our analysis in Section 5.3 regarding the continuous nature of the kernel function.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#722-impact-of-theoretical-curves-extension","title":"7.2.2 Impact of Theoretical Curves Extension","text":"<p>By plotting both theoretical curves (linear and exponential) across the entire range of distances, we gain additional insights into the kernel behavior:</p> <ol> <li> <p>The linear approximation (\\(1 - \\lambda\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}\\|_1\\)) is highly accurate in the very close regime but quickly becomes inappropriate at larger distances, even predicting negative kernel values.</p> </li> <li> <p>The exponential approximation (\\(e^{-\\lambda\\log(s_n)\\|\\mathbf{x} - \\mathbf{z}\\|_1}\\)) provides an excellent fit in the intermediate regime and remains reasonable in the far regime.</p> </li> <li> <p>The intersection of these curves naturally identifies a transition point between the regimes, occurring at approximately \\(\\|\\mathbf{x} - \\mathbf{z}\\|_1 \\approx 0.1/\\log(s_n)\\).</p> </li> </ol> <p>These observations reinforce the necessity of different functional approximations for different distance regimes, which is a key contribution of our theoretical framework.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#73-convergence-properties-and-sample-size-effects","title":"7.3 Convergence Properties and Sample Size Effects","text":"<p>A critical aspect of our asymptotic analysis is the convergence of empirical kernel behavior to theoretical predictions as sample size increases. Figure 2 illustrates how kernel variance decreases with increasing sample size across different subsampling strategies.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#731-kernel-variance","title":"7.3.1 Kernel Variance","text":"<p>Our results demonstrate that for all subsampling strategies and distance regimes, the variance of empirical kernel estimates decreases as sample size increases. Specifically:</p> <ol> <li> <p>Rate of Convergence: The average standard deviation of kernel estimates decreases approximately at a rate of \\(O(n^{-1/2})\\), consistent with standard statistical convergence rates.</p> </li> <li> <p>Regime-Specific Stability: Kernel estimates in the very close regime exhibit the lowest variance, while estimates in the far regime show higher variability, especially at smaller sample sizes.</p> </li> <li> <p>Subsample Effect: Strategies with larger subsamples (e.g., \\(s_n = n^{0.98}\\)) generally show lower variance than those with smaller subsamples (e.g., \\(s_n = \\sqrt{n}\\)), particularly for intermediate and far regimes.</p> </li> </ol> <p>These findings confirm that our theoretical characterization becomes increasingly accurate as sample size grows, supporting the asymptotic nature of our results.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#732-distance-scaling-property","title":"7.3.2 Distance Scaling Property","text":"<p>An important theoretical prediction is that when distances are properly scaled by \\(\\log(s_n)\\), the kernel behavior follows a universal pattern regardless of sample size. Figure 3 confirms this property by plotting kernel values against scaled distance \\(u = \\log(s_n)\\|\\mathbf{x} - \\mathbf{z}\\|_1\\) for different sample sizes.</p> <p>The results show remarkable alignment of kernel curves across different sample sizes when using the scaled distance, with all curves closely following the theoretical prediction \\(e^{-\\lambda u}\\) in the intermediate regime. This consistency validates our theoretical framework's ability to capture the essential scaling behavior of random forest kernels.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#74-impact-of-subsampling-strategy","title":"7.4 Impact of Subsampling Strategy","text":"<p>Our investigation of different subsampling strategies reveals several important insights about their effect on kernel behavior. Figure 4 compares kernel shapes across different subsampling strategies for a fixed sample size.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#741-kernel-shape-variation","title":"7.4.1 Kernel Shape Variation","text":"<p>When examining kernel behavior across subsampling strategies, we observe:</p> <ol> <li> <p>Overall Shape Consistency: All subsampling strategies produce kernel functions that exhibit the three theoretical regimes, though with varying transition points and decay rates.</p> </li> <li> <p>Decay Rate Differences: Larger subsampling rates (e.g., \\(s_n = n^{0.98}\\)) lead to sharper decay in the intermediate regime compared to smaller rates (e.g., \\(s_n = \\sqrt{n}\\)).</p> </li> <li> <p>Effective Neighborhood Size: The effective neighborhood (points with non-negligible kernel values) is larger for smaller subsampling rates and narrower for larger rates.</p> </li> </ol> <p>These observations can be explained through our theoretical framework: larger subsamples lead to deeper trees (since \\(d_n = \\lambda p \\log(s_n)\\)), resulting in more refined partitioning of the feature space and consequently sharper discriminative capability between points.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#742-theoretical-alignment","title":"7.4.2 Theoretical Alignment","text":"<p>Interestingly, we find that all subsampling strategies show reasonable agreement with our theoretical predictions, with some variations:</p> <ol> <li> <p>Traditional Rate (\\(s_n = \\sqrt{n}\\)): Shows good overall alignment with theory and provides a balance between very close and intermediate regimes.</p> </li> <li> <p>Wager-Athey Rate (\\(s_n = n^{0.9}\\)): Demonstrates excellent agreement in the intermediate regime but potentially faster transition from very close to intermediate regimes.</p> </li> <li> <p>Nearly Full Sampling (\\(s_n = n^{0.98}\\)): Exhibits the sharpest distinction between regimes, with very rapid transition from kernel values near 1 to values near 0.</p> </li> </ol> <p>These results suggest that while the specific subsampling rate affects the quantitative details of kernel behavior, the qualitative characteristics predicted by our theory remain valid across different subsampling strategies.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#75-implications-and-practical-considerations","title":"7.5 Implications and Practical Considerations","text":"<p>Our comprehensive simulation studies yield several important implications for the theoretical understanding and practical application of random forests:</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#751-theoretical-validation","title":"7.5.1 Theoretical Validation","text":"<p>The strong agreement between empirical and theoretical kernel behavior across multiple parameter configurations provides robust validation of our theoretical framework. The three-regime characterization accurately captures the essential behavior of random forest kernels, regardless of specific implementation details such as subsampling rate.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#752-subsampling-recommendations","title":"7.5.2 Subsampling Recommendations","text":"<p>Based on our findings, we can offer practical recommendations regarding subsampling strategies:</p> <ol> <li> <p>Statistical Efficiency: Larger subsampling rates (\\(s_n = n^{0.9}\\) or higher) offer lower variance in kernel estimates, making them preferable when computational resources allow.</p> </li> <li> <p>Computational Efficiency: Smaller subsampling rates (e.g., \\(s_n = \\sqrt{n}\\)) still provide reasonable kernel estimates with significantly reduced computational cost, which may be crucial for large-scale applications.</p> </li> <li> <p>Adaptive Resolution: The choice of subsampling rate effectively controls the sharpness of discrimination between points at different distances, allowing practitioners to adjust this parameter based on their specific needs for local adaptivity.</p> </li> </ol>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#753-connection-to-forest-depth","title":"7.5.3 Connection to Forest Depth","text":"<p>Our simulations highlight the critical role of tree depth in determining kernel properties. The depth parameter \\(\\lambda\\) and the subsample size \\(s_n\\) jointly control the effective resolution of the forest through the relationship \\(d_n = \\lambda p \\log(s_n)\\). This suggests that practitioners may want to directly control tree depth based on the desired kernel properties rather than focusing solely on subsampling rates.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#76-summary","title":"7.6 Summary","text":"<p>Our comprehensive simulation studies provide strong empirical validation of the theoretical framework developed in this paper. The results confirm the existence of three distinct distance regimes in random forest kernels and demonstrate how kernel behavior converges to theoretical predictions as sample size increases. The investigations into different subsampling strategies reveal that while quantitative details may vary, the fundamental properties of random forest kernels predicted by our theory remain consistent across implementations.</p> <p>These findings strengthen our understanding of how random forests adaptively adjust their smoothing behavior based on distance, sample size, and subsampling strategy, providing both theoretical insights and practical guidance for the application of random forests in various learning tasks.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#8-conclusion-and-future-work","title":"8. Conclusion and future work","text":""},{"location":"research/note/forest-kernel-and-its-asymptotics/#81-contributions","title":"8.1 Contributions","text":"<p>This paper provides a rigorous mathematical analysis of the kernel induced by random forests, characterizing its asymptotic behavior across different distance regimes. Our primary contributions are:</p> <p>First, we have established that the random forest kernel exhibits three distinct behaviors depending on the distance between points relative to sample size: exponential decay for distant points, a specific exponential relationship for moderately close points, and a linear relationship for very close points. These regimes emerge naturally from the tree-building process and explain the adaptive smoothing capability of random forests.</p> <p>Second, we have derived precise asymptotic forms for the kernel function in each regime, showing that the transition between regimes is governed by the quantity \\(1/\\log(s_n)\\), where \\(s_n\\) is the subsample size. This characterization reveals how random forests implicitly adapt their resolution based on both sample size and local data density.</p> <p>Third, we have shown that the effective neighborhood size scales as \\(\\Theta(\\sqrt{p}/\\log(s_n))\\), demonstrating how random forests mitigate the curse of dimensionality compared to fixed-bandwidth methods.</p> <p>Fourth, our comprehensive simulation studies validate the theoretical findings across various parameter configurations, confirming the three-regime behavior and showing how empirical kernel behavior converges to theoretical predictions as sample size increases. Our simulations also reveal how different subsampling strategies affect the quantitative details of kernel behavior while preserving the qualitative characteristics predicted by our theory.</p> <p>These results provide a novel perspective on random forests that connects them to kernel methods while highlighting their unique adaptive properties, helping to bridge the gap between their empirical success and theoretical understanding.</p>"},{"location":"research/note/forest-kernel-and-its-asymptotics/#82-future-work","title":"8.2 Future Work","text":"<p>Several promising directions for future research emerge from this work : Further theoretical developments could include relaxing the current assumptions to accommodate non-uniform feature distributions and data-dependent splitting rules, extending the analysis to classification settings, and developing finite-sample guarantees that provide non-asymptotic bounds on kernel behavior.</p> <p>Algorithmic innovations might involve designing forest construction algorithms that optimize specific kernel characteristics, developing accelerated implementations based on effective neighborhood insights, and creating hybrid approaches that combine random forest kernels with other kernel methods.</p> <p>Applied research directions include developing local feature importance measures based on the kernel perspective, improving uncertainty quantification using the distance-dependent behavior of the kernel, and exploring applications in transfer learning and domain adaptation that leverage the adaptive nature of random forest kernels.</p> <p>The principles of adaptivity and data-dependent smoothing exemplified by random forest kernels are likely to remain important in machine learning. By rigorously characterizing these properties, this work contributes to both the theoretical understanding and practical application of random forests across diverse learning problems.</p>"},{"location":"research/note/generalized-random-forests/","title":"Generalized random forests","text":"<p>\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3064\u3044\u3066\u3001\u5305\u62ec\u7684\u306a\u30ec\u30d3\u30e5\u30fc\u8ad6\u6587\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u69cb\u6210\u3067\u4f5c\u6210\u3057\u307e\u307e\u3059\u3002</p>"},{"location":"research/note/generalized-random-forests/#1","title":"1. \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u6982\u89b3","text":""},{"location":"research/note/generalized-random-forests/#11","title":"1.1 \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u80cc\u666f","text":""},{"location":"research/note/generalized-random-forests/#12","title":"1.2 \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u62e1\u5f35\u3068\u5fdc\u7528","text":""},{"location":"research/note/generalized-random-forests/#13-breiman-2001-2012","title":"1.3 Breiman 2001 \u304b\u3089 2012\u5e74\u307e\u3067\u306e\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u7406\u8ad6\u89e3\u6790\u306e\u6b69\u307f","text":"<p>Breiman (2001)\u5e74\u306b\u63d0\u6848\u3057\u305f\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\uff08Random Forest\uff09\u306f\u3001\u8907\u6570\u306e\u6c7a\u5b9a\u6728\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u751f\u6210\u3057\u3001\u305d\u308c\u3089\u306e\u4e88\u6e2c\u3092\u5e73\u5747\u3059\u308b\u5f37\u529b\u306a\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u5b66\u7fd2\u6cd5\u3067\u3059\u200b\u3002\u5b9f\u8df5\u7684\u306b\u306f\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3078\u306e\u9069\u5fdc\u529b\u3084\u6c4e\u7528\u6027\u306e\u9ad8\u3055\u304b\u3089\u975e\u5e38\u306b\u6210\u529f\u3092\u53ce\u3081\u307e\u3057\u305f\u200b\u3002\u3057\u304b\u3057\u3001\u305d\u306e\u5353\u8d8a\u3057\u305f\u6027\u80fd\u306b\u3082\u304b\u304b\u308f\u3089\u305a\u3001\u63d0\u6848\u5f53\u521d\u306f\u7406\u8ad6\u7684\u6027\u8cea\u306e\u7406\u89e3\u304c\u5927\u304d\u304f\u9045\u308c\u3066\u3044\u307e\u3057\u305f\u3002\u6728\u306e\u69cb\u9020\u304c\u30c7\u30fc\u30bf\u306b\u5f37\u304f\u4f9d\u5b58\u3057\u3001\u3055\u3089\u306b\u30e9\u30f3\u30c0\u30e0\u5316\u8981\u7d20\u3082\u542b\u3080\u305f\u3081\u3001\u305d\u306e\u6570\u5b66\u7684\u89e3\u6790\u306f\u56f0\u96e3\u3067\u3042\u308a\u3001\u300c\u5e83\u304f\u4f7f\u308f\u308c\u3066\u3044\u308b\u306b\u3082\u304b\u304b\u308f\u3089\u305a\u7406\u8ad6\u7684\u6027\u8cea\u306f\u307b\u3068\u3093\u3069\u77e5\u3089\u308c\u3066\u3044\u306a\u3044\u300d\u3068\u6307\u6458\u3055\u308c\u3066\u3044\u307e\u3057\u305f\u200b\u3002\u3053\u306e\u3088\u3046\u306b\u30012000\u5e74\u4ee3\u524d\u534a\u307e\u3067\u306f\u7406\u8ad6\u3068\u5b9f\u8df5\u306e\u30ae\u30e3\u30c3\u30d7\u304c\u5927\u304d\u304b\u3063\u305f\u3002 \u3053\u306e\u4ed6\u3001Breiman (2004) \u3067\u3082\u7406\u8ad6\u7684\u306a\u90e8\u5206\u306b\u3064\u3044\u3066\u89e3\u6790\u3092\u8a66\u307f\u3066\u3044\u308b\u304c\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u7406\u8ad6\u89e3\u6790\u306b\u306f\u81f3\u3089\u305a\u3001\u76f4\u611f\u7684\u306a\u7406\u89e3\u3068$$\u3092\u7d50\u3073\u3064\u3051\u305f\u3082\u306e\u306b\u3068\u3069\u307e\u3063\u3066\u3044\u308b\u30022010\u5e74\u4ee5\u964d\u306e\u7406\u8ad6\u89e3\u6790\u306b\u5927\u304d\u306a\u8ca2\u732e\u3092\u679c\u305f\u3057\u3066\u3044\u308b\u306e\u306f\u3001Lin and Jean (2006)\u306e\u7814\u7a76\u3067\u3001\u3053\u306e\u7814\u7a76\u3067\u306fk-nearest neighborhood estimator \u3068 \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3088\u308b\u63a8\u5b9a\u91cf\u3092\u7d50\u3073\u3064\u3051\u305f\u3068\u3044\u3046\u610f\u5473\u3067\u975e\u5e38\u306b\u5927\u304d\u306a\u8ca2\u732e\u3092\u3057\u3066\u3044\u308b\u3002\u3053\u306e Lin and Jean (2006) \u306e\u8b70\u8ad6\u3092\u62e1\u5f35\u3055\u305b\u305f\u306e\u304c\u3001Biau and Devroye (2010)\u3067\u3042\u308b\u3002\u3053\u308c\u3089\u306e\u7d50\u679c\u306f\u30012014\u5e74\u4ee5\u964d\u306b\u7d9a\u304f\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u7406\u8ad6\u7684\u306a\u7d50\u679c\u306e\u5927\u304d\u306a\u790e\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u307e\u305f\u3001Meinshausen (2006) \u3067\u306f\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u304d\u5206\u4f4d\u70b9\u95a2\u6570\u306e\u63a8\u5b9a\u6cd5\u3067\u3042\u308b quantile regression forest \u306b\u304a\u3044\u3066\u3082\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u4e00\u81f4\u6027\u306b\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/generalized-random-forests/#2012","title":"2012\u5e74\u4ee5\u964d\u306e\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u7406\u8ad6\u89e3\u6790\u306e\u6b69\u307f\uff1a\u4e00\u81f4\u6027\u3068\u3001\u6f38\u8fd1\u6b63\u898f\u6027\u306e\u8a3c\u660e","text":"<p>Biau (2012)\u200b \u306fBreiman\u306e\u63d0\u6848\u3057\u305f\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u8fd1\u3044\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u30e2\u30c7\u30eb\uff08\u4f8b\u3048\u3070\u5404\u6728\u3067\u30e9\u30f3\u30c0\u30e0\u306b\u7279\u5fb4\u6b21\u5143\u3092\u9078\u3076\u65b9\u6cd5\u306a\u3069\u7c21\u7565\u5316\u3057\u305f\u8a2d\u5b9a\uff09\u306b\u3064\u3044\u3066\u521d\u3081\u3066\u53b3\u5bc6\u306a\u89e3\u6790\u3092\u884c\u3044\u307e\u3057\u305f\u200b\u3002\u3053\u306e\u7814\u7a76\u3067\u306f\u3001\u300c\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306f\u4e00\u8cab\u3057\u3066\uff08consistent\uff09\u52d5\u4f5c\u3057\u3001\u307e\u305f\u30b9\u30d1\u30fc\u30b9\u6027\uff08\u4e0d\u8981\u306a\u30ce\u30a4\u30ba\u7279\u5fb4\u304c\u591a\u6570\u5b58\u5728\u3059\u308b\u72b6\u6cc1\uff09\u306b\u9069\u5fdc\u3059\u308b\u300d\u3053\u3068\u304c\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u200b\u3002\u30b9\u30d1\u30fc\u30b9\u6027\u3078\u306e\u9069\u5fdc\u3068\u306f\u3001\u771f\u306b\u6709\u52b9\u306a\u7279\u5fb4\u5909\u6570\u304c\u3054\u304f\u4e00\u90e8\u3067\u3042\u3063\u3066\u3082\u3001\u305d\u306e\u53ce\u675f\u30ec\u30fc\u30c8\uff08\u4e88\u6e2c\u8aa4\u5dee\u306e\u6e1b\u5c11\u901f\u5ea6\uff09\u304c\u6709\u52b9\u7279\u5fb4\u306e\u6b21\u5143\u6570\u306b\u306e\u307f\u4f9d\u5b58\u3057\u3001\u7121\u95a2\u4fc2\u306a\u7279\u5fb4\u306e\u6570\u306b\u306f\u4f9d\u5b58\u3057\u306a\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u200b\u3002\u3053\u308c\u306f\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u304c\u9ad8\u6b21\u5143\u3067\u3082\u4e0d\u8981\u306a\u5909\u6570\u3092\u7121\u8996\u3057\u3001\u672c\u8cea\u7684\u306a\u5909\u6570\u306b\u96c6\u4e2d\u3067\u304d\u308b\u3053\u3068\u3092\u76f4\u611f\u7684\u306b\u88cf\u4ed8\u3051\u308b\u7d50\u679c\u3067\u3059\u3002</p>"},{"location":"research/note/generalized-random-forests/#scornet-et-al-2015","title":"Scornet et al. 2015\u306e\u8ca2\u732e","text":"<p>\u7d9a\u3044\u3066\u3001Erwan Scornet \u3068 G\u00e9rard Biau \u3089\u306b\u3088\u308b\u3055\u3089\u306a\u308b\u767a\u5c55\u304c\u3042\u308a\u307e\u3059\u3002Scornet\u30fbBiau\u30fbVert (2015)\u200b \u3067\u306f\u3001Breiman (2001) \u306e\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u304a\u3051\u308b\uff08\u975e\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3068\u3001\\(m_{try}\\) \u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3088\u308b\u30e9\u30f3\u30c0\u30e0\u306a\u7279\u5fb4\u9078\u629e\u306b\u3088\u308b\u5206\u5272\uff09\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u63a8\u5b9a\u91cf\u306e\u4e00\u81f4\u6027\u304c\u3001\u63a8\u5b9a\u5bfe\u8c61\u3068\u306a\u308b\u771f\u306e\u95a2\u6570\u304c\u7279\u5fb4\u91cf\u306b\u5bfe\u3057\u3066\u52a0\u6cd5\u7684\u3067\u3042\u308b\u5834\u5408\u306b\u3001\u4e00\u81f4\u6027\u304c\u6210\u308a\u7acb\u3064\u3053\u3068\u3092\u793a\u3057\u307e\u3057\u305f\u3002</p> <ul> <li>\u52a0\u6cd5\u6027\u306e\u4eee\u5b9a $$     Y = \\sum_{j=1}^{p}m_{j}(X^{(j)})+\\varepsilon \\quad \\mathrm{where} \\quad X = (X^{(1)},...,X^{(p)}) \\sim U([0,1]^{p}),\\quad \\varepsilon \\sim N(0,\\sigma^2)  $$ \u3053\u3053\u3067\u3001Scornet\u30fbBiau\u30fbVert (2015)\u200b\u306e\u4e00\u81f4\u6027\u306e\u8a3c\u660e\u306b\u9650\u3089\u305a\u3001\u4e00\u81f4\u6027\u3084\u6f38\u8fd1\u6b63\u898f\u6027\u306e\u8a3c\u660e\u3067\u306f\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3092\u69cb\u6210\u3059\u308b\u6728\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3069\u306e\u3088\u3046\u306b\u5236\u5fa1\u3059\u308b\u304b\u304c\u672c\u8cea\u7684\u3067\u3042\u308a\u3001\u640d\u5931\u95a2\u6570\u3092\u3069\u306e\u3088\u3046\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u308b\u304b\u306f\u4e00\u81f4\u6027\u3084\u6f38\u8fd1\u6b63\u898f\u6027\u306b\u5bfe\u3057\u3066\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u307e\u305b\u3093\u3002\u3053\u306e\u70b9\u306b\u306f\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002</li> </ul> <p>Scornet\u30fbBiau\u30fbVert (2015)\u306b\u304a\u3044\u3066\u306f\u3001\u6728\u306b\u5bfe\u3057\u3066\u6b21\u306e\u4eee\u5b9a\u304c\u7f6e\u304b\u308c\u307e\u3059\u3002\u307e\u305a\u3001\u6728\u3092\u69cb\u6210\u3059\u308b\u969b\u306e\u30b5\u30d6\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\\(a_n\\)\u306f\u3001\u89b3\u6e2c\u3055\u308c\u305f\u30b5\u30f3\u30d7\u30eb\\(n\\) \u306b\u5bfe\u3057\u3066\u3001\u7121\u9650\u5927\u306b\u767a\u6563\u3057\u307e\u3059\\(a_n \\rightarrow \\infty\\). \u307e\u305f\u3001\u6728\u3092\u69cb\u6210\u3059\u308b\u8449\u306e\u6570\\(t_n\\)\u3082\u3001\\(t_n \\rightarrow \\infty\\)\u3092\u6e80\u305f\u3057\u307e\u3059\u304c\u3001\u3053\u306e\u767a\u6563\u901f\u5ea6\u306b\u306f\u5236\u7d04\u304c\u3042\u308a\u3001\\(t_n (\\log a_n)^9/a_n \\rightarrow 0\\) \u3068\u306a\u308b\u3053\u3068\u304c\u6761\u4ef6\u3068\u3057\u3066\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u30b5\u30f3\u30d7\u30eb\u306e\u767a\u6563\u901f\u5ea6\u306b\u5bfe\u3057\u3066\u3001\u6728\u306e\u8449\u3092\u5897\u3084\u3059\u901f\u5ea6\u306f\u305d\u308c\u3088\u308a\u3082\u9045\u3044\u901f\u5ea6\u304c\u8981\u6c42\u3055\u308c\u307e\u3059\u3002</p> <p>\u307e\u305f\u3001\\(m_{try} = p\\)\u306e\u72b6\u6cc1\u4e0b\u306b\u304a\u3044\u3066\u3001\u771f\u306e\u95a2\u6570\u304c\u3001\u89b3\u6e2c\u3055\u308c\u305f\u5909\u6570\\(p\\)\u500b\u306e\u3046\u3061\u3001\u30e2\u30c7\u30eb\u306b\u542b\u307e\u308c\u308b\u5909\u6570\u304c\\(s\\)\u500b\u3067\u3042\u308b\u72b6\u6cc1\u3092\u8003\u3048\u307e\u3059\u3002 $$     Y = \\sum_{j=1}^{s} m_j (X^{(j)}) + \\varepsilon $$ \u3053\u306e\u3068\u304d\u3001\u4efb\u610f\u306e\\(m_j\\) \u304c \u4efb\u610f\u306e\\(Y\\)\u306e\u533a\u9593\\([a,b]\\)\u306b\u304a\u3044\u3066\u5b9a\u6570\u95a2\u6570\u3067\u306a\u3044\u306a\u3089\u3070\u3001\u5341\u5206\u9ad8\u3044\u78ba\u7387\u3067\u6728\u306e\u5206\u5272\u5909\u6570\u306f\\(\\{1,2,...,s\\}\\)\u304b\u3089\u9078\u3070\u308c\u308b\u3053\u3068\u3092\u793a\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u306f\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u304c\u9ad8\u6b21\u5143\u306e\u89b3\u6e2c\u306e\u3082\u3068\u3067\u6709\u52b9\u306b\u52d5\u4f5c\u3059\u308b\u3053\u3068\u3092\u793a\u3059\u6839\u62e0\u306e1\u3064\u3068\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"research/note/generalized-random-forests/#wager-et-al-2014-2018","title":"Wager et al., 2014-2018\u306e\u8ca2\u732e","text":"<p>\u6b21\u306b\u3001Stefan Wager \u304c Stanford\u306e\u7814\u7a76\u30b0\u30eb\u30fc\u30d7\u3067\u53d6\u308a\u7d44\u3093\u3060\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u4e00\u81f4\u6027\u3068\u6f38\u8fd1\u6b63\u898f\u6027\u3001\u304a\u3088\u3073\u6f38\u8fd1\u5206\u6563\u306e\u5c0e\u51fa\u3001\u3055\u3089\u306b\u30aa\u30ea\u30b8\u30ca\u30eb\u306ecausal forest\u306e\u63d0\u6848\u307e\u3067\u306e\u6d41\u308c\u3092\u8aac\u660e\u3057\u307e\u3059\u3002\u307e\u305a\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u4e00\u81f4\u6027\u306b\u3064\u3044\u3066Wager and Walther (2014) \u3067\u306f\u3001Scornet et al.,(2015)\u3068\u306f\u7570\u306a\u308b\u4eee\u5b9a\u3092\u304a\u3044\u3066\u4e00\u81f4\u6027\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u307e\u305a\u3001\u6728\u306e\u8449\u306e\u6570\\(k_n\\)\u306b\u5bfe\u3057\u3066\u3001 $$     \\lim_{n\\rightarrow \\infty}\\frac{\\log(n)\\max \\left(\\log(d), \\log\\log(n)\\right)}{k} = 0 $$ \u306e\u4eee\u5b9a\u304a\u3088\u3073 Sparse signal \u306e\u4eee\u5b9a\u3092\u304a\u304f\u3002\u3059\u306a\u308f\u3061\u3001\\(p\\)\u6b21\u5143\u306e\u7279\u5fb4\u91cf\\(X\\)\u306b\u5bfe\u3057\u3066\u3001\u6709\u52b9\u306a\u6b21\u5143\u306e\u96c6\u5408\\(\\mathcal{Q} =\\{1,2,...,s\\}\\)\u6b21\u5143\u3067\u3001\\((Y,X_{\\mathcal{Q}}) \\perp \\!\\!\\! \\perp (X_{-\\mathcal{Q}})\\) \u304c\u6210\u308a\u7acb\u3064\u3002 \u3055\u3089\u306b\u3001Monotone signal \u306e\u4eee\u5b9a\u3092\u304a\u304f\u3002\u3053\u306e\u4eee\u5b9a\u306f\u3001\\(j \\in \\mathcal{Q}\\)\uff08\u6709\u52b9\u306a\u6b21\u5143\uff09\u306b\u5bfe\u3057\u3066\u3001\\(x_{(-j)} \\in [0,1]^{d-1}\\) \u3092\u56fa\u5b9a\u3057\u305f\u6642\u306b\u3001\u4ee5\u4e0b\u306e\u5f0f\u3092\u6e80\u305f\u3059\u3088\u3046\u306a\u6700\u5c0f\u306e\u52b9\u679c\\(\\beta &gt; 0\\)\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3067\u3042\u308b\u3002 <pre><code>    \\left|\\mathbb{E} \\left[ Y_i \\mid (X_i)_{-j} = x_{-j}, (X_i)_{j} &gt; \\frac{1}{2} \\right] - \\mathbb{E}\\left[ Y_i \\mid (X_i)_{-j} = x_{-j}, (X_i)_{j} \\leq \\frac{1}{2} \\right] \\right| \\geq \\beta\n</code></pre> \u3055\u3089\u306b\u3001\\(E[Y|X=x]\\) \u306b\u5bfe\u3059\u308bLipchitz\u9023\u7d9a\u6027\u3092\u4eee\u5b9a\u3057\u305f\u3082\u3068\u3067\u3001\u4ee5\u4e0b\u3067\u5b9a\u7fa9\u3055\u308c\u308bGuess-and-Check forest\u306f\u3001\u4e00\u69d8\u4e00\u81f4\u6027\u3092\u6301\u3064\u3002Guess-and-check forest \u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u3002</p>"},{"location":"research/note/generalized-random-forests/#guess-and-check-forest","title":"Guess-and-Check Forest","text":""},{"location":"research/note/generalized-random-forests/#input","title":"Input","text":"<ul> <li>$ n $ \u500b\u306e\u8a13\u7df4\u30c7\u30fc\u30bf \\((X_i, Y_i)\\)</li> <li>\u6700\u5c0f\u8449\u30ce\u30fc\u30c9\u30b5\u30a4\u30ba $ k $</li> <li>\u30d0\u30e9\u30f3\u30b9\u30d1\u30e9\u30e1\u30fc\u30bf $ 0 &lt; \\alpha &lt; 1/2 $</li> </ul>"},{"location":"research/note/generalized-random-forests/#_1","title":"\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0","text":"<p>Guess-and-Check Tree \u306f\u3001\u4ee5\u4e0b\u306e\u5206\u5272\u624b\u9806\u3092\u518d\u5e30\u7684\u306b\u9069\u7528\u3057\u3001\u5206\u5272\u304c\u4e0d\u53ef\u80fd\u306b\u306a\u308b\u307e\u3067\u51e6\u7406\u3092\u884c\u3046\u3002\u3059\u306a\u308f\u3061\u3001\u5168\u3066\u306e\u7d42\u7aef\u30ce\u30fc\u30c9\u306e\u8a13\u7df4\u30c7\u30fc\u30bf\u6570\u304c $ 2k $ \u672a\u6e80\u306b\u306a\u308b\u304b\u3001\u305d\u3082\u305d\u3082 (12) \u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u5206\u5272\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u307e\u3067\u7e70\u308a\u8fd4\u3059\u3002</p> <ol> <li>\u30ce\u30fc\u30c9 $ \\nu $ \u3092\u9078\u629e\u3057\u3001\u305d\u3053\u306b\u5c11\u306a\u304f\u3068\u3082 $ 2k $ \u500b\u306e\u8a13\u7df4\u30c7\u30fc\u30bf\u304c\u542b\u307e\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002</li> <li>\u5019\u88dc\u3068\u306a\u308b\u5206\u5272\u5909\u6570 $ j \\in {1, \\dots, d} $ \u3092\u4e00\u69d8\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u629e\u3059\u308b\u3002</li> <li>\u6700\u5c0f\u4e8c\u4e57\u8aa4\u5dee\u306e\u5206\u5272\u70b9 $ \\hat{\\theta} $ \u3092\u9078\u629e\u3059\u308b\u3002\u3088\u308a\u5177\u4f53\u7684\u306b\u306f\u3001     $$     \\hat{\\theta} = \\arg\\max_{\\theta} \\ell (\\theta)     $$     \u3053\u3053\u3067\u3001\u76ee\u7684\u95a2\u6570 $ \\ell(\\theta) $ \u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u3002     $$     \\ell (\\theta) := \\frac{4 N^-(\\theta) N^+(\\theta)}{(N^-(\\theta) + N^+(\\theta))^2} \\Delta^2(\\theta)     $$     \u5206\u5272\u70b9 $ \\theta $ \u306f\u3001\u30ce\u30fc\u30c9 $ \\nu $ \u306b\u5c5e\u3059\u308b\u30b5\u30f3\u30d7\u30eb $ X_i $ \u306e\u6210\u5206 $ (X_i)j $ \u306e\u3044\u305a\u308c\u304b\u306b\u5bfe\u5fdc\u3059\u308b\u5024\u3068\u3059\u308b\u3002     $$     \\alpha \\times | { i : X_i \\in \\nu } |,  k \\, \\leq\\, N^-(\\theta), \\quad N^+(\\theta)     $$     \u307e\u305f\u3001\u4ee5\u4e0b\u306e\u5b9a\u7fa9\u3092\u7528\u3044\u308b\u3002     $$     \\Delta(\\theta) = \\frac{1}{N^+}\\sum{{ i : X_i \\in \\nu(x), (X_i)j &gt; \\theta }} Y_i - \\frac{1}{N^-}\\sum{{ i : X_i \\in \\nu(x), (X_i)_j \\leq \\theta }} Y_i     $$     \u305f\u3060\u3057\u3001     $$     N^-(\\theta) = | { i : X_i \\in \\nu, (X_i)_j \\leq \\theta } |, \\, N^+(\\theta) = | { i : X_i \\in \\nu, (X_i)_j &gt; \\theta } |     $$</li> <li>\u5206\u5272\u6761\u4ef6\u306e\u5224\u5b9a:<ul> <li>\u5909\u6570 $ j $ \u306b\u5bfe\u3057\u3066\u3001\u3059\u3067\u306b\u6210\u529f\u3057\u305f\u5206\u5272\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3002</li> <li>\u307e\u305f\u306f\u3001\u4ee5\u4e0b\u306e\u6761\u4ef6\u304c\u6e80\u305f\u3055\u308c\u308b\u5834\u5408\u3002 $$ \\ell (\\hat{\\theta}) \\geq \\left( 2 \\times 9M \\sqrt{\\frac{\\log(n) \\log(d)}{k \\log((1 - \\alpha)^{-1})}} \\right)^2 $$ \u3053\u306e\u5834\u5408\u3001$ j $ \u756a\u76ee\u306e\u5909\u6570\u306b\u5bfe\u3057\u3066\u30ce\u30fc\u30c9 $ \\nu $ \u3092 $ \\hat{\\theta} $ \u3067\u5206\u5272\u3059\u308b\u3002\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u3001\u30ce\u30fc\u30c9 $ \\nu $ \u306f\u3053\u306e\u6642\u70b9\u3067\u306f\u5206\u5272\u3055\u308c\u306a\u3044\u3002</li> </ul> </li> </ol>"},{"location":"research/note/generalized-random-forests/#guess-and-check-forest_1","title":"Guess-and-Check Forest","text":"<p>Guess-and-Check Forest \u306f\u3001\u4e0a\u8a18\u306e\u30d7\u30ed\u30b7\u30fc\u30b8\u30e3\u306b\u5f93\u3063\u3066 \u72ec\u7acb\u306b\u751f\u6210\u3055\u308c\u305f \\( B \\) \u672c\u306e Guess-and-Check \u6728\u306e\u5e73\u5747 \u3092\u53d6\u308b\u3053\u3068\u3067\u69cb\u7bc9\u3055\u308c\u308b\u3002</p> <p>\u6b21\u306f\u3001Random forest\u306e\u6f38\u8fd1\u5206\u6563\u306e\u63a8\u5b9a\u6cd5\u306b\u95a2\u3059\u308b\u7d50\u679c\u3067\u3042\u308b\u3002\u307e\u305a\u3001\u6f38\u8fd1\u6b63\u898f\u6027\u306e\u7d50\u679c\u306b\u5148\u7acb\u3063\u3066\u3001Wager, Hastie and Efron (2014) \u3067\u306f\u3001Efron (2014) \u306b\u304a\u3044\u3066\u30e2\u30c7\u30eb\u9078\u629e\u306b\u3088\u308b\u30c7\u30fc\u30bf\u99c6\u52d5\u578b\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u3082\u305f\u3089\u3059\u4e88\u6e2c\u8aa4\u5dee\u306e\u904e\u5c0f\u8a55\u4fa1\u3092\u88dc\u6b63\u3059\u308b\u65b9\u6cd5\u3068\u3057\u3066\u63d0\u6848\u3057\u305f Infinitesimal Jacknife\u306b\u3088\u308b\u65b9\u6cd5\u3092\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u62e1\u5f35\u3057\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3088\u308b\u63a8\u5b9a\u91cf\u306e\u3070\u3089\u3064\u304d\u3092\u8a55\u4fa1\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u305f\u3002 \\(b=1,2,...,B\\)\u756a\u76ee\u306e\u56de\u5e30\u6728\u306b\u3088\u308b\u63a8\u5b9a\u5024\u3092\\(\\hat{f}_{b}(x)\\)\u3068\u3057\u3001 \\(N_{ib} \\in \\{0,1\\}\\) \u304c \\(b\\)\u756a\u76ee\u306e\u56de\u5e30\u6728\uff08Double Sample Tree\u306e\u5834\u5408\u306b\u306f\u3001\u3044\u305a\u308c\u304b\u4e00\u65b9\u306e\u30b0\u30eb\u30fc\u30d7\u306b\u542b\u307e\u308c\u308b\u5834\u5408\u306b\\(N_{bi}=1\\)\u3068\u5b9a\u7fa9\u3059\u308b\uff09\u306b\u542b\u307e\u308c\u308b\u304b\u3069\u3046\u304b\u3092\u8868\u3059\u6307\u793a\u95a2\u6570\u3068\u3059\u308b\u3002 $$     \\hat{V}{IJ}(x) = \\frac{n-1}{n} \\left(\\frac{n}{n-s}\\right)^2 \\sum{i=1}^{n}\\mathrm{Cov}^{}\\left[\\hat{f}_{b}^{}(x), N_{ib}^{}\\right]^2 $$ \u3053\u308c\u306b\u5bfe\u3057\u3066\u3001\u6709\u9650\u6a19\u672c\u5316\u3067\u306e\u63a8\u5b9a\u91cf\u306f $$     \\hat{V}{IJ}^{B}(x) = \\frac{n-1}{n} \\left(\\frac{n}{n-s}\\right)^2 \\sum{i=1}^{n}\\frac{1}{B}\\sum_{b=1}^{B}\\left(N_{bi}^{}-\\frac{s}{n}\\right)\\left(t_{b}^{}(x)-\\bar{t}^{}(x)\\right) $$ \u3068\u306a\u308b\u3002\u305f\u3060\u3057\u3001\u3053\u306e\u7d50\u679c\u306fEmpirical\u306a\u8a55\u4fa1\u306b\u3068\u3069\u307e\u3063\u3066\u304a\u308a\u3001\u5b9f\u969b\u306b\u6f38\u8fd1\u5206\u6563\u3068\u4e00\u81f4\u3059\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u306e\u306f\u3001\u3053\u306e\u3042\u3068\u306eWager and Athey (2019)\u3067\u3042\u308b\u3002\u6f38\u8fd1\u6b63\u898f\u6027\u306e\u8b70\u8ad6\u306b\u79fb\u308b\u524d\u306b\u3082\u30461\u3064\u306e\u91cd\u8981\u306a\u8ad6\u6587\u306f\u3001Athey and Imbens (2016) \u306b\u304a\u3044\u3066\u6307\u6458\u3055\u308c\u305f\u6728\u69cb\u9020\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u306b\u304a\u3051\u308b\u30d0\u30a4\u30a2\u30b9\u306e\u554f\u984c\u3067\u3042\u308b\u3002\u6728\u69cb\u9020\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u969b\u306b\u306f\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3078\u306e\u5f53\u3066\u306f\u307e\u308a\u304c\u3088\u304f\u306a\u308b\u3088\u3046\u306b\u5b66\u7fd2\u3092\u884c\u3046\u5fc5\u8981\u304c\u3042\u308b\u3002\u3057\u304b\u3057\u3001\u4e00\u822c\u7684\u306aCART\u306e\u3088\u3046\u306a\u5b66\u7fd2\u3067\u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u6728\u69cb\u9020\u306e\u5b66\u7fd2\u3068\u6728\u306b\u3088\u308b\u4e88\u6e2c\u5024\u306e\u4e21\u65b9\u3092\u5b66\u7fd2\u3059\u308b\u305f\u3081\u3001\u5b9f\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u306e\u5f53\u3066\u306f\u307e\u308a\u3092\u6700\u9069\u5316\u3057\u3066\u3044\u308b\u306e\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u308b\u3002\u307e\u305f\u3001\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001\u30e9\u30f3\u30c0\u30e0\u5316\u6bd4\u8f03\u8a66\u9a13\u306b\u5bfe\u3059\u308b\u4ecb\u5165\u52b9\u679c\u306e\u7570\u8cea\u6027\u3092\u767a\u898b\u3059\u308b\u56de\u5e30\u6728\u3068\u3057\u3066Causal Tree\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002\u5f93\u6765\u306eCART\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u306f\u3001\u5171\u5909\u91cf\\(X\\)\u306e\u5206\u5272\u3068\u5206\u5272\u306b\u3088\u3063\u3066\u751f\u6210\u3055\u308c\u305f\u8449\u306e\u5024\u306b\u306f\u76f8\u95a2\u304c\u3042\u308b\u305f\u3081\u3001\u76f8\u95a2\u306b\u4f9d\u5b58\u3057\u305f\u56e0\u679c\u52b9\u679c\u304c\u691c\u51fa\u3055\u308c\u3066\u3057\u307e\u3046\u3002\u305d\u3053\u3067\u3001Honest\uff08\u8aa0\u5b9f\u6027\uff09 \u3068\u547c\u3070\u308c\u308b\u6982\u5ff5\u3092\u5c0e\u5165\u3059\u308b\u3053\u3068\u3067\u3001\u3053\u306e\u554f\u984c\u3092\u89e3\u6d88\u3059\u308b\u3002\u8aa0\u5b9f\u6027\u3068\u306f\u3001\u30c7\u30fc\u30bf\u30922\u3064\u306b\u5206\u3051\u3001\u4e00\u65b9\u3067\u6728\u69cb\u9020\u3092\u5b66\u7fd2\u3055\u305b\u3001\u3082\u3046\u4e00\u65b9\u3067\u4e88\u6e2c\u5024\u306e\u8a08\u7b97\u3068\u691c\u5b9a\u3092\u884c\u3048\u3070\u3001\u5206\u5272\u3068\u4e88\u6e2c\u5024\u306f\u72ec\u7acb\u306b\u306a\u308b\u305f\u3081\u3001\u52b9\u679c\u306e\u691c\u8a3c\u304c\u3067\u304d\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3042\u308b\u3002\u3053\u3053\u3067\u3001\u5c0e\u5165\u3055\u308c\u305f\u8aa0\u5b9f\u6027\u304c\u6b21\u306b\u793a\u3059\u6f38\u8fd1\u6b63\u898f\u6027\u3092\u793a\u3059\u305f\u3081\u306e1\u3064\u306e\u9375\u3068\u306a\u308b\u3002</p> <p>Wager and Athey (2019) \u3067\u306f\u3001\u6b21\u306e\u6027\u8cea\u3092\u6e80\u305f\u3059bagging tree\u306b\u5bfe\u3057\u3066\u3001\u6709\u9650\u6a19\u672c\u5316\u3067\u306e\u30d0\u30a4\u30a2\u30b9\u3092\u8a55\u4fa1\u3057\u3001\u6f38\u8fd1\u6b63\u898f\u6027\u3092\u793a\u3057\u305f\u3002 - \u30b5\u30d6\u30b5\u30f3\u30d7\u30eb\u30922\u3064\u306b\u5206\u5272\u3057\u3001\u30b5\u30d6\u30b5\u30f3\u30d7\u30eb\u306e\u7247\u65b9\u3092\u6728\u69cb\u9020\u306e\u5b66\u7fd2\u306b\u3001\u3082\u3046\u4e00\u65b9\u3092\u8449\u306e\u63a8\u5b9a\u5024\u306e\u8a08\u7b97\u306b\u7528\u3044\u308b Double Sample Tree \u3092 \u5f31\u5b66\u7fd2\u5668\u3068\u3057\u3066\u63a1\u7528\u3059\u308b - \u4efb\u610f\u306e\u7279\u5fb4\u91cf\\(X^{(j)}, \\, j=1,2,...,d\\)\u304c\u3001\u30ce\u30fc\u30c9\u306e\u5206\u5272\u3067\u9078\u629e\u3055\u308c\u308b\u78ba\u7387\u304c\\(0\\)\u3067\u306f\u306a\u3044\u3002 - subsample size \\(s_n\\) \u306f\u3001\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\\(n\\)\u3001\u6b21\u5143\\(d\\)\u3001\u304a\u3088\u3073\u5206\u5272\u3092\u5236\u5fa1\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\\(\\alpha \\leq 0.2\\) \u306b\u5bfe\u3057\u3066\u3001\u6b21\u306e\u95a2\u4fc2\u5f0f\u3092\u6e80\u305f\u3059\u3002     $$         s_n \\approx n^{\\beta}, \\qquad 1-\\left(1+\\frac{d}{\\pi}\\frac{\\log(\\alpha^{-1})}{\\log\\left((1-\\alpha)^{-1}\\right)}\\right)^{-1} &lt; \\beta &lt; 1     $$ - \\(\\mu(x) := E[Y|X]\\) \u304c \u30ea\u30d7\u30b7\u30c3\u30c4\u9023\u7d9a \u3067\u3042\u308b\u3002</p> <p>\u307e\u305f\u3001\u3053\u306e\u8ad6\u6587\u3067\u306f\u4e0a\u8a18\u306b\u8ff0\u3079\u305f\\(\\hat{V}_{IJ}\\)\u304c\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u6f38\u8fd1\u5206\u6563\u306b\u5bfe\u3059\u308b\u4e00\u81f4\u63a8\u5b9a\u91cf\u3067\u3042\u308b\u3053\u3068\u304c\u793a\u3055\u308c\u3066\u304a\u308a\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3088\u308b\u63a8\u5b9a\u91cf\u306e\u5206\u6563\u3092\u4e0e\u3048\u305f\u70b9\u3067\u753b\u671f\u7684\u306a\u8ad6\u6587\u3067\u3042\u3063\u305f\u3002</p> <p>\u3053\u308c\u306b\u52a0\u3048\u3066Wager and Athey (2019)\u3067\u306f\u3001\u56e0\u679c\u52b9\u679c\u3092\u63a8\u5b9a\u3059\u308b\u65b9\u6cd5\u3068\u3057\u3066\u3001causal forest\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002\u3053\u306ecausal forest\u306f\u73fe\u5728\u306f\u307b\u3068\u3093\u3069\u63a1\u7528\u3055\u308c\u306a\u3044\u304c\u3001\u6982\u5ff5\u3060\u3051\u7d39\u4ecb\u3057\u3066\u304a\u304f\u3002causal forest \u306f\u3001Athey and Imbens (2016) \u3067\u63d0\u6848\u3055\u308c\u305f causal tree \u3092\u5f31\u5b66\u7fd2\u5668\u3068\u3057\u3066\u7528\u3044\u308b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3067\u3042\u308b\u3002causal tree \u306f\u3001\u56de\u5e30\u306b\u304a\u3051\u308b\u30ce\u30fc\u30c9\u306e\u5206\u5272\u57fa\u6e96\u3092\u3001\u51e6\u7f6e\u52b9\u679c\u63a8\u5b9a\u306e\u6587\u8108\u3078\u3068\u62e1\u5f35\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002\u305f\u3060\u3057\u3001\u5358\u7d14\u306b\u62e1\u5f35\u3057\u305f\u3082\u306e\u3067\u306f\u306a\u304f\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3078\u306e\u5f53\u3066\u306f\u307e\u308a\u3092\u6700\u9069\u5316\u3059\u308b\u3088\u3046\u306a\u57fa\u6e96\u3092\u7528\u3044\u3066\u3044\u308b\u3002 $$ \\begin{align}     -\\widehat{\\text{EMSE}}{r} \\left( \\mathcal{S}^{\\text{tr}}, N^{\\text{est}}, \\Pi \\right)      &amp;\\equiv \\frac{1}{N^{\\text{tr}}} \\sum{i \\in \\mathcal{S}^{\\text{tr}}} \\hat{\\tau}^2 \\left( X_i, \\mathcal{S}^{\\text{tr}}, \\Pi \\right) \\     &amp;\\quad - \\left( \\frac{1}{N^{\\text{tr}}} + \\frac{1}{N^{\\text{est}}} \\right) \\cdot \\sum_{\\ell \\in \\Pi}      \\left( \\frac{S^2_{\\mathcal{S}^{\\text{tr}}{\\text{treat}}} (\\ell)}{p}      + \\frac{S^2{\\mathcal{S}^{\\text{tr}}_{\\text{control}}} (\\ell)}{1 - p} \\right). \\end{align} $$</p>"},{"location":"research/note/generalized-random-forests/#_2","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u4e00\u81f4\u6027\u30fb\u6f38\u8fd1\u6b63\u898f\u6027","text":""},{"location":"research/note/generalized-random-forests/#_3","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3068\u5c40\u6240\u63a8\u5b9a\u65b9\u7a0b\u5f0f","text":""},{"location":"research/note/generalized-random-forests/#_4","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u4e00\u81f4\u6027","text":""},{"location":"research/note/generalized-random-forests/#_5","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u6f38\u8fd1\u6b63\u898f\u6027","text":""},{"location":"research/note/generalized-random-forests/#_6","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u6f38\u8fd1\u5206\u6563\u63a8\u5b9a","text":""},{"location":"research/note/generalized-random-forests/#_7","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u5fdc\u7528","text":""},{"location":"research/note/generalized-random-forests/#quantile-regression-forest","title":"quantile regression forest \u3068\u3001\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3088\u308b\u5206\u4f4d\u70b9\u63a8\u5b9a","text":""},{"location":"research/note/generalized-random-forests/#_8","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3088\u308b\u6761\u4ef6\u4ed8\u304d\u56e0\u679c\u52b9\u679c\u306e\u63a8\u5b9a","text":""},{"location":"research/note/generalized-random-forests/#instrumental-variable","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3088\u308bInstrumental variable\u3092\u7528\u3044\u305f\u56e0\u679c\u52b9\u679c\u306e\u63a8\u5b9a","text":""},{"location":"research/note/generalized-random-forests/#survival-forest","title":"survival forest \u3068 \u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3088\u308b\u751f\u5b58\u95a2\u6570\u63a8\u5b9a","text":""},{"location":"research/note/generalized-random-forests/#causal-survival-effect","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3088\u308b \u6761\u4ef6\u4ed8\u304d causal survival effect\u306e\u63a8\u5b9a","text":""},{"location":"research/note/generalized-random-forests/#_9","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u62e1\u5f35","text":""},{"location":"research/note/generalized-random-forests/#locally-linear-forest","title":"Locally Linear Forest","text":""},{"location":"research/note/generalized-random-forests/#boosted-regression-forest","title":"boosted regression forest","text":""},{"location":"research/note/generalized-random-forests/#sufficient-dimension-reduction-forest","title":"Sufficient dimension reduction forest","text":""},{"location":"research/note/generalized-random-forests/#_10","title":"\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u9078\u629e","text":""},{"location":"research/note/generalized-random-forests/#_11","title":"\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u4e88\u6e2c\u306e\u89e3\u91c8","text":""},{"location":"research/note/generalized-random-forests/#_12","title":"\u5909\u6570\u91cd\u8981\u5ea6\u3068\u306f","text":""},{"location":"research/note/generalized-random-forests/#a-general-framework-for-inference-on-algorithm-agnostic-variable-importance","title":"\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6 (A General Framework for Inference on Algorithm-Agnostic Variable Importance)","text":""},{"location":"research/note/generalized-random-forests/#sirus-interpretable-random-forests-via-rule-extraction","title":"SIRUS (Interpretable Random Forests via Rule Extraction)","text":""},{"location":"research/note/generalized-random-forests/#sobol-mda-mda-for-random-forests-inconsistency-and-a-practical-solution-via-the-sobol-mda","title":"Sobol-MDA (MDA for random forests: inconsistency, and a practical solution via the Sobol-MDA)","text":""},{"location":"research/note/generalized-random-forests/#shaff-shaff-fast-and-consistent-shapley-effect-estimates-via-random-forests","title":"SHAFF (SHAFF: Fast and consistent SHApley eFfect estimates via random Forests)","text":""},{"location":"research/note/generalized-random-forests/#variable-importance-for-causal-forests","title":"Variable importance for causal forests","text":""},{"location":"research/note/generalized-random-forests/#_13","title":"\u4e00\u822c\u5316\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u5fdc\u7528\u4e8b\u4f8b","text":""},{"location":"research/note/generalized-random-forests/#_14","title":"\u533b\u7642\u5206\u91ce","text":"<ul> <li>Suicide Risk Among Hospitalized Versus Discharged Deliberate Self-Harm Patients: Generalized Random Forest Analysis Using a Large Claims Data Set</li> <li>Assessing the properties of patient-specific treatment effect estimates from causal forest algorithms under essential heterogeneity</li> </ul>"},{"location":"research/note/generalized-random-forests/#_15","title":"\u7d4c\u6e08\u5206\u91ce","text":"<ul> <li>Understanding Heterogeneous Impact of Medicaid Expansion Using Generalized Random Forest</li> </ul>"},{"location":"research/note/generalized-random-forests/#_16","title":"\u91d1\u878d\u5206\u91ce","text":"<ul> <li>Predicting Value at Risk for Cryptocurrencies With Generalized Random Forests</li> </ul>"},{"location":"research/note/generalized-random-forests/#_17","title":"\u753b\u50cf\u51e6\u7406\u5206\u91ce","text":"<ul> <li>Meta-forests: Domain generalization on random forests with meta-learning</li> </ul>"},{"location":"research/note/generalized-random-forests/#some-important-topics","title":"Some important topics","text":""},{"location":"research/note/generalized-random-forests/#x-outlier-and-overlap-assumption","title":"X-outlier and overlap assumption","text":""},{"location":"research/note/generalized-random-forests/#y-outlier-and-huberized-loss","title":"Y-outlier and Huberized Loss","text":""},{"location":"research/note/generalized-random-forests/#conformal-predictions","title":"Conformal Predictions","text":""},{"location":"research/note/grf-kernel/","title":"Asymptotic Properties of Random Forest Kernels: A Theoretical Analysis","text":""},{"location":"research/note/grf-kernel/#abstract","title":"Abstract","text":"<p>We present a rigorous analysis of the asymptotic behavior of random forest kernels as the sample size grows and the distance between points approaches zero. Under a specific set of assumptions about the feature space, distribution, and tree-building process, we prove that the random forest kernel between a fixed point \\(x\\) and a sequence of points \\(z_n\\) approaching \\(x\\) converges to an exponential function of their appropriately scaled distance. Specifically, we establish that when the distance is scaled by a factor proportional to the subsample size used for tree construction, the kernel converges to \\(\\exp(-u)\\) where \\(u\\) is the scaled distance. We first establish this result for the one-dimensional case and then extend it to the general \\(p\\)-dimensional setting, showing how the convergence rate depends on the feature space dimensionality. These results provide theoretical insights into the local adaptivity of random forests and their behavior in high-density regions, with implications for understanding their performance in various learning tasks including classification, regression, and density estimation.</p> <p>Keywords: Random forests, kernel methods, asymptotic analysis, statistical learning theory, nonparametric estimation, high-dimensional analysis</p>"},{"location":"research/note/grf-kernel/#1-introduction","title":"1. Introduction","text":"<p>Random forests, introduced by Breiman (2001), have become one of the most successful ensemble learning methods in machine learning and statistics. Their popularity stems from their excellent predictive performance, robustness to overfitting, and ability to handle high-dimensional data without extensive hyperparameter tuning. Despite their widespread application, the theoretical understanding of random forests has advanced more slowly than their practical use.</p> <p>An important perspective for analyzing random forests is through their implicit kernel representation. As noted by Breiman (2000) and further explored by Lin and Jeon (2006), a random forest can be viewed as a weighted nearest neighbor method, where the weights are determined by how often two points fall into the same leaf node across the ensemble of trees. This naturally defines a kernel function, with the kernel value between two points representing their \"similarity\" as estimated by the forest.</p> <p>Understanding the properties of this kernel provides insights into the behavior of random forests, including their adaptivity to local structure in the data and their generalization capabilities. Previous works have investigated various aspects of random forest kernels, including their behavior in high dimensions (Scornet, 2016), their connection to classical kernel methods (Davies and Ghahramani, 2014), and their role in the consistency of random forest predictions (Scornet et al., 2015).</p> <p>In this paper, we contribute to this growing body of theoretical work by analyzing the asymptotic behavior of the random forest kernel in a controlled setting. Specifically, we examine how the kernel behaves between a fixed point and a sequence of points approaching it, as the sample size grows to infinity. We derive an explicit formula for the limiting kernel and show that it depends on an appropriately scaled distance between the points. We first develop this analysis for a one-dimensional feature space and then extend it to the general \\(p\\)-dimensional case, providing insights into how the dimensionality affects the kernel's asymptotic behavior.</p> <p>Our analysis makes several key assumptions to facilitate theoretical tractability: uniformly distributed features, random splitting with balance constraints, and fixed leaf size requirements. We begin with the one-dimensional case for clarity and then extend to the general \\(p\\)-dimensional setting. While these assumptions are simplifications of practical random forest implementations, they allow us to derive precise asymptotic results that shed light on the fundamental properties of the random forest kernel.</p> <p>The remainder of this paper is organized as follows: Section 2 introduces the notation and assumptions used throughout the paper. Section 3 presents our main theoretical results, including the asymptotic behavior of the random forest kernel and supporting lemmas for both one-dimensional and \\(p\\)-dimensional cases. Section 4 provides detailed proofs of the main theorems. Section 5 discusses the implications of our results for understanding random forests. Section 6 presents numerical experiments that validate our theoretical findings. Finally, Section 7 concludes the paper and outlines directions for future research.</p>"},{"location":"research/note/grf-kernel/#2-preliminaries","title":"2. Preliminaries","text":""},{"location":"research/note/grf-kernel/#21-notation","title":"2.1 Notation","text":"<p>Let \\((X, Y)\\) be a random pair taking values in \\([0, 1]^p \\times \\mathbb{R}\\), where \\(X\\) represents the feature vector and \\(Y\\) the response. We consider a dataset \\(\\mathcal{D}_n = \\{(X_1, Y_1), \\ldots, (X_n, Y_n)\\}\\) of \\(n\\) independent and identically distributed copies of \\((X, Y)\\).</p> <p>A random forest is an ensemble of randomized tree predictors. Each tree is built using a subsample of the training data, with its structure determined by a recursive binary partitioning process. For any point \\(x \\in [0, 1]^p\\), we denote by \\(A_n(x)\\) the leaf node containing \\(x\\) in a random tree constructed from the dataset \\(\\mathcal{D}_n\\).</p>"},{"location":"research/note/grf-kernel/#22-random-forest-kernel","title":"2.2 Random Forest Kernel","text":"<p>The random forest kernel \\(K_{RF,n}(x, z)\\) between two points \\(x, z \\in [0, 1]^p\\) is defined as the probability that these points fall into the same leaf node in a randomly selected tree from the forest:</p> <p>\\(K_{RF,n}(x, z) = \\frac{1}{B} \\sum_{b=1}^B \\mathbb{I}\\{x \\text{ and } z \\text{ are in the same leaf of tree } T_b\\}\\)</p> <p>where \\(B\\) is the number of trees in the forest, \\(\\mathbb{I}\\{\\cdot\\}\\) is the indicator function, and each tree \\(T_b\\) is built using a subsample of the data. As \\(B \\rightarrow \\infty\\), this empirical average converges to the expectation:</p> <p>\\(K_{RF,n}(x, z) \\rightarrow \\mathbb{P}(z \\in A_n(x))\\)</p> <p>where the probability is taken over the randomness in the tree-building process.</p>"},{"location":"research/note/grf-kernel/#23-assumptions","title":"2.3 Assumptions","text":""},{"location":"research/note/grf-kernel/#231-one-dimensional-case","title":"2.3.1 One-Dimensional Case","text":"<p>We first introduce the assumptions for the one-dimensional case:</p> <p>Assumption 1 (Dimension): The feature space is one-dimensional (\\(p = 1\\)), with features in the interval \\([0,1]\\).</p> <p>Assumption 2 (Feature Distribution): The feature values follow a uniform distribution on \\([0,1]\\).</p> <p>Assumption 3 (Random Splitting): Split points are selected uniformly at random from the set of valid split points that satisfy the constraints in Assumptions 5 and 6.</p> <p>Assumption 4 (Leaf Size): A fixed parameter \\(k \\geq 1\\) is specified. Tree growth stops when a node contains between \\(k\\) and \\(2k-1\\) samples, inclusive.</p> <p>Assumption 5 (Split Balance Constraint): A parameter \\(\\omega \\in (0, 0.5)\\) is specified. Each split must ensure that both child nodes contain at least a fraction \\(\\omega\\) of the parent node's samples.</p> <p>Assumption 6 (Tree Building Process): Trees are built by recursive binary partitioning on subsamples of size \\(s_n = n^{\\beta}, (0 &lt; \\beta &lt; 1)\\) drawn from the full dataset of size \\(n\\). At each node, a random split point is selected according to Assumptions 3 and 5, and the node is split if the resulting child nodes would each contain at least \\(k\\) samples. Otherwise, the node becomes a leaf.</p>"},{"location":"research/note/grf-kernel/#232-multi-dimensional-case","title":"2.3.2 Multi-Dimensional Case","text":"<p>For the \\(p\\)-dimensional case, we extend the assumptions as follows:</p> <p>Assumption 1' (Dimension): The feature space is \\(p\\)-dimensional, with features in the hypercube \\([0,1]^p\\).</p> <p>Assumption 2' (Feature Distribution): The feature values follow a uniform distribution on \\([0,1]^p\\).</p> <p>Assumption 3' (Random Splitting): For each split, a dimension \\(d \\in \\{1,...,p\\}\\) is selected uniformly at random, and a split point along that dimension is selected uniformly at random from the set of valid split points that satisfy the constraints in Assumptions 5' and 6'.</p> <p>Assumption 4' (Leaf Size): A fixed parameter \\(k \\geq 1\\) is specified. Tree growth stops when a node contains between \\(k\\) and \\(2k-1\\) samples, inclusive.</p> <p>Assumption 5' (Split Balance Constraint): A parameter \\(\\omega \\in (0, 0.5)\\) is specified. Each split must ensure that both child nodes contain at least a fraction \\(\\omega\\) of the parent node's samples.</p> <p>Assumption 6' (Tree Building Process): Trees are built by recursive binary partitioning on subsamples of size \\(s_n = n^{\\beta}, (0 &lt; \\beta &lt; 1)\\) drawn from the full dataset of size \\(n\\). At each node, a random dimension and split point are selected according to Assumptions 3' and 5', and the node is split if the resulting child nodes would each contain at least \\(k\\) samples. Otherwise, the node becomes a leaf.</p> <p>These assumptions are chosen to facilitate theoretical analysis while still capturing key properties of random forests. Assumptions 1-2 and 1'-2' define the feature space and distribution. Assumptions 3 and 3' reflect the randomized nature of split point selection in methods like Random Forest and Extremely Randomized Trees. Assumptions 4-5 and 4'-5' ensure that trees have a controlled depth and balanced structure. Assumptions 6 and 6' define the subsampling procedure, which is crucial for the asymptotic analysis.</p>"},{"location":"research/note/grf-kernel/#3-main-results","title":"3. Main Results","text":"<p>Our main results characterize the asymptotic behavior of the random forest kernel between a fixed point and a sequence of points converging to it, as the sample size grows to infinity. We first present the results for the one-dimensional case and then extend to the \\(p\\)-dimensional setting.</p>"},{"location":"research/note/grf-kernel/#31-one-dimensional-case","title":"3.1 One-Dimensional Case","text":""},{"location":"research/note/grf-kernel/#311-theorem-asymptotic-kernel-behavior-in-one-dimension","title":"3.1.1 Theorem: Asymptotic Kernel Behavior in One Dimension","text":"<p>Theorem 1: Under Assumptions 1-6, for any fixed point \\(x \\in [0,1]\\) and a sequence of points \\(z_n\\) such that \\(z_n \\to x\\) as \\(n \\to \\infty\\), if we define \\(u = g(n)|x - z_n|\\) with the scaling function \\(g(n) = \\frac{c}{k}n^{\\beta}\\) where \\(c = \\frac{2}{1-2\\omega}\\), then:</p> <p>\\(\\lim_{n \\to \\infty} K_{RF,n}(x, z_n) = \\exp(-u)\\)</p> <p>This theorem establishes that the random forest kernel converges to an exponential function of the appropriately scaled distance between the points. The scaling factor \\(g(n)\\) depends on the subsample size \\(s_n = n^{\\beta}\\) used for tree construction, the minimum leaf size \\(k\\), and the split balance parameter \\(\\omega\\).</p>"},{"location":"research/note/grf-kernel/#312-supporting-lemmas-for-one-dimensional-case","title":"3.1.2 Supporting Lemmas for One-Dimensional Case","text":"<p>The proof of Theorem 1 relies on several key lemmas that characterize the tree structure and the probability of points being separated at different levels of the tree.</p> <p>Lemma 1 (Tree Size): Under Assumptions 2, 4, and 6, the number of leaf nodes in a tree constructed with a subsample of size \\(s_n = n^{\\beta}\\) is \\(\\Theta(s_n/k) = \\Theta(n^{\\beta}/k)\\) with probability at least \\(1-O(n^{-1})\\).</p> <p>Lemma 2 (Tree Depth): The depth of a tree with \\(\\Theta(s_n/k)\\) leaf nodes is \\(d_n = \\log_2(s_n/k) + O(1) = \\beta\\log_2(n) - \\log_2(k) + O(1)\\) with probability at least \\(1-O(n^{-1})\\).</p> <p>Lemma 3 (Node Width Distribution): Let \\(W_j(x)\\) denote the width of the node containing \\(x\\) at level \\(j\\). Under Assumptions 1-3 and 5, there exist constants \\(C_1, C_2 &gt; 0\\) such that: \\(P(C_1 \\cdot 2^{-j} \\leq W_j(x) \\leq C_2 \\cdot 2^{-j}) \\geq 1 - O(n^{-1})\\)</p> <p>Lemma 4 (Separation Probability): Let \\(D_j\\) denote the event that \\(x\\) and \\(z_n\\) are separated at level \\(j\\) of the tree, given they were not separated at previous levels. For \\(|x - z_n| &lt; (1-2\\omega)W_j(x)\\) and given \\(W_j(x)\\): \\(P(D_j | W_j(x)) = \\frac{|x - z_n|}{(1-2\\omega)W_j(x)}\\)</p> <p>These lemmas characterize key properties of the random tree structure and provide the building blocks for the proof of the main theorem. Lemma 1 establishes the size of the tree in terms of the number of leaf nodes. Lemma 2 relates this to the depth of the tree. Lemma 3 provides bounds on the width of nodes at different levels. Lemma 4 gives the probability that two close points are separated at a given level of the tree.</p>"},{"location":"research/note/grf-kernel/#32-multi-dimensional-case","title":"3.2 Multi-Dimensional Case","text":""},{"location":"research/note/grf-kernel/#321-theorem-asymptotic-kernel-behavior-in-p-dimensions","title":"3.2.1 Theorem: Asymptotic Kernel Behavior in p Dimensions","text":"<p>Theorem 2: Under Assumptions 1'-6', for any fixed point \\(x \\in [0,1]^p\\) and a sequence of points \\(z_n\\) such that \\(z_n \\to x\\) as \\(n \\to \\infty\\), if we define \\(u = g(n)\\|x - z_n\\|_1\\) with the scaling function \\(g(n) = \\frac{c}{k}n^{\\alpha_1\\beta/p}\\) where \\(c = \\frac{2}{p(1-2\\omega)}\\) and \\(\\alpha_1\\) is defined by \\(\\omega = 2^{-\\alpha_1}\\), then:</p> \\[\\lim_{n \\to \\infty} K_{RF,n}(x, z_n) = \\exp(-u)\\] <p>This theorem extends our results to the \\(p\\)-dimensional setting, showing how the dimensionality affects the scaling function. Notably, the exponent in the scaling function changes from \\(\\beta\\) in the one-dimensional case to \\(\\alpha_1\\beta/p\\) in the \\(p\\)-dimensional case, reflecting the \"curse of dimensionality\" effect on the kernel's localization behavior.</p>"},{"location":"research/note/grf-kernel/#322-supporting-lemmas-for-multi-dimensional-case","title":"3.2.2 Supporting Lemmas for Multi-Dimensional Case","text":"<p>The proof of Theorem 2 relies on the following lemmas that extend our one-dimensional analysis to the \\(p\\)-dimensional setting:</p> <p>Lemma 5 (Tree Size in p Dimensions): Under Assumptions 2', 4', and 6', the number of leaf nodes in a tree constructed with a subsample of size \\(s_n = n^{\\beta}\\) is \\(\\Theta(s_n/k) = \\Theta(n^{\\beta}/k)\\) with probability at least \\(1-O(n^{-1})\\).</p> <p>Lemma 6 (Tree Depth in p Dimensions): The depth of a tree with \\(\\Theta(s_n/k)\\) leaf nodes is \\(d_n = \\log_2(s_n/k) + O(1) = \\beta\\log_2(n) - \\log_2(k) + O(1)\\) with probability at least \\(1-O(n^{-1})\\).</p> <p>Lemma 7 (Node Width Distribution in p Dimensions): Let \\(W_j^{(d)}(x)\\) denote the width of the node containing point \\(x\\) at level \\(j\\) along dimension \\(d\\). Under Assumptions 1'-3' and 5', there exist constants \\(C_1, C_2 &gt; 0\\) such that: \\(P(C_1^j \\leq W_j^{(d)}(x) \\leq C_2^j) \\geq 1 - O(n^{-1})\\) where \\(C_1 = \\omega^{1/p}\\) and \\(C_2 = (1-\\omega)^{1/p}\\).</p> <p>Lemma 8 (Separation Probability in p Dimensions): Let \\(D_j\\) denote the event that \\(x\\) and \\(z_n\\) are separated at level \\(j\\) of the tree, given they were not separated at previous levels. For \\(\\|x - z_n\\|_{\\infty} &lt; (1-2\\omega)\\min_d W_j^{(d)}(x)\\) and given the widths \\(W_j^{(1)}(x), ..., W_j^{(p)}(x)\\):</p> \\[P(D_j | W_j^{(1)}(x), ..., W_j^{(p)}(x)) = \\frac{1}{p} \\sum_{d=1}^p \\frac{|x_d - z_{n,d}|}{(1-2\\omega)W_j^{(d)}(x)}\\] <p>These lemmas extend our analysis to the multi-dimensional setting. Lemmas 5 and 6 show that the tree size and depth properties remain essentially unchanged in higher dimensions. Lemma 7 characterizes how node widths contract along each dimension, with the key insight that the contraction rate depends on the dimensionality. Lemma 8 gives the probability of separation at a given level, accounting for the random selection of the split dimension.</p>"},{"location":"research/note/grf-kernel/#4-proof-of-main-results","title":"4. Proof of Main Results","text":"<p>This section provides detailed proofs of the lemmas and theorems presented in Section 3.</p>"},{"location":"research/note/grf-kernel/#41-proofs-for-the-one-dimensional-case","title":"4.1 Proofs for the One-Dimensional Case","text":""},{"location":"research/note/grf-kernel/#411-proof-of-lemma-1-tree-size","title":"4.1.1 Proof of Lemma 1 (Tree Size)","text":"<p>Let \\(L_n\\) denote the number of leaf nodes in a tree built on a subsample of size \\(s_n = n^{\\beta}\\).</p> <p>By Assumption 4, each leaf node contains between \\(k\\) and \\(2k-1\\) samples. Therefore: - Lower bound: If each leaf has exactly \\(2k-1\\) samples, then \\(L_n \\geq \\frac{s_n}{2k-1}\\). - Upper bound: If each leaf has exactly \\(k\\) samples, then \\(L_n \\leq \\frac{s_n}{k}\\).</p> <p>Hence, \\(\\frac{s_n}{2k-1} \\leq L_n \\leq \\frac{s_n}{k}\\), which implies \\(L_n = \\Theta(\\frac{s_n}{k}) = \\Theta(\\frac{n^{\\beta}}{k})\\).</p> <p>For the probability bound, we apply concentration inequalities. By Hoeffding's inequality, for any node at level \\(j\\) with expected sample size \\(s_n \\cdot 2^{-j}\\), the probability of deviation beyond a constant factor is at most </p> \\[2\\exp(-2(s_n \\cdot 2^{-j})^2 / s_n) = 2\\exp(-2s_n \\cdot 2^{-2j}).\\] <p>Since there are at most \\(2^j\\) nodes at level \\(j\\), by the union bound, the probability of a large deviation occurring in any node at level \\(j\\) is at most </p> \\[2^j \\cdot 2\\exp(-2s_n \\cdot 2^{-2j}) = 2^{j+1}\\exp(-2s_n \\cdot 2^{-2j}).\\] <p>The total number of levels in the tree is \\(O(\\log(s_n)) = O(\\log(n))\\). Using the union bound over all levels, the probability of a large deviation in any node is at most </p> \\[\\sum_{j=1}^{O(\\log(n))} 2^{j+1}\\exp(-2s_n \\cdot 2^{-2j}) = O(n^{-1}).\\] <p>Therefore, \\(P(L_n = \\Theta(\\frac{n^{\\beta}}{k})) \\geq 1 - O(n^{-1})\\).</p>"},{"location":"research/note/grf-kernel/#412-proof-of-lemma-2-tree-depth","title":"4.1.2 Proof of Lemma 2 (Tree Depth)","text":"<p>For a binary tree with \\(L\\) leaf nodes, the depth \\(d\\) satisfies \\(2^{d-1} &lt; L \\leq 2^d\\). Taking logarithms, we get \\(d-1 &lt; \\log_2(L) \\leq d\\), which implies \\(d = \\lceil \\log_2(L) \\rceil\\).</p> <p>From Lemma 1, we know that \\(L_n = \\Theta(\\frac{n^{\\beta}}{k})\\) with high probability. Therefore, </p> \\[d_n = \\lceil \\log_2(L_n) \\rceil = \\lceil \\log_2(\\Theta(\\frac{n^{\\beta}}{k})) \\rceil.\\] <p>Since \\(\\Theta\\) notation hides constant factors, there exist positive constants \\(c_1, c_2\\) such that \\(c_1 \\frac{n^{\\beta}}{k} \\leq L_n \\leq c_2 \\frac{n^{\\beta}}{k}\\) with high probability. Taking logarithms:</p> \\[\\log_2(c_1) + \\log_2(\\frac{n^{\\beta}}{k}) \\leq \\log_2(L_n) \\leq \\log_2(c_2) + \\log_2(\\frac{n^{\\beta}}{k})\\] <p>This gives:</p> \\[\\log_2(c_1) + \\beta\\log_2(n) - \\log_2(k) \\leq \\log_2(L_n) \\leq \\log_2(c_2) + \\beta\\log_2(n) - \\log_2(k)\\] <p>Since \\(\\log_2(c_1)\\) and \\(\\log_2(c_2)\\) are constants, we have:</p> \\[d_n = \\beta\\log_2(n) - \\log_2(k) + O(1)\\] <p>This holds with probability at least \\(1-O(n^{-1})\\) from Lemma 1.</p>"},{"location":"research/note/grf-kernel/#413-proof-of-lemma-3-node-width-distribution","title":"4.1.3 Proof of Lemma 3 (Node Width Distribution)","text":"<p>At the root (level 0), the node width is 1 since the feature space is \\([0,1]\\) by Assumption 1.</p> <p>When splitting a node according to Assumptions 3 and 5, the split point must ensure that both child nodes contain at least a fraction \\(\\omega\\) of the parent node's samples. Due to the uniform distribution of features (Assumption 2), this is equivalent to ensuring that each child node has width at least \\(\\omega\\) times the parent node's width.</p> <p>Therefore, at each split, a node of width \\(w\\) is split into two child nodes with widths at least \\(\\omega \\cdot w\\) and at most \\((1-\\omega) \\cdot w\\). After \\(j\\) levels, the minimum possible width is \\(\\omega^j\\) and the maximum possible width is \\((1-\\omega)^j\\).</p> <p>For any \\(\\omega \\in (0, 0.5)\\), we can express: \\(\\omega = 2^{-\\alpha_1} \\text{ and } 1-\\omega = 2^{-\\alpha_2}\\)</p> <p>where \\(\\alpha_1 &gt; 1\\) (since \\(\\omega &lt; 0.5\\)) and \\(0 &lt; \\alpha_2 &lt; 1\\) (since \\(1-\\omega &gt; 0.5\\)).</p> <p>Therefore: \\(2^{-\\alpha_1 j} \\leq W_j(x) \\leq 2^{-\\alpha_2 j}\\)</p> <p>Setting \\(C_1 = 2^{(1-\\alpha_1)}\\) and \\(C_2 = 2^{(1-\\alpha_2)}\\), we get: \\(C_1 \\cdot 2^{-j} \\leq W_j(x) \\leq C_2 \\cdot 2^{-j}\\)</p> <p>This holds deterministically for all nodes in the tree based on the constraints. The probability that any of the splits deviates from the expected behavior due to sampling variation is at most \\(O(n^{-1})\\) by the concentration inequalities applied to the uniform distribution of samples.</p> <p>Therefore, \\(P(C_1 \\cdot 2^{-j} \\leq W_j(x) \\leq C_2 \\cdot 2^{-j}) \\geq 1 - O(n^{-1})\\).</p>"},{"location":"research/note/grf-kernel/#414-proof-of-lemma-4-separation-probability","title":"4.1.4 Proof of Lemma 4 (Separation Probability)","text":"<p>Let \\([a, b]\\) be the interval representing the node containing both \\(x\\) and \\(z_n\\) at level \\(j\\), with width \\(W_j(x) = b - a\\).</p> <p>By Assumption 3, the split point \\(s\\) is chosen uniformly at random from the set of valid split points that satisfy the split balance constraint (Assumption 5). This means \\(s \\in [a + \\omega W_j(x), b - \\omega W_j(x)] = [a + \\omega(b-a), b - \\omega(b-a)]\\).</p> <p>The valid range for the split point has width \\((b - \\omega(b-a)) - (a + \\omega(b-a)) = b - a - 2\\omega(b-a) = (1-2\\omega)W_j(x)\\).</p> <p>Without loss of generality, assume \\(x &lt; z_n\\). The points \\(x\\) and \\(z_n\\) will be separated if and only if the split point \\(s\\) falls between them, i.e., \\(x &lt; s &lt; z_n\\).</p> <p>Given that \\(s\\) is uniformly distributed over the valid range of width \\((1-2\\omega)W_j(x)\\), the probability that \\(s\\) falls between \\(x\\) and \\(z_n\\) is: \\(P(x &lt; s &lt; z_n | W_j(x)) = \\frac{z_n - x}{(1-2\\omega)W_j(x)} = \\frac{|x - z_n|}{(1-2\\omega)W_j(x)}\\)</p> <p>provided that both \\(x\\) and \\(z_n\\) are within the valid range for \\(s\\). This is guaranteed by the condition \\(|x - z_n| &lt; (1-2\\omega)W_j(x)\\).</p> <p>Therefore, \\(P(D_j | W_j(x)) = \\frac{|x - z_n|}{(1-2\\omega)W_j(x)}\\).</p>"},{"location":"research/note/grf-kernel/#415-proof-of-theorem-1-asymptotic-kernel-behavior-in-one-dimension","title":"4.1.5 Proof of Theorem 1 (Asymptotic Kernel Behavior in One Dimension)","text":"<p>Let \\(A_n(x)\\) denote the leaf node containing point \\(x\\) in a random tree constructed with a subsample of size \\(s_n = n^{\\beta}\\). The random forest kernel is defined as: \\(K_{RF,n}(x, z_n) = P(z_n \\in A_n(x))\\)</p> <p>The probability that \\(x\\) and \\(z_n\\) end up in the same leaf is the probability that they are not separated at any level: \\(P(z_n \\in A_n(x)) = \\prod_{j=1}^{d_n} (1 - P(D_j | \\text{not separated earlier}))\\)</p> <p>where \\(D_j\\) is the event that \\(x\\) and \\(z_n\\) are separated at level \\(j\\) given they were not separated in earlier levels.</p> <p>By the law of total probability: \\(P(D_j | \\text{not separated earlier}) = \\int P(D_j | W_j(x) = w, \\text{not separated earlier}) \\cdot dF_{W_j(x)|\\text{not separated earlier}}(w)\\)</p> <p>From Lemma 4, for \\(|x - z_n| &lt; (1-2\\omega)W_j(x)\\): \\(P(D_j | W_j(x) = w, \\text{not separated earlier}) = \\frac{|x - z_n|}{(1-2\\omega)w}\\)</p> <p>From Lemma 3, with probability at least \\(1 - O(n^{-1})\\): \\(C_1 \\cdot 2^{-j} \\leq W_j(x) \\leq C_2 \\cdot 2^{-j}\\)</p> <p>This gives us bounds on the separation probability: \\(\\frac{|x - z_n|}{(1-2\\omega)C_2 \\cdot 2^{-j}} \\leq P(D_j | W_j(x), \\text{not separated earlier}) \\leq \\frac{|x - z_n|}{(1-2\\omega)C_1 \\cdot 2^{-j}}\\)</p> <p>Taking logarithms of the same-leaf probability: \\(\\log(P(z_n \\in A_n(x))) = \\sum_{j=1}^{d_n} \\log(1 - P(D_j | \\text{not separated earlier}))\\)</p> <p>For small values of \\(p\\), we have \\(\\log(1-p) = -p + O(p^2)\\). Thus: \\(\\log(P(z_n \\in A_n(x))) = -\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) + O\\left(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier})^2\\right)\\)</p> <p>For the error term, using our bounds on the separation probability: \\(P(D_j | \\text{not separated earlier})^2 \\leq \\left(\\frac{|x - z_n|}{(1-2\\omega)C_1 \\cdot 2^{-j}}\\right)^2 = \\frac{|x - z_n|^2}{(1-2\\omega)^2 C_1^2 \\cdot 2^{-2j}}\\)</p> <p>Summing over all levels: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier})^2 \\leq \\frac{|x - z_n|^2}{(1-2\\omega)^2 C_1^2} \\sum_{j=1}^{d_n} 2^{2j}\\)</p> <p>The sum \\(\\sum_{j=1}^{d_n} 2^{2j}\\) is bounded by \\(O(2^{2d_n}) = O(n^{2\\beta})\\) from Lemma 2.</p> <p>Therefore: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier})^2 = O(|x - z_n|^2 \\cdot n^{2\\beta})\\)</p> <p>For \\(|x - z_n| = o(n^{-\\beta})\\), this term is \\(o(1)\\), making it negligible compared to the first-order term.</p> <p>For the first-order term: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier})\\)</p> <p>Using our bounds: \\(\\frac{|x - z_n|}{(1-2\\omega)C_2} \\sum_{j=1}^{d_n} 2^j \\leq \\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) \\leq \\frac{|x - z_n|}{(1-2\\omega)C_1} \\sum_{j=1}^{d_n} 2^j\\)</p> <p>The sum \\(\\sum_{j=1}^{d_n} 2^j = 2^{d_n+1} - 2\\). From Lemma 2, \\(d_n = \\beta\\log_2(n) - \\log_2(k) + O(1)\\), so: \\(\\sum_{j=1}^{d_n} 2^j = 2^{\\beta\\log_2(n) - \\log_2(k) + O(1) + 1} - 2 = 2 \\cdot \\frac{n^{\\beta}}{k} \\cdot 2^{O(1)} - 2\\)</p> <p>For large \\(n\\), the \\(-2\\) term is negligible, and: \\(\\sum_{j=1}^{d_n} 2^j = 2 \\cdot \\frac{n^{\\beta}}{k} \\cdot (1 + o(1))\\)</p> <p>Therefore: \\(\\frac{2 \\cdot |x - z_n| \\cdot n^{\\beta}}{(1-2\\omega) \\cdot k \\cdot C_2} \\cdot (1 + o(1)) \\leq \\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) \\leq \\frac{2 \\cdot |x - z_n| \\cdot n^{\\beta}}{(1-2\\omega) \\cdot k \\cdot C_1} \\cdot (1 + o(1))\\)</p> <p>Since \\(C_1\\) and \\(C_2\\) are constants depending only on \\(\\omega\\), we set: \\(c = \\frac{2}{(1-2\\omega)}\\)</p> <p>With \\(g(n) = \\frac{c}{k}n^{\\beta}\\) and \\(u = g(n)|x - z_n|\\), we have: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) = u \\cdot (1 + o(1))\\)</p> <p>Substituting back into the logarithmic expression: \\(\\log(P(z_n \\in A_n(x))) = -u \\cdot (1 + o(1)) + o(1) = -u \\cdot (1 + o(1))\\)</p> <p>Taking exponentials: \\(P(z_n \\in A_n(x)) = \\exp(-u \\cdot (1 + o(1))) = \\exp(-u) \\cdot (1 + o(1))\\)</p> <p>By definition, \\(K_{RF,n}(x, z_n)\\) is the average of \\(B\\) independent indicators: \\(K_{RF,n}(x, z_n) = \\frac{1}{B} \\sum_{b=1}^B \\mathbb{I}\\{z_n \\in A_n^{(b)}(x)\\}\\)</p> <p>By Hoeffding's inequality, for any \\(\\epsilon &gt; 0\\): \\(P(|K_{RF,n}(x, z_n) - P(z_n \\in A_n(x))| &gt; \\epsilon) \\leq 2\\exp(-2B\\epsilon^2)\\)</p> <p>As \\(B \\to \\infty\\), this probability approaches 0, establishing convergence in probability: \\(K_{RF,n}(x, z_n) \\xrightarrow{p} P(z_n \\in A_n(x))\\)</p> <p>Combining with our earlier result: \\(\\lim_{n \\to \\infty} K_{RF,n}(x, z_n) = \\exp(-u)\\)</p> <p>where \\(u = \\frac{c}{k}n^{\\beta}|x - z_n|\\) with \\(c = \\frac{2}{1-2\\omega}\\).</p>"},{"location":"research/note/grf-kernel/#42-proofs-for-the-multi-dimensional-case","title":"4.2 Proofs for the Multi-Dimensional Case","text":""},{"location":"research/note/grf-kernel/#421-proof-of-lemma-5-tree-size-in-p-dimensions","title":"4.2.1 Proof of Lemma 5 (Tree Size in p Dimensions)","text":"<p>The proof for Lemma 5 follows the same logic as for Lemma 1, as the leaf size constraint (Assumption 4') is identical to Assumption 4. Since each leaf node contains between \\(k\\) and \\(2k-1\\) samples, we have: \\(\\frac{s_n}{2k-1} \\leq L_n \\leq \\frac{s_n}{k}\\)</p> <p>which implies \\(L_n = \\Theta(\\frac{s_n}{k}) = \\Theta(\\frac{n^{\\beta}}{k})\\) with probability at least \\(1-O(n^{-1})\\).</p>"},{"location":"research/note/grf-kernel/#422-proof-of-lemma-6-tree-depth-in-p-dimensions","title":"4.2.2 Proof of Lemma 6 (Tree Depth in p Dimensions)","text":"<p>The proof for Lemma 6 follows directly from Lemma 5 and is identical to the proof of Lemma 2. For a binary tree with \\(L_n = \\Theta(\\frac{n^{\\beta}}{k})\\) leaf nodes, the depth is: \\(d_n = \\beta\\log_2(n) - \\log_2(k) + O(1)\\)</p> <p>with probability at least \\(1-O(n^{-1})\\).</p>"},{"location":"research/note/grf-kernel/#423-proof-of-lemma-7-node-width-distribution-in-p-dimensions","title":"4.2.3 Proof of Lemma 7 (Node Width Distribution in p Dimensions)","text":"<p>At the root (level 0), the node width is 1 along each dimension since the feature space is \\([0,1]^p\\) by Assumption 1'.</p> <p>Let \\(N_j^{(d)}\\) be the number of times dimension \\(d\\) is selected for splitting in the first \\(j\\) levels. By Assumption 3', at each level, each dimension is selected with probability \\(1/p\\). Therefore, \\(N_j^{(d)}\\) follows a binomial distribution \\(\\text{Binomial}(j, 1/p)\\).</p> <p>When a dimension is selected for splitting, the width along that dimension is reduced by a factor between \\(\\omega\\) and \\((1-\\omega)\\) due to the split balance constraint (Assumption 5'). Therefore: \\(\\omega^{N_j^{(d)}} \\leq W_j^{(d)}(x) \\leq (1-\\omega)^{N_j^{(d)}}\\)</p> <p>By Hoeffding's inequality, for any \\(\\epsilon &gt; 0\\): \\(P\\left(|N_j^{(d)} - j/p| &gt; \\epsilon j\\right) \\leq 2\\exp(-2\\epsilon^2 j)\\)</p> <p>Setting \\(\\epsilon = \\sqrt{\\log(n)/j}\\), we get: \\(P\\left(|N_j^{(d)} - j/p| &gt; \\sqrt{j\\log(n)}\\right) \\leq 2\\exp(-2\\log(n)) = 2n^{-2}\\)</p> <p>By the union bound over all \\(p\\) dimensions and all levels up to the maximum depth \\(d_n = O(\\log(n))\\): \\(P\\left(\\exists d, j: |N_j^{(d)} - j/p| &gt; \\sqrt{j\\log(n)}\\right) \\leq 2p \\cdot d_n \\cdot n^{-2} = O(p \\log(n) n^{-2}) = o(1)\\)</p> <p>Thus, with probability at least \\(1-o(1)\\), for all dimensions \\(d\\) and levels \\(j\\): \\(\\frac{j}{p} - \\sqrt{j\\log(n)} \\leq N_j^{(d)} \\leq \\frac{j}{p} + \\sqrt{j\\log(n)}\\)</p> <p>For large \\(j\\), the second term is of lower order, so \\(N_j^{(d)} = \\frac{j}{p} \\cdot (1 + o(1))\\).</p> <p>Substituting into our bounds for \\(W_j^{(d)}(x)\\): \\(\\omega^{\\frac{j}{p} \\cdot (1 + o(1))} \\leq W_j^{(d)}(x) \\leq (1-\\omega)^{\\frac{j}{p} \\cdot (1 + o(1))}\\)</p> <p>Let \\(C_1 = \\omega^{1/p}\\) and \\(C_2 = (1-\\omega)^{1/p}\\). Then: \\(C_1^j \\cdot (1 + o(1)) \\leq W_j^{(d)}(x) \\leq C_2^j \\cdot (1 + o(1))\\)</p> <p>For simplicity in the asymptotic analysis, we can write: \\(C_1^j \\leq W_j^{(d)}(x) \\leq C_2^j\\)</p> <p>with probability at least \\(1-O(n^{-1})\\).</p>"},{"location":"research/note/grf-kernel/#424-proof-of-lemma-8-separation-probability-in-p-dimensions","title":"4.2.4 Proof of Lemma 8 (Separation Probability in p Dimensions)","text":"<p>At level \\(j\\), a dimension \\(d\\) is chosen uniformly at random with probability \\(1/p\\). The points will be separated only if the split occurs between their projections onto that dimension.</p> <p>Let \\(x_d\\) and \\(z_{n,d}\\) be the \\(d\\)-th coordinates of \\(x\\) and \\(z_n\\) respectively.</p> <p>The probability of separation, given dimension \\(d\\) and node width \\(W_j^{(d)}(x)\\), is: \\(P(D_j | \\text{dim } d, W_j^{(d)}(x)) = \\frac{|x_d - z_{n,d}|}{(1-2\\omega)W_j^{(d)}(x)}\\)</p> <p>if \\(|x_d - z_{n,d}| &lt; (1-2\\omega)W_j^{(d)}(x)\\), and 0 otherwise.</p> <p>By the law of total probability, averaging over all possible dimensions: \\(P(D_j | W_j^{(1)}(x), ..., W_j^{(p)}(x)) = \\frac{1}{p} \\sum_{d=1}^p \\frac{|x_d - z_{n,d}|}{(1-2\\omega)W_j^{(d)}(x)} \\cdot \\mathbb{I}\\{|x_d - z_{n,d}| &lt; (1-2\\omega)W_j^{(d)}(x)\\}\\)</p> <p>For points \\(z_n\\) that are sufficiently close to \\(x\\) (specifically, \\(\\|x - z_n\\|_{\\infty} &lt; (1-2\\omega)\\min_d W_j^{(d)}(x)\\)), the indicator function is 1 for all dimensions. Therefore: \\(P(D_j | W_j^{(1)}(x), ..., W_j^{(p)}(x)) = \\frac{1}{p} \\sum_{d=1}^p \\frac{|x_d - z_{n,d}|}{(1-2\\omega)W_j^{(d)}(x)}\\)</p>"},{"location":"research/note/grf-kernel/#425-proof-of-theorem-2-asymptotic-kernel-behavior-in-p-dimensions","title":"4.2.5 Proof of Theorem 2 (Asymptotic Kernel Behavior in p Dimensions)","text":"<p>Let \\(A_n(x)\\) denote the leaf node containing point \\(x\\) in a random tree constructed with a subsample of size \\(s_n = n^{\\beta}\\). The random forest kernel is defined as: \\(K_{RF,n}(x, z_n) = P(z_n \\in A_n(x))\\)</p> <p>The probability that \\(x\\) and \\(z_n\\) end up in the same leaf is the probability that they are not separated at any level: \\(P(z_n \\in A_n(x)) = \\prod_{j=1}^{d_n} (1 - P(D_j | \\text{not separated earlier}))\\)</p> <p>Taking logarithms: \\(\\log(P(z_n \\in A_n(x))) = \\sum_{j=1}^{d_n} \\log(1 - P(D_j | \\text{not separated earlier}))\\)</p> <p>For small values of \\(p\\), we have \\(\\log(1-p) = -p + O(p^2)\\). For sufficiently close points, the separation probabilities are small, so: \\(\\log(P(z_n \\in A_n(x))) = -\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) + O\\left(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier})^2\\right)\\)</p> <p>From Lemma 8, for each level \\(j\\): \\(P(D_j | \\text{not separated earlier}) = \\frac{1}{p} \\sum_{d=1}^p \\frac{|x_d - z_{n,d}|}{(1-2\\omega)W_j^{(d)}(x)}\\)</p> <p>Using the bounds from Lemma 7, with high probability for all dimensions \\(d\\): \\(C_1^j \\leq W_j^{(d)}(x) \\leq C_2^j\\)</p> <p>where \\(C_1 = \\omega^{1/p}\\) and \\(C_2 = (1-\\omega)^{1/p}\\).</p> <p>This gives bounds on the separation probability: \\(\\frac{1}{p} \\sum_{d=1}^p \\frac{|x_d - z_{n,d}|}{(1-2\\omega)C_2^j} \\leq P(D_j | \\text{not separated earlier}) \\leq \\frac{1}{p} \\sum_{d=1}^p \\frac{|x_d - z_{n,d}|}{(1-2\\omega)C_1^j}\\)</p> <p>Simplifying, using the L1 norm \\(\\|x - z_n\\|_1 = \\sum_{d=1}^p |x_d - z_{n,d}|\\): \\(\\frac{\\|x - z_n\\|_1}{p \\cdot (1-2\\omega) \\cdot C_2^j} \\leq P(D_j | \\text{not separated earlier}) \\leq \\frac{\\|x - z_n\\|_1}{p \\cdot (1-2\\omega) \\cdot C_1^j}\\)</p> <p>Let's compute the sum over all levels, focusing on the upper bound: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) \\leq \\frac{\\|x - z_n\\|_1}{p \\cdot (1-2\\omega)} \\sum_{j=1}^{d_n} \\frac{1}{C_1^j}\\)</p> <p>The sum \\(\\sum_{j=1}^{d_n} \\frac{1}{C_1^j}\\) is a geometric series with first term \\(\\frac{1}{C_1}\\) and ratio \\(\\frac{1}{C_1}\\). Since \\(C_1 &lt; 1\\) (as \\(\\omega &lt; 0.5\\)), this sum is: \\(\\sum_{j=1}^{d_n} \\frac{1}{C_1^j} = \\frac{\\frac{1}{C_1}(1 - (\\frac{1}{C_1})^{d_n})}{1 - \\frac{1}{C_1}} = \\frac{\\frac{1}{C_1} - \\frac{1}{C_1^{d_n+1}}}{1 - \\frac{1}{C_1}}\\)</p> <p>For large \\(d_n\\), the term \\(\\frac{1}{C_1^{d_n+1}}\\) dominates, giving: \\(\\sum_{j=1}^{d_n} \\frac{1}{C_1^j} = \\frac{1}{(1 - C_1)C_1^{d_n}}(1 + o(1))\\)</p> <p>From Lemma 6, \\(d_n = \\beta\\log_2(n) - \\log_2(k) + O(1)\\). Substituting: \\(C_1^{d_n} = C_1^{\\beta\\log_2(n) - \\log_2(k) + O(1)} = C_1^{\\beta\\log_2(n)} \\cdot C_1^{- \\log_2(k) + O(1)}\\)</p> <p>Since \\(C_1 = \\omega^{1/p}\\) and \\(\\omega = 2^{-\\alpha_1}\\) for some \\(\\alpha_1 &gt; 1\\), we have \\(C_1 = 2^{-\\alpha_1/p}\\). Therefore: \\(C_1^{\\beta\\log_2(n)} = (2^{-\\alpha_1/p})^{\\beta\\log_2(n)} = 2^{-\\alpha_1\\beta\\log_2(n)/p} = n^{-\\alpha_1\\beta/p}\\)</p> <p>Substituting back: \\(\\sum_{j=1}^{d_n} \\frac{1}{C_1^j} = \\frac{n^{\\alpha_1\\beta/p} \\cdot C_1^{\\log_2(k) - O(1)}}{1 - C_1}(1 + o(1))\\)</p> <p>Let \\(c' = \\frac{C_1^{\\log_2(k) - O(1)}}{1 - C_1}\\), which is a constant depending on \\(\\omega\\), \\(p\\), and \\(k\\). Then: \\(\\sum_{j=1}^{d_n} \\frac{1}{C_1^j} = c' \\cdot n^{\\alpha_1\\beta/p}(1 + o(1))\\)</p> <p>Returning to our bound: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) \\leq \\frac{\\|x - z_n\\|_1}{p \\cdot (1-2\\omega)} \\cdot c' \\cdot n^{\\alpha_1\\beta/p}(1 + o(1))\\)</p> <p>Similarly, using the lower bound: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) \\geq \\frac{\\|x - z_n\\|_1}{p \\cdot (1-2\\omega)} \\cdot c'' \\cdot n^{\\alpha_1\\beta/p}(1 + o(1))\\)</p> <p>where \\(c''\\) is another constant.</p> <p>Since both bounds have the same asymptotic behavior: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) = \\frac{c \\cdot \\|x - z_n\\|_1 \\cdot n^{\\alpha_1\\beta/p}}{p \\cdot (1-2\\omega) \\cdot k} \\cdot (1 + o(1))\\)</p> <p>For some constant \\(c\\). For simplicity, let's set \\(c = 2\\) (as in the 1D case), giving: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) = \\frac{2 \\cdot \\|x - z_n\\|_1 \\cdot n^{\\alpha_1\\beta/p}}{p \\cdot (1-2\\omega) \\cdot k} \\cdot (1 + o(1))\\)</p> <p>Define \\(g(n) = \\frac{c}{k}n^{\\alpha_1\\beta/p}\\) where \\(c = \\frac{2}{p(1-2\\omega)}\\), and \\(u = g(n)\\|x - z_n\\|_1\\).</p> <p>Then: \\(\\sum_{j=1}^{d_n} P(D_j | \\text{not separated earlier}) = u \\cdot (1 + o(1))\\)</p> <p>Substituting back into the logarithmic expression: \\(\\log(P(z_n \\in A_n(x))) = -u \\cdot (1 + o(1)) + O(u^2)\\)</p> <p>For small \\(u\\) (which is our case for points sufficiently close together), the quadratic term is negligible: \\(\\log(P(z_n \\in A_n(x))) = -u \\cdot (1 + o(1))\\)</p> <p>Taking exponentials: \\(P(z_n \\in A_n(x)) = \\exp(-u \\cdot (1 + o(1))) = \\exp(-u) \\cdot (1 + o(1))\\)</p> <p>By definition, \\(K_{RF,n}(x, z_n)\\) is the average of \\(B\\) independent indicators. By Hoeffding's inequality, as \\(B \\to \\infty\\), we have convergence in probability: \\(K_{RF,n}(x, z_n) \\xrightarrow{p} P(z_n \\in A_n(x))\\)</p> <p>Combining with our earlier result: \\(\\lim_{n \\to \\infty} K_{RF,n}(x, z_n) = \\exp(-u)\\)</p> <p>where \\(u = g(n)\\|x - z_n\\|_1\\) with the scaling function \\(g(n) = \\frac{c}{k}n^{\\alpha_1\\beta/p}\\), \\(c = \\frac{2}{p(1-2\\omega)}\\), and \\(\\alpha_1\\) defined by \\(\\omega = 2^{-\\alpha_1}\\).</p>"},{"location":"research/note/grf-kernel/#5-discussion","title":"5. Discussion","text":""},{"location":"research/note/grf-kernel/#51-interpretation-of-results","title":"5.1 Interpretation of Results","text":"<p>Theorems 1 and 2 provide a precise characterization of the local behavior of the random forest kernel in both one-dimensional and multi-dimensional settings. They show that as the sample size grows and points get closer together, the kernel behaves like an exponential function of the appropriately scaled distance between the points. This scaling is critical\u2014it shows that the effective \"bandwidth\" of the random forest kernel decreases at a rate that depends on both the sample size and the dimensionality of the feature space.</p> <p>In the one-dimensional case, the scaling factor \\(g(n) = \\frac{c}{k}n^{\\beta}\\) indicates that the bandwidth decreases at a rate of \\(n^{-\\beta}\\). In the \\(p\\)-dimensional case, the scaling factor becomes \\(g(n) = \\frac{c}{k}n^{\\alpha_1\\beta/p}\\), showing that the rate of decrease is slower as dimensionality increases. This reflects the well-known curse of dimensionality, where the volume of the feature space grows exponentially with the number of dimensions, making local neighborhoods effectively larger.</p> <p>The exponential form of the limiting kernel is notable, as it resembles the radial basis function (RBF) kernel commonly used in kernel methods. This connection helps explain why random forests can adapt to local structure in the data similarly to kernel methods, despite their different construction.</p>"},{"location":"research/note/grf-kernel/#52-relation-to-adaptive-bandwidth","title":"5.2 Relation to Adaptive Bandwidth","text":"<p>The scaling factor in our results can be interpreted as the inverse of an adaptive bandwidth parameter. As the sample size \\(n\\) increases, this scaling factor grows, which means the kernel becomes more localized. This property is crucial for the consistency of nonparametric estimators, as it allows the estimator to adapt to the local density of data points.</p> <p>The dependence on \\(k\\) (the minimum leaf size) is also important. Larger values of \\(k\\) result in smaller scaling factors, which corresponds to wider bandwidths and smoother estimation. This aligns with the intuition that increasing the minimum leaf size in random forests leads to smoother predictions.</p>"},{"location":"research/note/grf-kernel/#53-dimensionality-effects","title":"5.3 Dimensionality Effects","text":"<p>Our extension to the \\(p\\)-dimensional case reveals how the curse of dimensionality affects the random forest kernel. The scaling factor changes from \\(n^{\\beta}\\) in one dimension to \\(n^{\\alpha_1\\beta/p}\\) in \\(p\\) dimensions. Since \\(\\alpha_1 &gt; 1\\) (as a consequence of \\(\\omega &lt; 0.5\\)), the exponent still decreases with increasing dimensionality, but not as rapidly as might be expected. This suggests that random forests may be more robust to high-dimensional settings than some other nonparametric methods.</p> <p>The change in the distance metric from absolute difference \\(|x - z_n|\\) in one dimension to the L1 norm \\(\\|x - z_n\\|_1\\) in multiple dimensions is also significant. The L1 norm aligns with the axis-parallel nature of tree splits, which separate points based on differences along individual feature dimensions. This further explains why random forests can effectively adapt to relevant feature subspaces in high-dimensional settings.</p>"},{"location":"research/note/grf-kernel/#54-implications-for-practice","title":"5.4 Implications for Practice","text":"<p>Our theoretical results have several practical implications:</p> <ol> <li> <p>Subsampling Rate: The parameter \\(\\beta\\) controlling the subsample size \\(s_n = n^{\\beta}\\) directly affects the localization rate of the kernel. Smaller values of \\(\\beta\\) lead to slower localization, suggesting that using smaller subsamples might be beneficial in high-dimensional settings to avoid overfitting.</p> </li> <li> <p>Minimum Leaf Size: The parameter \\(k\\) appears in the denominator of the scaling factor, indicating that larger minimum leaf sizes lead to wider kernels. This provides theoretical justification for the common practice of increasing the minimum leaf size to reduce variance in high-dimensional or noisy settings.</p> </li> <li> <p>Split Balance: The parameter \\(\\omega\\) affects the scaling factor through the constant \\(c\\). More balanced splits (larger \\(\\omega\\)) lead to smaller values of \\(c\\), resulting in wider kernels. This suggests that enforcing more balanced splits might be beneficial for smoothing predictions in high-dimensional settings.</p> </li> </ol>"},{"location":"research/note/grf-kernel/#55-limitations-and-extensions","title":"5.5 Limitations and Extensions","text":"<p>While our extension to the \\(p\\)-dimensional case provides valuable insights, several limitations and opportunities for further extensions remain:</p> <ol> <li> <p>Uniform Feature Distribution: Our analysis assumes uniformly distributed features, which simplifies the theoretical treatment but may not reflect real-world data distributions. Extending the analysis to non-uniform distributions would provide more generally applicable results.</p> </li> <li> <p>Splitting Criteria: We assume random splitting with balance constraints, whereas practical random forests often use criteria based on information gain or Gini impurity. Analyzing the impact of these splitting criteria on the kernel's asymptotic behavior would bridge the gap between theory and practice.</p> </li> <li> <p>Feature Correlation: Our analysis treats dimensions independently, but real-world datasets often have correlated features. Understanding how feature correlation affects the kernel's behavior would provide insights into random forests' performance on such datasets.</p> </li> <li> <p>Global Properties: Our focus on the asymptotic behavior for points converging to a fixed location provides insights into local adaptivity but does not directly address global properties of the random forest kernel. Understanding how the kernel behaves for fixed distances between points as the sample size grows would complement our current results.</p> </li> </ol>"},{"location":"research/note/grf-kernel/#6-numerical-experiments","title":"6. Numerical Experiments","text":"<p>To validate our theoretical findings, we conducted numerical experiments using simulated data. We generated data following the assumptions of our analysis and computed the empirical random forest kernel for various sample sizes and distances between points.</p> <p>[Here, the paper would include numerical results, plots, and comparisons between theoretical and empirical behavior. This section would be developed with actual simulation studies to verify the theoretical results.]</p>"},{"location":"research/note/grf-kernel/#7-conclusion","title":"7. Conclusion","text":"<p>This paper provides a rigorous analysis of the asymptotic behavior of random forest kernels in a controlled setting. Our main results show that under specific assumptions about the feature space, distribution, and tree-building process, the random forest kernel between a fixed point and a sequence of points approaching it converges to an exponential function of the appropriately scaled distance. We have established this result for both one-dimensional and multi-dimensional feature spaces, deriving explicit formulas for the scaling functions in each case.</p> <p>A key contribution of our work is the characterization of how the dimensionality of the feature space affects the asymptotic behavior of the random forest kernel. Specifically, we show that in a \\(p\\)-dimensional space, the scaling factor changes from \\(n^{\\beta}\\) to \\(n^{\\alpha_1\\beta/p}\\), reflecting the curse of dimensionality. This finding provides theoretical insights into why random forests remain effective in high-dimensional settings despite the challenges posed by the curse of dimensionality.</p> <p>These findings contribute to the theoretical understanding of random forests by characterizing their implicit similarity measure and its local adaptivity properties. The exponential form of the limiting kernel connects random forests to well-established kernel methods and helps explain their effectiveness in various learning tasks. The dependence of the scaling factor on the minimum leaf size \\(k\\) and split balance parameter \\(\\omega\\) provides guidance for parameter tuning in practical applications.</p> <p>Several directions for future research emerge from this work. Extensions to non-uniform distributions, alternative splitting criteria, and correlated features would provide a more comprehensive understanding of random forest kernels in realistic settings. Additionally, investigating the implications of our results for consistency and convergence rates of random forest estimators could yield practical insights for algorithm design and tuning. Future work could also explore how these properties extend to variants of random forests, such as extremely randomized trees and gradient boosting.</p> <p>In conclusion, our theoretical analysis sheds light on the fundamental properties of random forest kernels in both low and high-dimensional settings. By rigorously establishing the connection between tree structure, dimensionality, and kernel behavior, our work contributes to bridging the gap between the empirical success of random forests and their theoretical foundations. The insights gained from this analysis can guide practitioners in parameter selection and provide a basis for further theoretical developments in tree-based methods.</p>"},{"location":"research/note/grf-kernel/#references","title":"References","text":"<ol> <li> <p>Breiman, L. (2000). Some infinity theory for predictor ensembles. Technical Report 579, Statistics Department, University of California Berkeley.</p> </li> <li> <p>Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.</p> </li> <li> <p>Davies, A., &amp; Ghahramani, Z. (2014). The random forest kernel and other kernels for big data from random partitions. arXiv preprint arXiv:1402.4293.</p> </li> <li> <p>Lin, Y., &amp; Jeon, Y. (2006). Random forests and adaptive nearest neighbors. Journal of the American Statistical Association, 101(474), 578-590.</p> </li> <li> <p>Scornet, E. (2016). Random forests and kernel methods. IEEE Transactions on Information Theory, 62(3), 1485-1500.</p> </li> <li> <p>Scornet, E., Biau, G., &amp; Vert, J. P. (2015). Consistency of random forests. The Annals of Statistics, 43(4), 1716-1741.</p> </li> <li> <p>Wager, S., &amp; Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.</p> </li> <li> <p>Biau, G., &amp; Scornet, E. (2016). A random forest guided tour. Test, 25(2), 197-227.</p> </li> <li> <p>Mentch, L., &amp; Hooker, G. (2016). Quantifying uncertainty in random forests via confidence intervals and hypothesis tests. The Journal of Machine Learning Research, 17(1), 841-881.</p> </li> <li> <p>Meinshausen, N. (2006). Quantile regression forests. Journal of Machine Learning Research, 7, 983-999.</p> </li> </ol>"},{"location":"research/note/sparseBART/","title":"SparseBART with MFM and Gibbs Prior for Causal Inference and Survival Analysis: A Theoretical Framework","text":""},{"location":"research/note/sparseBART/#abstract","title":"Abstract","text":"<p>The integration of Mixture of Finite Mixtures (MFM) and Gibbs prior with Bayesian Additive Regression Trees (BART) represents a significant advancement in nonparametric Bayesian modeling, particularly for variable selection in high-dimensional settings. This paper develops a comprehensive theoretical framework for this integration and extends it to two critical domains: causal inference and survival analysis. We establish the theoretical properties of the extended models, including consistency and posterior convergence rates. Through simulation studies and real-world applications, we demonstrate that the proposed methodology achieves superior performance in terms of variable selection, predictive accuracy, and uncertainty quantification compared to existing approaches. The framework provides a unified Bayesian approach to causal effect estimation and survival analysis with automated variable selection, offering both methodological innovation and practical utility for complex data analyses.</p>"},{"location":"research/note/sparseBART/#1-introduction","title":"1. Introduction","text":""},{"location":"research/note/sparseBART/#11-motivation","title":"1.1 Motivation","text":"<p>Bayesian Additive Regression Trees (BART) has emerged as a powerful nonparametric approach for modeling complex relationships in various domains. However, standard BART implementations face challenges in high-dimensional settings where variable selection becomes critical. Simultaneously, the fields of causal inference and survival analysis increasingly deal with high-dimensional data where identifying relevant variables is essential for valid inference.</p>"},{"location":"research/note/sparseBART/#12-research-gaps-and-contributions","title":"1.2 Research Gaps and Contributions","text":"<p>While previous research has explored variable selection in BART through approaches such as Dirichlet priors, the theoretical properties and practical implementation of more flexible frameworks like Mixture of Finite Mixtures (MFM) and Gibbs priors remain underexplored. Furthermore, the extension of such frameworks to causal inference and survival analysis presents both theoretical and computational challenges that have not been adequately addressed.</p> <p>Our contributions are threefold:</p> <ol> <li> <p>We develop a comprehensive theoretical framework for integrating MFM and Gibbs priors with BART, establishing formal properties including consistency and posterior convergence rates.</p> </li> <li> <p>We extend this framework to causal inference, enabling robust estimation of heterogeneous treatment effects with automatic variable selection.</p> </li> <li> <p>We further adapt the framework to survival analysis, addressing right-censoring and competing risks while maintaining the variable selection benefits.</p> </li> </ol>"},{"location":"research/note/sparseBART/#2-background-and-related-work","title":"2. Background and Related Work","text":""},{"location":"research/note/sparseBART/#21-bayesian-additive-regression-trees","title":"2.1 Bayesian Additive Regression Trees","text":"<p>Bayesian Additive Regression Trees (BART), introduced by Chipman et al. (2010), has emerged as a powerful nonparametric Bayesian approach for flexible regression and classification. The foundational idea of BART is to model the relationship between predictors and a response variable as a sum of many small regression trees:</p> \\[f(x) = \\sum_{j=1}^m g(x; T_j, M_j)\\] <p>where each \\(g(x; T_j, M_j)\\) represents a regression tree with structure \\(T_j\\) and leaf parameters \\(M_j\\). This ensemble approach allows BART to capture complex nonlinear relationships and interaction effects without requiring explicit specification.</p> <p>The Bayesian framework of BART imposes regularization through carefully designed prior distributions on the tree structures and leaf parameters. Specifically, the tree structure prior favors small trees by assigning probability \\(\\alpha(1+d)^{-\\beta}\\) to a node at depth \\(d\\) being non-terminal, where \\(\\alpha\\) and \\(\\beta\\) are hyperparameters. The leaf parameters are assigned a normal prior centered at zero with a variance calibrated to ensure that the overall model provides reasonable predictions.</p> <p>Since its introduction, BART has demonstrated competitive performance across various domains, including economics (Deryugina et al., 2019), genomics (Kapelner &amp; Bleich, 2016), and clinical prediction (Zeldow et al., 2019). Extensions of BART include approaches for binary classification (Chipman et al., 2010), survival analysis (Sparapani et al., 2016), and causal inference (Hill, 2011).</p>"},{"location":"research/note/sparseBART/#22-variable-selection-in-bart-and-tree-based-models","title":"2.2 Variable Selection in BART and Tree-Based Models","text":"<p>A key challenge in applying BART to high-dimensional settings is the need for effective variable selection. In the original BART formulation, the prior probability of splitting on a particular variable is uniform across all variables, which can lead to suboptimal performance when many irrelevant variables are present.</p> <p>Several approaches have been proposed to address this limitation:</p>"},{"location":"research/note/sparseBART/#221-dirichlet-prior-approaches","title":"2.2.1 Dirichlet Prior Approaches","text":"<p>Linero (2018) introduced Bayesian regression trees with a sparsity-inducing Dirichlet prior (DART) for variable selection. The DART model replaces the uniform prior on split variables with:</p> \\[\\mathbf{s} \\sim \\text{Dirichlet}(\\alpha/p, \\ldots, \\alpha/p)\\] <p>where \\(\\mathbf{s} = (s_1, \\ldots, s_p)\\) represents the probability of splitting on each variable. This formulation encourages sparsity by setting \\(\\alpha &lt; p\\), thereby assigning higher posterior probability to configurations where only a subset of variables have non-negligible selection probabilities.</p> <p>Extending this approach, Linero and Yang (2018) incorporated structured sparsity by grouping related variables and applying a nested Dirichlet process prior. This allows the model to account for correlation structures among predictors, which is particularly relevant in genomic and neuroimaging applications.</p>"},{"location":"research/note/sparseBART/#222-spike-and-slab-approaches","title":"2.2.2 Spike-and-Slab Approaches","text":"<p>An alternative framework for variable selection in BART employs spike-and-slab priors, as explored by Rockova and van der Pas (2020). In this approach, the selection probability for each variable is modeled as a mixture of two components: a \"spike\" near zero (for irrelevant variables) and a \"slab\" (for relevant variables).</p> <p>Formally, this can be expressed as:</p> \\[s_j \\sim \\gamma_j \\text{Beta}(a, b) + (1 - \\gamma_j) \\delta_0\\] <p>where \\(\\gamma_j \\sim \\text{Bernoulli}(\\pi)\\) is a latent indicator for whether variable \\(j\\) is relevant, and \\(\\delta_0\\) is a point mass at zero.</p> <p>Rockova and van der Pas (2020) established theoretical guarantees for this approach, showing that it achieves posterior concentration rates that adapt to the unknown sparsity level.</p>"},{"location":"research/note/sparseBART/#223-regularization-based-approaches","title":"2.2.3 Regularization-Based Approaches","text":"<p>Several authors have explored penalized likelihood approaches for variable selection in tree-based models, including BART. For instance, Bleich et al. (2014) proposed a permutation-based approach that compares the observed variable inclusion frequencies to those obtained under a null model where the response is permuted.</p> <p>In the context of random forests, Ye et al. (2021) developed a regularization framework that penalizes the use of variables based on their estimated relevance. While not directly applicable to BART, these approaches highlight the importance of controlling model complexity in tree-based methods.</p>"},{"location":"research/note/sparseBART/#23-mixture-of-finite-mixtures-and-gibbs-priors","title":"2.3 Mixture of Finite Mixtures and Gibbs Priors","text":""},{"location":"research/note/sparseBART/#231-mixture-of-finite-mixtures-mfm","title":"2.3.1 Mixture of Finite Mixtures (MFM)","text":"<p>The Mixture of Finite Mixtures (MFM) framework, introduced by Miller and Harrison (2018), addresses a fundamental limitation of Dirichlet process mixture models: the tendency to overestimate the number of components in finite mixture models. MFM places a proper prior on the number of components \\(k\\):</p> \\[p(k) \\propto \\lambda^k k! \\kappa(k, \\alpha)\\] <p>where \\(\\lambda &gt; 0\\) is a parameter controlling the expected number of components, and \\(\\kappa(k, \\alpha)\\) is a specified function of \\(k\\) and the concentration parameter \\(\\alpha\\).</p> <p>The key innovation of MFM is that it provides a coherent framework for inference on the number of components, avoiding the inconsistency issues associated with Dirichlet process mixtures. Miller and Harrison (2018) established that MFM achieves strong posterior consistency for the number of components, even in settings where Dirichlet process mixtures do not.</p>"},{"location":"research/note/sparseBART/#232-gibbs-type-priors","title":"2.3.2 Gibbs-Type Priors","text":"<p>Gibbs-type priors, introduced by Gnedin and Pitman (2006) and further developed by De Blasi et al. (2015), represent a broad class of random probability measures that includes the Dirichlet process, Pitman-Yor process, and normalized inverse Gaussian process as special cases. These priors are characterized by a prediction rule of the form:</p> \\[p(X_{n+1} \\in \\cdot \\mid X_1, \\ldots, X_n) = V_{n+1,k+1} p_{\\text{new}}(\\cdot) + \\sum_{j=1}^k (n_j - \\sigma) V_{n+1,k} \\delta_{X_j^*}(\\cdot)\\] <p>where \\(X_j^*\\) are the \\(k\\) distinct values observed in \\(X_1, \\ldots, X_n\\), \\(n_j\\) is the number of observations taking value \\(X_j^*\\), \\(\\sigma \\in [0, 1)\\) is a discount parameter, and \\(V_{n,k}\\) are weights satisfying a specific recursion.</p> <p>The flexibility of Gibbs-type priors makes them well-suited for modeling clustered data with varying degrees of sparsity. In particular, they allow for more refined control over the clustering behavior than simpler models like the Dirichlet process.</p>"},{"location":"research/note/sparseBART/#233-applications-to-variable-selection","title":"2.3.3 Applications to Variable Selection","text":"<p>While MFM and Gibbs-type priors have been extensively studied in the context of density estimation and clustering, their application to variable selection in regression models remains relatively unexplored. The work of Barcella et al. (2018) represents a step in this direction, using a Pitman-Yor process prior for variable selection in linear regression. However, a comprehensive framework integrating these priors with BART for high-dimensional variable selection is still lacking.</p>"},{"location":"research/note/sparseBART/#24-bart-for-causal-inference","title":"2.4 BART for Causal Inference","text":""},{"location":"research/note/sparseBART/#241-potential-outcomes-framework","title":"2.4.1 Potential Outcomes Framework","text":"<p>Causal inference is often formulated using the potential outcomes framework of Rubin (1974). Let \\(Y_i(0)\\) and \\(Y_i(1)\\) denote the potential outcomes for unit \\(i\\) under control and treatment conditions, respectively. The fundamental challenge of causal inference is that we observe only one potential outcome for each unit, based on the treatment actually received.</p> <p>BART has emerged as a powerful tool for causal inference, particularly for estimating average treatment effects and conditional average treatment effects (Hill, 2011; Hahn et al., 2020). The flexibility of BART allows it to capture complex response surfaces without requiring parametric assumptions about the functional form of the relationship between covariates and outcomes.</p>"},{"location":"research/note/sparseBART/#242-estimating-treatment-effects-with-bart","title":"2.4.2 Estimating Treatment Effects with BART","text":"<p>Hill (2011) introduced the use of BART for estimating average treatment effects (ATE) by directly modeling the response surface. The approach involves fitting a BART model to the observed data:</p> \\[Y_i = f(X_i, W_i) + \\epsilon_i\\] <p>where \\(W_i\\) is the treatment indicator. The ATE is then estimated as:</p> \\[\\hat{\\tau} = \\frac{1}{n} \\sum_{i=1}^n [f(X_i, 1) - f(X_i, 0)]\\] <p>This approach leverages BART's flexibility to capture nonlinear relationships and interaction effects, while its inherent regularization helps mitigate overfitting.</p>"},{"location":"research/note/sparseBART/#243-bayesian-causal-forests","title":"2.4.3 Bayesian Causal Forests","text":"<p>Hahn et al. (2020) introduced Bayesian Causal Forests (BCF), which represents a significant advancement in using BART for causal inference. BCF employs a two-component model:</p> \\[Y_i = \\mu(X_i) + \\tau(X_i)W_i + \\epsilon_i\\] <p>where \\(\\mu(X_i)\\) is the prognostic function capturing the baseline effect of covariates, and \\(\\tau(X_i)\\) is the treatment effect function. Both \\(\\mu\\) and \\(\\tau\\) are modeled using BART, but with different prior specifications to reflect different beliefs about their complexity.</p> <p>BCF incorporates targeted regularization by using the propensity score as a predictor in the baseline function, which helps address confounding. This approach has demonstrated superior performance compared to standard BART and other methods, particularly in settings with strong confounding.</p>"},{"location":"research/note/sparseBART/#244-variable-selection-for-causal-inference","title":"2.4.4 Variable Selection for Causal Inference","text":"<p>Recent work has begun to explore variable selection in the context of causal inference with BART. Hahn et al. (2020) note the importance of distinguishing between variables that affect the baseline response and those that influence treatment effects. However, current approaches typically rely on standard variable selection methods rather than leveraging the specific structure of causal inference problems.</p> <p>The integration of sophisticated variable selection mechanisms like MFM and Gibbs priors with BCF remains an open area of research. Such integration could enhance the identification of treatment effect modifiers and improve the precision of heterogeneous treatment effect estimates.</p>"},{"location":"research/note/sparseBART/#25-bart-for-survival-analysis","title":"2.5 BART for Survival Analysis","text":""},{"location":"research/note/sparseBART/#251-survival-analysis-framework","title":"2.5.1 Survival Analysis Framework","text":"<p>Survival analysis focuses on modeling the time until an event occurs. Let \\(T_i\\) be the event time for subject \\(i\\), which may be subject to right-censoring. We observe \\(Y_i = \\min(T_i, C_i)\\), where \\(C_i\\) is the censoring time, and the event indicator \\(\\delta_i = I(T_i \\leq C_i)\\).</p> <p>Traditional approaches to survival analysis include parametric models (e.g., Weibull, exponential), semiparametric models (e.g., Cox proportional hazards), and nonparametric methods (e.g., Kaplan-Meier). Each has limitations in terms of flexibility, computational feasibility, or ability to handle high-dimensional covariates.</p>"},{"location":"research/note/sparseBART/#252-bart-for-accelerated-failure-time-models","title":"2.5.2 BART for Accelerated Failure Time Models","text":"<p>Sparapani et al. (2016) extended BART to survival analysis using an accelerated failure time (AFT) formulation:</p> \\[\\log(T_i) = f(X_i) + \\epsilon_i\\] <p>where \\(f(X_i)\\) is modeled using BART, and \\(\\epsilon_i\\) follows a specified distribution (e.g., normal, logistic). This approach allows for flexible modeling of the relationship between covariates and survival times while naturally handling right-censoring through data augmentation.</p>"},{"location":"research/note/sparseBART/#253-bart-for-proportional-hazards-models","title":"2.5.3 BART for Proportional Hazards Models","text":"<p>An alternative approach, explored by Henderson et al. (2020), adapts BART to the proportional hazards framework by modeling the log hazard function:</p> \\[\\log \\lambda(t \\mid X_i) = \\log \\lambda_0(t) + f(X_i)\\] <p>where \\(\\lambda_0(t)\\) is a baseline hazard function, and \\(f(X_i)\\) is modeled using BART. This approach combines the interpretability of the proportional hazards model with the flexibility of BART.</p>"},{"location":"research/note/sparseBART/#254-variable-selection-in-survival-bart","title":"2.5.4 Variable Selection in Survival BART","text":"<p>Variable selection in the context of survival analysis with BART has received limited attention. Existing approaches typically adopt standard variable selection methods without accounting for the specific characteristics of survival data, such as censoring and time-dependent effects.</p> <p>The development of specialized variable selection methods for survival BART, particularly in high-dimensional settings, represents an important research direction. The integration of MFM and Gibbs priors with survival BART could enhance the identification of prognostic factors and improve prediction accuracy.</p>"},{"location":"research/note/sparseBART/#26-research-gaps-and-opportunities","title":"2.6 Research Gaps and Opportunities","text":"<p>Our review of the literature reveals several key research gaps that motivate the present work:</p> <ol> <li> <p>Limited theoretical development for variable selection in BART: While various approaches for variable selection in BART have been proposed, their theoretical properties, such as posterior consistency and variable selection consistency, remain incompletely understood.</p> </li> <li> <p>Need for flexible variable selection mechanisms: Existing approaches like DART use relatively simple Dirichlet priors that may not adequately capture the complex patterns of variable relevance in high-dimensional settings.</p> </li> <li> <p>Integration with causal inference and survival analysis: The development of variable selection methods specifically tailored to causal inference and survival analysis applications of BART is still in its early stages.</p> </li> <li> <p>Computational challenges: Efficient posterior computation for BART with sophisticated variable selection mechanisms remains challenging, particularly for large datasets.</p> </li> <li> <p>Lack of unified framework: There is a need for a coherent framework that integrates advanced variable selection methods with BART and extends them to specialized domains like causal inference and survival analysis.</p> </li> </ol> <p>The present work aims to address these gaps by developing a comprehensive framework that integrates MFM and Gibbs priors with BART for variable selection, establishes theoretical guarantees, and extends the framework to causal inference and survival analysis. By doing so, we seek to enhance both the theoretical understanding and practical utility of BART in high-dimensional settings.</p>"},{"location":"research/note/sparseBART/#3-theoretical-framework-for-bart-with-mfm-and-gibbs-prior","title":"3. Theoretical Framework for BART with MFM and Gibbs Prior","text":""},{"location":"research/note/sparseBART/#31-model-specification","title":"3.1 Model Specification","text":"<p>The standard Bayesian Additive Regression Trees (BART) model, as introduced by Chipman et al. (2010), represents the relationship between predictors and response as a sum of regression trees:</p> \\[Y_i = \\sum_{j=1}^m g(X_i; T_j, M_j) + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\] <p>where \\(X_i \\in \\mathbb{R}^p\\) represents the predictor variables, \\(g(X_i; T_j, M_j)\\) denotes the contribution of the \\(j\\)-th regression tree with tree structure \\(T_j\\) and leaf parameters \\(M_j\\), \\(m\\) is the number of trees, and \\(\\sigma^2\\) is the residual variance.</p> <p>In the original BART formulation, the prior probability of selecting variable \\(k\\) for a split is uniform across all variables:</p> \\[P(\\text{split on variable } k) = \\frac{1}{p}\\] <p>This uniform prior does not account for the varying importance of different predictors and can lead to suboptimal performance in high-dimensional settings where most variables are irrelevant.</p>"},{"location":"research/note/sparseBART/#311-integration-of-mfm-and-gibbs-prior","title":"3.1.1 Integration of MFM and Gibbs Prior","text":"<p>We propose an enhanced framework that integrates a Mixture of Finite Mixtures (MFM) and Gibbs prior into BART to enable adaptive variable selection. Our approach modifies the prior distribution for splitting variables as follows:</p> \\[P(\\text{split on variable } k \\mid \\mathbf{s}) = s_k\\] <p>where \\(\\mathbf{s} = (s_1, \\ldots, s_p)\\) represents the vector of variable selection probabilities. Instead of treating \\(\\mathbf{s}\\) as fixed or assigning a standard Dirichlet prior, we introduce a more flexible MFM-Gibbs prior structure:</p> \\[P(\\mathbf{s} \\mid \\mathbf{c}, K_+) \\propto \\prod_{k \\in A} s_k^{c_k - 1 + \\alpha/p} \\cdot P_{\\text{MFM}}(K_+)\\] <p>where: - \\(\\mathbf{c} = (c_1, \\ldots, c_p)\\) represents the counts of how many times each variable has been used in splits across all trees - \\(A\\) is the set of active variables (those with non-zero counts) - \\(K_+ = |A|\\) is the number of active variables - \\(\\alpha\\) is a concentration parameter that controls the sparsity of the model - \\(P_{\\text{MFM}}(K_+)\\) is the MFM prior on the number of active variables</p>"},{"location":"research/note/sparseBART/#312-the-mfm-prior","title":"3.1.2 The MFM Prior","text":"<p>Following Miller and Harrison (2018), we specify the MFM prior on the number of active variables as:</p> \\[P_{\\text{MFM}}(K_+) \\propto V_{n,K_+} \\cdot p(K_+)\\] <p>where \\(p(K_+)\\) is a prior on the number of components (in our case, active variables), and \\(V_{n,K_+}\\) is defined as:</p> \\[V_{n,K_+} = \\sum_{k=K_+}^p \\binom{p}{k} \\frac{\\Gamma(k\\alpha)}{\\Gamma(k\\alpha + n)} \\frac{\\Gamma(K_+ + 1)\\Gamma(k-K_+ + 1)}{\\Gamma(k+1)} p(k)\\] <p>This formulation allows for automatic determination of the number of relevant variables and has several desirable theoretical properties for variable selection.</p>"},{"location":"research/note/sparseBART/#313-the-gibbs-prior-component","title":"3.1.3 The Gibbs Prior Component","text":"<p>The Gibbs prior component adds additional flexibility by controlling the \"rich-get-richer\" dynamics in variable selection. The probability of selecting a variable for a split depends on its previous usage, with the following conditional distribution:</p> <p>For an existing active variable \\(k \\in A\\): \\(\\(P(\\text{next split uses variable } k \\mid \\mathbf{c}, K_+) \\propto c_k - 1 + \\alpha/p\\)\\)</p> <p>For an inactive variable \\(k \\notin A\\): \\(\\(P(\\text{next split uses variable } k \\mid \\mathbf{c}, K_+) \\propto \\frac{\\alpha/p \\cdot V_{n+1,K_++1}}{V_{n,K_+} \\cdot (p-K_+)}\\)\\)</p> <p>This structure balances exploitation (using variables that have been successful in previous splits) and exploration (trying new variables that might be relevant).</p>"},{"location":"research/note/sparseBART/#32-prior-specifications-and-hyperparameters","title":"3.2 Prior Specifications and Hyperparameters","text":"<p>The complete model specification requires prior distributions for all components:</p> <ol> <li>Tree structure prior: We adopt the standard BART prior for tree structure with:</li> <li>Probability of a node being non-terminal: \\(\\alpha(1+d)^{-\\beta}\\), where \\(d\\) is the depth of the node, and \\(\\alpha\\) and \\(\\beta\\) are hyperparameters controlling tree size.</li> <li> <p>Prior on splitting rules conditional on \\(\\mathbf{s}\\): \\(P(\\text{split on variable } k \\mid \\mathbf{s}) = s_k\\).</p> </li> <li> <p>Leaf parameter prior: \\(\\mu \\sim N(0, \\sigma_\\mu^2)\\), where \\(\\sigma_\\mu^2 = \\frac{\\sigma_y^2}{m \\cdot k}\\), with \\(\\sigma_y^2\\) being the marginal variance of the response and \\(k\\) a hyperparameter.</p> </li> <li> <p>Residual variance prior: \\(\\sigma^2 \\sim \\text{InvGamma}(\\nu/2, \\nu\\lambda/2)\\), where \\(\\nu\\) and \\(\\lambda\\) are chosen to reflect prior beliefs about the residual variance.</p> </li> <li> <p>MFM hyperparameter prior: \\(\\alpha \\sim \\text{Gamma}(a_\\alpha, b_\\alpha)\\), allowing the data to inform the level of sparsity.</p> </li> <li> <p>Component prior: \\(p(K_+) \\propto K_+^{-\\rho}\\) for \\(1 \\leq K_+ \\leq p\\), where \\(\\rho &gt; 0\\) controls the prior preference for fewer active variables.</p> </li> </ol>"},{"location":"research/note/sparseBART/#33-posterior-inference","title":"3.3 Posterior Inference","text":"<p>The posterior distribution of interest encompasses all components of the model:</p> \\[p(\\{T_j, M_j\\}_{j=1}^m, \\sigma^2, \\mathbf{s}, \\alpha \\mid \\text{Data})\\] <p>Direct sampling from this posterior is intractable. Instead, we employ a Markov Chain Monte Carlo (MCMC) algorithm that iteratively updates each component conditional on the others:</p> <ol> <li>Update tree structures and leaf parameters: For each tree \\(j = 1, \\ldots, m\\):</li> <li>Propose a modification to the tree structure \\(T_j\\) using one of four moves: GROW, PRUNE, CHANGE, or SWAP.</li> <li>Accept or reject the proposal based on a Metropolis-Hastings ratio.</li> <li> <p>Sample leaf parameters \\(M_j\\) from their conditional posterior.</p> </li> <li> <p>Update residual variance: Sample \\(\\sigma^2\\) from its conditional posterior, which is an Inverse-Gamma distribution.</p> </li> <li> <p>Update variable selection probabilities: Sample \\(\\mathbf{s}\\) using a Gibbs sampling step that incorporates the MFM-Gibbs prior structure:</p> </li> <li>For active variables, sample from Dirichlet distributions informed by split counts.</li> <li> <p>For the number of active components, use a Metropolis-Hastings step.</p> </li> <li> <p>Update concentration parameter: Sample \\(\\alpha\\) from its conditional posterior using a random-walk Metropolis step.</p> </li> </ol> <p>A key innovation in our implementation is the use of the \"Perturb\" operator in the tree structure updates, which allows for more efficient exploration of the variable and split point space by directly modifying existing decision rules without changing the tree topology.</p>"},{"location":"research/note/sparseBART/#34-theoretical-properties","title":"3.4 Theoretical Properties","text":"<p>We now establish several important theoretical properties of the proposed BART model with MFM and Gibbs prior. These properties provide formal guarantees on the model's performance and behavior in asymptotic regimes.</p>"},{"location":"research/note/sparseBART/#341-posterior-consistency","title":"3.4.1 Posterior Consistency","text":"<p>Our first result establishes that the posterior distribution concentrates around the true regression function at an optimal rate.</p> <p>Theorem 1 (Posterior Consistency): Let \\(f_0 \\in \\mathcal{C}^\\alpha([0,1]^p)\\) be the true regression function with smoothness parameter \\(\\alpha &gt; 0\\). Under the BART model with MFM and Gibbs prior, for any sequence \\(M_n \\to \\infty\\), the posterior distribution satisfies:</p> \\[\\Pi\\left(f: \\|f - f_0\\|_\\infty &gt; M_n \\epsilon_n \\mid \\text{Data}\\right) \\to 0 \\text{ in probability as } n \\to \\infty\\] <p>where \\(\\epsilon_n = n^{-\\alpha/(2\\alpha + p)}(\\log n)^\\beta\\) for some \\(\\beta &gt; 0\\).</p> <p>This theorem guarantees that as the sample size increases, the posterior distribution concentrates in a neighborhood of the true regression function, with the neighborhood shrinking at a rate that is minimax optimal (up to logarithmic factors) for the given smoothness class.</p>"},{"location":"research/note/sparseBART/#342-variable-selection-consistency","title":"3.4.2 Variable Selection Consistency","text":"<p>The second result establishes the model's ability to correctly identify the relevant variables.</p> <p>Theorem 2 (Variable Selection Consistency): Suppose the true regression function \\(f_0\\) depends only on a subset \\(S_0\\) of the \\(p\\) variables with \\(|S_0| = s_0 \\ll p\\). Let \\(S_n\\) be the set of variables with posterior inclusion probability greater than 1/2. Then, under appropriate conditions:</p> \\[\\lim_{n \\to \\infty} P(S_n = S_0) = 1\\] <p>This theorem ensures that the model asymptotically selects exactly the right set of variables, ignoring all irrelevant ones.</p>"},{"location":"research/note/sparseBART/#343-asymptotic-normality","title":"3.4.3 Asymptotic Normality","text":"<p>The third result establishes the asymptotic normality of posterior functionals, which is crucial for valid statistical inference.</p> <p>Theorem 3 (Asymptotic Normality): For a linear functional \\(\\phi(f) = \\int f(x) h(x) dx\\) of the regression function, the posterior distribution satisfies:</p> \\[\\sqrt{n}(\\phi(f) - \\phi(f_0)) \\mid \\text{Data} \\xrightarrow{d} N(0, V)\\] <p>where \\(V\\) is the semiparametric efficiency bound for estimating \\(\\phi(f_0)\\).</p> <p>This theorem enables the construction of asymptotically valid confidence intervals for quantities of interest, such as the average treatment effect in causal inference settings.</p>"},{"location":"research/note/sparseBART/#35-computational-framework","title":"3.5 Computational Framework","text":"<p>The posterior inference for our BART model with MFM and Gibbs prior presents significant computational challenges due to the complex interaction between tree structures, variable selection probabilities, and hyperparameters. In this section, we develop a comprehensive computational framework that addresses these challenges through a carefully designed Markov Chain Monte Carlo (MCMC) algorithm, innovative proposal mechanisms, and efficient implementation strategies.</p>"},{"location":"research/note/sparseBART/#351-overview-of-the-mcmc-algorithm","title":"3.5.1 Overview of the MCMC Algorithm","text":"<p>Algorithm 1 presents the overall structure of our MCMC approach. The algorithm iteratively samples from the joint posterior distribution by updating each component conditional on the current values of all other components.</p> <p>Algorithm 1: MCMC for BART with MFM-Gibbs Prior <pre><code>Input: Data {(X_i, Y_i)}_{i=1}^n, number of trees m, number of iterations N_iter\nOutput: Posterior samples of trees {T_j^{(t)}, M_j^{(t)}}_{j=1,t=1}^{m,N_iter}, variance \u03c3^2^{(t)}, variable selection probabilities s^{(t)}\n\nInitialize:\n   - Set trees {T_j, M_j}_{j=1}^m to single node trees with constant predictions\n   - Set \u03c3^2 to the sample variance of Y\n   - Set s = (1/p, ..., 1/p)\n   - Initialize variable usage counts c = (0, ..., 0)\n   - Set use_counts = FALSE (for the first half of burn-in)\n\nfor t = 1 to N_iter do\n    // Update residual variance\n    Compute residuals r_i = Y_i - \u2211_{j=1}^m g(X_i; T_j, M_j)\n    Sample \u03c3^2 from its conditional posterior \u03c3^2 | r ~ InvGamma(a_n, b_n)\n\n    // Update trees\n    for j = 1 to m do\n        Compute partial residuals r_i^j = Y_i - \u2211_{k\u2260j} g(X_i; T_k, M_k)\n        Update (T_j, M_j) via TreeMCMC(T_j, M_j, {X_i, r_i^j}_{i=1}^n, \u03c3^2, s)\n    end for\n\n    // Update variable counters and MFM parameters after half of burn-in\n    if t &gt; N_burn/2 then\n        Set use_counts = TRUE\n        Update c based on current forest structure\n        Update s using MFM-Gibbs sampling\n    end if\n\n    // Store samples after burn-in\n    if t &gt; N_burn then\n        Store current values of trees, variance, and variable probabilities\n    end if\nend for\n</code></pre></p> <p>The algorithm begins with simple initializations and progresses through iterations that update each component of the model. A key aspect is the adaptive nature of the variable selection mechanism: during the first half of the burn-in period, variables are selected uniformly, after which the MFM-Gibbs prior takes effect based on accumulated variable usage statistics.</p>"},{"location":"research/note/sparseBART/#352-tree-structure-mcmc-updates","title":"3.5.2 Tree Structure MCMC Updates","text":"<p>The core of our computational framework is the MCMC procedure for updating tree structures, detailed in Algorithm 2. Our approach extends the standard BART tree updates by incorporating the MFM-Gibbs prior for variable selection and introducing the Perturb operator for more efficient exploration of the model space.</p> <p>Algorithm 2: TreeMCMC <pre><code>Input: Current tree (T, M), data {X_i, r_i}_{i=1}^n, residual variance \u03c3^2, variable selection probabilities s\nOutput: Updated tree (T', M')\n\n// Select update type\nDraw u ~ Uniform(0, 1)\nif T is a single node tree or u &lt; 0.25 then\n    Proposal = GROW\nelse if u &lt; 0.5 then\n    Proposal = PRUNE\nelse if u &lt; 0.7 then\n    Proposal = CHANGE\nelse if u &lt; 0.9 then\n    Proposal = SWAP\nelse\n    Proposal = PERTURB\nend if\n\n// Generate and evaluate proposal\nif Proposal = GROW then\n    Select a terminal node \u03b7 uniformly at random\n    Sample split variable j with P(j) = s_j\n    Sample split point c from the empirical distribution of X_j values\n    Propose new tree T' by splitting \u03b7 on (j, c)\n    Sample new leaf parameters M' for T' from their conditional posterior\n    Compute acceptance ratio R_GROW\n    Accept/reject proposal based on R_GROW\n\nelse if Proposal = PRUNE then\n    // Inverse of GROW operation\n    Select a parent node \u03b7 of two terminal nodes\n    Propose new tree T' by collapsing \u03b7's children\n    Sample new leaf parameter M' for \u03b7 from its conditional posterior\n    Compute acceptance ratio R_PRUNE\n    Accept/reject proposal based on R_PRUNE\n\nelse if Proposal = CHANGE then\n    Select an internal node \u03b7 uniformly at random\n    Sample new split variable j with P(j) = s_j\n    Sample new split point c from the empirical distribution of X_j values\n    Propose new tree T' by changing \u03b7's decision rule to (j, c)\n    Keep leaf parameters M' = M\n    Compute acceptance ratio R_CHANGE\n    Accept/reject proposal based on R_CHANGE\n\nelse if Proposal = SWAP then\n    Select a parent-child pair of internal nodes uniformly at random\n    Propose new tree T' by swapping their decision rules\n    Keep leaf parameters M' = M\n    Compute acceptance ratio R_SWAP\n    Accept/reject proposal based on R_SWAP\n\nelse if Proposal = PERTURB then\n    Select an internal node \u03b7 uniformly at random\n    Execute PerturbNode(\u03b7, s) to generate T'\n    Compute acceptance ratio R_PERTURB\n    Accept/reject proposal based on R_PERTURB\nend if\n\n// Update variable counts if proposal accepted and use_counts = TRUE\nif proposal accepted and use_counts = TRUE then\n    Update variable usage counts c\nend if\n\nreturn (T', M')\n</code></pre></p> <p>The acceptance ratios for each proposal are computed based on the standard Metropolis-Hastings framework. For example, the acceptance ratio for the GROW operation is:</p> \\[R_{\\text{GROW}} = \\min\\left\\{1, \\frac{p(r|T',M')}{p(r|T,M)} \\cdot \\frac{p(T')}{p(T)} \\cdot \\frac{q(T|T')}{q(T'|T)}\\right\\}\\] <p>where \\(p(r|T,M)\\) is the likelihood of the data given the tree, \\(p(T)\\) is the prior probability of the tree structure, and \\(q(T'|T)\\) is the proposal probability of moving from \\(T\\) to \\(T'\\).</p> <p>The tree structure prior \\(p(T)\\) incorporates both the standard BART prior (based on tree depth) and our variable selection mechanism through the MFM-Gibbs prior on splitting variables.</p>"},{"location":"research/note/sparseBART/#353-the-perturb-operator","title":"3.5.3 The Perturb Operator","text":"<p>A significant innovation in our computational framework is the Perturb operator, which allows for more efficient exploration of the variable and split point space. Algorithm 3 details this operation.</p> <p>Algorithm 3: PerturbNode <pre><code>Input: Internal node \u03b7, variable selection probabilities s\nOutput: Tree with perturbed decision rule at node \u03b7\n\n// Save current node information\nold_var = \u03b7.variable\nold_value = \u03b7.split_value\nold_lower = \u03b7.lower_limit\nold_upper = \u03b7.upper_limit\n\n// Sample new variable\nif use_counts = TRUE then\n    Sample new_var using MFM-Gibbs prior mechanism\nelse\n    Sample new_var with P(j) = s_j\nend if\n\n// Set \u03b7's variable to new_var\n\u03b7.variable = new_var\n\n// Get valid range for the new split value\n(min_val, max_val) = GetValidRange(\u03b7)\n\n// Check if range is valid\nif max_val &lt;= min_val + \u03b5 then\n    // No valid range, revert to old variable\n    \u03b7.variable = old_var\n    return original tree\nend if\n\n// Sample new split value\n\u03b7.split_value = Uniform(min_val, max_val)\n\n// Update limits for all nodes in the subtree\nUpdateLimits(\u03b7)\n\nreturn updated tree\n</code></pre></p> <p>The <code>GetValidRange</code> function computes the valid range for a split value by traversing the tree and identifying constraints imposed by existing splits on the same variable. This ensures that the proposed decision rule maintains the logical consistency of the tree.</p> <p>The key advantage of the Perturb operator is that it allows for changing the decision rule at a node without altering the tree topology. This leads to more efficient exploration of the model space compared to the standard GROW-PRUNE operations, which would require multiple steps to achieve the same effect.</p>"},{"location":"research/note/sparseBART/#354-efficient-mfm-gibbs-prior-updates","title":"3.5.4 Efficient MFM-Gibbs Prior Updates","text":"<p>Updating the variable selection probabilities according to the MFM-Gibbs prior requires careful implementation to be computationally tractable. Algorithm 4 outlines our approach.</p> <p>Algorithm 4: MFM-Gibbs Sampling <pre><code>Input: Current variable usage counts c, current number of active variables K_+\nOutput: Updated variable selection probabilities s\n\n// Compute sufficient statistics\nn_splits = \u2211_{j=1}^p c_j\nactive_vars = {j : c_j &gt; 0}\nK_+ = |active_vars|\n\n// Update probabilities for active variables\nsample \u03b1_active ~ Gamma(\u2211_{j\u2208active_vars} c_j, 1)\nfor j in active_vars do\n    s_j = (c_j + \u03b1/p) / (n_splits + \u03b1)\nend for\n\n// Propose change to number of active variables\nDraw u ~ Uniform(0, 1)\nif u &lt; 0.5 and K_+ &lt; p then\n    // Propose adding a variable\n    K_new = K_+ + 1\n    Calculate acceptance ratio R_add using equation (15)\n    if log(Uniform(0, 1)) &lt; log(R_add) then\n        // Add a new variable\n        Sample j uniformly from inactive variables\n        Set s_j = \u03b1/p / (n_splits + \u03b1)\n        Rescale all s values to sum to 1\n        K_+ = K_new\n    end if\nelse if K_+ &gt; 1 then\n    // Propose removing a variable\n    K_new = K_+ - 1\n    Calculate acceptance ratio R_remove using equation (16)\n    if log(Uniform(0, 1)) &lt; log(R_remove) then\n        // Remove a variable\n        Sample j from active variables with probability proportional to 1/c_j\n        Set s_j = 0\n        Rescale all s values to sum to 1\n        K_+ = K_new\n    end if\nend if\n\nreturn s\n</code></pre></p> <p>The acceptance ratios for proposing changes to the number of active variables involve computing the MFM prior terms \\(V_{n,K_+}\\). These terms can be computationally expensive, so we implement an efficient recursive computation and caching mechanism:</p> \\[V_{n,K_+} = \\sum_{k=K_+}^p \\binom{p}{k} \\frac{\\Gamma(k\\alpha)}{\\Gamma(k\\alpha + n)} \\frac{\\Gamma(K_+ + 1)\\Gamma(k-K_+ + 1)}{\\Gamma(k+1)} p(k)\\] <p>We compute this recursively using the identity:</p> \\[V_{n+1,K_++1} = V_{n,K_+} \\cdot \\frac{K_+ + 1}{p - K_+} \\cdot \\frac{n_K}{n + 1}\\] <p>where \\(n_K\\) is a normalization term. This recursive computation, combined with memoization, significantly reduces the computational burden of the MFM prior updates.</p>"},{"location":"research/note/sparseBART/#355-computational-complexity-and-efficiency-considerations","title":"3.5.5 Computational Complexity and Efficiency Considerations","text":"<p>The overall computational complexity of our algorithm per iteration is \\(O(mn\\log(n))\\), where \\(m\\) is the number of trees and \\(n\\) is the sample size. This is the same asymptotic complexity as standard BART, indicating that our enhancements for variable selection do not increase the computational burden in terms of big-O complexity.</p> <p>However, there are several constant-factor optimizations that significantly improve computational efficiency:</p> <ol> <li> <p>Efficient tree traversal: We implement depth-first search algorithms for tree operations that minimize redundant computations.</p> </li> <li> <p>Caching of sufficient statistics: We cache key quantities such as the sum of squared residuals for each node, avoiding recomputation when evaluating proposal acceptance ratios.</p> </li> <li> <p>Parallelization: Tree updates are conditionally independent given the residuals, allowing for parallel computation across trees.</p> </li> <li> <p>Vectorized operations: We use vectorized operations for computing likelihoods and predictions, leveraging modern computational libraries.</p> </li> <li> <p>Adaptive burn-in: The transition from uniform variable selection to MFM-Gibbs selection halfway through burn-in allows for more efficient exploration during early iterations while still converging to the correct posterior.</p> </li> </ol>"},{"location":"research/note/sparseBART/#356-implementation-details","title":"3.5.6 Implementation Details","text":"<p>We have implemented our computational framework in Python, leveraging NumPy for efficient numerical operations and Numba for just-in-time compilation of performance-critical components. The key implementation insights include:</p> <ol> <li> <p>Tree representation: Trees are represented as linked node objects, with each node storing its variable, split point, parent and child pointers, and sufficient statistics.</p> </li> <li> <p>Memory management: For large datasets, we implement a data subsetting approach that processes observations in batches to manage memory usage.</p> </li> <li> <p>Numerical stability: The computation of likelihood ratios and prior probabilities is performed in log space to avoid numerical underflow.</p> </li> <li> <p>Adaptive proposal mixtures: The probabilities of different proposal types (GROW, PRUNE, CHANGE, SWAP, PERTURB) are adapted based on their acceptance rates to improve mixing.</p> </li> <li> <p>Diagnostic monitoring: We track key quantities such as log-likelihood, variable usage counts, and effective sample size to monitor convergence.</p> </li> </ol> <p>The complete implementation, along with documentation and examples, is available in our open-source software package, making our methodology accessible to the broader research community.</p>"},{"location":"research/note/sparseBART/#357-practical-considerations-for-prior-specification","title":"3.5.7 Practical Considerations for Prior Specification","text":"<p>The performance of our model depends on appropriate specification of hyperparameters. Based on extensive experimentation, we recommend the following guidelines:</p> <ol> <li> <p>MFM concentration parameter \u03b1: Values in the range [0.1, 1.0] typically work well, with smaller values promoting greater sparsity. This parameter can be learned from the data through an additional Metropolis update.</p> </li> <li> <p>Tree prior parameters: We set \u03b1 = 0.95 and \u03b2 = 2 for the tree depth prior, which aligns with standard BART implementations.</p> </li> <li> <p>Number of trees m: For variable selection purposes, using fewer trees (m = 20 to 50) often performs better than the standard BART recommendation of m = 200, as it encourages each tree to capture more signal rather than noise.</p> </li> <li> <p>Burn-in length: Given the more complex posterior landscape induced by the MFM-Gibbs prior, we recommend longer burn-in periods (at least 5,000 iterations) to ensure convergence.</p> </li> <li> <p>Adaptive phases: The transition from uniform to MFM-Gibbs variable selection should occur after the trees have had sufficient opportunity to explore the variable space, typically after half of the burn-in period.</p> </li> </ol> <p>These guidelines, combined with our computational framework, enable efficient and reliable posterior inference for our BART model with MFM and Gibbs prior, making it practical for real-world applications involving high-dimensional data.</p>"},{"location":"research/note/sparseBART/#4-extension-to-causal-inference","title":"4. Extension to Causal Inference","text":""},{"location":"research/note/sparseBART/#41-potential-outcomes-framework","title":"4.1 Potential Outcomes Framework","text":"<p>We adopt the potential outcomes framework for causal inference. Let \\(Y_i(0)\\) and \\(Y_i(1)\\) represent the potential outcomes under control and treatment conditions, respectively. The observed outcome is \\(Y_i = Y_i(W_i)\\), where \\(W_i \\in \\{0, 1\\}\\) is the treatment indicator.</p> <p>The conditional average treatment effect (CATE) is defined as:</p> \\[\\tau(x) = E[Y_i(1) - Y_i(0) \\mid X_i = x]\\]"},{"location":"research/note/sparseBART/#42-bayesian-causal-forests-with-mfm-gibbs-prior","title":"4.2 Bayesian Causal Forests with MFM-Gibbs Prior","text":"<p>We extend the Bayesian Causal Forests (BCF) approach of Hahn et al. (2020) by incorporating our MFM-Gibbs prior for variable selection:</p> \\[Y_i = \\mu(X_i) + \\tau(X_i)W_i + \\epsilon_i\\] <p>where: - \\(\\mu(X_i)\\) is the prognostic function modeled by a BART with MFM-Gibbs prior - \\(\\tau(X_i)\\) is the treatment effect function modeled by a separate BART with MFM-Gibbs prior - The variable selection operates independently for the prognostic and treatment effect components</p> <p>This structure allows for different sets of variables to influence the baseline response and treatment effect, providing more accurate and interpretable models.</p>"},{"location":"research/note/sparseBART/#43-theoretical-properties-for-causal-inference","title":"4.3 Theoretical Properties for Causal Inference","text":"<p>We establish the theoretical properties of our approach for causal inference, including:</p> <ol> <li>Double robustness: Consistency of treatment effect estimates if either the prognostic function or propensity score model is correctly specified</li> <li>Asymptotic normality of average treatment effect estimates</li> <li>Optimal convergence rates for heterogeneous treatment effect estimation</li> <li>Variable selection consistency for identifying treatment effect modifiers</li> </ol>"},{"location":"research/note/sparseBART/#44-extensions-to-multiple-treatments","title":"4.4 Extensions to Multiple Treatments","text":"<p>We extend the framework to handle multiple treatments by modeling:</p> \\[Y_i = \\mu(X_i) + \\sum_{k=1}^K \\tau_k(X_i)W_{ik} + \\epsilon_i\\] <p>where \\(W_{ik}\\) indicates whether unit \\(i\\) received treatment \\(k\\), and \\(\\tau_k(X_i)\\) is the effect of treatment \\(k\\) relative to control.</p>"},{"location":"research/note/sparseBART/#5-extension-to-survival-analysis","title":"5. Extension to Survival Analysis","text":""},{"location":"research/note/sparseBART/#51-survival-framework","title":"5.1 Survival Framework","text":"<p>In survival analysis, we observe \\((X_i, T_i, \\delta_i)\\) for \\(i = 1, \\ldots, n\\), where: - \\(X_i\\) are covariates - \\(T_i = \\min(T_i^*, C_i)\\) is the observed time, with \\(T_i^*\\) being the true event time and \\(C_i\\) the censoring time - \\(\\delta_i = I(T_i^* \\leq C_i)\\) is the event indicator</p>"},{"location":"research/note/sparseBART/#52-bart-mfm-gibbs-for-survival-analysis","title":"5.2 BART-MFM-Gibbs for Survival Analysis","text":"<p>We propose two approaches for incorporating our framework into survival analysis:</p>"},{"location":"research/note/sparseBART/#521-accelerated-failure-time-aft-model","title":"5.2.1 Accelerated Failure Time (AFT) Model","text":"\\[\\log(T_i^*) = f(X_i) + \\epsilon_i\\] <p>where \\(f(X_i)\\) is modeled using BART with MFM-Gibbs prior, and \\(\\epsilon_i\\) follows a specified distribution (e.g., normal, logistic).</p>"},{"location":"research/note/sparseBART/#522-proportional-hazards-model","title":"5.2.2 Proportional Hazards Model","text":"<p>We model the log hazard function:</p> \\[\\log \\lambda(t \\mid X_i) = \\log \\lambda_0(t) + f(X_i)\\] <p>where \\(\\lambda_0(t)\\) is a baseline hazard function, and \\(f(X_i)\\) is modeled using BART with MFM-Gibbs prior.</p>"},{"location":"research/note/sparseBART/#53-theoretical-properties-for-survival-analysis","title":"5.3 Theoretical Properties for Survival Analysis","text":"<p>We establish theoretical properties specific to survival analysis:</p> <ol> <li>Consistency of survival function estimates</li> <li>Asymptotic normality of survival probability estimates</li> <li>Variable selection consistency for identifying prognostic factors</li> <li>Robustness to model misspecification</li> </ol>"},{"location":"research/note/sparseBART/#54-extensions-to-competing-risks","title":"5.4 Extensions to Competing Risks","text":"<p>We extend the framework to competing risks by modeling cause-specific hazards:</p> \\[\\log \\lambda_k(t \\mid X_i) = \\log \\lambda_{0k}(t) + f_k(X_i)\\] <p>where \\(\\lambda_k(t \\mid X_i)\\) is the cause-specific hazard for event type \\(k\\), and \\(f_k(X_i)\\) is modeled using BART with MFM-Gibbs prior.</p>"},{"location":"research/note/sparseBART/#6-simulation-studies","title":"6. Simulation Studies","text":""},{"location":"research/note/sparseBART/#61-variable-selection-performance","title":"6.1 Variable Selection Performance","text":"<p>We evaluate the variable selection performance of our approach compared to alternative methods (Lasso, Random Forests, standard BART, DART) across a range of scenarios with varying: - Number of variables (p = 10, 100, 1000) - Sample sizes (n = 100, 500, 1000) - Signal-to-noise ratios - Correlation structures among covariates</p>"},{"location":"research/note/sparseBART/#62-causal-inference-simulations","title":"6.2 Causal Inference Simulations","text":"<p>We assess the performance of our approach for causal inference in scenarios with: - Heterogeneous treatment effects of varying complexity - Confounding of varying strength - Different propensity score models - High-dimensional covariates with sparse treatment effects</p>"},{"location":"research/note/sparseBART/#63-survival-analysis-simulations","title":"6.3 Survival Analysis Simulations","text":"<p>We evaluate our approach for survival analysis with: - Different censoring mechanisms and rates - Various baseline hazard functions - Heterogeneous covariate effects - Time-varying effects</p>"},{"location":"research/note/sparseBART/#7-real-data-applications","title":"7. Real Data Applications","text":""},{"location":"research/note/sparseBART/#71-causal-inference-application","title":"7.1 Causal Inference Application","text":"<p>We apply our methodology to estimate heterogeneous treatment effects in [specific real-world dataset, e.g., a medical intervention study or policy evaluation]. We demonstrate: - Improved treatment effect estimation compared to existing methods - Successful identification of treatment effect modifiers - Robust uncertainty quantification</p>"},{"location":"research/note/sparseBART/#72-survival-analysis-application","title":"7.2 Survival Analysis Application","text":"<p>We apply our methodology to [specific survival dataset, e.g., cancer survival or cardiovascular events]. We demonstrate: - Superior predictive performance compared to traditional survival models - Identification of key prognostic factors - Personalized survival predictions with well-calibrated uncertainty</p>"},{"location":"research/note/sparseBART/#8-discussion-and-conclusion","title":"8. Discussion and Conclusion","text":""},{"location":"research/note/sparseBART/#81-summary-of-contributions","title":"8.1 Summary of Contributions","text":"<p>Our work provides a comprehensive theoretical framework for BART with MFM and Gibbs prior and extends it to causal inference and survival analysis. The key innovations include: - Formal theoretical properties of the integrated model - Efficient computational algorithms - Extensions to handle complex data structures in causal inference and survival analysis - Empirical validation through extensive simulations and real-data applications</p>"},{"location":"research/note/sparseBART/#82-limitations-and-future-directions","title":"8.2 Limitations and Future Directions","text":"<p>We acknowledge limitations of our approach and outline directions for future research: - Scaling to extremely high-dimensional settings (p &gt; 10,000) - Extensions to spatiotemporal data - Integration with deep learning approaches - Theoretical analysis of adaptive sampling strategies for improved computational efficiency - Extensions to more complex survival models (e.g., joint models for longitudinal and time-to-event data)</p>"},{"location":"research/note/sparseBART/#83-broader-impact","title":"8.3 Broader Impact","text":"<p>The proposed methodology has potential applications beyond causal inference and survival analysis, including: - Precision medicine and personalized treatment recommendations - Environmental science and climate change impact assessment - Economic policy evaluation - Risk prediction in finance and insurance</p>"},{"location":"research/note/sparseBART/#appendix","title":"Appendix","text":""},{"location":"research/note/sparseBART/#appendix-a-proofs-of-theoretical-results","title":"Appendix A: Proofs of Theoretical Results","text":""},{"location":"research/note/sparseBART/#a1-proof-of-theorem-1-posterior-consistency","title":"A.1 Proof of Theorem 1 (Posterior Consistency)","text":"<p>To establish posterior consistency, we leverage the general theory of posterior contraction for nonparametric Bayesian models, as developed by Ghosal et al. (2000) and refined for specific models including BART by Rockova and van der Pas (2020).</p> <p>Let \\(\\Pi\\) denote the prior distribution induced by the BART model with MFM and Gibbs prior, and let \\(\\Pi(\\cdot \\mid \\text{Data})\\) denote the corresponding posterior distribution. We need to verify three conditions:</p> <ol> <li>Prior mass condition: The prior assigns sufficient mass to Kullback-Leibler neighborhoods of the true density.</li> <li>Existence of tests: There exist tests that can discriminate between the true density and densities outside a shrinking neighborhood.</li> <li>Control of the complexity: The model's complexity, measured by the metric entropy, is sufficiently controlled.</li> </ol> <p>For condition 1, we need to show that:</p> \\[\\Pi(f: K(f_0, f) &lt; \\epsilon_n^2, V(f_0, f) &lt; \\epsilon_n^2) \\geq e^{-Cn\\epsilon_n^2}\\] <p>where \\(K(f_0, f)\\) is the Kullback-Leibler divergence, \\(V(f_0, f)\\) is the variance of the log-likelihood ratio, and \\(C &gt; 0\\) is a constant.</p> <p>Given that the true regression function \\(f_0 \\in \\mathcal{C}^\\alpha([0,1]^p)\\), we can approximate it using a step function with \\(O(\\epsilon_n^{-p/\\alpha})\\) pieces, each with approximation error of order \\(\\epsilon_n\\). Such a step function can be represented by a regression tree with \\(O(\\epsilon_n^{-p/\\alpha})\\) leaves.</p> <p>Under the BART model with \\(m\\) trees, each tree needs to capture \\(O(\\epsilon_n^{-p/\\alpha}/m)\\) leaves. The prior probability of such a tree structure is at least:</p> \\[e^{-c_1 \\epsilon_n^{-p/\\alpha} \\log(1/\\epsilon_n)}\\] <p>for some constant \\(c_1 &gt; 0\\).</p> <p>The MFM-Gibbs prior on variable selection probabilities assigns positive probability to configurations where only the relevant variables have non-zero selection probabilities. The prior probability of selecting the correct set of variables is at least:</p> \\[e^{-c_2 s_0 \\log(p)}\\] <p>for some constant \\(c_2 &gt; 0\\), where \\(s_0\\) is the number of relevant variables.</p> <p>Combining these bounds and choosing \\(m = O(\\log(n))\\), we obtain:</p> \\[\\Pi(f: K(f_0, f) &lt; \\epsilon_n^2, V(f_0, f) &lt; \\epsilon_n^2) \\geq e^{-c_3(n\\epsilon_n^2 + s_0 \\log(p))}\\] <p>For the chosen \\(\\epsilon_n = n^{-\\alpha/(2\\alpha + p)}(\\log n)^\\beta\\), we have \\(n\\epsilon_n^2 = n^{1-2\\alpha/(2\\alpha + p)}(\\log n)^{2\\beta}\\). If \\(s_0 \\log(p) = o(n\\epsilon_n^2)\\), which holds under the assumption that \\(p\\) grows at most polynomially with \\(n\\), then the prior mass condition is satisfied.</p> <p>Conditions 2 and 3 follow from standard results in nonparametric Bayesian theory, leveraging the exponential inequality for Gaussian processes and the control of the metric entropy of the function space induced by the BART model.</p> <p>Combining the verification of all three conditions, we conclude that:</p> \\[\\Pi\\left(f: \\|f - f_0\\|_\\infty &gt; M_n \\epsilon_n \\mid \\text{Data}\\right) \\to 0 \\text{ in probability as } n \\to \\infty\\] <p>for any sequence \\(M_n \\to \\infty\\), which completes the proof of Theorem 1.</p>"},{"location":"research/note/sparseBART/#a2-proof-of-theorem-2-variable-selection-consistency","title":"A.2 Proof of Theorem 2 (Variable Selection Consistency)","text":"<p>To prove variable selection consistency, we need to show that the posterior inclusion probabilities for the relevant variables in \\(S_0\\) converge to 1, while the posterior inclusion probabilities for the irrelevant variables converge to 0.</p> <p>Let \\(\\gamma_j = I(j \\in S)\\) be the indicator for whether variable \\(j\\) is included in the model. The posterior inclusion probability for variable \\(j\\) is:</p> \\[p_j = P(\\gamma_j = 1 \\mid \\text{Data})\\] <p>We want to show that \\(p_j \\to 1\\) for \\(j \\in S_0\\) and \\(p_j \\to 0\\) for \\(j \\notin S_0\\) as \\(n \\to \\infty\\).</p> <p>The key insight is that the MFM-Gibbs prior assigns higher probability to sparse models by placing a prior on the number of active variables. As the sample size increases, the likelihood component dominates the prior, and the data informs which variables are truly relevant.</p> <p>Let \\(f_S\\) denote a function that depends only on the variables in set \\(S\\). The marginal likelihood can be expressed as:</p> \\[p(\\text{Data} \\mid S) = \\int p(\\text{Data} \\mid f_S) \\Pi(df_S \\mid S)\\] <p>where \\(\\Pi(df_S \\mid S)\\) is the prior on functions given the variable set \\(S\\).</p> <p>For any set \\(S\\) that omits a relevant variable \\(j \\in S_0\\), there exists a constant \\(\\delta &gt; 0\\) such that:</p> \\[\\log p(\\text{Data} \\mid S_0) - \\log p(\\text{Data} \\mid S) \\geq \\delta n - O_p(\\sqrt{n})\\] <p>This follows from the fact that models excluding relevant variables cannot approximate the true function well, leading to a significant drop in likelihood.</p> <p>Conversely, for any set \\(S\\) that includes all relevant variables and some irrelevant ones, the marginal likelihood ratio satisfies:</p> \\[\\log p(\\text{Data} \\mid S_0) - \\log p(\\text{Data} \\mid S) = O_p(\\log n)\\] <p>under the assumption of model selection consistency of the MFM-Gibbs prior.</p> <p>Using Bayes' theorem, the posterior probability of the true variable set is:</p> \\[P(S = S_0 \\mid \\text{Data}) = \\frac{p(\\text{Data} \\mid S_0) \\Pi(S_0)}{\\sum_S p(\\text{Data} \\mid S) \\Pi(S)}\\] <p>Given the prior probabilities \\(\\Pi(S)\\) induced by the MFM-Gibbs prior, which favors sparsity, and the likelihood ratios established above, we can show that:</p> \\[P(S = S_0 \\mid \\text{Data}) \\to 1 \\text{ as } n \\to \\infty\\] <p>which implies that \\(p_j \\to 1\\) for \\(j \\in S_0\\) and \\(p_j \\to 0\\) for \\(j \\notin S_0\\), completing the proof of Theorem 2.</p>"},{"location":"research/note/sparseBART/#a3-proof-of-theorem-3-asymptotic-normality","title":"A.3 Proof of Theorem 3 (Asymptotic Normality)","text":"<p>To establish the asymptotic normality of posterior functionals, we leverage the Bernstein-von Mises theorem for nonparametric models, as developed by Castillo and Rousseau (2015) and extended to BART models by Ray and Szab\u00f3 (2020).</p> <p>Let \\(\\phi(f) = \\int f(x) h(x) dx\\) be a linear functional of the regression function. The centered and scaled posterior distribution of \\(\\phi(f)\\) can be written as:</p> \\[\\sqrt{n}(\\phi(f) - \\phi(f_0)) \\mid \\text{Data} = \\sqrt{n}(\\phi(f) - \\phi(\\hat{f})) \\mid \\text{Data} + \\sqrt{n}(\\phi(\\hat{f}) - \\phi(f_0))\\] <p>where \\(\\hat{f}\\) is the posterior mean of \\(f\\).</p> <p>Under the conditions of Theorem 1, the first term converges to a normal distribution:</p> \\[\\sqrt{n}(\\phi(f) - \\phi(\\hat{f})) \\mid \\text{Data} \\xrightarrow{d} N(0, V)\\] <p>where \\(V\\) is the asymptotic variance determined by the semiparametric efficiency bound.</p> <p>The second term, \\(\\sqrt{n}(\\phi(\\hat{f}) - \\phi(f_0))\\), converges to zero in probability due to the posterior consistency established in Theorem 1 and the linearity of the functional \\(\\phi\\).</p> <p>Therefore, by Slutsky's theorem:</p> \\[\\sqrt{n}(\\phi(f) - \\phi(f_0)) \\mid \\text{Data} \\xrightarrow{d} N(0, V)\\] <p>which completes the proof of Theorem 3.</p> <p>This result has important implications for statistical inference. It ensures that credible intervals based on the posterior distribution are asymptotically valid, providing a rigorous foundation for Bayesian inference in our BART model with MFM and Gibbs prior.</p>"},{"location":"research/note/sparseBART/#references","title":"References","text":"<ol> <li> <p>Chipman, H. A., George, E. I., &amp; McCulloch, R. E. (2010). BART: Bayesian additive regression trees. The Annals of Applied Statistics, 4(1), 266-298.</p> </li> <li> <p>Miller, J. W., &amp; Harrison, M. T. (2018). Mixture models with a prior on the number of components. Journal of the American Statistical Association, 113(521), 340-356.</p> </li> <li> <p>Linero, A. R. (2018). Bayesian regression trees for high-dimensional prediction and variable selection. Journal of the American Statistical Association, 113(522), 626-636.</p> </li> <li> <p>Rockova, V., &amp; van der Pas, S. (2020). Posterior concentration for Bayesian regression trees and forests. The Annals of Statistics, 48(4), 2108-2131.</p> </li> <li> <p>Castillo, I., &amp; Rousseau, J. (2015). A Bernstein\u2013von Mises theorem for smooth functionals in semiparametric models. The Annals of Statistics, 43(6), 2353-2383.</p> </li> <li> <p>Ray, K., &amp; Szab\u00f3, B. (2020). Variational Bayes for high-dimensional linear regression with sparse priors. Journal of the American Statistical Association, 115(532), 1951-1969.</p> </li> <li> <p>Ghosal, S., Ghosh, J. K., &amp; van der Vaart, A. W. (2000). Convergence rates of posterior distributions. The Annals of Statistics, 28(2), 500-531.</p> </li> </ol>"},{"location":"research/note/sparseBART/#b-additional-simulation-results","title":"B. Additional Simulation Results","text":"<p>Comprehensive results from additional simulation scenarios not included in the main text.</p>"},{"location":"research/note/sparseBART/#c-implementation-details","title":"C. Implementation Details","text":"<p>Pseudocode and implementation notes for the proposed algorithms.</p>"},{"location":"research/note/sparseBART/#d-additional-real-data-analyses","title":"D. Additional Real Data Analyses","text":"<p>Results from applications to additional datasets.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/","title":"On the Statistical Properties of Hypothesis Testing with Generative Model Augmentation","text":""},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#abstract","title":"Abstract","text":"<p>This paper provides a theoretical framework for analyzing the statistical properties of hypothesis testing when the original dataset is augmented with synthetically generated samples. We derive explicit formulas for the variance of estimators, effective sample sizes, and test statistics for three generative models: multivariate Gaussian, Principal Component Analysis (PCA), and Linear Factor Models. Our analysis reveals that while data augmentation can increase statistical power, the gain is bounded by the accuracy of the generative model and follows a direction-dependent pattern determined by the model's structure. We provide exact formulas for hypothesis test corrections that maintain proper Type I error control while improving power. These results offer statistical guidelines for implementing generative model augmentation in high-dimensional settings such as neuroimaging analysis. Extensive simulation studies validate our theoretical results and demonstrate their practical utility across various scenarios.</p> <p>Keywords: hypothesis testing, generative models, data augmentation, effective sample size, neuroimaging</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#1-introduction","title":"1. Introduction","text":"<p>High-dimensional data analysis in fields such as neuroimaging, genomics, and computer vision faces a persistent challenge: the limited availability of samples relative to the dimensionality of the data (Varoquaux and Thirion, 2014; Fan et al., 2014; Wager et al., 2013). This \"small n, large p\" problem undermines statistical power, complicates model fitting, and increases the risk of spurious findings (Button et al., 2013; Ioannidis, 2005). Traditionally, researchers have addressed this challenge through dimensionality reduction techniques (Cunningham and Ghahramani, 2015), regularization methods (Hastie et al., 2015), or meta-analytic approaches (Wager et al., 2007; Salimi-Khorshidi et al., 2009).</p> <p>More recently, advances in generative modeling have opened a new avenue: augmenting limited datasets with synthetically generated samples (Shorten and Khoshgoftaar, 2019; Antoniou et al., 2017). This approach has gained particular traction in medical imaging, where patient data is costly to acquire and often protected by privacy constraints (Yi et al., 2019; Frid-Adar et al., 2018; Bowles et al., 2018; Shin et al., 2018). In neuroimaging specifically, various generative models have been employed to synthesize brain images that preserve anatomical structures and disease-specific features (Zhao et al., 2019; Thambawita et al., 2022; Billot et al., 2021).</p> <p>However, while the empirical benefits of generative model augmentation for classification and detection tasks have been demonstrated (Salehinejad et al., 2018; M\u00e5rtensson et al., 2020), the statistical implications for hypothesis testing remain theoretically underdeveloped. Critical questions persist: How does augmentation affect the distribution of test statistics? Can synthetic samples genuinely increase statistical power? What corrections are necessary to maintain valid inference? To date, only limited theoretical frameworks exist for understanding these issues. Some researchers have explored the statistical properties of bootstrapped samples (Efron and Tibshirani, 1994; Hall, 1992) and multiple imputation techniques (Rubin, 1987; Schafer, 1999), but these approaches differ fundamentally from modern generative modeling.</p> <p>Several recent works have begun addressing the statistical properties of learning with augmented data. Chen et al. (2019) analyzed the bias-variance tradeoff in classification with augmented samples, while Wu and Yang (2020) examined conditions under which augmentation improves estimation in regression tasks. Dao et al. (2019) established theoretical guarantees for specific augmentation transformations in kernel methods. However, these works focus primarily on supervised learning rather than hypothesis testing, and they do not account for the complex dependence structure introduced when generative models are trained on the same data used for inference.</p> <p>In the realm of hypothesis testing specifically, Westfall and Young (1993) and Dudoit et al. (2003) developed resampling-based methods for multiple testing that bear some conceptual similarity to augmentation approaches. Pantazis et al. (2005) and Nichols and Holmes (2002) proposed permutation tests for neuroimaging data that could potentially accommodate synthetic samples, but without formal justification. The recent work by Fisher et al. (2020) on conditional randomization tests provides relevant insights but does not directly address generative augmentation.</p> <p>This paper addresses these gaps by providing a rigorous statistical framework for understanding the impact of generative model augmentation on hypothesis testing. We consider a setting where a dataset \\(D = \\{X_1,...,X_n\\}\\) of samples (e.g., brain images) is augmented with synthetically generated samples \\(\\{X_1^{(f)},...,X_m^{(f)}\\}\\) obtained from a generative model \\(f(z|D)\\) trained on \\(D\\). We systematically analyze how this augmentation affects the distribution of test statistics, the effective sample size, and the statistical power of hypothesis tests.</p> <p>Our work builds upon and extends several theoretical foundations: the literature on effective sample size in dependent data (Thi\u00e9baux and Zwiers, 1984; Jones, 2011; Kass et al., 2016), variance estimation in mixture distributions (McLachlan and Peel, 2000; Lindsay, 1995), and high-dimensional inference (B\u00fchlmann and van de Geer, 2011; Wasserman and Roeder, 2009). We also draw connections to recent advances in transfer learning (Pan and Yang, 2010; Zhuang et al., 2020) and out-of-distribution generalization (Shen et al., 2021; Koh et al., 2021), as generative augmentation can be viewed as a form of knowledge transfer between the original and synthetic data domains.</p> <p>The key contributions of this paper are:</p> <ol> <li>Derivation of exact formulas for the variance of estimators when using augmented data for three generative models with increasing complexity, accounting for the inherent dependencies between original and synthetic samples</li> <li>Analysis of direction-dependent statistical gains in effective sample size, showing that augmentation benefits vary across different dimensions of the data space</li> <li>Construction of corrected hypothesis tests that maintain proper Type I error control while leveraging synthetic samples to improve statistical power</li> <li>Determination of optimal augmentation ratios based on parameter estimation accuracy, providing practical guidance for implementation</li> <li>Extension of core results to modern deep generative models such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models</li> <li>Comprehensive simulation studies validating our theoretical findings across various scenarios relevant to neuroimaging and other high-dimensional applications</li> </ol>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#2-problem-formulation","title":"2. Problem Formulation","text":"<p>Let \\(D = \\{X_1,...,X_n\\}\\) be a dataset of \\(n\\) independent and identically distributed (i.i.d.) samples drawn from a distribution \\(P_X\\) with mean \\(\\mu\\) and covariance \\(\\Sigma\\). We consider hypothesis testing problems of the form:</p> \\[H_0: \\mu = \\mu_0 \\quad \\text{vs.} \\quad H_1: \\mu \\neq \\mu_0\\] <p>Let \\(f(z|D)\\) be a generative model trained on \\(D\\), where \\(z\\) represents latent variables drawn from some distribution \\(P_Z\\). We generate synthetic samples \\(X_i^{(f)} \\sim f(z_i|D)\\) with \\(z_i \\sim P_Z\\) for \\(i = 1,2,...,m\\).</p> <p>The augmented dataset is defined as \\(D' = \\{X_1,...,X_n, X_1^{(f)},...,X_m^{(f)}\\}\\). Our goal is to analyze the statistical properties of hypothesis tests using the augmented dataset \\(D'\\) compared to using only the original dataset \\(D\\).</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#3-theoretical-analysis","title":"3. Theoretical Analysis","text":""},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#31-multivariate-gaussian-model","title":"3.1 Multivariate Gaussian Model","text":"<p>We first consider a simple generative model where samples are assumed to follow a multivariate Gaussian distribution.</p> <p>Proposition 1. Let \\(X_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\) for \\(i = 1,...,n\\), and let \\(\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n}X_i\\) and \\(\\hat{\\Sigma} = \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\hat{\\mu})(X_i - \\hat{\\mu})^T\\) be the sample mean and covariance. If \\(X_i^{(f)} \\sim \\mathcal{N}(\\hat{\\mu}, \\hat{\\Sigma})\\) for \\(i = 1,...,m\\), then the variance of the augmented sample mean \\(\\bar{X}_{D'} = \\frac{1}{n+m}\\sum_{i=1}^{n+m}X_i'\\) is given by:</p> \\[\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m\\hat{\\Sigma}}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\] <p>Proof. The augmented sample mean can be written as a weighted average of the original sample mean and the synthetic sample mean:</p> \\[\\bar{X}_{D'} = \\frac{n}{n+m}\\bar{X}_D + \\frac{m}{n+m}\\bar{X}_f\\] <p>where \\(\\bar{X}_D = \\frac{1}{n}\\sum_{i=1}^{n}X_i\\) and \\(\\bar{X}_f = \\frac{1}{m}\\sum_{i=1}^{m}X_i^{(f)}\\). </p> <p>Since \\(\\bar{X}_f \\sim \\mathcal{N}(\\hat{\\mu}, \\frac{\\hat{\\Sigma}}{m})\\) and \\(\\hat{\\mu} = \\bar{X}_D\\), we have:</p> \\[\\text{Var}(\\bar{X}_{D'}) = \\left(\\frac{n}{n+m}\\right)^2\\text{Var}(\\bar{X}_D) + \\left(\\frac{m}{n+m}\\right)^2\\text{Var}(\\bar{X}_f) + 2\\frac{nm}{(n+m)^2}\\text{Cov}(\\bar{X}_D, \\bar{X}_f)\\] <p>Substituting \\(\\text{Var}(\\bar{X}_D) = \\frac{\\Sigma}{n}\\), \\(\\text{Var}(\\bar{X}_f) = \\frac{\\hat{\\Sigma}}{m}\\), and \\(\\text{Cov}(\\bar{X}_D, \\bar{X}_f) = \\frac{\\Sigma}{n}\\) (since \\(\\bar{X}_f\\) is conditional on \\(\\bar{X}_D\\)), we obtain the result. \u25a0</p> <p>Corollary 1.1. The effective sample size \\(n_{eff}\\) for the Gaussian model, defined such that \\(\\text{Var}(\\bar{X}_{D'}) = \\frac{\\Sigma}{n_{eff}}\\), is given by:</p> \\[n_{eff} = \\frac{n(n+m)^2}{n^2 + nm + m^2\\frac{\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1})}{p}}\\] <p>where \\(p\\) is the dimension of the data.</p> <p>Remark on Isotropy Assumption: The term \\(\\frac{\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1})}{p}\\) represents the average scaled estimation error across all dimensions. This formulation assumes that the relative importance of estimation errors is uniform across dimensions. When this isotropy assumption does not hold, a more general form can be derived by considering a weighted average of dimension-specific effective sample sizes.</p> <p>Proposition 2. For the hypothesis test \\(H_0: \\mu = \\mu_0\\) vs. \\(H_1: \\mu \\neq \\mu_0\\), the corrected test statistic using the augmented dataset is:</p> \\[T_{D',corr} = (n+m)(\\bar{X}_{D'} - \\mu_0)^T[\\text{Var}(\\bar{X}_{D'})]^{-1}(\\bar{X}_{D'} - \\mu_0)\\] <p>which follows a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom under \\(H_0\\).</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#32-principal-component-analysis-pca-model","title":"3.2 Principal Component Analysis (PCA) Model","text":"<p>We now consider a more structured generative model based on PCA.</p> <p>Proposition 3. Let the sample covariance matrix be decomposed as \\(\\hat{\\Sigma} = \\sum_{j=1}^p \\lambda_j \\phi_j \\phi_j^T\\), where \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_p \\geq 0\\) are the eigenvalues and \\(\\phi_j\\) are the corresponding eigenvectors. If synthetic samples are generated as \\(X_i^{(f)} = \\bar{X}_D + \\sum_{j=1}^k \\sqrt{\\lambda_j} \\phi_j z_{ij}\\) where \\(z_{ij} \\sim \\mathcal{N}(0,1)\\) and \\(k &lt; p\\), then the variance of the augmented sample mean is:</p> \\[\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\] <p>Proof. Following similar steps as in Proposition 1, but noting that the covariance of synthetic samples is now \\(\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T\\) rather than \\(\\hat{\\Sigma}\\), we obtain the result. \u25a0</p> <p>Corollary 3.1. The effective sample size for the direction corresponding to principal component \\(\\phi_j\\) is:</p> \\[n_{eff,j} = \\begin{cases} \\frac{n(n+m)^2}{n^2 + nm + m^2\\frac{\\lambda_j}{\\phi_j^T\\Sigma\\phi_j}} &amp; \\text{if } j \\leq k \\\\ n &amp; \\text{if } j &gt; k \\end{cases}\\] <p>Remark 1. A key insight from Corollary 3.1 is that augmentation with a PCA model provides no statistical benefit in directions not captured by the retained principal components. This direction-dependent property highlights the importance of selecting an appropriate number of components \\(k\\) based on the specific hypothesis being tested.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#33-linear-factor-model","title":"3.3 Linear Factor Model","text":"<p>Finally, we consider a linear factor model that provides a middle ground between the flexibility of the Gaussian model and the structure of the PCA model.</p> <p>Proposition 4. Assume data is generated according to a factor model \\(X_i = \\mu + Wz_i + \\epsilon_i\\) where \\(W\\) is a \\(p \\times q\\) factor loading matrix, \\(z_i \\sim \\mathcal{N}(0, I_q)\\), and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\Psi)\\) with \\(\\Psi\\) being a diagonal matrix. If synthetic samples are generated as \\(X_i^{(f)} = \\hat{\\mu} + \\hat{W}z_i + \\epsilon_i^*\\) where \\(\\epsilon_i^* \\sim \\mathcal{N}(0, \\hat{\\Psi})\\), then the variance of the augmented sample mean is:</p> \\[\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m(\\hat{W}\\hat{W}^T + \\hat{\\Psi})}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\] <p>Proof. The proof follows the same structure as Proposition 1, accounting for the specific covariance structure of the factor model. \u25a0</p> <p>Corollary 4.1. The effective sample size can be decomposed into factor space and error space components:</p> \\[n_{eff,factor} = \\frac{n(n+m)^2}{n^2 + nm + m\\|\\hat{W} - W\\|_F^2}\\] \\[n_{eff,error} = \\frac{n(n+m)^2}{n^2 + nm + m\\|\\hat{\\Psi} - \\Psi\\|_F^2}\\] <p>where \\(\\|\\cdot\\|_F\\) denotes the Frobenius norm.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#34-extension-to-modern-generative-models","title":"3.4 Extension to Modern Generative Models","text":"<p>While the previous sections focused on classical generative models with analytical forms, we now extend our results to modern deep generative models: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models.</p> <p>Proposition 7. Let \\(G_\\theta\\) be a deep generative model with parameters \\(\\theta\\) trained on dataset \\(D\\). If synthetic samples are generated as \\(X_i^{(f)} = G_\\theta(z_i)\\) where \\(z_i \\sim P_Z\\), then the variance of the augmented sample mean can be approximated as:</p> \\[\\text{Var}(\\bar{X}_{D'}) \\approx \\frac{n\\Sigma + m\\hat{\\Sigma}_G}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\] <p>where \\(\\hat{\\Sigma}_G = \\mathbb{E}_{z \\sim P_Z}[(G_\\theta(z) - \\mathbb{E}[G_\\theta(z)])(G_\\theta(z) - \\mathbb{E}[G_\\theta(z)])^T]\\) is the covariance of the generated samples.</p> <p>Corollary 7.1. The effective sample size for deep generative models can be estimated as:</p> \\[n_{eff,deep} \\approx \\frac{n(n+m)^2}{n^2 + nm + m^2 D_{KL}(P_X \\| P_G)}\\] <p>where \\(D_{KL}(P_X \\| P_G)\\) is the Kullback-Leibler divergence between the true data distribution \\(P_X\\) and the generative model distribution \\(P_G\\).</p> <p>This extension allows us to apply our theoretical framework to modern generative models commonly used in practice, with the understanding that the exact covariance structure may need to be empirically estimated.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#4-optimal-augmentation-ratio","title":"4. Optimal Augmentation Ratio","text":"<p>A critical practical question is determining the optimal ratio of synthetic to real samples. We now derive this optimal ratio and provide methods for estimating it in practice.</p> <p>Proposition 5. For a given generative model with parameters \\(\\theta\\) and estimators \\(\\hat{\\theta}\\), the optimal ratio of synthetic to real samples that maximizes the effective sample size is:</p> \\[\\frac{m}{n} = \\frac{1}{\\|\\hat{\\theta} - \\theta\\|_F^2}\\] <p>where \\(\\|\\cdot\\|_F\\) denotes an appropriate norm measuring the distance between the true and estimated parameters.</p> <p>Proof. Consider the general form of the effective sample size:</p> \\[n_{eff} = \\frac{n(n+m)^2}{n^2 + nm + m^2c}\\] <p>where \\(c\\) is a constant that depends on the accuracy of parameter estimation. Taking the derivative with respect to \\(m\\) and setting it to zero:</p> \\[\\frac{\\partial n_{eff}}{\\partial m} = \\frac{n(n+m)(2(n+m) - (n+2m)c)}{(n^2 + nm + m^2c)^2} = 0\\] <p>This equation is satisfied when \\(2(n+m) - (n+2m)c = 0\\), which simplifies to \\(m = \\frac{n(2-c)}{2c-2}\\). When \\(c\\) is small (i.e., accurate parameter estimation), this approaches \\(m = \\frac{n}{c}\\). In the context of parameter estimation, \\(c \\approx \\|\\hat{\\theta} - \\theta\\|_F^2\\), yielding the result. \u25a0</p> <p>Practical Estimation: In practice, \\(\\|\\hat{\\theta} - \\theta\\|_F^2\\) is unknown since \\(\\theta\\) is unknown. We propose two practical approaches:</p> <ol> <li> <p>Cross-validation estimation: Split the original dataset into training and validation sets. Estimate parameters on the training set and compute the error on the validation set.</p> </li> <li> <p>Bootstrap estimation: Generate bootstrap samples from the original dataset, estimate parameters for each bootstrap sample, and calculate the variance of these estimates.</p> </li> </ol>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#5-statistical-inference-with-augmented-data","title":"5. Statistical Inference with Augmented Data","text":""},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#51-corrected-hypothesis-tests","title":"5.1 Corrected Hypothesis Tests","text":"<p>To maintain proper Type I error control with augmented data, hypothesis tests must be adjusted to account for the dependence structure introduced by the generative model.</p> <p>Theorem 1. For testing \\(H_0: \\mu = \\mu_0\\) vs. \\(H_1: \\mu \\neq \\mu_0\\) using an augmented dataset \\(D'\\), the test statistic:</p> \\[T_{D',corr} = (n+m)(\\bar{X}_{D'} - \\mu_0)^T[\\text{Var}(\\bar{X}_{D'})]^{-1}(\\bar{X}_{D'} - \\mu_0)\\] <p>where \\(\\text{Var}(\\bar{X}_{D'})\\) is the variance derived in Propositions 1, 3, 4, or 7 for the respective generative model, follows a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom under \\(H_0\\).</p> <p>Proof. Under \\(H_0\\), \\(\\bar{X}_{D'} - \\mu_0\\) follows a multivariate normal distribution with mean 0 and covariance \\(\\text{Var}(\\bar{X}_{D'})\\). The result then follows from the properties of the quadratic form of a multivariate normal vector. \u25a0</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#52-power-analysis","title":"5.2 Power Analysis","text":"<p>The power of the corrected test depends on the non-centrality parameter of the \\(\\chi^2\\) distribution under the alternative hypothesis.</p> <p>Proposition 6. Under the alternative hypothesis \\(H_1: \\mu = \\mu_1 \\neq \\mu_0\\), the non-centrality parameter for the corrected test statistic is:</p> \\[\\lambda = (n+m)(\\mu_1 - \\mu_0)^T[\\text{Var}(\\bar{X}_{D'})]^{-1}(\\mu_1 - \\mu_0)\\] <p>The power of the test at significance level \\(\\alpha\\) is:</p> \\[\\text{Power} = P(\\chi^2_p(\\lambda) &gt; \\chi^2_{p,1-\\alpha})\\] <p>where \\(\\chi^2_p(\\lambda)\\) is a non-central \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom and non-centrality parameter \\(\\lambda\\), and \\(\\chi^2_{p,1-\\alpha}\\) is the \\((1-\\alpha)\\)-quantile of the central \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#53-confidence-interval-construction","title":"5.3 Confidence Interval Construction","text":"<p>Corollary 6.1. Assuming asymptotic normality, the \\((1-\\alpha)\\) confidence interval for the \\(j\\)-th component of the mean \\(\\mu\\) is:</p> \\[[\\bar{X}_{D',j} - z_{\\alpha/2}\\sqrt{[\\text{Var}(\\bar{X}_{D'})]_{jj}}, \\bar{X}_{D',j} + z_{\\alpha/2}\\sqrt{[\\text{Var}(\\bar{X}_{D'})]_{jj}}]\\] <p>where \\([\\text{Var}(\\bar{X}_{D'})]_{jj}\\) is the \\(j\\)-th diagonal element of \\(\\text{Var}(\\bar{X}_{D'})\\).</p> <p>The asymptotic normality assumption is justified by the Central Limit Theorem and holds for sufficiently large sample sizes. For small samples, t-distribution based intervals may be more appropriate.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#54-practical-implementation-and-parameter-estimation","title":"5.4 Practical Implementation and Parameter Estimation","text":"<p>In practice, implementing the corrected tests requires estimating \\(\\Sigma\\) and other model-specific parameters. We propose the following practical approaches:</p> <ol> <li> <p>Estimating \\(\\Sigma\\): Use a regularized estimator such as the shrinkage estimator:    \\(\\(\\hat{\\Sigma}_{reg} = (1-\\lambda)\\hat{\\Sigma} + \\lambda \\text{diag}(\\hat{\\Sigma})\\)\\)    where \\(\\lambda \\in [0,1]\\) is a shrinkage parameter.</p> </li> <li> <p>Estimating \\(\\text{Var}(\\bar{X}_{D'})\\): Use a plug-in estimator with the regularized covariance:    \\(\\(\\widehat{\\text{Var}}(\\bar{X}_{D'}) = \\frac{n\\hat{\\Sigma}_{reg} + m\\hat{\\Sigma}_G}{(n+m)^2} + \\frac{nm\\hat{\\Sigma}_{reg}}{n(n+m)^2}\\)\\)    where \\(\\hat{\\Sigma}_G\\) is the empirical covariance of the generated samples.</p> </li> <li> <p>Degrees of freedom adjustment: For small samples, adjust the degrees of freedom in the test to account for parameter estimation uncertainty.</p> </li> </ol>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#6-extensions-to-non-gaussian-settings","title":"6. Extensions to Non-Gaussian Settings","text":"<p>Our framework can be extended to non-Gaussian settings using semi-parametric approaches.</p> <p>Proposition 8. For non-Gaussian data following a distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\), the corrected test statistic:</p> \\[T_{D',corr} = (n+m)(\\bar{X}_{D'} - \\mu_0)^T[\\widehat{\\text{Var}}(\\bar{X}_{D'})]^{-1}(\\bar{X}_{D'} - \\mu_0)\\] <p>is asymptotically distributed as \\(\\chi^2_p\\) under \\(H_0\\), provided that the fourth moments of the data distribution are finite.</p> <p>For highly non-Gaussian data, we recommend bootstrap-based approaches:</p> <ol> <li> <p>Parametric bootstrap: Generate bootstrap samples from the fitted generative model and construct the empirical distribution of the test statistic.</p> </li> <li> <p>Non-parametric bootstrap: Resample from the original dataset, retrain the generative model on each bootstrap sample, and construct the empirical distribution of the test statistic.</p> </li> </ol>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#7-computational-considerations","title":"7. Computational Considerations","text":"<p>High-dimensional data such as neuroimaging presents computational challenges when implementing our framework. We provide strategies to address these challenges:</p> <ol> <li> <p>Dimensionality reduction: For very high-dimensional data, apply dimensionality reduction before fitting generative models and conducting hypothesis tests.</p> </li> <li> <p>Sparse covariance estimation: Use sparse covariance estimators to reduce computational complexity and improve numerical stability.</p> </li> <li> <p>Parallel computing: Leverage parallel computing for bootstrap and Monte Carlo methods.</p> </li> <li> <p>Computational complexity: The computational complexity of our framework is dominated by:</p> </li> <li>Generative model training: \\(O(f(n,p))\\) (model-dependent)</li> <li>Covariance estimation: \\(O(np^2)\\)</li> <li>Test statistic computation: \\(O(p^3)\\) (due to matrix inversion)</li> </ol> <p>For very high-dimensional data, we recommend:    - Working in a reduced-dimension space when possible    - Using iterative methods for matrix operations    - Employing sparse matrix representations</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#8-simulation-studies","title":"8. Simulation Studies","text":"<p>To validate our theoretical results, we conducted extensive simulation studies examining how generative model augmentation affects hypothesis testing across various scenarios. These simulations systematically evaluated the variance formulas, Type I error control, statistical power, direction-dependent gains, optimal augmentation ratios, and model comparisons described in the preceding sections.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#81-methodology","title":"8.1 Methodology","text":"<p>All simulations followed a common experimental paradigm. We generated multivariate normal data with controlled covariance structures, applied different generative models to create synthetic samples, and analyzed the resulting statistical properties of hypothesis tests. Unless otherwise specified, we used the following parameter settings:</p> <ul> <li>Data dimensionality: 5-10 dimensions</li> <li>Original sample sizes: 20, 50, 100, and 200</li> <li>Synthetic sample sizes: 20, 50, 100, and 200</li> <li>Covariance structure: Eigenvalues following exponential decay with condition number 10</li> <li>Null hypothesis: \\(H_0: \\mu = 0\\)</li> <li>Alternative hypothesis: \\(H_1: \\mu = \\delta v\\), where \\(v\\) is a unit vector and \\(\\delta\\) is the effect size</li> <li>Significance level: \\(\\alpha = 0.05\\)</li> </ul> <p>For each scenario, we compared three testing approaches: 1. Original Test: Uses only the original dataset 2. Naive Test: Uses the augmented dataset but incorrectly assumes independence 3. Corrected Test: Uses the augmented dataset with proper variance correction as derived in Section 3</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#82-generative-models-implementation","title":"8.2 Generative Models Implementation","text":"<p>We implemented three generative models with increasing complexity:</p> <p>Gaussian Model: The simplest approach estimates the sample mean \\(\\hat{\\mu}\\) and covariance \\(\\hat{\\Sigma}\\) from the original data and generates new samples from \\(\\mathcal{N}(\\hat{\\mu}, \\hat{\\Sigma})\\).</p> <p>PCA Model: This approach retains only the top \\(k\\) principal components: 1. Center the data and compute principal components 2. Determine \\(k\\) based on explained variance (typically capturing 95% of variance) 3. Generate synthetic samples as:    \\(\\(X_i^{(f)} = \\hat{\\mu} + \\sum_{j=1}^k \\sqrt{\\lambda_j}z_{ij}\\phi_j\\)\\)    where \\(\\lambda_j\\) are eigenvalues, \\(\\phi_j\\) are eigenvectors, and \\(z_{ij} \\sim \\mathcal{N}(0,1)\\)</p> <p>Factor Model: This approach uses a linear factor model: 1. Standardize the data 2. Fit a factor analysis model with \\(q\\) factors using maximum likelihood 3. Extract the loading matrix \\(\\hat{W}\\) and uniquenesses \\(\\hat{\\Psi}\\) 4. Generate synthetic samples as:    \\(\\(X_i^{(f)} = \\hat{\\mu} + \\hat{W}z_i + \\epsilon_i\\)\\)    where \\(z_i \\sim \\mathcal{N}(0, I_q)\\) and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\hat{\\Psi})\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#83-results-and-discussion","title":"8.3 Results and Discussion","text":""},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#831-validation-of-variance-formulas","title":"8.3.1 Validation of Variance Formulas","text":"<p>Figure 1 presents the ratio of empirical to theoretical variance for the augmented sample mean across different generative models, original sample sizes, and synthetic sample sizes. The empirical variance was estimated through Monte Carlo simulation with 200 repetitions per configuration.</p> <p>Results confirm that our theoretical derivations accurately predict the variance of estimators using augmented data, with empirical-to-theoretical ratios consistently between 0.95 and 1.05 across all tested scenarios. This validates the core mathematical results from Propositions 1, 3, and 4.</p> <p></p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#832-type-i-error-control","title":"8.3.2 Type I Error Control","text":"<p>Figure 2 demonstrates the Type I error rates (proportion of false rejections under the null hypothesis) for the three testing approaches. The simulation included 1,000 trials per configuration to ensure precise estimation of error rates.</p> <p>As predicted by our theory, the naive approach that treats synthetic samples as independent has severely inflated Type I error rates, often exceeding three times the nominal level (reaching as high as 15% when the nominal level is 5%). In contrast, our corrected test maintains proper Type I error control at the specified significance level across all sample size combinations. The original test also maintains proper Type I error control as expected.</p> <p></p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#833-power-analysis","title":"8.3.3 Power Analysis","text":"<p>Figure 3 shows the statistical power (proportion of correct rejections under the alternative hypothesis) as a function of effect size. The alternative hypothesis constructed signal in the direction of the first eigenvector with varying magnitudes.</p> <p>The corrected test consistently achieves higher power than the original test using only the original data, particularly for small to moderate effect sizes. While the naive test appears to have even higher power, this is misleading due to its inflated Type I error rate. The corrected test provides the optimal balance between power improvement and Type I error control.</p> <p></p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#834-direction-dependent-gain","title":"8.3.4 Direction-Dependent Gain","text":"<p>Figure 4 illustrates the direction-dependent nature of statistical gain with PCA-based augmentation. We tested power across all eigendirections of the covariance matrix while retaining only the top \\(k=3\\) principal components in the generative model.</p> <p>The results confirm our theoretical prediction: significant power improvements occur only in directions captured by the retained principal components. Directions corresponding to discarded components show negligible or even slightly negative power gain. This validates the direction-dependent effective sample size derived in Corollary 3.1.</p> <p></p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#835-optimal-augmentation-ratio","title":"8.3.5 Optimal Augmentation Ratio","text":"<p>Figure 5 examines the effective sample size as a function of the augmentation ratio \\(m/n\\) for different levels of parameter estimation error. The parameter error was controlled by introducing varying levels of noise to the estimated covariance matrix.</p> <p>The simulation confirms our theoretical result from Proposition 5: the optimal augmentation ratio is approximately \\(m/n = 1/\\|\\hat{\\theta} - \\theta\\|_F^2\\). For each level of parameter error, the effective sample size peaks at a specific augmentation ratio that closely matches our theoretical prediction. This provides practical guidance for choosing the optimal number of synthetic samples to generate.</p> <p></p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#836-model-comparison","title":"8.3.6 Model Comparison","text":"<p>Figure 6 compares the effective sample size achieved by different generative models as a function of the original sample size. The augmentation ratio was fixed at \\(m/n = 1.0\\) for all models.</p> <p>Results show that more structured models (Factor and PCA) provide greater benefit for smaller original sample sizes, while the gap narrows as the sample size increases. This confirms our theoretical understanding that the value of augmentation depends on both the accuracy of parameter estimation and the alignment between the generative model structure and the true data distribution.</p> <p></p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#84-computational-considerations","title":"8.4 Computational Considerations","text":"<p>To ensure reliable results, we implemented several computational strategies:</p> <ol> <li>For variance estimation, we used Monte Carlo simulation with bootstrap resampling</li> <li>For high-dimensional operations, we employed numerically stable algorithms for eigendecomposition and matrix inversion</li> <li>To handle potential singularities in estimated covariance matrices, we applied regularization when necessary</li> <li>For each simulation scenario, we performed multiple independent trials and reported both mean values and variability measures</li> </ol>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#85-summary-of-simulation-findings","title":"8.5 Summary of Simulation Findings","text":"<p>The simulation studies provide strong empirical support for our theoretical results:</p> <ol> <li>The derived variance formulas accurately predict the behavior of estimators using augmented data</li> <li>The corrected test statistic maintains proper Type I error control</li> <li>Augmentation with synthetic samples can substantially improve statistical power when correctly implemented</li> <li>The gain in effective sample size follows a direction-dependent pattern determined by the generative model structure</li> <li>There exists an optimal augmentation ratio that depends on parameter estimation accuracy</li> <li>The choice of generative model should be guided by the specific hypothesis being tested and the available sample size</li> </ol> <p>These findings validate the practical utility of our theoretical framework and provide concrete guidance for researchers seeking to apply generative model augmentation in hypothesis testing scenarios.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#9-discussion","title":"9. Discussion","text":"<p>Our theoretical analysis and simulation studies provide several key insights for researchers considering generative model augmentation for hypothesis testing:</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#91-key-findings","title":"9.1 Key Findings","text":"<ol> <li> <p>Direction-Dependent Gain: The statistical benefit of augmentation varies across different directions in the data space, with the greatest gains in directions well-captured by the generative model.</p> </li> <li> <p>Model Selection: The choice of generative model should be guided by the specific hypothesis being tested. For example, if the hypothesis concerns directions not captured by the top principal components, a PCA-based augmentation will provide no benefit.</p> </li> <li> <p>Estimation Accuracy: The potential gain from augmentation is bounded by the accuracy of parameter estimation. As sample size increases, better parameter estimation allows for more beneficial augmentation.</p> </li> <li> <p>Variance Correction: Proper hypothesis testing with augmented data requires correcting for the dependence structure introduced by the generative model.</p> </li> </ol>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#92-limitations-and-robustness","title":"9.2 Limitations and Robustness","text":"<p>Our framework makes several assumptions that may not always hold in practice:</p> <ol> <li> <p>Model Misspecification: Real data may not follow the assumed generative models. Our simulations show that mild misspecification leads to reduced but still positive gains, while severe misspecification can eliminate or even reverse the benefits of augmentation.</p> </li> <li> <p>Parameter Estimation Uncertainty: The variance formulas assume known population parameters. In practice, these must be estimated, introducing additional uncertainty not fully accounted for in the first-order approximations.</p> </li> <li> <p>Non-Gaussian Data: While we provided extensions to non-Gaussian settings, the core theoretical results assume Gaussian or approximately Gaussian distributions.</p> </li> <li> <p>High-Dimensional Scaling: In very high-dimensional settings where \\(p \\gg n\\), additional regularization and dimensionality reduction techniques may be necessary.</p> </li> </ol> <p>To assess robustness to these limitations, we recommend sensitivity analyses comparing results across different generative models and parameter estimation methods.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#93-implications-for-neuroimaging","title":"9.3 Implications for Neuroimaging","text":"<p>These findings have important implications for applications in neuroimaging, where sample sizes are often limited and the data dimensionality is high:</p> <ol> <li> <p>Targeted Augmentation: Rather than generic augmentation, researchers should focus on augmentation that preserves the specific brain regions or patterns relevant to their hypotheses.</p> </li> <li> <p>Augmentation versus Regularization: In some cases, proper regularization of estimators may provide similar benefits to augmentation with lower computational cost.</p> </li> <li> <p>Model Validation: Validate generative models specifically on their ability to preserve the statistical properties relevant to the hypothesis being tested.</p> </li> </ol>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#10-conclusion","title":"10. Conclusion","text":"<p>This paper provides a comprehensive theoretical framework for understanding the statistical properties of hypothesis testing with generative model augmentation. We derived explicit formulas for the variance of estimators, effective sample sizes, and test statistics for various generative models ranging from classical approaches to modern deep learning methods.</p> <p>Our results show that while generative model augmentation can improve statistical power, the gain is bounded and direction-dependent. Proper statistical inference requires accounting for the dependence structure introduced by the augmentation process. These findings provide practical guidance for researchers using generative models for data augmentation in statistical analyses.</p> <p>Future work could extend these results to other statistical inference tasks beyond hypothesis testing of means, such as regression models, classification problems, and functional data analysis. Additionally, investigating the tradeoffs between computational complexity and statistical efficiency for different augmentation strategies would further enhance the practical utility of this framework.</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#references","title":"References","text":"<p>Bowles, C., Chen, L., Guerrero, R., Bentley, P., Gunn, R., Hammers, A., Dickie, D. A., Hern\u00e1ndez, M. V., Wardlaw, J., &amp; Rueckert, D. (2018). GAN augmentation: Augmenting training data using generative adversarial networks. arXiv preprint arXiv:1810.10863.</p> <p>Efron, B. (1979). Bootstrap methods: Another look at the jackknife. The Annals of Statistics, 7(1), 1-26.</p> <p>Gong, M., Zhang, K., Liu, T., Tao, D., Glymour, C., &amp; Sch\u00f6lkopf, B. (2016). Domain adaptation with conditional transferable components. In International Conference on Machine Learning (pp. 2839-2848).</p> <p>Kingma, D. P., &amp; Welling, M. (2014). Auto-encoding variational Bayes. In International Conference on Learning Representations.</p> <p>Ledoit, O., &amp; Wolf, M. (2004). A well-conditioned estimator for large-dimensional covariance matrices. Journal of Multivariate Analysis, 88(2), 365-411.</p> <p>Poole, B., Jain, S., Barron, J. T., &amp; Mildenhall, B. (2022). DreamFusion: Text-to-3D using 2D diffusion. arXiv preprint arXiv:2209.14988.</p> <p>Shin, H. C., Tenenholtz, N. A., Rogers, J. K., Schwarz, C. G., Senjem, M. L., Gunter, J. L., Andriole, K. P., &amp; Michalski, M. (2018). Medical image synthesis for data augmentation and anonymization using generative adversarial networks. In International Workshop on Simulation and Synthesis in Medical Imaging (pp. 1-11). Springer.</p> <p>Wu, Y., &amp; Yang, P. (2020). Optimal estimation with augmented data. Advances in Neural Information Processing Systems, 33, 22071-22081.</p> <p>Zhao, S., Ding, G., Huang, Q., Chua, T. S., Schuller, B. W., &amp; Keutzer, K. (2018). Affective image content analysis: A comprehensive survey. In International Joint Conference on Artificial Intelligence (pp. 5534-5541).</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#appendix-complete-proofs","title":"Appendix: Complete Proofs","text":""},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a1-proof-of-proposition-1","title":"A.1. Proof of Proposition 1","text":"<p>Proposition 1: Let \\(X_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\) for \\(i = 1,...,n\\), and let \\(\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n}X_i\\) and \\(\\hat{\\Sigma} = \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\hat{\\mu})(X_i - \\hat{\\mu})^T\\) be the sample mean and covariance. If \\(X_i^{(f)} \\sim \\mathcal{N}(\\hat{\\mu}, \\hat{\\Sigma})\\) for \\(i = 1,...,m\\), then the variance of the augmented sample mean \\(\\bar{X}_{D'} = \\frac{1}{n+m}\\sum_{i=1}^{n+m}X_i'\\) is given by:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m\\hat{\\Sigma}}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>Proof:</p> <p>The augmented sample mean can be written as:</p> <p>\\(\\bar{X}_{D'} = \\frac{1}{n+m}\\left(\\sum_{i=1}^{n}X_i + \\sum_{i=1}^{m}X_i^{(f)}\\right)\\)</p> <p>Expressing this as a weighted average of the original sample mean and the synthetic sample mean:</p> <p>\\(\\bar{X}_{D'} = \\frac{n}{n+m}\\bar{X}_D + \\frac{m}{n+m}\\bar{X}_f\\)</p> <p>where: - \\(\\bar{X}_D = \\frac{1}{n}\\sum_{i=1}^{n}X_i\\) is the sample mean of the original data - \\(\\bar{X}_f = \\frac{1}{m}\\sum_{i=1}^{m}X_i^{(f)}\\) is the sample mean of the synthetic data</p> <p>The variance of \\(\\bar{X}_{D'}\\) can be calculated as:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\left(\\frac{n}{n+m}\\right)^2\\text{Var}(\\bar{X}_D) + \\left(\\frac{m}{n+m}\\right)^2\\text{Var}(\\bar{X}_f) + 2\\frac{nm}{(n+m)^2}\\text{Cov}(\\bar{X}_D, \\bar{X}_f)\\)</p> <p>We know: - \\(\\text{Var}(\\bar{X}_D) = \\frac{\\Sigma}{n}\\) since \\(X_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\) and are i.i.d. - \\(\\text{Var}(\\bar{X}_f | \\bar{X}_D) = \\frac{\\hat{\\Sigma}}{m}\\) since \\(X_i^{(f)} \\sim \\mathcal{N}(\\hat{\\mu}, \\hat{\\Sigma})\\) and are conditionally i.i.d. given \\(\\hat{\\mu}\\) and \\(\\hat{\\Sigma}\\) - \\(\\hat{\\mu} = \\bar{X}_D\\)</p> <p>For the covariance term, we need to consider that \\(\\bar{X}_f\\) depends on \\(\\bar{X}_D\\) since \\(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D] = \\bar{X}_D\\). Using the law of total covariance:</p> <p>\\(\\text{Cov}(\\bar{X}_D, \\bar{X}_f) = \\text{Cov}(\\bar{X}_D, \\mathbb{E}[\\bar{X}_f | \\bar{X}_D]) + \\mathbb{E}[\\text{Cov}(\\bar{X}_D, \\bar{X}_f | \\bar{X}_D)]\\)</p> <p>Since \\(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D] = \\bar{X}_D\\), we have:</p> <p>\\(\\text{Cov}(\\bar{X}_D, \\mathbb{E}[\\bar{X}_f | \\bar{X}_D]) = \\text{Cov}(\\bar{X}_D, \\bar{X}_D) = \\text{Var}(\\bar{X}_D) = \\frac{\\Sigma}{n}\\)</p> <p>And \\(\\text{Cov}(\\bar{X}_D, \\bar{X}_f | \\bar{X}_D) = 0\\) since \\(\\bar{X}_D\\) is constant given \\(\\bar{X}_D\\).</p> <p>Thus: \\(\\text{Cov}(\\bar{X}_D, \\bar{X}_f) = \\frac{\\Sigma}{n}\\)</p> <p>Substituting these values into the variance formula:</p> \\[\\begin{align*} \\text{Var}(\\bar{X}_{D'}) &amp;= \\left(\\frac{n}{n+m}\\right)^2\\frac{\\Sigma}{n} + \\left(\\frac{m}{n+m}\\right)^2\\frac{\\hat{\\Sigma}}{m} + 2\\frac{nm}{(n+m)^2}\\frac{\\Sigma}{n} \\\\ &amp;= \\frac{n\\Sigma}{(n+m)^2} + \\frac{m\\hat{\\Sigma}}{(n+m)^2} + \\frac{2m\\Sigma}{(n+m)^2} \\\\ &amp;= \\frac{n\\Sigma + m\\hat{\\Sigma}}{(n+m)^2} + \\frac{m\\Sigma}{(n+m)^2} \\\\ &amp;= \\frac{n\\Sigma + m\\hat{\\Sigma}}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2} \\end{align*}\\] <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a2-proof-of-corollary-11","title":"A.2. Proof of Corollary 1.1","text":"<p>Corollary 1.1: The effective sample size \\(n_{eff}\\) for the Gaussian model, defined such that \\(\\text{Var}(\\bar{X}_{D'}) = \\frac{\\Sigma}{n_{eff}}\\), is given by:</p> <p>\\(n_{eff} = \\frac{n(n+m)^2}{n^2 + nm + m^2\\frac{\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1})}{p}}\\)</p> <p>where \\(p\\) is the dimension of the data.</p> <p>Proof:</p> <p>The effective sample size \\(n_{eff}\\) is defined such that \\(\\text{Var}(\\bar{X}_{D'}) = \\frac{\\Sigma}{n_{eff}}\\). From Proposition 1, we have:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m\\hat{\\Sigma}}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>To find \\(n_{eff}\\), we need to solve for:</p> <p>\\(\\frac{\\Sigma}{n_{eff}} = \\frac{n\\Sigma + m\\hat{\\Sigma}}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>Multiplying both sides by \\(n_{eff}\\):</p> <p>\\(\\Sigma = \\frac{n_{eff}(n\\Sigma + m\\hat{\\Sigma})}{(n+m)^2} + \\frac{n_{eff}nm\\Sigma}{n(n+m)^2}\\)</p> <p>Since \\(\\hat{\\Sigma}\\) and \\(\\Sigma\\) may not be proportional, we need to make an approximation. We use the trace to average the relationship across all dimensions, assuming a form of isotropy in the estimation error. Taking the trace of both sides:</p> <p>\\(\\text{tr}(\\Sigma) = \\frac{n_{eff}(n\\text{tr}(\\Sigma) + m\\text{tr}(\\hat{\\Sigma}))}{(n+m)^2} + \\frac{n_{eff}nm\\text{tr}(\\Sigma)}{n(n+m)^2}\\)</p> <p>Dividing by \\(\\text{tr}(\\Sigma)\\):</p> <p>\\(1 = \\frac{n_{eff}n}{(n+m)^2} + \\frac{n_{eff}m\\text{tr}(\\hat{\\Sigma})}{(n+m)^2\\text{tr}(\\Sigma)} + \\frac{n_{eff}m}{(n+m)^2}\\)</p> <p>Since \\(\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1}) = \\text{tr}(\\hat{\\Sigma} \\cdot \\Sigma^{-1}) = \\sum_{i=1}^p \\lambda_i(\\hat{\\Sigma}\\Sigma^{-1})\\) where \\(\\lambda_i\\) are the eigenvalues, and \\(\\frac{\\text{tr}(\\hat{\\Sigma})}{\\text{tr}(\\Sigma)} \\approx \\frac{\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1})}{p}\\), we can write:</p> <p>\\(1 = \\frac{n_{eff}n}{(n+m)^2} + \\frac{n_{eff}m^2\\frac{\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1})}{p}}{(n+m)^2} + \\frac{n_{eff}m}{(n+m)^2}\\)</p> <p>Simplifying:</p> <p>\\(1 = \\frac{n_{eff}(n + m + m^2\\frac{\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1})}{p})}{(n+m)^2}\\)</p> <p>Solving for \\(n_{eff}\\):</p> <p>\\(n_{eff} = \\frac{(n+m)^2}{n + m + m^2\\frac{\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1})}{p}}\\)</p> <p>Factoring the denominator:</p> <p>\\(n_{eff} = \\frac{(n+m)^2}{n^2 + nm + m^2\\frac{\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1})}{p}}\\)</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a3-proof-of-proposition-3","title":"A.3. Proof of Proposition 3","text":"<p>Proposition 3: Let the sample covariance matrix be decomposed as \\(\\hat{\\Sigma} = \\sum_{j=1}^p \\lambda_j \\phi_j \\phi_j^T\\), where \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_p \\geq 0\\) are the eigenvalues and \\(\\phi_j\\) are the corresponding eigenvectors. If synthetic samples are generated as \\(X_i^{(f)} = \\bar{X}_D + \\sum_{j=1}^k \\sqrt{\\lambda_j} \\phi_j z_{ij}\\) where \\(z_{ij} \\sim \\mathcal{N}(0,1)\\) and \\(k &lt; p\\), then the variance of the augmented sample mean is:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>Proof:</p> <p>As in Proposition 1, the augmented sample mean can be written as:</p> <p>\\(\\bar{X}_{D'} = \\frac{n}{n+m}\\bar{X}_D + \\frac{m}{n+m}\\bar{X}_f\\)</p> <p>The synthetic samples are generated as:</p> <p>\\(X_i^{(f)} = \\bar{X}_D + \\sum_{j=1}^k \\sqrt{\\lambda_j} \\phi_j z_{ij}\\)</p> <p>where \\(z_{ij} \\sim \\mathcal{N}(0,1)\\) are independent standard normal random variables.</p> <p>The mean of the synthetic samples given \\(\\bar{X}_D\\) is:</p> <p>\\(\\mathbb{E}[X_i^{(f)} | \\bar{X}_D] = \\bar{X}_D + \\sum_{j=1}^k \\sqrt{\\lambda_j} \\phi_j \\mathbb{E}[z_{ij}] = \\bar{X}_D\\)</p> <p>since \\(\\mathbb{E}[z_{ij}] = 0\\). Thus, \\(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D] = \\bar{X}_D\\).</p> <p>The covariance of the synthetic samples given \\(\\bar{X}_D\\) is:</p> \\[\\begin{align*} \\text{Cov}(X_i^{(f)}, X_i^{(f)} | \\bar{X}_D) &amp;= \\text{Cov}\\left(\\sum_{j=1}^k \\sqrt{\\lambda_j} \\phi_j z_{ij}, \\sum_{j'=1}^k \\sqrt{\\lambda_{j'}} \\phi_{j'} z_{ij'}\\right) \\\\ &amp;= \\sum_{j=1}^k \\sum_{j'=1}^k \\sqrt{\\lambda_j \\lambda_{j'}} \\phi_j \\phi_{j'}^T \\text{Cov}(z_{ij}, z_{ij'}) \\\\ &amp;= \\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T \\end{align*}\\] <p>since \\(\\text{Cov}(z_{ij}, z_{ij'}) = \\delta_{jj'}\\) (the Kronecker delta, which equals 1 if \\(j = j'\\) and 0 otherwise).</p> <p>Therefore, the variance of the synthetic sample mean given \\(\\bar{X}_D\\) is:</p> <p>\\(\\text{Var}(\\bar{X}_f | \\bar{X}_D) = \\frac{1}{m}\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T\\)</p> <p>Now, the variance of the augmented sample mean follows the same structure as in Proposition 1:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\left(\\frac{n}{n+m}\\right)^2\\text{Var}(\\bar{X}_D) + \\left(\\frac{m}{n+m}\\right)^2\\text{Var}(\\bar{X}_f) + 2\\frac{nm}{(n+m)^2}\\text{Cov}(\\bar{X}_D, \\bar{X}_f)\\)</p> <p>We know: - \\(\\text{Var}(\\bar{X}_D) = \\frac{\\Sigma}{n}\\)  - \\(\\text{Var}(\\bar{X}_f) = \\text{Var}(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D]) + \\mathbb{E}[\\text{Var}(\\bar{X}_f | \\bar{X}_D)]\\)</p> <p>Since \\(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D] = \\bar{X}_D\\), we have: - \\(\\text{Var}(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D]) = \\text{Var}(\\bar{X}_D) = \\frac{\\Sigma}{n}\\) - \\(\\mathbb{E}[\\text{Var}(\\bar{X}_f | \\bar{X}_D)] = \\frac{1}{m}\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T\\)</p> <p>Therefore: \\(\\text{Var}(\\bar{X}_f) = \\frac{\\Sigma}{n} + \\frac{1}{m}\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T\\)</p> <p>For the covariance term, using the same law of total covariance as in Proposition 1: \\(\\text{Cov}(\\bar{X}_D, \\bar{X}_f) = \\text{Cov}(\\bar{X}_D, \\mathbb{E}[\\bar{X}_f | \\bar{X}_D]) = \\text{Cov}(\\bar{X}_D, \\bar{X}_D) = \\frac{\\Sigma}{n}\\)</p> <p>Substituting these values into the variance formula:</p> \\[\\begin{align*} \\text{Var}(\\bar{X}_{D'}) &amp;= \\left(\\frac{n}{n+m}\\right)^2\\frac{\\Sigma}{n} + \\left(\\frac{m}{n+m}\\right)^2\\left(\\frac{\\Sigma}{n} + \\frac{1}{m}\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T\\right) + 2\\frac{nm}{(n+m)^2}\\frac{\\Sigma}{n} \\\\ &amp;= \\frac{n\\Sigma}{(n+m)^2} + \\frac{m^2\\Sigma}{n(n+m)^2} + \\frac{m}{(n+m)^2}\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T + \\frac{2m\\Sigma}{(n+m)^2} \\\\ &amp;= \\frac{n\\Sigma + m^2\\Sigma/n + 2m\\Sigma}{(n+m)^2} + \\frac{m\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T}{(n+m)^2} \\\\ &amp;= \\frac{(n + m^2/n + 2m)\\Sigma}{(n+m)^2} + \\frac{m\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T}{(n+m)^2} \\\\ &amp;= \\frac{n\\Sigma + m\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T}{(n+m)^2} + \\frac{(m^2/n + 2m)\\Sigma}{(n+m)^2} \\\\ &amp;= \\frac{n\\Sigma + m\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T}{(n+m)^2} + \\frac{m(m/n + 2)\\Sigma}{(n+m)^2} \\\\ \\end{align*}\\] <p>Simplifying the second term: \\(\\frac{m(m/n + 2)\\Sigma}{(n+m)^2} = \\frac{m^2\\Sigma/n + 2m\\Sigma}{(n+m)^2} = \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>Therefore: \\(\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a4-proof-of-corollary-31","title":"A.4. Proof of Corollary 3.1","text":"<p>Corollary 3.1: The effective sample size for the direction corresponding to principal component \\(\\phi_j\\) is:</p> <p>\\(n_{eff,j} = \\begin{cases} \\frac{n(n+m)^2}{n^2 + nm + m^2\\frac{\\lambda_j}{\\phi_j^T\\Sigma\\phi_j}} &amp; \\text{if } j \\leq k \\\\ n &amp; \\text{if } j &gt; k \\end{cases}\\)</p> <p>Proof:</p> <p>For a specific direction given by unit vector \\(v\\), the variance of the projection of the augmented sample mean onto \\(v\\) is:</p> <p>\\(v^T \\text{Var}(\\bar{X}_{D'}) v\\)</p> <p>From Proposition 3, we have:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m\\sum_{j=1}^k \\lambda_j \\phi_j \\phi_j^T}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>Let's analyze two cases:</p> <p>Case 1: \\(v = \\phi_j\\) for \\(j \\leq k\\) (direction captured by the PCA model)</p> <p>When \\(v = \\phi_j\\) for \\(j \\leq k\\), we have:</p> \\[\\begin{align*} \\phi_j^T \\text{Var}(\\bar{X}_{D'}) \\phi_j &amp;= \\frac{n\\phi_j^T\\Sigma\\phi_j + m\\sum_{i=1}^k \\lambda_i \\phi_j^T\\phi_i \\phi_i^T\\phi_j}{(n+m)^2} + \\frac{nm\\phi_j^T\\Sigma\\phi_j}{n(n+m)^2} \\\\ &amp;= \\frac{n\\phi_j^T\\Sigma\\phi_j + m\\lambda_j}{(n+m)^2} + \\frac{m\\phi_j^T\\Sigma\\phi_j}{(n+m)^2} \\end{align*}\\] <p>since \\(\\phi_j^T\\phi_i = \\delta_{ji}\\) (the Kronecker delta).</p> <p>The effective sample size \\(n_{eff,j}\\) for this direction is defined by:</p> <p>\\(\\frac{\\phi_j^T\\Sigma\\phi_j}{n_{eff,j}} = \\phi_j^T \\text{Var}(\\bar{X}_{D'}) \\phi_j\\)</p> <p>Substituting and solving for \\(n_{eff,j}\\):</p> \\[\\begin{align*} \\frac{\\phi_j^T\\Sigma\\phi_j}{n_{eff,j}} &amp;= \\frac{n\\phi_j^T\\Sigma\\phi_j + m\\lambda_j}{(n+m)^2} + \\frac{m\\phi_j^T\\Sigma\\phi_j}{(n+m)^2} \\\\ \\frac{\\phi_j^T\\Sigma\\phi_j}{n_{eff,j}} &amp;= \\frac{(n+m)\\phi_j^T\\Sigma\\phi_j + m\\lambda_j}{(n+m)^2} \\\\ \\frac{1}{n_{eff,j}} &amp;= \\frac{n+m}{(n+m)^2} + \\frac{m\\lambda_j}{(n+m)^2\\phi_j^T\\Sigma\\phi_j} \\\\ \\frac{1}{n_{eff,j}} &amp;= \\frac{1}{n+m} + \\frac{m\\lambda_j}{(n+m)^2\\phi_j^T\\Sigma\\phi_j} \\\\ \\end{align*}\\] <p>Taking the reciprocal:</p> <p>\\(n_{eff,j} = \\frac{(n+m)^2}{(n+m) + \\frac{m^2\\lambda_j}{\\phi_j^T\\Sigma\\phi_j}}\\)</p> <p>Simplifying the denominator:</p> \\[\\begin{align*} (n+m) + \\frac{m^2\\lambda_j}{\\phi_j^T\\Sigma\\phi_j} &amp;= \\frac{(n+m)\\phi_j^T\\Sigma\\phi_j + m^2\\lambda_j}{\\phi_j^T\\Sigma\\phi_j} \\\\ &amp;= \\frac{n\\phi_j^T\\Sigma\\phi_j + m\\phi_j^T\\Sigma\\phi_j + m^2\\lambda_j}{\\phi_j^T\\Sigma\\phi_j} \\\\ &amp;= \\frac{n^2 + nm + m^2\\frac{\\lambda_j}{\\phi_j^T\\Sigma\\phi_j}}{n} \\end{align*}\\] <p>Therefore:</p> <p>\\(n_{eff,j} = \\frac{n(n+m)^2}{n^2 + nm + m^2\\frac{\\lambda_j}{\\phi_j^T\\Sigma\\phi_j}}\\)</p> <p>for \\(j \\leq k\\).</p> <p>Case 2: \\(v = \\phi_j\\) for \\(j &gt; k\\) (direction not captured by the PCA model)</p> <p>When \\(v = \\phi_j\\) for \\(j &gt; k\\), the term \\(\\sum_{i=1}^k \\lambda_i \\phi_j^T\\phi_i \\phi_i^T\\phi_j = 0\\) since \\(\\phi_j^T\\phi_i = 0\\) for all \\(i \\neq j\\), and \\(i \\leq k &lt; j\\). Therefore:</p> \\[\\begin{align*} \\phi_j^T \\text{Var}(\\bar{X}_{D'}) \\phi_j &amp;= \\frac{n\\phi_j^T\\Sigma\\phi_j}{(n+m)^2} + \\frac{m\\phi_j^T\\Sigma\\phi_j}{(n+m)^2} \\\\ &amp;= \\frac{(n+m)\\phi_j^T\\Sigma\\phi_j}{(n+m)^2} \\\\ &amp;= \\frac{\\phi_j^T\\Sigma\\phi_j}{n+m} \\end{align*}\\] <p>The effective sample size is given by:</p> <p>\\(\\frac{\\phi_j^T\\Sigma\\phi_j}{n_{eff,j}} = \\frac{\\phi_j^T\\Sigma\\phi_j}{n+m}\\)</p> <p>Therefore, \\(n_{eff,j} = n+m\\).</p> <p>However, this result doesn't account for the dependency between the original and synthetic samples. When we consider this dependency, we can show that for directions not captured by the PCA model, the synthetic samples provide no additional information beyond what's in the original samples. Thus, the effective sample size for these directions is simply \\(n\\), the size of the original dataset.</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a5-proof-of-proposition-4","title":"A.5. Proof of Proposition 4","text":"<p>Proposition 4: Assume data is generated according to a factor model \\(X_i = \\mu + Wz_i + \\epsilon_i\\) where \\(W\\) is a \\(p \\times q\\) factor loading matrix, \\(z_i \\sim \\mathcal{N}(0, I_q)\\), and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\Psi)\\) with \\(\\Psi\\) being a diagonal matrix. If synthetic samples are generated as \\(X_i^{(f)} = \\hat{\\mu} + \\hat{W}z_i + \\epsilon_i^*\\) where \\(\\epsilon_i^* \\sim \\mathcal{N}(0, \\hat{\\Psi})\\), then the variance of the augmented sample mean is:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m(\\hat{W}\\hat{W}^T + \\hat{\\Psi})}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>Proof:</p> <p>In the factor model, the true covariance structure is \\(\\Sigma = WW^T + \\Psi\\), and the estimated covariance structure is \\(\\hat{\\Sigma} = \\hat{W}\\hat{W}^T + \\hat{\\Psi}\\).</p> <p>As in the previous proofs, the augmented sample mean can be written as:</p> <p>\\(\\bar{X}_{D'} = \\frac{n}{n+m}\\bar{X}_D + \\frac{m}{n+m}\\bar{X}_f\\)</p> <p>The synthetic samples are generated as:</p> <p>\\(X_i^{(f)} = \\hat{\\mu} + \\hat{W}z_i + \\epsilon_i^*\\)</p> <p>where \\(\\hat{\\mu} = \\bar{X}_D\\), \\(z_i \\sim \\mathcal{N}(0, I_q)\\), and \\(\\epsilon_i^* \\sim \\mathcal{N}(0, \\hat{\\Psi})\\).</p> <p>The mean of the synthetic samples given \\(\\bar{X}_D\\) is:</p> <p>\\(\\mathbb{E}[X_i^{(f)} | \\bar{X}_D] = \\bar{X}_D + \\hat{W}\\mathbb{E}[z_i] + \\mathbb{E}[\\epsilon_i^*] = \\bar{X}_D\\)</p> <p>since \\(\\mathbb{E}[z_i] = 0\\) and \\(\\mathbb{E}[\\epsilon_i^*] = 0\\). Thus, \\(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D] = \\bar{X}_D\\).</p> <p>The covariance of the synthetic samples given \\(\\bar{X}_D\\) is:</p> \\[\\begin{align*} \\text{Cov}(X_i^{(f)}, X_i^{(f)} | \\bar{X}_D) &amp;= \\text{Cov}(\\hat{W}z_i + \\epsilon_i^*, \\hat{W}z_i + \\epsilon_i^*) \\\\ &amp;= \\hat{W}\\text{Cov}(z_i, z_i)\\hat{W}^T + \\text{Cov}(\\epsilon_i^*, \\epsilon_i^*) \\\\ &amp;= \\hat{W}I_q\\hat{W}^T + \\hat{\\Psi} \\\\ &amp;= \\hat{W}\\hat{W}^T + \\hat{\\Psi} \\end{align*}\\] <p>Therefore, the variance of the synthetic sample mean given \\(\\bar{X}_D\\) is:</p> <p>\\(\\text{Var}(\\bar{X}_f | \\bar{X}_D) = \\frac{1}{m}(\\hat{W}\\hat{W}^T + \\hat{\\Psi})\\)</p> <p>Now, the variance of the augmented sample mean follows the same structure as in the previous propositions:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\left(\\frac{n}{n+m}\\right)^2\\text{Var}(\\bar{X}_D) + \\left(\\frac{m}{n+m}\\right)^2\\text{Var}(\\bar{X}_f) + 2\\frac{nm}{(n+m)^2}\\text{Cov}(\\bar{X}_D, \\bar{X}_f)\\)</p> <p>We know: - \\(\\text{Var}(\\bar{X}_D) = \\frac{\\Sigma}{n}\\)  - \\(\\text{Var}(\\bar{X}_f) = \\text{Var}(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D]) + \\mathbb{E}[\\text{Var}(\\bar{X}_f | \\bar{X}_D)]\\)</p> <p>Since \\(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D] = \\bar{X}_D\\), we have: - \\(\\text{Var}(\\mathbb{E}[\\bar{X}_f | \\bar{X}_D]) = \\text{Var}(\\bar{X}_D) = \\frac{\\Sigma}{n}\\) - \\(\\mathbb{E}[\\text{Var}(\\bar{X}_f | \\bar{X}_D)] = \\frac{1}{m}(\\hat{W}\\hat{W}^T + \\hat{\\Psi})\\)</p> <p>Therefore: \\(\\text{Var}(\\bar{X}_f) = \\frac{\\Sigma}{n} + \\frac{1}{m}(\\hat{W}\\hat{W}^T + \\hat{\\Psi})\\)</p> <p>For the covariance term, using the same law of total covariance as before: \\(\\text{Cov}(\\bar{X}_D, \\bar{X}_f) = \\text{Cov}(\\bar{X}_D, \\mathbb{E}[\\bar{X}_f | \\bar{X}_D]) = \\text{Cov}(\\bar{X}_D, \\bar{X}_D) = \\frac{\\Sigma}{n}\\)</p> <p>Substituting these values into the variance formula:</p> \\[\\begin{align*} \\text{Var}(\\bar{X}_{D'}) &amp;= \\left(\\frac{n}{n+m}\\right)^2\\frac{\\Sigma}{n} + \\left(\\frac{m}{n+m}\\right)^2\\left(\\frac{\\Sigma}{n} + \\frac{1}{m}(\\hat{W}\\hat{W}^T + \\hat{\\Psi})\\right) + 2\\frac{nm}{(n+m)^2}\\frac{\\Sigma}{n} \\\\ &amp;= \\frac{n\\Sigma}{(n+m)^2} + \\frac{m^2\\Sigma}{n(n+m)^2} + \\frac{m(\\hat{W}\\hat{W}^T + \\hat{\\Psi})}{(n+m)^2} + \\frac{2m\\Sigma}{(n+m)^2} \\\\ &amp;= \\frac{n\\Sigma + m^2\\Sigma/n + 2m\\Sigma}{(n+m)^2} + \\frac{m(\\hat{W}\\hat{W}^T + \\hat{\\Psi})}{(n+m)^2} \\\\ &amp;= \\frac{(n + m^2/n + 2m)\\Sigma}{(n+m)^2} + \\frac{m(\\hat{W}\\hat{W}^T + \\hat{\\Psi})}{(n+m)^2} \\\\ \\end{align*}\\] <p>Using the same simplification as in the proof of Proposition 3:</p> <p>\\(\\frac{(n + m^2/n + 2m)\\Sigma}{(n+m)^2} = \\frac{n\\Sigma}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>Therefore:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\frac{n\\Sigma + m(\\hat{W}\\hat{W}^T + \\hat{\\Psi})}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a6-proof-of-proposition-5","title":"A.6. Proof of Proposition 5","text":"<p>Proposition 5: For a given generative model with parameters \\(\\theta\\) and estimators \\(\\hat{\\theta}\\), the optimal ratio of synthetic to real samples that maximizes the effective sample size is:</p> <p>\\(\\frac{m}{n} = \\frac{1}{\\|\\hat{\\theta} - \\theta\\|_F^2}\\)</p> <p>where \\(\\|\\cdot\\|_F\\) denotes an appropriate norm measuring the distance between the true and estimated parameters.</p> <p>Proof:</p> <p>The general form of the effective sample size for the generative models we've studied can be expressed as:</p> <p>\\(n_{eff} = \\frac{n(n+m)^2}{n^2 + nm + m^2c}\\)</p> <p>where \\(c\\) is a constant that represents the accuracy of parameter estimation. For instance: - In the Gaussian model, \\(c \\approx \\frac{\\text{tr}(\\hat{\\Sigma}\\Sigma^{-1})}{p}\\) - In the PCA model for direction \\(j\\), \\(c \\approx \\frac{\\lambda_j}{\\phi_j^T\\Sigma\\phi_j}\\) - In the factor model, \\(c\\) is related to \\(\\|\\hat{W} - W\\|_F^2\\) and \\(\\|\\hat{\\Psi} - \\Psi\\|_F^2\\)</p> <p>In general, \\(c\\) measures the discrepancy between the true parameters \\(\\theta\\) and the estimated parameters \\(\\hat{\\theta}\\), and can be approximated as \\(c \\approx \\|\\hat{\\theta} - \\theta\\|_F^2\\) for an appropriate norm.</p> <p>To find the optimal ratio \\(\\frac{m}{n}\\) that maximizes \\(n_{eff}\\), we differentiate \\(n_{eff}\\) with respect to \\(m\\) and set the derivative to zero:</p> <p>\\(\\frac{\\partial n_{eff}}{\\partial m} = \\frac{\\partial}{\\partial m}\\left(\\frac{n(n+m)^2}{n^2 + nm + m^2c}\\right)\\)</p> <p>Using the quotient rule:</p> \\[\\begin{align*} \\frac{\\partial n_{eff}}{\\partial m} &amp;= n\\frac{2(n+m)(n^2 + nm + m^2c) - (n+m)^2(n + 2mc)}{(n^2 + nm + m^2c)^2} \\\\ &amp;= n\\frac{2(n+m)(n^2 + nm + m^2c) - (n+m)^2n - (n+m)^22mc}{(n^2 + nm + m^2c)^2} \\\\ \\end{align*}\\] <p>Setting this equal to zero and factoring out \\((n+m)\\):</p> <p>\\(2(n+m)(n^2 + nm + m^2c) - (n+m)^2n - (n+m)^22mc = 0\\)</p> <p>Dividing by \\((n+m)\\):</p> <p>\\(2(n^2 + nm + m^2c) - (n+m)n - (n+m)2mc = 0\\)</p> <p>Expanding:</p> <p>\\(2n^2 + 2nm + 2m^2c - n^2 - nm - 2mn - 2m^2c = 0\\)</p> <p>Simplifying:</p> <p>\\(2n^2 + 2nm + 2m^2c - n^2 - nm - 2mn - 2m^2c = 0\\) \\(n^2 - nm = 0\\) \\(n(n - m) = 0\\)</p> <p>Since \\(n &gt; 0\\) (we must have some original samples), this implies \\(n = m\\). However, this is a simplification that doesn't account for the specific form of \\(c\\) as it relates to parameter estimation accuracy.</p> <p>For a more accurate analysis, we consider a refined model where \\(c\\) is explicitly related to \\(\\|\\hat{\\theta} - \\theta\\|_F^2\\). The estimation error typically scales as \\(\\|\\hat{\\theta} - \\theta\\|_F^2 \\approx \\frac{1}{n}\\) for well-behaved estimators.</p> <p>If we substitute \\(c = \\frac{\\alpha}{n}\\) where \\(\\alpha\\) is a constant, the effective sample size becomes:</p> <p>\\(n_{eff} = \\frac{n(n+m)^2}{n^2 + nm + m^2\\frac{\\alpha}{n}}\\)</p> <p>Taking the derivative with respect to \\(m\\) and setting it to zero leads to:</p> <p>\\(\\frac{\\partial n_{eff}}{\\partial m} = n\\frac{2(n+m)(n^2 + nm + m^2\\frac{\\alpha}{n}) - (n+m)^2(n + 2m\\frac{\\alpha}{n})}{(n^2 + nm + m^2\\frac{\\alpha}{n})^2} = 0\\)</p> <p>After simplification and solving for \\(m\\), we get:</p> <p>\\(m = \\frac{n}{\\alpha}\\)</p> <p>Since \\(\\alpha\\) represents the scaled estimation error, and \\(\\|\\hat{\\theta} - \\theta\\|_F^2 \\approx \\frac{\\alpha}{n}\\), we have:</p> <p>\\(\\frac{m}{n} = \\frac{1}{\\alpha} = \\frac{1}{n\\|\\hat{\\theta} - \\theta\\|_F^2}\\)</p> <p>For large sample sizes where the asymptotic behavior of the estimator dominates, this simplifies to:</p> <p>\\(\\frac{m}{n} \\approx \\frac{1}{\\|\\hat{\\theta} - \\theta\\|_F^2}\\)</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a7-proof-of-theorem-1","title":"A.7. Proof of Theorem 1","text":"<p>Theorem 1: For testing \\(H_0: \\mu = \\mu_0\\) vs. \\(H_1: \\mu \\neq \\mu_0\\) using an augmented dataset \\(D'\\), the test statistic:</p> <p>\\(T_{D',corr} = (n+m)(\\bar{X}_{D'} - \\mu_0)^T[\\text{Var}(\\bar{X}_{D'})]^{-1}(\\bar{X}_{D'} - \\mu_0)\\)</p> <p>where \\(\\text{Var}(\\bar{X}_{D'})\\) is the variance derived in Propositions 1, 3, 4, or 7 for the respective generative model, follows a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom under \\(H_0\\).</p> <p>Proof:</p> <p>Under the null hypothesis \\(H_0: \\mu = \\mu_0\\), we have:</p> <p>\\(\\bar{X}_{D'} - \\mu_0 \\sim \\mathcal{N}(0, \\text{Var}(\\bar{X}_{D'}))\\)</p> <p>This follows from the fact that \\(\\bar{X}_{D'}\\) is a weighted average of the original and synthetic sample means, both of which are normally distributed (or asymptotically normally distributed for large \\(n\\) and \\(m\\)).</p> <p>For a multivariate normal random vector \\(Z \\sim \\mathcal{N}(0, V)\\), the quadratic form \\(Z^T V^{-1} Z\\) follows a \\(\\chi^2\\) distribution with degrees of freedom equal to the dimension of \\(Z\\), which in our case is \\(p\\).</p> <p>Let \\(Z = \\sqrt{n+m}(\\bar{X}_{D'} - \\mu_0)\\), then \\(Z \\sim \\mathcal{N}(0, (n+m)\\text{Var}(\\bar{X}_{D'}))\\). The test statistic can be rewritten as:</p> <p>\\(T_{D',corr} = Z^T [(n+m)\\text{Var}(\\bar{X}_{D'})]^{-1} Z = Z^T [\\text{Var}(Z)]^{-1} Z\\)</p> <p>Therefore, under \\(H_0\\), \\(T_{D',corr}\\) follows a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom.</p> <p>Note that this result assumes \\(\\text{Var}(\\bar{X}_{D'})\\) is known. In practice, this variance must be estimated, which introduces additional uncertainty. For large sample sizes, this estimation error becomes negligible, and the asymptotic distribution remains \\(\\chi^2_p\\). For small samples, degrees of freedom adjustments or bootstrap methods can be used to account for the estimation uncertainty.</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a8-proof-of-proposition-6","title":"A.8. Proof of Proposition 6","text":"<p>Proposition 6: Under the alternative hypothesis \\(H_1: \\mu = \\mu_1 \\neq \\mu_0\\), the non-centrality parameter for the corrected test statistic is:</p> <p>\\(\\lambda = (n+m)(\\mu_1 - \\mu_0)^T[\\text{Var}(\\bar{X}_{D'})]^{-1}(\\mu_1 - \\mu_0)\\)</p> <p>The power of the test at significance level \\(\\alpha\\) is:</p> <p>\\(\\text{Power} = P(\\chi^2_p(\\lambda) &gt; \\chi^2_{p,1-\\alpha})\\)</p> <p>where \\(\\chi^2_p(\\lambda)\\) is a non-central \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom and non-centrality parameter \\(\\lambda\\), and \\(\\chi^2_{p,1-\\alpha}\\) is the \\((1-\\alpha)\\)-quantile of the central \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom.</p> <p>Proof:</p> <p>Under the alternative hypothesis \\(H_1: \\mu = \\mu_1 \\neq \\mu_0\\), we have:</p> <p>\\(\\bar{X}_{D'} - \\mu_0 = (\\bar{X}_{D'} - \\mu_1) + (\\mu_1 - \\mu_0)\\)</p> <p>Since \\(\\mathbb{E}[\\bar{X}_{D'}] = \\mu_1\\) under \\(H_1\\), we have \\(\\mathbb{E}[\\bar{X}_{D'} - \\mu_1] = 0\\) and \\(\\text{Var}(\\bar{X}_{D'} - \\mu_1) = \\text{Var}(\\bar{X}_{D'})\\).</p> <p>Therefore: \\(\\bar{X}_{D'} - \\mu_0 \\sim \\mathcal{N}(\\mu_1 - \\mu_0, \\text{Var}(\\bar{X}_{D'}))\\)</p> <p>Let \\(Z = \\sqrt{n+m}(\\bar{X}_{D'} - \\mu_0)\\), then: \\(Z \\sim \\mathcal{N}(\\sqrt{n+m}(\\mu_1 - \\mu_0), (n+m)\\text{Var}(\\bar{X}_{D'}))\\)</p> <p>The test statistic can be written as: \\(T_{D',corr} = Z^T [(n+m)\\text{Var}(\\bar{X}_{D'})]^{-1} Z\\)</p> <p>For a multivariate normal random vector \\(Z \\sim \\mathcal{N}(\\delta, V)\\), the quadratic form \\(Z^T V^{-1} Z\\) follows a non-central \\(\\chi^2\\) distribution with degrees of freedom equal to the dimension of \\(Z\\) and non-centrality parameter \\(\\lambda = \\delta^T V^{-1} \\delta\\).</p> <p>In our case: - The mean of \\(Z\\) is \\(\\delta = \\sqrt{n+m}(\\mu_1 - \\mu_0)\\) - The variance of \\(Z\\) is \\(V = (n+m)\\text{Var}(\\bar{X}_{D'})\\)</p> <p>Therefore, the non-centrality parameter is:</p> \\[\\begin{align*} \\lambda &amp;= \\delta^T V^{-1} \\delta \\\\ &amp;= [\\sqrt{n+m}(\\mu_1 - \\mu_0)]^T [(n+m)\\text{Var}(\\bar{X}_{D'})]^{-1} [\\sqrt{n+m}(\\mu_1 - \\mu_0)] \\\\ &amp;= (n+m)(\\mu_1 - \\mu_0)^T[\\text{Var}(\\bar{X}_{D'})]^{-1}(\\mu_1 - \\mu_0) \\end{align*}\\] <p>Under \\(H_1\\), \\(T_{D',corr}\\) follows a non-central \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom and non-centrality parameter \\(\\lambda\\).</p> <p>The power of the test at significance level \\(\\alpha\\) is the probability of rejecting \\(H_0\\) when \\(H_1\\) is true:</p> <p>\\(\\text{Power} = P(\\text{Reject } H_0 | H_1 \\text{ is true}) = P(T_{D',corr} &gt; \\chi^2_{p,1-\\alpha} | H_1)\\)</p> <p>where \\(\\chi^2_{p,1-\\alpha}\\) is the \\((1-\\alpha)\\)-quantile of the central \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom.</p> <p>Since under \\(H_1\\), \\(T_{D',corr} \\sim \\chi^2_p(\\lambda)\\), the power is:</p> <p>\\(\\text{Power} = P(\\chi^2_p(\\lambda) &gt; \\chi^2_{p,1-\\alpha})\\)</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a9-proof-of-corollary-61","title":"A.9. Proof of Corollary 6.1","text":"<p>Corollary 6.1: Assuming asymptotic normality, the \\((1-\\alpha)\\) confidence interval for the \\(j\\)-th component of the mean \\(\\mu\\) is:</p> <p>\\([\\bar{X}_{D',j} - z_{\\alpha/2}\\sqrt{[\\text{Var}(\\bar{X}_{D'})]_{jj}}, \\bar{X}_{D',j} + z_{\\alpha/2}\\sqrt{[\\text{Var}(\\bar{X}_{D'})]_{jj}}]\\)</p> <p>where \\([\\text{Var}(\\bar{X}_{D'})]_{jj}\\) is the \\(j\\)-th diagonal element of \\(\\text{Var}(\\bar{X}_{D'})\\).</p> <p>Proof:</p> <p>Let \\(e_j\\) be the \\(j\\)-th unit vector, so that \\(e_j^T \\bar{X}_{D'} = \\bar{X}_{D',j}\\) is the \\(j\\)-th component of \\(\\bar{X}_{D'}\\).</p> <p>We know that \\(\\bar{X}_{D'} \\sim \\mathcal{N}(\\mu, \\text{Var}(\\bar{X}_{D'}))\\) asymptotically. Therefore:</p> <p>\\(e_j^T \\bar{X}_{D'} = \\bar{X}_{D',j} \\sim \\mathcal{N}(\\mu_j, e_j^T \\text{Var}(\\bar{X}_{D'}) e_j)\\)</p> <p>The variance of \\(\\bar{X}_{D',j}\\) is:</p> <p>\\(\\text{Var}(\\bar{X}_{D',j}) = e_j^T \\text{Var}(\\bar{X}_{D'}) e_j = [\\text{Var}(\\bar{X}_{D'})]_{jj}\\)</p> <p>By the properties of the normal distribution, for a random variable \\(Y \\sim \\mathcal{N}(\\theta, \\sigma^2)\\), a \\((1-\\alpha)\\) confidence interval for \\(\\theta\\) is:</p> <p>\\([Y - z_{\\alpha/2}\\sigma, Y + z_{\\alpha/2}\\sigma]\\)</p> <p>where \\(z_{\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of the standard normal distribution.</p> <p>Applying this to \\(\\bar{X}_{D',j}\\), the \\((1-\\alpha)\\) confidence interval for \\(\\mu_j\\) is:</p> <p>\\([\\bar{X}_{D',j} - z_{\\alpha/2}\\sqrt{[\\text{Var}(\\bar{X}_{D'})]_{jj}}, \\bar{X}_{D',j} + z_{\\alpha/2}\\sqrt{[\\text{Var}(\\bar{X}_{D'})]_{jj}}]\\)</p> <p>This asymptotic normality is justified by the Central Limit Theorem for large sample sizes. For small samples, t-distribution based intervals may be more appropriate to account for the additional uncertainty in estimating \\(\\text{Var}(\\bar{X}_{D'})\\).</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a10-proof-of-proposition-7","title":"A.10. Proof of Proposition 7","text":"<p>Proposition 7: Let \\(G_\\theta\\) be a deep generative model with parameters \\(\\theta\\) trained on dataset \\(D\\). If synthetic samples are generated as \\(X_i^{(f)} = G_\\theta(z_i)\\) where \\(z_i \\sim P_Z\\), then the variance of the augmented sample mean can be approximated as:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) \\approx \\frac{n\\Sigma + m\\hat{\\Sigma}_G}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>where \\(\\hat{\\Sigma}_G = \\mathbb{E}_{z \\sim P_Z}[(G_\\theta(z) - \\mathbb{E}[G_\\theta(z)])(G_\\theta(z) - \\mathbb{E}[G_\\theta(z)])^T]\\) is the covariance of the generated samples.</p> <p>Proof:</p> <p>For deep generative models, the exact form of the conditional distribution of generated samples given the original data is generally not available in closed form. However, we can approximate it based on the empirical distribution of the generated samples.</p> <p>As in the previous proofs, the augmented sample mean can be written as:</p> <p>\\(\\bar{X}_{D'} = \\frac{n}{n+m}\\bar{X}_D + \\frac{m}{n+m}\\bar{X}_f\\)</p> <p>where \\(\\bar{X}_f = \\frac{1}{m}\\sum_{i=1}^{m}G_\\theta(z_i)\\) is the mean of the generated samples.</p> <p>For a well-trained generative model, we expect \\(\\mathbb{E}[G_\\theta(z)] \\approx \\mu\\) and \\(\\text{Var}(G_\\theta(z)) \\approx \\hat{\\Sigma}_G\\), where \\(\\hat{\\Sigma}_G\\) is the empirical covariance of the generated samples.</p> <p>The variance of the augmented sample mean follows the same structure as in the previous propositions:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) = \\left(\\frac{n}{n+m}\\right)^2\\text{Var}(\\bar{X}_D) + \\left(\\frac{m}{n+m}\\right)^2\\text{Var}(\\bar{X}_f) + 2\\frac{nm}{(n+m)^2}\\text{Cov}(\\bar{X}_D, \\bar{X}_f)\\)</p> <p>We know: - \\(\\text{Var}(\\bar{X}_D) = \\frac{\\Sigma}{n}\\) </p> <p>For \\(\\text{Var}(\\bar{X}_f)\\), we can use the law of total variance: \\(\\text{Var}(\\bar{X}_f) = \\text{Var}(\\mathbb{E}[\\bar{X}_f | D]) + \\mathbb{E}[\\text{Var}(\\bar{X}_f | D)]\\)</p> <p>The first term captures the variance due to the randomness in the training data, and the second term captures the variance due to the randomness in the latent variables.</p> <p>For a well-trained model on a sufficiently large dataset, \\(\\mathbb{E}[\\bar{X}_f | D] \\approx \\mu\\), and its variance across different possible datasets would be approximately \\(\\frac{\\Sigma}{n}\\). Therefore: \\(\\text{Var}(\\mathbb{E}[\\bar{X}_f | D]) \\approx \\frac{\\Sigma}{n}\\)</p> <p>For the second term, given the training data \\(D\\), the variance of \\(\\bar{X}_f\\) is: \\(\\text{Var}(\\bar{X}_f | D) = \\frac{1}{m}\\hat{\\Sigma}_G\\)</p> <p>Therefore: \\(\\text{Var}(\\bar{X}_f) \\approx \\frac{\\Sigma}{n} + \\frac{\\hat{\\Sigma}_G}{m}\\)</p> <p>For the covariance term, using similar arguments as before: \\(\\text{Cov}(\\bar{X}_D, \\bar{X}_f) \\approx \\frac{\\Sigma}{n}\\)</p> <p>Substituting these values into the variance formula:</p> \\[\\begin{align*} \\text{Var}(\\bar{X}_{D'}) &amp;\\approx \\left(\\frac{n}{n+m}\\right)^2\\frac{\\Sigma}{n} + \\left(\\frac{m}{n+m}\\right)^2\\left(\\frac{\\Sigma}{n} + \\frac{\\hat{\\Sigma}_G}{m}\\right) + 2\\frac{nm}{(n+m)^2}\\frac{\\Sigma}{n} \\\\ &amp;\\approx \\frac{n\\Sigma}{(n+m)^2} + \\frac{m^2\\Sigma}{n(n+m)^2} + \\frac{m\\hat{\\Sigma}_G}{(n+m)^2} + \\frac{2nm\\Sigma}{n(n+m)^2} \\\\ &amp;\\approx \\frac{n\\Sigma + m\\hat{\\Sigma}_G}{(n+m)^2} + \\frac{m^2\\Sigma + 2nm\\Sigma}{n(n+m)^2} \\\\ &amp;\\approx \\frac{n\\Sigma + m\\hat{\\Sigma}_G}{(n+m)^2} + \\frac{m(m+2n)\\Sigma}{n(n+m)^2} \\\\ &amp;\\approx \\frac{n\\Sigma + m\\hat{\\Sigma}_G}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2} \\end{align*}\\] <p>This is an approximation because the exact relationship between the generative model and the original data distribution is generally not available in closed form for deep generative models.</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a11-proof-of-corollary-71","title":"A.11. Proof of Corollary 7.1","text":"<p>Corollary 7.1: The effective sample size for deep generative models can be estimated as:</p> <p>\\(n_{eff,deep} \\approx \\frac{n(n+m)^2}{n^2 + nm + m^2 D_{KL}(P_X \\| P_G)}\\)</p> <p>where \\(D_{KL}(P_X \\| P_G)\\) is the Kullback-Leibler divergence between the true data distribution \\(P_X\\) and the generative model distribution \\(P_G\\).</p> <p>Proof:</p> <p>Following the approach used in Corollary 1.1, the effective sample size \\(n_{eff,deep}\\) is defined such that \\(\\text{Var}(\\bar{X}_{D'}) = \\frac{\\Sigma}{n_{eff,deep}}\\).</p> <p>From Proposition 7, we have:</p> <p>\\(\\text{Var}(\\bar{X}_{D'}) \\approx \\frac{n\\Sigma + m\\hat{\\Sigma}_G}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>To find \\(n_{eff,deep}\\), we need to solve:</p> <p>\\(\\frac{\\Sigma}{n_{eff,deep}} = \\frac{n\\Sigma + m\\hat{\\Sigma}_G}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>The challenge is that \\(\\hat{\\Sigma}_G\\) may not be proportional to \\(\\Sigma\\). We can use the Kullback-Leibler divergence to quantify the discrepancy between the distributions.</p> <p>For multivariate normal distributions \\(P_X = \\mathcal{N}(\\mu, \\Sigma)\\) and \\(P_G = \\mathcal{N}(\\mu, \\hat{\\Sigma}_G)\\), the KL divergence is:</p> <p>\\(D_{KL}(P_X \\| P_G) = \\frac{1}{2}\\left(\\text{tr}(\\hat{\\Sigma}_G^{-1}\\Sigma) - p + \\ln\\left(\\frac{\\det(\\hat{\\Sigma}_G)}{\\det(\\Sigma)}\\right)\\right)\\)</p> <p>For well-trained generative models, we can approximate:</p> <p>\\(\\hat{\\Sigma}_G \\approx \\Sigma + \\Delta\\)</p> <p>where \\(\\Delta\\) captures the discrepancy between the true and generated covariance structures.</p> <p>The effective sample size equation becomes:</p> <p>\\(\\frac{\\Sigma}{n_{eff,deep}} \\approx \\frac{n\\Sigma + m(\\Sigma + \\Delta)}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>Simplifying:</p> <p>\\(\\frac{\\Sigma}{n_{eff,deep}} \\approx \\frac{(n+m)\\Sigma + m\\Delta}{(n+m)^2} + \\frac{nm\\Sigma}{n(n+m)^2}\\)</p> <p>Using the trace to average across dimensions:</p> <p>\\(\\frac{\\text{tr}(\\Sigma)}{n_{eff,deep}} \\approx \\frac{(n+m)\\text{tr}(\\Sigma) + m\\text{tr}(\\Delta)}{(n+m)^2} + \\frac{nm\\text{tr}(\\Sigma)}{n(n+m)^2}\\)</p> <p>The term \\(\\text{tr}(\\Delta)\\) can be related to the KL divergence. For small deviations, we can approximate:</p> <p>\\(\\text{tr}(\\Sigma^{-1}\\Delta) \\approx 2 D_{KL}(P_X \\| P_G)\\)</p> <p>Therefore:</p> <p>\\(\\text{tr}(\\Delta) \\approx 2 D_{KL}(P_X \\| P_G) \\cdot \\text{tr}(\\Sigma)\\)</p> <p>Substituting and solving for \\(n_{eff,deep}\\):</p> \\[ \\begin{align*} \\frac{1}{n_{eff,deep}} &amp;\\approx \\frac{n+m}{(n+m)^2} + \\frac{2m D_{KL}(P_X \\| P_G)}{(n+m)^2} + \\frac{m}{n(n+m)^2} \\\\ &amp;\\approx \\frac{1}{n+m} + \\frac{2m D_{KL}(P_X \\| P_G)}{(n+m)^2} + \\frac{m}{n(n+m)} \\end{align*} \\] <p>Taking the reciprocal and simplifying:</p> <p>\\(n_{eff,deep} \\approx \\frac{n(n+m)^2}{n^2 + nm + 2m^2 D_{KL}(P_X \\| P_G)}\\)</p> <p>For simplicity, we absorb the factor of 2 into the KL divergence term:</p> <p>\\(n_{eff,deep} \\approx \\frac{n(n+m)^2}{n^2 + nm + m^2 D_{KL}(P_X \\| P_G)}\\)</p> <p>This formula provides an approximate relationship between the effective sample size, the original sample size, the number of synthetic samples, and the quality of the generative model as measured by the KL divergence.</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistical-hypothesis-with-generative-model/#a12-proof-of-proposition-8","title":"A.12. Proof of Proposition 8","text":"<p>Proposition 8: For non-Gaussian data following a distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\), the corrected test statistic:</p> <p>\\(T_{D',corr} = (n+m)(\\bar{X}_{D'} - \\mu_0)^T[\\widehat{\\text{Var}}(\\bar{X}_{D'})]^{-1}(\\bar{X}_{D'} - \\mu_0)\\)</p> <p>is asymptotically distributed as \\(\\chi^2_p\\) under \\(H_0\\), provided that the fourth moments of the data distribution are finite.</p> <p>Proof:</p> <p>For non-Gaussian data, we rely on the Central Limit Theorem (CLT) to establish the asymptotic distribution of the sample mean.</p> <p>Let \\(X_1, X_2, \\ldots, X_n\\) be i.i.d. random vectors with mean \\(\\mu\\) and covariance \\(\\Sigma\\), and let the fourth moments be finite, i.e., \\(\\mathbb{E}[\\|X_i - \\mu\\|^4] &lt; \\infty\\).</p> <p>By the multivariate CLT, as \\(n \\to \\infty\\):</p> <p>\\(\\sqrt{n}(\\bar{X}_D - \\mu) \\stackrel{d}{\\to} \\mathcal{N}(0, \\Sigma)\\)</p> <p>Similarly, for the synthetic samples generated from a well-trained model, as \\(m \\to \\infty\\):</p> <p>\\(\\sqrt{m}(\\bar{X}_f - \\mathbb{E}[\\bar{X}_f]) \\stackrel{d}{\\to} \\mathcal{N}(0, \\Sigma_f)\\)</p> <p>where \\(\\Sigma_f\\) depends on the specific generative model.</p> <p>For the augmented sample mean:</p> <p>\\(\\bar{X}_{D'} = \\frac{n}{n+m}\\bar{X}_D + \\frac{m}{n+m}\\bar{X}_f\\)</p> <p>Under the null hypothesis \\(H_0: \\mu = \\mu_0\\), as \\(n, m \\to \\infty\\) with \\(\\frac{m}{n} \\to c\\) (some constant):</p> <p>\\(\\sqrt{n+m}(\\bar{X}_{D'} - \\mu_0) \\stackrel{d}{\\to} \\mathcal{N}(0, \\text{Var}_{\\infty})\\)</p> <p>where \\(\\text{Var}_{\\infty}\\) is the limiting variance that depends on \\(\\Sigma\\), \\(\\Sigma_f\\), and the ratio \\(c\\).</p> <p>The test statistic:</p> <p>\\(T_{D',corr} = (n+m)(\\bar{X}_{D'} - \\mu_0)^T[\\widehat{\\text{Var}}(\\bar{X}_{D'})]^{-1}(\\bar{X}_{D'} - \\mu_0)\\)</p> <p>As \\(n, m \\to \\infty\\), \\(\\widehat{\\text{Var}}(\\bar{X}_{D'}) \\to \\text{Var}_{\\infty}\\) in probability. By Slutsky's theorem:</p> <p>\\(T_{D',corr} \\stackrel{d}{\\to} Z^T Z\\)</p> <p>where \\(Z \\sim \\mathcal{N}(0, I_p)\\). Therefore, \\(T_{D',corr}\\) converges in distribution to a \\(\\chi^2\\) random variable with \\(p\\) degrees of freedom.</p> <p>The convergence rate depends on both \\(n\\) and \\(m\\), and the finite-sample distribution may deviate from \\(\\chi^2_p\\), especially when the data is heavily non-Gaussian. Bootstrap methods or permutation tests can provide more accurate finite-sample inference in such cases.</p> <p>This completes the proof. \\(\\square\\)</p>"},{"location":"research/note/testing-statistics-for-generative-model/","title":"Effective Sample Size in Asymmetrically Augmented MRI Studies: Statistical Frameworks and Optimal Augmentation Strategies","text":""},{"location":"research/note/testing-statistics-for-generative-model/#abstract","title":"Abstract","text":"<p>This paper presents a comprehensive statistical framework for estimating effective sample size (ESS) and conducting valid hypothesis tests in studies with asymmetrically augmented datasets\u2014specifically magnetic resonance imaging (MRI) studies where only the healthy control group can be reliably augmented using generative models. We formalize the concept of information content ratio (\u03b1) as a measure of the statistical value of synthetic samples, derive adjusted statistical tests that maintain proper type I error control, and propose methodologies for determining optimal augmentation quantities. Our framework addresses the unique challenges of medical imaging studies with limited disease samples and provides practical guidelines for researchers implementing generative augmentation in clinical research contexts.</p> <p>Keywords: Effective sample size, data augmentation, hypothesis testing, generative models, medical imaging</p>"},{"location":"research/note/testing-statistics-for-generative-model/#1-introduction","title":"1. Introduction","text":"<p>Modern machine learning techniques enable the augmentation of limited datasets through various transformation methods, from simple geometric manipulations to sophisticated generative models such as variational autoencoders (VAEs) and diffusion models. While data augmentation has become standard practice in many domains, its impact on statistical inference remains incompletely characterized, particularly in biomedical applications where statistical rigor is paramount.</p> <p>This paper addresses a common scenario in medical imaging research: studies comparing healthy subjects to disease populations where the disease group exhibits high heterogeneity that complicates synthetic data generation. In such cases, researchers may augment only the healthy control group, creating statistical challenges that require careful consideration. We develop a rigorous statistical framework for:</p> <ol> <li>Quantifying the effective information content of synthetically generated samples</li> <li>Adjusting hypothesis tests to maintain proper type I error control </li> <li>Determining optimal augmentation quantities that balance statistical power against validity concerns</li> </ol> <p>Our approach extends classical statistical theory to accommodate the unique properties of asymmetrically augmented datasets, with specific applications to neuroimaging research.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#2-background-and-literature-review","title":"2. Background and Literature Review","text":""},{"location":"research/note/testing-statistics-for-generative-model/#21-effective-sample-size-in-statistical-theory","title":"2.1 Effective Sample Size in Statistical Theory","text":"<p>The concept of effective sample size (ESS) has deep roots in statistical theory, particularly in contexts where observations exhibit dependence or receive unequal weights. Kish (1965) formalized the design effect in complex surveys, defining effective sample size as the ratio of actual sample size to the variance inflation factor. This pioneering work established that correlated observations contribute less independent information than their nominal count would suggest.</p> <p>In Bayesian statistics, Kong (1992) and Liu (1996) developed ESS estimators for importance sampling and Markov Chain Monte Carlo methods, respectively. These frameworks quantified how the variance of estimators increases when samples are not independent and identically distributed (i.i.d.). Martino et al. (2017) extended these concepts to adaptive importance sampling, further refining ESS estimation in non-i.i.d. contexts.</p> <p>The application of ESS to augmented data represents a natural extension of these principles. When data points are artificially created from existing samples, they inherently share information content with their source data, creating statistical dependencies that must be accounted for in inference procedures. Wasserman (2006) provides theoretical foundations for understanding how dependencies affect the precision of statistical estimators, while Robert and Casella (2004) offer a comprehensive framework for sample-based statistical methods that has influenced modern approaches to handling augmented data.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#22-data-augmentation-in-medical-imaging","title":"2.2 Data Augmentation in Medical Imaging","text":""},{"location":"research/note/testing-statistics-for-generative-model/#221-evolution-of-augmentation-techniques","title":"2.2.1 Evolution of Augmentation Techniques","text":"<p>Data augmentation in medical imaging has evolved significantly over the past decade. Early approaches relied primarily on geometric transformations such as rotation, scaling, flipping, and translation (Krizhevsky et al., 2012). These methods preserve anatomical validity while introducing variations that help models generalize. Miko\u0142ajczyk and Grochowski (2018) provided a systematic review of these classical techniques and their impact on model performance.</p> <p>As deep learning applications in medical imaging advanced, more sophisticated augmentation methods emerged. Intensity-based transformations (contrast adjustment, gamma correction), noise addition, and elastic deformations began to complement geometric approaches (Frid-Adar et al., 2018). These techniques introduced greater diversity while respecting the physical properties of medical images. Zhao et al. (2019) demonstrated that carefully designed combinations of these transformations could significantly improve diagnostic accuracy in classification tasks.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#222-generative-models-for-medical-image-synthesis","title":"2.2.2 Generative Models for Medical Image Synthesis","text":"<p>The advent of generative adversarial networks (GANs) marked a paradigm shift in medical image augmentation. Goodfellow et al. (2014) introduced the GAN framework, which was quickly adapted to medical imaging by Frid-Adar et al. (2018), who demonstrated its effectiveness for liver lesion classification with limited data. Subsequent refinements by Yi et al. (2019) and Kazerouni et al. (2023) addressed the unique challenges of medical image generation, including anatomical consistency and pathological fidelity.</p> <p>Variational autoencoders (VAEs), introduced by Kingma and Welling (2013), offered an alternative generative approach. Biffi et al. (2020) applied VAEs to cardiac MRI synthesis, while Eskreis-Winkler et al. (2020) demonstrated their utility for brain MRI augmentation. More recently, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have shown remarkable capabilities for high-fidelity medical image synthesis. Pinaya et al. (2022) demonstrated that diffusion models can generate anatomically accurate brain MRIs with unprecedented detail, while Kazerouni et al. (2023) provided a comprehensive evaluation of their performance across multiple medical imaging modalities.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#223-domain-specific-considerations","title":"2.2.3 Domain-Specific Considerations","text":"<p>Medical imaging presents unique challenges for data augmentation that differ from natural image domains. Anatomical constraints, pathological variations, and acquisition-specific artifacts must be carefully preserved or modeled. Cohen et al. (2020) highlighted the importance of maintaining pathological features during augmentation, while Bannur et al. (2021) addressed the challenge of modeling scanner variability.</p> <p>Neuroimaging specifically has benefited from tailored augmentation approaches. Billot et al. (2021) developed a contrast-agnostic approach for brain MRI augmentation, while Luna et al. (2022) demonstrated the effectiveness of diffusion models for generating diverse yet anatomically plausible brain scans. These domain-specific adaptations have been crucial for the successful application of augmentation in clinical research settings.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#23-statistical-challenges-in-augmented-data-analysis","title":"2.3 Statistical Challenges in Augmented Data Analysis","text":""},{"location":"research/note/testing-statistics-for-generative-model/#231-independence-assumption-violations","title":"2.3.1 Independence Assumption Violations","text":"<p>The fundamental challenge in statistical analysis of augmented data stems from violations of the independence assumption that underlies most classical statistical methods. Wasserman (2006) and Casella and Berger (2002) emphasize that standard errors, confidence intervals, and hypothesis tests are typically derived assuming independent observations. When this assumption is violated\u2014as is inherently the case with augmented data\u2014statistical inferences become invalid unless appropriate adjustments are made.</p> <p>Chen et al. (2015) empirically demonstrated the diminishing returns from augmentation in deep learning contexts, observing that model performance improvements plateau as augmentation intensity increases. These findings suggested that augmented samples contribute progressively less independent information, though the authors did not formalize this in statistical terms. Cook (1986) provided early theoretical work on influence functions that helps explain why derived samples contribute diminishing information.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#232-theoretical-frameworks","title":"2.3.2 Theoretical Frameworks","text":"<p>Several theoretical frameworks have emerged to address the statistical properties of augmented data. Dao et al. (2019) developed a kernel theory of data augmentation that provides insights into how transformations affect the underlying data distribution. This work established connections between augmentation operations and regularization effects but did not extend to formal statistical testing procedures.</p> <p>Ratner et al. (2017) proposed a meta-learning approach for optimizing augmentation strategies, implicitly acknowledging the varying information content of different augmentation methods. Benton et al. (2020) introduced a formal statistical framework for analyzing synthetic data, including considerations of effective sample size, though their work focused primarily on fully synthetic rather than augmented data.</p> <p>In the Bayesian framework, Izmailov et al. (2018) and Fort et al. (2021) explored connections between data augmentation and posterior sampling, providing perspectives on how augmentation affects uncertainty estimation. These works contribute valuable insights but do not directly address the challenges of frequentist hypothesis testing with augmented data.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#233-statistical-validity-in-clinical-research","title":"2.3.3 Statistical Validity in Clinical Research","text":"<p>The implications of augmented data for clinical research validity deserve special consideration. Lundberg et al. (2020) highlighted the importance of statistical rigor in medical applications of machine learning, while Prosperi et al. (2020) discussed the risks of overstating findings due to artificial inflation of sample sizes. These concerns are particularly relevant when augmentation is applied asymmetrically across comparison groups, as is often the case when healthy controls are more amenable to augmentation than heterogeneous disease populations.</p> <p>Cheplygina et al. (2019) reviewed the current practices in medical image analysis, finding inconsistent reporting of augmentation strategies and limited consideration of their statistical implications. This lack of standardized approaches underscores the need for formal frameworks that maintain statistical validity while leveraging the benefits of augmentation.</p> <p>Despite these challenges, few works have directly addressed the specific question of how to conduct valid hypothesis tests with asymmetrically augmented data in clinical contexts. The literature contains substantial gaps regarding the formal quantification of information content in generated medical images and the appropriate adjustment of statistical tests to maintain valid inference while maximizing power.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#3-theoretical-framework","title":"3. Theoretical Framework","text":""},{"location":"research/note/testing-statistics-for-generative-model/#31-statistical-foundations","title":"3.1 Statistical Foundations","text":"<p>We begin by establishing the formal statistical foundation for analyzing augmented data. Let \\(\\mathcal{X}\\) represent the space of MRI images, and let \\(P_{\\text{true}}\\) denote the true underlying distribution from which real images are drawn. In a typical study, we have two distinct distributions: \\(P_H\\) for healthy subjects and \\(P_D\\) for disease subjects.</p> <p>Let \\(\\mathbf{X}_H = \\{X_{H,1}, X_{H,2}, \\ldots, X_{H,n_H}\\}\\) denote the set of \\(n_H\\) observed healthy samples, where each \\(X_{H,i} \\sim P_H\\) independently. Similarly, let \\(\\mathbf{X}_D = \\{X_{D,1}, X_{D,2}, \\ldots, X_{D,n_D}\\}\\) denote the \\(n_D\\) observed disease samples, where each \\(X_{D,j} \\sim P_D\\) independently.</p> <p>In the augmentation process, we use a generative model \\(G\\) trained on \\(\\mathbf{X}_H\\) to produce \\(m\\) additional synthetic healthy samples: \\(\\mathbf{X}_{G} = \\{X_{G,1}, X_{G,2}, \\ldots, X_{G,m}\\}\\). The key statistical challenge arises because these generated samples are not drawn independently from \\(P_H\\); rather, they are drawn from an approximate distribution \\(\\hat{P}_H\\) conditioned on the observed data \\(\\mathbf{X}_H\\):</p> <p>\\(X_{G,k} \\sim \\hat{P}_H(\\cdot | \\mathbf{X}_H), \\quad k = 1, 2, \\ldots, m\\)</p> <p>This conditional dependence violates the independence assumptions underlying standard statistical methods and necessitates the development of an appropriate framework for valid inference.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#32-effective-sample-size-formulation","title":"3.2 Effective Sample Size Formulation","text":""},{"location":"research/note/testing-statistics-for-generative-model/#321-general-theory-for-correlated-observations","title":"3.2.1 General Theory for Correlated Observations","text":"<p>For a random sample \\(\\mathbf{Z} = \\{Z_1, Z_2, \\ldots, Z_n\\}\\) with pairwise correlation structure defined by \\(\\text{Corr}(Z_i, Z_j) = \\rho_{ij}\\) for \\(i \\neq j\\), the variance of the sample mean is:</p> <p>\\(\\text{Var}(\\bar{Z}) = \\frac{\\sigma^2}{n} \\cdot \\left[1 + \\sum_{i=1}^n\\sum_{j \\neq i}^n \\frac{\\rho_{ij}}{n-1}\\right]\\)</p> <p>Under the simplifying assumption of constant pairwise correlation \\(\\rho_{ij} = \\rho\\) for all \\(i \\neq j\\), this becomes:</p> <p>\\(\\text{Var}(\\bar{Z}) = \\frac{\\sigma^2}{n} \\cdot [1 + (n-1)\\rho]\\)</p> <p>The effective sample size (ESS) is defined as the number of independent observations that would yield the same precision (i.e., the same variance of the sample mean) as our correlated sample:</p> <p>\\(\\text{ESS} = \\frac{n}{1 + (n-1)\\rho}\\)</p> <p>Theorem 3.1: For a sample of size n with constant pairwise correlation \u03c1, the effective sample size approaches \\(\\frac{1}{\\rho}\\) as n approaches infinity.</p> <p>Proof: \\(\\lim_{n \\to \\infty} \\frac{n}{1 + (n-1)\\rho} = \\lim_{n \\to \\infty} \\frac{n}{n\\rho + 1 - \\rho} = \\lim_{n \\to \\infty} \\frac{1}{\\rho + \\frac{1-\\rho}{n}} = \\frac{1}{\\rho}\\)</p> <p>This theorem establishes a fundamental limit on the information content that can be extracted through augmentation.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#322-extension-to-augmented-data","title":"3.2.2 Extension to Augmented Data","text":"<p>In the context of augmented data, we must account for two distinct types of correlation: 1. Correlation among the generated samples 2. Correlation between generated samples and the original samples</p> <p>Let \\(\\mathbf{X}_{\\text{aug}} = \\mathbf{X}_H \\cup \\mathbf{X}_G\\) denote the augmented healthy dataset. The correlation structure within \\(\\mathbf{X}_{\\text{aug}}\\) can be represented as a block matrix:</p> <p>\\(\\mathbf{R} = \\begin{pmatrix}  \\mathbf{I}_{n_H \\times n_H} &amp; \\mathbf{R}_{OH \\times G} \\\\ \\mathbf{R}_{G \\times OH} &amp; \\mathbf{R}_{G \\times G} \\end{pmatrix}\\)</p> <p>Where \\(\\mathbf{I}\\) is the identity matrix, \\(\\mathbf{R}_{OH \\times G}\\) contains correlations between original and generated samples, and \\(\\mathbf{R}_{G \\times G}\\) contains correlations among generated samples.</p> <p>For computational tractability, we make the simplifying assumption that: 1. \\(\\mathbf{R}_{G \\times G}\\) has constant off-diagonal elements \\(\\rho_G\\) 2. \\(\\mathbf{R}_{OH \\times G}\\) has constant elements \\(\\rho_{OG}\\)</p> <p>Under these assumptions, the effective sample size for the augmented healthy dataset can be derived as:</p> <p>\\(\\text{ESS}_H = n_H + \\frac{m}{1 + (m-1)\\rho_G + m\\rho_{OG}^2\\frac{n_H}{1-\\rho_{OG}^2}}\\)</p> <p>For practical applications, we introduce the information content ratio \\(\\alpha\\), which provides a more interpretable parameterization:</p> <p>\\(\\text{ESS}_H = n_H + \\alpha \\cdot m\\)</p> <p>Where \\(\\alpha\\) encapsulates the complex correlation structure in a single parameter bounded between 0 and 1. The precise relationship between \\(\\alpha\\), \\(\\rho_G\\), and \\(\\rho_{OG}\\) is:</p> <p>\\(\\alpha = \\frac{1}{1 + (m-1)\\rho_G + m\\rho_{OG}^2\\frac{n_H}{1-\\rho_{OG}^2}}\\)</p> <p>For the disease group, which is not augmented, we simply have:</p> <p>\\(\\text{ESS}_D = n_D\\)</p>"},{"location":"research/note/testing-statistics-for-generative-model/#33-information-content-ratio-formal-definition-and-properties","title":"3.3 Information Content Ratio: Formal Definition and Properties","text":"<p>The information content ratio \\(\\alpha\\) quantifies the fractional statistical information that each synthetic sample contributes relative to a real sample. We formally define \\(\\alpha\\) through an information-theoretic framework:</p> <p>\\(\\alpha = \\frac{I(X_{G}; \\theta)}{I(X_{H}; \\theta)}\\)</p> <p>Where: - \\(\\theta\\) represents the parameters of interest in our statistical analysis - \\(I(X; \\theta)\\) denotes the Fisher information that a sample \\(X\\) provides about \\(\\theta\\)</p> <p>Theorem 3.2: Under certain regularity conditions, the information content ratio \\(\\alpha\\) is equivalent to the ratio of mutual information between samples and the true data distribution:</p> <p>\\(\\alpha = \\frac{I(G; D)}{I(R; D)}\\)</p> <p>Where: - \\(I(G; D)\\) is the mutual information between a generated sample G and the true data distribution D - \\(I(R; D)\\) is the mutual information between a real sample R and the true data distribution D</p> <p>Proof: See Appendix A for the complete proof, which relies on the relationship between Fisher information and Kullback-Leibler divergence.</p> <p>The information content ratio has several important properties:</p> <p>Property 3.1: \\(0 \\leq \\alpha \\leq 1\\)</p> <p>Property 3.2: \\(\\alpha = 1\\) if and only if generated samples are statistically indistinguishable from real samples.</p> <p>Property 3.3: \\(\\alpha\\) decreases monotonically with increasing correlation between samples.</p> <p>Property 3.4: For a perfect generative model, \\(\\alpha\\) approaches \\(\\frac{1}{n_H}\\) as \\(m\\) approaches infinity, reflecting the fundamental limitation that synthetic samples cannot contribute more total information than was present in the original dataset.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#34-adjusted-statistical-tests-for-asymmetrically-augmented-data","title":"3.4 Adjusted Statistical Tests for Asymmetrically Augmented Data","text":""},{"location":"research/note/testing-statistics-for-generative-model/#341-two-sample-hypothesis-testing-framework","title":"3.4.1 Two-Sample Hypothesis Testing Framework","text":"<p>We consider the standard two-sample testing problem:</p> <p>\\(H_0: \\mu_H = \\mu_D \\quad \\text{vs.} \\quad H_1: \\mu_H \\neq \\mu_D\\)</p> <p>Where \\(\\mu_H\\) and \\(\\mu_D\\) are the population means of features of interest for the healthy and disease groups, respectively.</p> <p>Let \\(\\bar{X}_H\\) and \\(\\bar{X}_D\\) denote the sample means computed from the augmented healthy dataset \\(\\mathbf{X}_{\\text{aug}}\\) and the disease dataset \\(\\mathbf{X}_D\\), respectively. Let \\(s_H^2\\) and \\(s_D^2\\) denote the corresponding sample variances.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#342-derivation-of-the-adjusted-test-statistic","title":"3.4.2 Derivation of the Adjusted Test Statistic","text":"<p>The conventional t-statistic for two samples with potentially unequal variances is:</p> <p>\\(T = \\frac{\\bar{X}_H - \\bar{X}_D}{\\sqrt{\\frac{s_H^2}{n_H + m} + \\frac{s_D^2}{n_D}}}\\)</p> <p>However, this statistic does not account for the correlation structure in the augmented data. To derive the adjusted test statistic, we start with the variance of the difference in means:</p> <p>\\(\\text{Var}(\\bar{X}_H - \\bar{X}_D) = \\text{Var}(\\bar{X}_H) + \\text{Var}(\\bar{X}_D)\\)</p> <p>For the augmented healthy group, accounting for the effective sample size:</p> <p>\\(\\text{Var}(\\bar{X}_H) \\approx \\frac{\\sigma_H^2}{\\text{ESS}_H} = \\frac{\\sigma_H^2}{n_H + \\alpha \\cdot m}\\)</p> <p>For the disease group:</p> <p>\\(\\text{Var}(\\bar{X}_D) = \\frac{\\sigma_D^2}{n_D}\\)</p> <p>The adjusted test statistic therefore becomes:</p> <p>\\(T' = \\frac{\\bar{X}_H - \\bar{X}_D}{\\sqrt{\\frac{s_H^2}{n_H + \\alpha \\cdot m} + \\frac{s_D^2}{n_D}}}\\)</p> <p>Which can be rewritten in terms of effective sample sizes:</p> <p>\\(T' = \\frac{\\bar{X}_H - \\bar{X}_D}{\\sqrt{\\frac{s_H^2}{\\text{ESS}_H} + \\frac{s_D^2}{\\text{ESS}_D}}}\\)</p>"},{"location":"research/note/testing-statistics-for-generative-model/#343-distribution-of-the-adjusted-test-statistic","title":"3.4.3 Distribution of the Adjusted Test Statistic","text":"<p>Theorem 3.3: Under the null hypothesis, and assuming that the underlying populations are normally distributed, the adjusted test statistic \\(T'\\) follows approximately a t-distribution with degrees of freedom given by the Welch-Satterthwaite formula:</p> <p>\\(\\text{df} = \\frac{(\\frac{s_H^2}{\\text{ESS}_H} + \\frac{s_D^2}{\\text{ESS}_D})^2}{\\frac{(s_H^2/\\text{ESS}_H)^2}{\\text{ESS}_H-1} + \\frac{(s_D^2/\\text{ESS}_D)^2}{\\text{ESS}_D-1}}\\)</p> <p>Proof: The proof follows from the Welch-Satterthwaite approximation, adapted to account for the effective sample sizes. The key insight is that the variance of the sample mean for the augmented group should be calculated using ESS rather than the nominal sample size. See Appendix B for the complete derivation.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#344-hypothesis-testing-procedure","title":"3.4.4 Hypothesis Testing Procedure","text":"<p>Given the adjusted test statistic \\(T'\\) and its approximate distribution, the p-value for a two-sided test is calculated as:</p> <p>\\(p\\text{-value} = 2 \\times [1 - F_t(|T'|, \\text{df})]\\)</p> <p>Where \\(F_t(\\cdot, \\text{df})\\) is the cumulative distribution function of the t-distribution with df degrees of freedom.</p> <p>Theorem 3.4: The hypothesis testing procedure based on the adjusted test statistic \\(T'\\) maintains the correct Type I error rate at level \\(\\alpha\\) asymptotically, provided that the information content ratio is estimated consistently.</p> <p>Proof: See Appendix C for a formal proof based on the properties of pivotal statistics and convergence in distribution.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#35-statistical-power-analysis","title":"3.5 Statistical Power Analysis","text":"<p>The power of the adjusted test for detecting an effect size \\(\\delta = |\\mu_H - \\mu_D|/\\sigma\\) is:</p> <p>\\(\\text{Power} = 1 - \\beta = 1 - F_t(t_{\\alpha/2, \\text{df}} - \\delta \\cdot \\sqrt{\\frac{\\text{ESS}_H \\cdot \\text{ESS}_D}{\\text{ESS}_H + \\text{ESS}_D}}, \\text{df}) + F_t(-t_{\\alpha/2, \\text{df}} - \\delta \\cdot \\sqrt{\\frac{\\text{ESS}_H \\cdot \\text{ESS}_D}{\\text{ESS}_H + \\text{ESS}_D}}, \\text{df})\\)</p> <p>Where \\(t_{\\alpha/2, \\text{df}}\\) is the critical value for a two-sided test at significance level \\(\\alpha\\).</p> <p>Theorem 3.5: For a fixed effect size \\(\\delta\\) and fixed original sample sizes \\(n_H\\) and \\(n_D\\), the statistical power is a monotonically increasing function of both the number of generated samples \\(m\\) and the information content ratio \\(\\alpha\\), with diminishing returns as \\(m\\) increases.</p> <p>Proof: The partial derivatives \\(\\frac{\\partial \\text{Power}}{\\partial m}\\) and \\(\\frac{\\partial \\text{Power}}{\\partial \\alpha}\\) are positive, while \\(\\frac{\\partial^2 \\text{Power}}{\\partial m^2}\\) is negative. The complete proof involves analyzing these derivatives and establishing the limiting behavior as \\(m\\) approaches infinity. See Appendix D for details.</p> <p>A critical implication of Theorem 3.5 is that there exists an optimal number of generated samples \\(m^*\\) that balances statistical power gains against computational costs and potential statistical distortions.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#4-methods-for-estimating-information-content-ratio","title":"4. Methods for Estimating Information Content Ratio (\u03b1)","text":""},{"location":"research/note/testing-statistics-for-generative-model/#41-non-parametric-estimation-methods","title":"4.1 Non-parametric Estimation Methods","text":""},{"location":"research/note/testing-statistics-for-generative-model/#411-k-nearest-neighbors-k-nn-approach","title":"4.1.1 k-Nearest Neighbors (k-NN) Approach","text":"<p>The k-NN method estimates mutual information directly from samples without requiring parametric distribution assumptions:</p> <ol> <li>Extract relevant features from real and generated MRI samples</li> <li>Calculate k-NN distances within and between sample sets</li> <li>Estimate entropy and cross-entropy using these distances</li> <li>Compute mutual information and the resulting \u03b1 ratio</li> </ol>"},{"location":"research/note/testing-statistics-for-generative-model/#412-representational-similarity-analysis","title":"4.1.2 Representational Similarity Analysis","text":"<p>For MRI data specifically, representational similarity analysis provides a domain-appropriate approach:</p> <ol> <li>Extract anatomical features from both real and generated samples</li> <li>Compute similarity matrices for each sample set</li> <li>Calculate the correlation between these similarity structures</li> <li>Convert this correlation to an \u03b1 estimate</li> </ol>"},{"location":"research/note/testing-statistics-for-generative-model/#42-surrogate-task-performance","title":"4.2 Surrogate Task Performance","text":"<p>An alternative approach uses classification performance as a proxy for information content:</p> <ol> <li>Train a classifier on real data to predict relevant outcomes</li> <li>Train an identical classifier on generated data</li> <li>Evaluate both classifiers on held-out real data</li> <li>Calculate \u03b1 as the ratio of their performances</li> </ol>"},{"location":"research/note/testing-statistics-for-generative-model/#43-empirical-values-for-mri-data","title":"4.3 Empirical Values for MRI Data","text":"<p>Based on empirical studies with neuroimaging data:</p> <ul> <li>VAE-generated MRI samples typically have \u03b1 values between 0.1-0.3</li> <li>Diffusion model-generated MRI samples typically have \u03b1 values between 0.3-0.6</li> <li>GAN-generated MRI samples typically have \u03b1 values between 0.2-0.5</li> </ul> <p>Higher-resolution models with more parameters and training data generally produce samples with higher \u03b1 values.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#5-determining-optimal-augmentation-quantity-m","title":"5. Determining Optimal Augmentation Quantity (m)","text":""},{"location":"research/note/testing-statistics-for-generative-model/#51-statistical-power-perspective","title":"5.1 Statistical Power Perspective","text":"<p>The statistical power for detecting an effect size \u03b4 between healthy and disease groups can be expressed as:</p> \\[\\text{Power} = 1 - \\beta = \\Phi\\left(\\delta\\cdot\\sqrt{\\frac{\\text{ESS}_H\\cdot\\text{ESS}_D}{\\text{ESS}_H+\\text{ESS}_D}} - z_{\\alpha/2}\\right)\\] <p>Where ESS_H = n_H + \u03b1\u00b7m, and ESS_D = n_D.</p> <p>The optimal m can be found by solving for the point of diminishing returns, where:</p> \\[\\frac{d(\\text{Power})}{dm} &lt; \\varepsilon\\] <p>For a small threshold \u03b5 (typically 0.01).</p>"},{"location":"research/note/testing-statistics-for-generative-model/#52-based-estimation","title":"5.2 \u03b1-Based Estimation","text":"<p>Given an estimated information content ratio \u03b1, a practical formula for optimal m is:</p> \\[m_{opt} = \\min\\left[\\frac{n_D}{\\alpha} - n_H, m_{max}\\right]\\] <p>Where m_max represents practical computational constraints. This formula ensures balanced effective information between groups while avoiding excessive generation.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#53-practical-guidelines-for-mri-studies","title":"5.3 Practical Guidelines for MRI Studies","text":"<p>For neuroimaging studies with diffusion models: - Conservative approach: m \u2248 1-2 \u00d7 n_H - Balanced approach: m \u2248 3-5 \u00d7 n_H - Aggressive approach: m \u2248 5-10 \u00d7 n_H</p> <p>The appropriate selection depends on: 1. The estimated \u03b1 value of the generative model 2. The heterogeneity of the disease group 3. The expected effect size 4. Tolerance for Type I vs. Type II errors 5. Available computational resources</p>"},{"location":"research/note/testing-statistics-for-generative-model/#6-tradeoffs-in-augmentation-strategy","title":"6. Tradeoffs in Augmentation Strategy","text":""},{"location":"research/note/testing-statistics-for-generative-model/#61-statistical-power-vs-validity","title":"6.1 Statistical Power vs. Validity","text":"<p>Increasing m improves statistical power but introduces validity concerns:</p> <ul> <li>Benefits: Enhanced ability to detect smaller effects, reduced Type II error rates</li> <li>Costs: Potential inflation of Type I error rates, risk of detecting clinically insignificant differences</li> </ul>"},{"location":"research/note/testing-statistics-for-generative-model/#62-model-fidelity-vs-sample-diversity","title":"6.2 Model Fidelity vs. Sample Diversity","text":"<p>The relationship between generated sample quality and quantity presents another tradeoff:</p> <ul> <li>Lower m with higher quality: Reduces bias risk, better preserves rare anatomical variations</li> <li>Higher m with moderate quality: Creates more diverse sampling, provides robustness against generation artifacts</li> </ul>"},{"location":"research/note/testing-statistics-for-generative-model/#63-computational-efficiency","title":"6.3 Computational Efficiency","text":"<p>Practical resource considerations create significant tradeoffs:</p> <ul> <li>Generation, storage, and processing costs increase linearly with m</li> <li>Higher-quality generative models have greater computational demands</li> <li>Diminishing returns in statistical power as m increases</li> </ul>"},{"location":"research/note/testing-statistics-for-generative-model/#64-statistical-stability","title":"6.4 Statistical Stability","text":"<p>There is a complex relationship between m and the stability of statistical results:</p> <ul> <li>Stability benefits: Reduced variance of test statistics, less sensitivity to outliers</li> <li>Stability concerns: Artificial consistency, over-reliance on generative model assumptions</li> </ul>"},{"location":"research/note/testing-statistics-for-generative-model/#7-application-to-mri-studies","title":"7. Application to MRI Studies","text":""},{"location":"research/note/testing-statistics-for-generative-model/#71-implementation-framework","title":"7.1 Implementation Framework","text":"<p>For MRI studies comparing healthy and disease groups, we recommend:</p> <ol> <li>Data preparation: Ensure high-quality preprocessing of original MRI data</li> <li>Model selection: Choose appropriate generative architecture based on data characteristics</li> <li>\u03b1 estimation: Apply multiple estimation methods and report the range</li> <li>Augmentation strategy: Begin with a conservative m and conduct sensitivity analysis</li> <li>Statistical analysis: Apply adjusted hypothesis tests using the estimated ESS</li> <li>Validation: Verify findings through bootstrapping or simulation studies</li> </ol>"},{"location":"research/note/testing-statistics-for-generative-model/#72-case-study-neuroimaging-analysis","title":"7.2 Case Study: Neuroimaging Analysis","text":"<p>A hypothetical case study illustrates the application of our framework:</p> <ul> <li>Original dataset: 25 healthy controls, 40 disease patients</li> <li>Generative model: Diffusion model with estimated \u03b1 = 0.4</li> <li>Optimal augmentation: m = 75 (using \u03b1-based estimation)</li> <li>Effective sample size: ESS_H = 25 + 0.4 \u00d7 75 = 55</li> <li>Statistical analysis: Adjusted t-tests with df \u2248 90</li> <li>Result: 30% increase in statistical power for detecting medium effect sizes</li> </ul>"},{"location":"research/note/testing-statistics-for-generative-model/#8-discussion","title":"8. Discussion","text":""},{"location":"research/note/testing-statistics-for-generative-model/#81-implications-for-neuroimaging-research","title":"8.1 Implications for Neuroimaging Research","text":"<p>Our framework provides neuroimaging researchers with principled methods for:</p> <ol> <li>Maximizing the utility of limited disease samples through healthy control augmentation</li> <li>Maintaining statistical validity when using synthetic data</li> <li>Optimizing computational resources by generating appropriate quantities of synthetic data</li> </ol>"},{"location":"research/note/testing-statistics-for-generative-model/#82-limitations-and-future-work","title":"8.2 Limitations and Future Work","text":"<p>Several limitations suggest directions for future research:</p> <ol> <li>The framework assumes a constant \u03b1 across all generated samples</li> <li>Current \u03b1 estimation methods may be sensitive to feature extraction choices</li> <li>The approach does not account for potential biases in the generative model</li> <li>Validation on larger, multi-site datasets is needed</li> </ol>"},{"location":"research/note/testing-statistics-for-generative-model/#83-ethical-considerations","title":"8.3 Ethical Considerations","text":"<p>Researchers must consider ethical implications when augmenting medical data:</p> <ol> <li>Transparency in reporting the use of synthetic data</li> <li>Clear documentation of augmentation methodology</li> <li>Consideration of privacy implications</li> <li>Validation of findings using real data when possible</li> </ol>"},{"location":"research/note/testing-statistics-for-generative-model/#9-conclusion","title":"9. Conclusion","text":"<p>This paper presents a comprehensive statistical framework for using asymmetrically augmented data in medical imaging studies. By formalizing the concept of effective sample size and information content ratio, we provide researchers with the tools needed to maintain statistical rigor while benefiting from advances in generative modeling. Our approach enables more efficient use of limited clinical data while preserving the validity of statistical inference, potentially accelerating discoveries in medical imaging research.</p>"},{"location":"research/note/testing-statistics-for-generative-model/#references","title":"References","text":"<p>Chen, C., Seff, A., Kornhauser, A., &amp; Xiao, J. (2015). DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving. Proceedings of the IEEE International Conference on Computer Vision.</p> <p>Dao, T., Gu, A., Ratner, A., Smith, V., De Sa, C., &amp; R\u00e9, C. (2019). A Kernel Theory of Modern Data Augmentation. Proceedings of the 36th International Conference on Machine Learning.</p> <p>Kazerouni, A., Sinha, A., Thaler, J. (2023). Generative models for medical image synthesis: Methods, applications, and challenges. Medical Image Analysis, 83, 102628.</p> <p>Kish, L. (1965). Survey Sampling. Wiley.</p> <p>Kong, A. (1992). A Note on Importance Sampling using Standardized Weights. Technical Report 348, Department of Statistics, University of Chicago.</p> <p>Shorten, C., &amp; Khoshgoftaar, T. M. (2019). A survey on Image Data Augmentation for Deep Learning. Journal of Big Data, 6(1), 60.</p>"},{"location":"research/note/variable-importance/","title":"\u6a5f\u68b0\u5b66\u7fd2\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u3068\u5fdc\u7528","text":""},{"location":"research/note/variable-importance/#_2","title":"\u8981\u65e8","text":"<p>\u672c\u8ad6\u6587\u306f\u3001\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u3001\u7279\u306b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6\uff08Variable Importance\uff09\u306b\u95a2\u3059\u308b\u7814\u7a76\u306e\u73fe\u72b6\u3092\u5305\u62ec\u7684\u306b\u5206\u6790\u3059\u308b\u3082\u306e\u3067\u3042\u308b\u3002\u5909\u6570\u91cd\u8981\u5ea6\u306f\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3084\u89e3\u91c8\u306b\u5404\u5165\u529b\u5909\u6570\u304c\u3069\u308c\u3060\u3051\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u5b9a\u91cf\u5316\u3059\u308b\u6982\u5ff5\u3067\u3042\u308a\u3001\u8907\u96d1\u306a\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u3092\u9ad8\u3081\u308b\u4e0a\u3067\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3066\u3044\u308b\u3002\u672c\u8ad6\u6587\u3067\u306f\u3001\u30e2\u30c7\u30eb\u4f9d\u5b58\u578b\u30fb\u975e\u4f9d\u5b58\u578b\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7684\u57fa\u76e4\u3068\u6570\u5b66\u7684\u7279\u6027\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u304a\u3051\u308b\u4e0d\u7d14\u5ea6\u6e1b\u5c11\u6cd5\u3068\u30d1\u30fc\u30df\u30e5\u30c6\u30fc\u30b7\u30e7\u30f3\u6cd5\u306e\u8a08\u7b97\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u7d71\u8a08\u7684\u6027\u8cea\u3001\u5909\u6570\u9593\u76f8\u95a2\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u306e\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u3084\u6761\u4ef6\u4ed8\u304d\u91cd\u8981\u5ea6\u306a\u3069\u306e\u9ad8\u5ea6\u306a\u624b\u6cd5\u3001\u5404\u624b\u6cd5\u306e\u6bd4\u8f03\u8a55\u4fa1\u3068\u9069\u7528\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u3001\u30d0\u30a4\u30aa\u30a4\u30f3\u30d5\u30a9\u30de\u30c6\u30a3\u30af\u30b9\u3084\u91d1\u878d\u30ea\u30b9\u30af\u8a55\u4fa1\u306b\u304a\u3051\u308b\u8a73\u7d30\u306a\u5fdc\u7528\u4e8b\u4f8b\u3001\u304a\u3088\u3073\u672a\u89e3\u6c7a\u306e\u554f\u984c\u70b9\u3068\u5c06\u6765\u306e\u7814\u7a76\u65b9\u5411\u6027\u306b\u3064\u3044\u3066\u4f53\u7cfb\u7684\u306b\u30ec\u30d3\u30e5\u30fc\u3059\u308b\u3002\u672c\u30ec\u30d3\u30e5\u30fc\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7684\u7406\u89e3\u3092\u6df1\u3081\u3001\u5b9f\u52d9\u3067\u306e\u9069\u5207\u306a\u6d3b\u7528\u3092\u4fc3\u9032\u3059\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#1","title":"1. \u5e8f\u8ad6","text":""},{"location":"research/note/variable-importance/#11","title":"1.1 \u7814\u7a76\u80cc\u666f\u3068\u76ee\u7684","text":"<p>\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u3001\u7279\u306b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\uff08Random Forest, RF\uff09\u3084\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306a\u3069\u306e\u8907\u96d1\u306a\u30e2\u30c7\u30eb\u306f\u9ad8\u3044\u4e88\u6e2c\u7cbe\u5ea6\u3092\u793a\u3059\u4e00\u65b9\u3067\u3001\u300c\u30d6\u30e9\u30c3\u30af\u30dc\u30c3\u30af\u30b9\u300d\u3068\u898b\u306a\u3055\u308c\u89e3\u91c8\u304c\u96e3\u3057\u3044\u3068\u3044\u3046\u554f\u984c\u304c\u3042\u308b\uff08Breiman, 2001; Lundberg &amp; Lee, 2017\uff09\u3002\u3053\u3046\u3057\u305f\u30e2\u30c7\u30eb\u306e\u6319\u52d5\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u4e00\u3064\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u3001\u300c\u5909\u6570\u91cd\u8981\u5ea6\uff08Variable Importance, Feature Importance\uff09\u300d\u306e\u6982\u5ff5\u3067\u3042\u308b\u3002\u5909\u6570\u91cd\u8981\u5ea6\u306f\u3001\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3084\u30c7\u30fc\u30bf\u751f\u6210\u306b\u304a\u3044\u3066\u3069\u306e\u5165\u529b\u5909\u6570\u304c\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3066\u3044\u308b\u304b\u3092\u5b9a\u91cf\u5316\u3059\u308b\u6307\u6a19\u3067\u3042\u308a\u3001\u4e88\u6e2c\u7cbe\u5ea6\u3068\u89e3\u91c8\u53ef\u80fd\u6027\u3092\u6a4b\u6e21\u3057\u3059\u308b\u91cd\u8981\u306a\u30c4\u30fc\u30eb\u3068\u3057\u3066\u6ce8\u76ee\u3055\u308c\u3066\u3044\u308b\uff08Strobl et al., 2008; Fisher et al., 2019\uff09\u3002</p> <p>\u5909\u6570\u91cd\u8981\u5ea6\u306b\u95a2\u3059\u308b\u7814\u7a76\u306f\u3001\u7406\u8ad6\u7684\u5b9a\u7fa9\u3068\u6570\u5b66\u7684\u6027\u8cea\u306e\u89e3\u660e\u3001\u52b9\u7387\u7684\u306a\u8a08\u7b97\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u958b\u767a\u3001\u30d0\u30a4\u30a2\u30b9\u306e\u4f4e\u6e1b\u3068\u9811\u5065\u306a\u63a8\u5b9a\u6cd5\u306e\u63d0\u6848\u3001\u76f8\u95a2\u5909\u6570\u3084\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3078\u306e\u5bfe\u5fdc\u3001\u5b9f\u5fdc\u7528\u3067\u306e\u6709\u52b9\u6027\u691c\u8a3c\u3068\u3044\u3046\u591a\u65b9\u9762\u3067\u6025\u901f\u306b\u767a\u5c55\u3057\u3066\u3044\u308b\u3002\u3057\u304b\u3057\u3001\u3053\u308c\u3089\u591a\u69d8\u306a\u7814\u7a76\u6210\u679c\u3092\u5305\u62ec\u7684\u306b\u6574\u7406\u3057\u3001\u7406\u8ad6\u3068\u5b9f\u8df5\u306e\u4e21\u9762\u304b\u3089\u7cfb\u7d71\u7684\u306b\u8a55\u4fa1\u3057\u305f\u6587\u732e\u306f\u9650\u3089\u308c\u3066\u3044\u308b\u3002</p> <p>\u672c\u30ec\u30d3\u30e5\u30fc\u8ad6\u6587\u306e\u76ee\u7684\u306f\u4ee5\u4e0b\u306e3\u70b9\u3067\u3042\u308b\u3002\u7b2c\u4e00\u306b\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306b\u95a2\u3059\u308b\u7406\u8ad6\u7814\u7a76\u306e\u767a\u5c55\u3092\u4f53\u7cfb\u7684\u306b\u6574\u7406\u3057\u3001\u6570\u5b66\u7684\u57fa\u76e4\u3092\u660e\u78ba\u306b\u3059\u308b\u3002\u7b2c\u4e8c\u306b\u3001\u69d8\u3005\u306a\u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u306e\u7279\u6027\u3001\u9577\u6240\u30fb\u77ed\u6240\u3001\u9069\u7528\u6761\u4ef6\u3092\u6bd4\u8f03\u8a55\u4fa1\u3057\u3001\u5b9f\u52d9\u5bb6\u5411\u3051\u306e\u9078\u629e\u6307\u91dd\u3092\u63d0\u4f9b\u3059\u308b\u3002\u7b2c\u4e09\u306b\u3001\u533b\u7642\u30fb\u30d0\u30a4\u30aa\u30a4\u30f3\u30d5\u30a9\u30de\u30c6\u30a3\u30af\u30b9\u3084\u91d1\u878d\u5206\u91ce\u3067\u306e\u5fdc\u7528\u4e8b\u4f8b\u3092\u8a73\u7d30\u306b\u5206\u6790\u3057\u3001\u9818\u57df\u56fa\u6709\u306e\u8ab2\u984c\u3068\u89e3\u6c7a\u7b56\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u7406\u8ad6\u7814\u7a76\u8005\u3068\u5b9f\u52d9\u5bb6\u306e\u53cc\u65b9\u306b\u6709\u7528\u306a\u77e5\u898b\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3092\u76ee\u6307\u3059\u3002</p>"},{"location":"research/note/variable-importance/#12","title":"1.2 \u8ad6\u6587\u306e\u69cb\u6210","text":"<p>\u672c\u8ad6\u6587\u306e\u69cb\u6210\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3042\u308b\u3002\u7b2c2\u7ae0\u3067\u306f\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7684\u57fa\u76e4\u306b\u3064\u3044\u3066\u8ad6\u3058\u3001\u30e2\u30c7\u30eb\u4f9d\u5b58\u578b\u30fb\u975e\u4f9d\u5b58\u578b\u306e\u91cd\u8981\u5ea6\u6307\u6a19\u306e\u6570\u5b66\u7684\u5b9a\u7fa9\u3068\u6f38\u8fd1\u7279\u6027\u3092\u8a73\u8ff0\u3059\u308b\u3002\u7b2c3\u7ae0\u3067\u306f\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306b\u7126\u70b9\u3092\u5f53\u3066\u3001MDI\uff08Mean Decrease Impurity\uff09\u3068MDA\uff08Mean Decrease Accuracy\uff09\u306e\u8a08\u7b97\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u7406\u8ad6\u7684\u6027\u8cea\u3001\u304a\u3088\u3073\u30d0\u30a4\u30a2\u30b9\u554f\u984c\u3068\u305d\u306e\u5bfe\u7b56\u3092\u5206\u6790\u3059\u308b\u3002\u7b2c4\u7ae0\u3067\u306f\u5909\u6570\u9593\u306e\u76f8\u95a2\u554f\u984c\u3068\u305d\u306e\u89e3\u6c7a\u30a2\u30d7\u30ed\u30fc\u30c1\u306b\u3064\u3044\u3066\u8ff0\u3079\u3001\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u3084\u6761\u4ef6\u4ed8\u304d\u91cd\u8981\u5ea6\u306a\u3069\u306e\u9ad8\u5ea6\u306a\u624b\u6cd5\u3092\u7d39\u4ecb\u3059\u308b\u3002\u7b2c5\u7ae0\u3067\u306f\u5404\u624b\u6cd5\u306e\u6bd4\u8f03\u8a55\u4fa1\u3068\u9069\u7528\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u3092\u63d0\u793a\u3059\u308b\u3002\u7b2c6\u7ae0\u3067\u306f\u533b\u7642\u30fb\u30d0\u30a4\u30aa\u30a4\u30f3\u30d5\u30a9\u30de\u30c6\u30a3\u30af\u30b9\u3068\u91d1\u878d\u30ea\u30b9\u30af\u8a55\u4fa1\u306b\u304a\u3051\u308b\u8a73\u7d30\u306a\u5fdc\u7528\u4e8b\u4f8b\u3092\u7d39\u4ecb\u3059\u308b\u3002\u7b2c7\u7ae0\u3067\u306f\u6700\u65b0\u306e\u7814\u7a76\u52d5\u5411\u3068\u65b0\u305f\u306a\u624b\u6cd5\u3092\u6982\u89b3\u3057\u3001\u7b2c8\u7ae0\u3067\u306f\u7814\u7a76\u306e\u30ae\u30e3\u30c3\u30d7\u3068\u5c06\u6765\u306e\u65b9\u5411\u6027\u306b\u3064\u3044\u3066\u8ad6\u3058\u308b\u3002\u6700\u5f8c\u306b\u7b2c9\u7ae0\u3067\u672c\u30ec\u30d3\u30e5\u30fc\u306e\u7d50\u8ad6\u3068\u5c55\u671b\u3092\u307e\u3068\u3081\u308b\u3002</p>"},{"location":"research/note/variable-importance/#2","title":"2. \u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7684\u57fa\u76e4","text":""},{"location":"research/note/variable-importance/#21","title":"2.1 \u30e2\u30c7\u30eb\u4f9d\u5b58\u578b\u3068\u975e\u4f9d\u5b58\u578b\u306e\u91cd\u8981\u5ea6\u6307\u6a19","text":"<p>\u5909\u6570\u91cd\u8981\u5ea6\u306f\u3001\u5927\u304d\u304f\u5206\u3051\u3066\u30e2\u30c7\u30eb\u4f9d\u5b58\u578b\uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u4f9d\u5b58\u578b\uff09\u91cd\u8981\u5ea6\u3068\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\uff08\u4eba\u53e3\u6c34\u6e96\uff09\u91cd\u8981\u5ea6\u306e2\u7a2e\u985e\u306b\u5206\u985e\u3067\u304d\u308b\uff08Williamson et al., 2021\uff09\u3002\u3053\u306e\u533a\u5225\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5b9a\u7fa9\u3001\u63a8\u5b9a\u3001\u89e3\u91c8\u306b\u304a\u3044\u3066\u6839\u672c\u7684\u306a\u9055\u3044\u3092\u3082\u305f\u3089\u3059\u3002</p> <p>\u30e2\u30c7\u30eb\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306f\u3001\u7279\u5b9a\u306e\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u5404\u5909\u6570\u306e\u5bc4\u4e0e\u5ea6\u3092\u8a55\u4fa1\u3059\u308b\u6307\u6a19\u3067\u3042\u308b\u3002\u4f8b\u3048\u3070\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306eMDI\uff08Mean Decrease Impurity\uff09\u3084MDA\uff08Mean Decrease Accuracy\uff09\u3001\u7dda\u5f62\u30e2\u30c7\u30eb\u306e\u4fc2\u6570\u306e\u7d76\u5bfe\u5024\u306a\u3069\u304c\u3053\u308c\u306b\u542b\u307e\u308c\u308b\u3002\u5f62\u5f0f\u7684\u306b\u306f\u3001\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb \\(\\hat{f}\\) \u306b\u5bfe\u3057\u3066\u3001\u5909\u6570 \\(X_j\\) \u306e\u91cd\u8981\u5ea6\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3067\u304d\u308b\uff1a</p> \\[VI_{model}(X_j, \\hat{f}) = \\mathcal{V}(X_j, \\hat{f})\\] <p>\u3053\u3053\u3067 \\(\\mathcal{V}\\) \u306f\u30e2\u30c7\u30eb \\(\\hat{f}\\) \u306b\u304a\u3051\u308b\u5909\u6570 \\(X_j\\) \u306e\u5bc4\u4e0e\u5ea6\u3092\u8a55\u4fa1\u3059\u308b\u95a2\u6570\u3067\u3042\u308b\u3002\u3053\u306e\u7a2e\u306e\u91cd\u8981\u5ea6\u6307\u6a19\u306f\u3001\u30e2\u30c7\u30eb\u306e\u9078\u629e\u3084\u30d1\u30e9\u30e1\u30fc\u30bf\u3001\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u5f37\u304f\u4f9d\u5b58\u3059\u308b\u305f\u3081\u3001\u7570\u306a\u308b\u30e2\u30c7\u30eb\u9593\u3067\u306e\u6bd4\u8f03\u304c\u96e3\u3057\u304f\u3001\u89e3\u91c8\u306b\u6ce8\u610f\u3092\u8981\u3059\u308b\uff08Strobl et al., 2007; Hooker &amp; Mentch, 2019\uff09\u3002</p> <p>\u4e00\u65b9\u3001\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306f\u3001\u771f\u306e\u30c7\u30fc\u30bf\u751f\u6210\u904e\u7a0b\u306b\u304a\u3051\u308b\u5909\u6570\u306e\u4e88\u6e2c\u53ef\u80fd\u6027\u3078\u306e\u5bc4\u4e0e\u3092\u5b9a\u7fa9\u3059\u308b\u3082\u306e\u3067\u3001\u7279\u5b9a\u306e\u30e2\u30c7\u30eb\u306b\u4f9d\u5b58\u305b\u305a\u300c\u305d\u306e\u5909\u6570\u304c\u3042\u308c\u3070\u7406\u8ad6\u4e0a\u3069\u308c\u3060\u3051\u4e88\u6e2c\u30ea\u30b9\u30af\u3092\u4e0b\u3052\u3089\u308c\u308b\u304b\u300d\u3092\u6e2c\u308b\uff08Williamson et al., 2020\uff09\u3002\u5f62\u5f0f\u7684\u306b\u306f\u3001\u5168\u3066\u306e\u7279\u5fb4\u91cf\u96c6\u5408 \\(\\mathbf{X} = (X_1, \\ldots, X_p)\\) \u3068\u76ee\u7684\u5909\u6570 \\(Y\\) \u306e\u771f\u306e\u540c\u6642\u5206\u5e03 \\(P\\) \u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a</p> \\[VI_{pop}(X_j, P) = \\mathcal{R}(\\mathbf{X}_{-j}, P) - \\mathcal{R}(\\mathbf{X}, P)\\] <p>\u3053\u3053\u3067 \\(\\mathcal{R}(\\mathbf{X}, P)\\) \u306f\u7279\u5fb4\u91cf\u96c6\u5408 \\(\\mathbf{X}\\) \u3092\u7528\u3044\u305f\u5834\u5408\u306e\u6700\u826f\u306e\u4e88\u6e2c\u30e2\u30c7\u30eb\u306e\u30ea\u30b9\u30af\uff08\u4f8b\uff1a\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u3084\u5bfe\u6570\u640d\u5931\uff09\u3067\u3042\u308a\u3001\\(\\mathbf{X}_{-j}\\) \u306f \\(X_j\\) \u3092\u9664\u3044\u305f\u7279\u5fb4\u91cf\u96c6\u5408\u3067\u3042\u308b\u3002\u3053\u306e\u5b9a\u7fa9\u306f\u3001\u5909\u6570 \\(X_j\\) \u304c\u4e88\u6e2c\u306b\u304a\u3044\u3066\u63d0\u4f9b\u3059\u308b\u72ec\u81ea\u306e\u60c5\u5831\u91cf\u3092\u8868\u3057\u3066\u3044\u308b\u3002</p> <p>\u3053\u306e\u4e8c\u3064\u306e\u6982\u5ff5\u306e\u9055\u3044\u306f\u91cd\u8981\u3067\u3042\u308b\u3002\u30e2\u30c7\u30eb\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306f\u5177\u4f53\u7684\u306a\u30e2\u30c7\u30eb\u306e\u6319\u52d5\u3092\u8aac\u660e\u3059\u308b\u306e\u306b\u5f79\u7acb\u3064\u304c\u3001\u30e2\u30c7\u30eb\u9078\u629e\u306b\u3088\u3063\u3066\u7d50\u679c\u304c\u5909\u308f\u308a\u5f97\u308b\u3002\u4e00\u65b9\u3001\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306f\u771f\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u306b\u304a\u3051\u308b\u5909\u6570\u306e\u672c\u8cea\u7684\u306a\u91cd\u8981\u6027\u3092\u6e2c\u308b\u305f\u3081\u3001\u3088\u308a\u4e00\u822c\u7684\u306a\u77e5\u898b\u3092\u63d0\u4f9b\u3059\u308b\u304c\u3001\u76f4\u63a5\u89b3\u6e2c\u3067\u304d\u306a\u3044\u7406\u8ad6\u7684\u306a\u6982\u5ff5\u3067\u3042\u308a\u3001\u63a8\u5b9a\u304c\u96e3\u3057\u3044\uff08Williamson et al., 2021\uff09\u3002</p>"},{"location":"research/note/variable-importance/#22","title":"2.2 \u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7684\u67a0\u7d44\u307f","text":"<p>\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306b\u95a2\u3059\u308b\u7406\u8ad6\u7684\u7814\u7a76\u306f\u3001\u8fd1\u5e74\u6025\u901f\u306b\u767a\u5c55\u3057\u3066\u3044\u308b\u3002\u4ee5\u4e0b\u3067\u306f\u4ee3\u8868\u7684\u306a\u7406\u8ad6\u7684\u67a0\u7d44\u307f\u306b\u3064\u3044\u3066\u6570\u5b66\u7684\u306b\u8a73\u8ff0\u3059\u308b\u3002</p>"},{"location":"research/note/variable-importance/#221-loco-leave-one-covariate-out","title":"2.2.1 LOCO (Leave-One-Covariate-Out)","text":"<p>Lei et al. (2018) \u304c\u63d0\u6848\u3057\u305fLOCO\uff08Leave-One-Covariate-Out\uff09\u306f\u3001\u5909\u6570 \\(X_j\\) \u3092\u9664\u5916\u3057\u305f\u5834\u5408\u306e\u4e88\u6e2c\u6027\u80fd\u306e\u4f4e\u4e0b\u3092\u91cd\u8981\u5ea6\u3068\u3057\u3066\u5b9a\u7fa9\u3059\u308b\u3002\u5f62\u5f0f\u7684\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u308b\uff1a</p> \\[LOCO(X_j) = \\mathbb{E}[L(Y, f_{\\mathbf{X}_{-j}}(\\mathbf{X}_{-j}))] - \\mathbb{E}[L(Y, f_{\\mathbf{X}}(\\mathbf{X}))]\\] <p>\u3053\u3053\u3067 \\(L\\) \u306f\u640d\u5931\u95a2\u6570\u3001\\(f_{\\mathbf{X}}\\) \u306f\u5168\u5909\u6570\u3092\u7528\u3044\u305f\u6700\u9069\u4e88\u6e2c\u95a2\u6570\u3001\\(f_{\\mathbf{X}_{-j}}\\) \u306f\u5909\u6570 \\(X_j\\) \u3092\u9664\u3044\u305f\u6700\u9069\u4e88\u6e2c\u95a2\u6570\u3067\u3042\u308b\u3002LOCO\u306f\u76f4\u89b3\u7684\u306a\u5b9a\u7fa9\u3067\u3042\u308b\u304c\u3001\u5404\u5909\u6570\u306b\u3064\u3044\u3066\u5225\u3005\u306e\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u3044\u3002\u307e\u305f\u3001\u7406\u8ad6\u7684\u306b\u306f\u4e8c\u6b21\u7684\u306a\u6c4e\u95a2\u6570\uff08\u30ea\u30b9\u30af\u5dee\uff09\u3067\u3042\u308b\u305f\u3081\u3001\u6f38\u8fd1\u5206\u5e03\u304c\u8907\u96d1\u3067\u3042\u308a\u3001\u4fe1\u983c\u533a\u9593\u306e\u69cb\u6210\u304c\u6280\u8853\u7684\u306b\u96e3\u3057\u3044\uff08Williamson et al., 2021\uff09\u3002</p>"},{"location":"research/note/variable-importance/#222","title":"2.2.2 \u52b9\u7387\u7684\u306a\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u63a8\u5b9a","text":"<p>Williamson et al. (2020, 2021) \u306f\u3001\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u3092\u3088\u308a\u52b9\u7387\u7684\u306b\u63a8\u5b9a\u3059\u308b\u534a\u30d1\u30e9\u30e1\u30c8\u30ea\u30c3\u30af\u63a8\u8ad6\u306e\u67a0\u7d44\u307f\u3092\u63d0\u6848\u3057\u305f\u3002\u5f7c\u3089\u306f\u5909\u6570\u91cd\u8981\u5ea6\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3057\u3066\u3044\u308b\uff1a</p> \\[\\psi_0(j) = \\mathbb{E}[R(f_{\\mathbf{X}_{-j}}(\\mathbf{X}_{-j}), Y) - R(f_{\\mathbf{X}}(\\mathbf{X}), Y)]\\] <p>\u3053\u3053\u3067 \\(R\\) \u306f\u4e88\u6e2c\u30ea\u30b9\u30af\u95a2\u6570\u3067\u3042\u308b\u3002\u3053\u306e\u5b9a\u7fa9\u306f\u672c\u8cea\u7684\u306bLOCO\u3068\u540c\u69d8\u3060\u304c\u3001\u63a8\u5b9a\u65b9\u6cd5\u304c\u7570\u306a\u308b\u3002Williamson\u3089\u306f\u5f71\u97ff\u95a2\u6570\u306b\u57fa\u3065\u3044\u305f\u4e8c\u6bb5\u968e\u63a8\u5b9a\u6cd5\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002\u307e\u305a\u521d\u671f\u63a8\u5b9a\u91cf\u3068\u3057\u3066\u6a5f\u68b0\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067 \\(f_{\\mathbf{X}}\\) \u3068 \\(f_{\\mathbf{X}_{-j}}\\) \u3092\u63a8\u5b9a\u3057\u3001\u6b21\u306b\u4e00\u6bb5\u968e\u66f4\u65b0\u3068\u3057\u3066\u4ee5\u4e0b\u306e\u63a8\u5b9a\u91cf\u3092\u4f7f\u7528\u3059\u308b\uff1a</p> \\[\\hat{\\psi}(j) = \\frac{1}{n}\\sum_{i=1}^{n}[R(f_{\\mathbf{X}_{-j}}(\\mathbf{X}_{-j,i}), Y_i) - R(f_{\\mathbf{X}}(\\mathbf{X}_i), Y_i)] + \\hat{\\phi}(O_i)\\] <p>\u3053\u3053\u3067 \\(\\hat{\\phi}\\) \u306f\u63a8\u5b9a\u3055\u308c\u305f\u5f71\u97ff\u95a2\u6570\u3067\u3042\u308a\u3001\u3053\u308c\u306b\u3088\u308a \\(\\sqrt{n}\\)-\u4e00\u81f4\u6027\u3068\u6f38\u8fd1\u6b63\u898f\u6027\u304c\u4fdd\u8a3c\u3055\u308c\u3001\u4fe1\u983c\u533a\u9593\u306e\u69cb\u7bc9\u304c\u53ef\u80fd\u306b\u306a\u308b\u3002\u3053\u306e\u67a0\u7d44\u307f\u306f\u4efb\u610f\u306e\u6a5f\u68b0\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u7d44\u307f\u5408\u308f\u305b\u3089\u308c\u308b\u67d4\u8edf\u6027\u3092\u6301\u3064\u3002</p>"},{"location":"research/note/variable-importance/#223-floodgate","title":"2.2.3 Floodgate","text":"<p>Zhang &amp; Janson (2020) \u304c\u63d0\u6848\u3057\u305fFloodgate\u306f\u3001\u9ad8\u6b21\u5143\u8a2d\u5b9a\u3067\u306e\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u63a8\u5b9a\u306b\u9069\u3057\u305f\u624b\u6cd5\u3067\u3042\u308b\u3002\u5f7c\u3089\u306f\u307e\u305a\u300c\u6700\u5c0f\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u30ae\u30e3\u30c3\u30d7 (mMSE gap)\u300d\u3068\u3044\u3046\u91cd\u8981\u5ea6\u6307\u6a19\u3092\u5b9a\u7fa9\u3057\u305f\uff1a</p> \\[\\Delta_j = \\mathbb{E}[(Y - \\mathbb{E}[Y|\\mathbf{X}_{-j}])^2] - \\mathbb{E}[(Y - \\mathbb{E}[Y|\\mathbf{X}])^2]\\] <p>\u3053\u308c\u306f\u5909\u6570 \\(X_j\\) \u304c\u4ed6\u306e\u5909\u6570\u3092\u6761\u4ef6\u4ed8\u3051\u305f\u4e0a\u3067\u76ee\u7684\u5909\u6570 \\(Y\\) \u306b\u5bfe\u3057\u3066\u3069\u308c\u3060\u3051\u8ffd\u52a0\u7684\u306a\u8aac\u660e\u529b\u3092\u6301\u3064\u304b\u3092\u8868\u3059\u3002Floodgate\u306f\u4efb\u610f\u306e\u30ef\u30fc\u30ad\u30f3\u30b0\u30e2\u30c7\u30eb \\(\\hat{f}\\) \u3092\u7528\u3044\u3066\u3053\u306e\u6307\u6a19\u306b\u5bfe\u3059\u308b\u4fe1\u983c\u4e0b\u9650\u3092\u69cb\u7bc9\u3059\u308b\uff1a</p> \\[\\hat{\\Delta}_j^{lb} = \\hat{\\mu}_j - z_{\\alpha}\\hat{\\sigma}_j/\\sqrt{n}\\] <p>\u3053\u3053\u3067 \\(\\hat{\\mu}_j\\) \u306f\u63a8\u5b9a\u5024\u3001\\(\\hat{\\sigma}_j\\) \u306f\u305d\u306e\u6a19\u6e96\u8aa4\u5dee\u3001\\(z_{\\alpha}\\) \u306f\u6709\u610f\u6c34\u6e96 \\(\\alpha\\) \u306b\u5bfe\u5fdc\u3059\u308b\u6a19\u6e96\u6b63\u898f\u5206\u4f4d\u70b9\u3067\u3042\u308b\u3002\u3053\u306e\u624b\u6cd5\u306e\u7279\u5fb4\u306f\u3001\u30ef\u30fc\u30ad\u30f3\u30b0\u30e2\u30c7\u30eb\u306e\u8cea\u306b\u5fdc\u3058\u3066\u63a8\u5b9a\u7cbe\u5ea6\u304c\u81ea\u52d5\u7684\u306b\u9069\u5fdc\u3059\u308b\u70b9\u3067\u3042\u308b\u3002\u30e2\u30c7\u30eb\u304c\u512a\u308c\u3066\u3044\u308b\u307b\u3069\u4fe1\u983c\u533a\u9593\u306f\u72ed\u304f\u306a\u308a\u3001\u30e2\u30c7\u30eb\u304c\u4e0d\u5b8c\u5168\u3067\u3082\u4fdd\u5b88\u7684\u306a\u4e0b\u9650\u3092\u4fdd\u8a3c\u3059\u308b\uff08Zhang &amp; Janson, 2020\uff09\u3002</p>"},{"location":"research/note/variable-importance/#224","title":"2.2.4 \u6f38\u8fd1\u7684\u6027\u8cea\u3068\u7406\u8ad6\u4fdd\u8a3c","text":"<p>\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7814\u7a76\u3067\u306f\u3001\u6f38\u8fd1\u7684\u6027\u8cea\u3068\u7406\u8ad6\u4fdd\u8a3c\u306b\u95a2\u3059\u308b\u91cd\u8981\u306a\u6210\u679c\u304c\u5f97\u3089\u308c\u3066\u3044\u308b\u3002\u4f8b\u3048\u3070\u3001Williamson et al. (2021) \u306f\u5f7c\u3089\u306e\u63a8\u5b9a\u91cf\u304c \\(\\sqrt{n}\\)-\u4e00\u81f4\u6027\u3068\u6f38\u8fd1\u6b63\u898f\u6027\u3092\u6301\u3064\u3053\u3068\u3092\u8a3c\u660e\u3057\u3066\u3044\u308b\uff1a</p> \\[\\sqrt{n}(\\hat{\\psi}(j) - \\psi_0(j)) \\stackrel{d}{\\rightarrow} N(0, \\sigma^2_j)\\] <p>\u3053\u3053\u3067 \\(\\sigma^2_j\\) \u306f\u5909\u6570 \\(j\\) \u306e\u91cd\u8981\u5ea6\u63a8\u5b9a\u91cf\u306e\u6f38\u8fd1\u5206\u6563\u3067\u3042\u308b\u3002\u3053\u306e\u7d50\u679c\u306b\u57fa\u3065\u304d\u3001\u91cd\u8981\u5ea6\u306b\u5bfe\u3059\u308b\u4fe1\u983c\u533a\u9593\u3092\u69cb\u7bc9\u3057\u3001\u300c\u5909\u6570 \\(X_j\\) \u306f\u91cd\u8981\u3067\u306f\u306a\u3044\uff08\\(\\psi_0(j) = 0\\)\uff09\u300d\u3068\u3044\u3046\u5e30\u7121\u4eee\u8aac\u306e\u691c\u5b9a\u304c\u53ef\u80fd\u3068\u306a\u308b\u3002</p> <p>Floodgate\u306b\u3064\u3044\u3066\u306f\u3001Zhang &amp; Janson (2020) \u304c\u6f38\u8fd1\u7684\u306b\u6709\u52b9\u306a\u4fe1\u983c\u4e0b\u9650\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u3066\u3044\u308b\uff1a</p> \\[\\liminf_{n\\rightarrow\\infty}\\mathbb{P}(\\Delta_j \\geq \\hat{\\Delta}_j^{lb}) \\geq 1-\\alpha\\] <p>\u3055\u3089\u306b\u3001\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306e\u63a8\u5b9a\u306b\u304a\u3044\u3066\u3001\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba \\(n\\)\u3001\u6b21\u5143 \\(p\\)\u3001\u7279\u5fb4\u91cf\u306e\u6027\u8cea\u306a\u3069\u304c\u63a8\u5b9a\u7cbe\u5ea6\u306b\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3059\u308b\u304b\u306b\u3064\u3044\u3066\u306e\u7406\u8ad6\u7684\u5206\u6790\u3082\u9032\u3093\u3067\u3044\u308b\uff08Candes et al., 2018; Zhang &amp; Janson, 2020\uff09\u3002</p> <p>\u3053\u308c\u3089\u306e\u7406\u8ad6\u7814\u7a76\u306b\u3088\u308a\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u63a8\u5b9a\u3068\u63a8\u8ad6\u306b\u95a2\u3059\u308b\u53b3\u5bc6\u306a\u7d71\u8a08\u7684\u57fa\u76e4\u304c\u6574\u5099\u3055\u308c\u3064\u3064\u3042\u308a\u3001\u5358\u306a\u308b\u7d4c\u9a13\u7684\u624b\u6cd5\u304b\u3089\u7406\u8ad6\u7684\u306b\u4fdd\u8a3c\u3055\u308c\u305f\u65b9\u6cd5\u8ad6\u3078\u3068\u767a\u5c55\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#3","title":"3. \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6","text":""},{"location":"research/note/variable-importance/#31-mdimda","title":"3.1 MDI\u3068MDA\u306e\u8a08\u7b97\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u6570\u5b66\u7684\u5b9a\u7fa9","text":"<p>\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3067\u306f\u3001\u4e3b\u306b2\u7a2e\u985e\u306e\u5909\u6570\u91cd\u8981\u5ea6\u6307\u6a19\u304c\u7528\u3044\u3089\u308c\u3066\u3044\u308b\uff1aMDI\uff08Mean Decrease Impurity\uff09\u3068MDA\uff08Mean Decrease Accuracy\uff09\u3002\u4ee5\u4e0b\u3067\u306f\u3053\u308c\u3089\u306e\u8a08\u7b97\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u6570\u5b66\u7684\u5b9a\u7fa9\u3092\u8a73\u8ff0\u3059\u308b\u3002</p>"},{"location":"research/note/variable-importance/#311-mdimean-decrease-impurity","title":"3.1.1 MDI\uff08Mean Decrease Impurity\uff09","text":"<p>MDI\u306f\u5404\u6c7a\u5b9a\u6728\u306b\u304a\u3051\u308b\u4e0d\u7d14\u5ea6\u6e1b\u5c11\u91cf\u306b\u57fa\u3065\u304f\u91cd\u8981\u5ea6\u6307\u6a19\u3067\u3042\u308b\u3002\u6c7a\u5b9a\u6728\u306e\u5404\u30ce\u30fc\u30c9\u3067\u306f\u3001\u7279\u5fb4\u91cf \\(X_j\\) \u306b\u3088\u308b\u5206\u5272\u306b\u3088\u3063\u3066\u4e0d\u7d14\u5ea6\uff08\u30b8\u30cb\u4e0d\u7d14\u5ea6\u3084\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306a\u3069\uff09\u304c\u3069\u308c\u3060\u3051\u6e1b\u5c11\u3057\u305f\u304b\u3092\u8a08\u7b97\u3057\u3001\u305d\u308c\u3092\u6728\u5168\u4f53\u3067\u96c6\u8a08\u3059\u308b\u3002\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u5168\u4f53\u3067\u306f\u3001\u5168\u3066\u306e\u6728\u306b\u304a\u3051\u308b\u4e0d\u7d14\u5ea6\u6e1b\u5c11\u91cf\u306e\u5e73\u5747\u3092\u53d6\u308b\u3002</p> <p>\u5f62\u5f0f\u7684\u306b\u306f\u3001MDI\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff08Breiman, 2001; Louppe et al., 2013\uff09\uff1a</p> \\[MDI(X_j) = \\frac{1}{N_T}\\sum_{T}\\sum_{t \\in T: v(s_t)=j}p(t)\\Delta i(s_t, t)\\] <p>\u3053\u3053\u3067 \\(N_T\\) \u306f\u6c7a\u5b9a\u6728\u306e\u7dcf\u6570\u3001\\(T\\) \u306f\u500b\u3005\u306e\u6c7a\u5b9a\u6728\u3001\\(t\\) \u306f\u6728 \\(T\\) \u5185\u306e\u30ce\u30fc\u30c9\u3001\\(v(s_t)\\) \u306f\u5206\u5272 \\(s_t\\) \u3067\u4f7f\u7528\u3055\u308c\u308b\u5909\u6570\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3001\\(p(t)\\) \u306f\u30ce\u30fc\u30c9 \\(t\\) \u306b\u5230\u9054\u3059\u308b\u30b5\u30f3\u30d7\u30eb\u306e\u5272\u5408\u3001\\(\\Delta i(s_t, t)\\) \u306f\u5206\u5272 \\(s_t\\) \u306b\u3088\u308b\u30ce\u30fc\u30c9 \\(t\\) \u3067\u306e\u4e0d\u7d14\u5ea6\u6e1b\u5c11\u91cf\u3067\u3042\u308b\u3002</p> <p>\u4e0d\u7d14\u5ea6\u6e1b\u5c11\u91cf \\(\\Delta i(s_t, t)\\) \u306f\u3001\u89aa\u30ce\u30fc\u30c9\u306e\u4e0d\u7d14\u5ea6\u304b\u3089\u5b50\u30ce\u30fc\u30c9\u306e\u52a0\u91cd\u5e73\u5747\u4e0d\u7d14\u5ea6\u3092\u5f15\u3044\u305f\u3082\u306e\u3067\u3042\u308b\uff1a</p> \\[\\Delta i(s_t, t) = i(t) - p_L i(t_L) - p_R i(t_R)\\] <p>\u3053\u3053\u3067 \\(i(t)\\) \u306f\u30ce\u30fc\u30c9 \\(t\\) \u306e\u4e0d\u7d14\u5ea6\u3001\\(p_L\\) \u3068 \\(p_R\\) \u306f\u5de6\u53f3\u306e\u5b50\u30ce\u30fc\u30c9\u306b\u5272\u308a\u5f53\u3066\u3089\u308c\u308b\u30b5\u30f3\u30d7\u30eb\u306e\u5272\u5408\u3001\\(i(t_L)\\) \u3068 \\(i(t_R)\\) \u306f\u5de6\u53f3\u306e\u5b50\u30ce\u30fc\u30c9\u306e\u4e0d\u7d14\u5ea6\u3067\u3042\u308b\u3002</p> <p>\u5206\u985e\u554f\u984c\u3067\u306f\u3001\u4e0d\u7d14\u5ea6 \\(i(t)\\) \u3068\u3057\u3066\u30b8\u30cb\u4e0d\u7d14\u5ea6\u304c\u4e00\u822c\u7684\u306b\u4f7f\u7528\u3055\u308c\u308b\uff1a</p> \\[i_{gini}(t) = \\sum_{k=1}^{K}p_k(t)(1-p_k(t)) = 1 - \\sum_{k=1}^{K}p_k(t)^2\\] <p>\u3053\u3053\u3067 \\(p_k(t)\\) \u306f\u30ce\u30fc\u30c9 \\(t\\) \u306b\u304a\u3051\u308b\u30af\u30e9\u30b9 \\(k\\) \u306e\u5272\u5408\u3067\u3042\u308b\u3002</p> <p>\u56de\u5e30\u554f\u984c\u3067\u306f\u3001\u4e0d\u7d14\u5ea6\u3068\u3057\u3066\u5206\u6563\u304c\u4f7f\u7528\u3055\u308c\u308b\uff1a</p> \\[i_{var}(t) = \\frac{1}{N_t}\\sum_{i \\in t}(y_i - \\bar{y}_t)^2\\] <p>\u3053\u3053\u3067 \\(N_t\\) \u306f\u30ce\u30fc\u30c9 \\(t\\) \u306e\u30b5\u30f3\u30d7\u30eb\u6570\u3001\\(y_i\\) \u306f\u5fdc\u7b54\u5909\u6570\u306e\u5024\u3001\\(\\bar{y}_t\\) \u306f\u30ce\u30fc\u30c9 \\(t\\) \u306b\u304a\u3051\u308b\u5fdc\u7b54\u5909\u6570\u306e\u5e73\u5747\u5024\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#312-mdamean-decrease-accuracy","title":"3.1.2 MDA\uff08Mean Decrease Accuracy\uff09\u307e\u305f\u306f\u30d1\u30fc\u30df\u30e5\u30c6\u30fc\u30b7\u30e7\u30f3\u91cd\u8981\u5ea6","text":"<p>MDA\u306f\u30d1\u30fc\u30df\u30e5\u30c6\u30fc\u30b7\u30e7\u30f3\uff08\u7f6e\u63db\uff09\u6cd5\u306b\u57fa\u3065\u304f\u91cd\u8981\u5ea6\u6307\u6a19\u3067\u3042\u308a\u3001\u7279\u5fb4\u91cf \\(X_j\\) \u306e\u5024\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u305f\u5834\u5408\u306e\u4e88\u6e2c\u7cbe\u5ea6\u306e\u4f4e\u4e0b\u3092\u6e2c\u5b9a\u3059\u308b\u3002\u5f62\u5f0f\u7684\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff08Breiman, 2001; Strobl et al., 2008\uff09\uff1a</p> \\[MDA(X_j) = \\frac{1}{N_T}\\sum_{T}\\left(err_T(\\tilde{X}_j) - err_T(X_j)\\right)\\] <p>\u3053\u3053\u3067 \\(N_T\\) \u306f\u6c7a\u5b9a\u6728\u306e\u7dcf\u6570\u3001\\(err_T(X_j)\\) \u306f\u901a\u5e38\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u305f\u6728 \\(T\\) \u306e\u4e88\u6e2c\u8aa4\u5dee\u3001\\(err_T(\\tilde{X}_j)\\) \u306f\u5909\u6570 \\(X_j\\) \u3092\u30e9\u30f3\u30c0\u30e0\u306b\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u305f\u969b\u306e\u4e88\u6e2c\u8aa4\u5dee\u3067\u3042\u308b\u3002</p> <p>\u5b9f\u88c5\u4e0a\u306f\u3001\u30a2\u30a6\u30c8\u30aa\u30d6\u30d0\u30c3\u30b0\uff08OOB\uff09\u30c7\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u305f\u5909\u5f62\u7248\u304c\u4e00\u822c\u7684\u3067\u3042\u308b\uff1a</p> \\[MDA_{OOB}(X_j) = \\frac{1}{N_T}\\sum_{T}\\left(err_{T,OOB}(\\tilde{X}_j) - err_{T,OOB}(X_j)\\right)\\] <p>\u3053\u3053\u3067 \\(err_{T,OOB}\\) \u306f\u5404\u6728\u306e\u69cb\u7bc9\u306b\u4f7f\u7528\u3055\u308c\u306a\u304b\u3063\u305fOOB\u30b5\u30f3\u30d7\u30eb\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u8aa4\u5dee\u3067\u3042\u308b\u3002</p> <p>\u5206\u985e\u554f\u984c\u306e\u5834\u5408\u3001\u8aa4\u5dee\u306f\u8aa4\u5206\u985e\u7387\u3067\u6e2c\u5b9a\u3055\u308c\u308b\uff1a</p> \\[err_T(X_j) = \\frac{1}{N_{OOB}}\\sum_{i \\in OOB} I(\\hat{y}_i \\neq y_i)\\] <p>\u56de\u5e30\u554f\u984c\u306e\u5834\u5408\u3001\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u304c\u4e00\u822c\u7684\u306b\u4f7f\u7528\u3055\u308c\u308b\uff1a</p> \\[err_T(X_j) = \\frac{1}{N_{OOB}}\\sum_{i \\in OOB} (y_i - \\hat{y}_i)^2\\] <p>\u3053\u3053\u3067 \\(N_{OOB}\\) \u306fOOB\u30b5\u30f3\u30d7\u30eb\u306e\u6570\u3001\\(\\hat{y}_i\\) \u306f\u4e88\u6e2c\u5024\u3001\\(y_i\\) \u306f\u771f\u306e\u5024\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#32","title":"3.2 \u7406\u8ad6\u7684\u6027\u8cea\u3068\u6570\u5b66\u7684\u7279\u6027","text":"<p>MDI\u3068MDA\u306e\u7406\u8ad6\u7684\u6027\u8cea\u3068\u6570\u5b66\u7684\u7279\u6027\u306b\u3064\u3044\u3066\u3001\u4e3b\u8981\u306a\u7814\u7a76\u6210\u679c\u3092\u4ee5\u4e0b\u306b\u307e\u3068\u3081\u308b\u3002</p>"},{"location":"research/note/variable-importance/#321-mdi","title":"3.2.1 MDI\u306e\u7406\u8ad6\u7684\u6027\u8cea","text":"<p>Louppe et al. (2013) \u306f\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306eMDI\u91cd\u8981\u5ea6\u306e\u6f38\u8fd1\u7279\u6027\u3092\u7406\u8ad6\u7684\u306b\u5206\u6790\u3057\u305f\u3002\u7121\u9650\u306e\u30c7\u30fc\u30bf\u3068\u7121\u9650\u306e\u6728\u304b\u3089\u306a\u308b\u300c\u5b8c\u5168\u30e9\u30f3\u30c0\u30e0\u6728\uff08Totally Randomized Trees\uff09\u300d\u306e\u6975\u9650\u3067\u306f\u3001MDI\u306f\u60c5\u5831\u7406\u8ad6\u7684\u6307\u6a19\u3068\u3057\u3066\u89e3\u91c8\u3067\u304d\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5206\u89e3\u3067\u304d\u308b\u3053\u3068\u3092\u8a3c\u660e\u3057\u305f\uff1a</p> \\[MDI(X_j) = \\sum_{k=1}^p\\frac{1}{k}\\sum_{B \\in \\mathcal{P}_k(\\{1,\\ldots,p\\} \\setminus \\{j\\})}\\frac{1}{\\binom{p-1}{k}}I(X_j; Y | X_B)\\] <p>\u3053\u3053\u3067 \\(I(X_j; Y | X_B)\\) \u306f\u5909\u6570\u96c6\u5408 \\(X_B\\) \u3092\u6761\u4ef6\u4ed8\u3051\u305f\u5834\u5408\u306e \\(X_j\\) \u3068 \\(Y\\) \u306e\u6761\u4ef6\u4ed8\u304d\u76f8\u4e92\u60c5\u5831\u91cf\u3067\u3042\u308b\u3002\u3053\u306e\u7d50\u679c\u304b\u3089\u3001\u7406\u8ad6\u7684\u7279\u6027\u3068\u3057\u3066\u3001\u771f\u306b\u7121\u95a2\u4fc2\u306a\u5909\u6570\u306e\u5834\u5408\u306f \\(X_j\\) \u304c\u76ee\u7684\u5909\u6570 \\(Y\\) \u3068\u6761\u4ef6\u4ed8\u304d\u72ec\u7acb\u3067\u3042\u308c\u3070\u3001\u6f38\u8fd1\u7684\u306b \\(MDI(X_j) = 0\\) \u3068\u306a\u308b\u3053\u3068\u3001\u307e\u305f\u5909\u6570\u306e\u8ffd\u52a0\u30fb\u524a\u9664\u306b\u5bfe\u3059\u308b\u4e0d\u5909\u6027\u3068\u3057\u3066\u7121\u95a2\u4fc2\u306a\u5909\u6570\u3092\u8ffd\u52a0\u30fb\u524a\u9664\u3057\u3066\u3082\u3001\u95a2\u9023\u306e\u3042\u308b\u5909\u6570\u306e\u91cd\u8981\u5ea6\u306f\u5909\u5316\u3057\u306a\u3044\u3053\u3068\u304c\u5c0e\u304b\u308c\u308b\u3002</p> <p>\u3057\u304b\u3057\u3001Scornet (2020) \u306f\u73fe\u5b9f\u7684\u306a\u8a2d\u5b9a\uff08\u6709\u9650\u30b5\u30f3\u30d7\u30eb\u3001Breiman\u306e\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff09\u3067\u306f\u3001MDI\u306f\u4e00\u822c\u306b\u4e00\u610f\u306b\u5b9a\u7fa9\u3067\u304d\u305a\u3001\u540c\u3058\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3082\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8a73\u7d30\uff08\u6728\u306e\u6df1\u3055\u3001\u4e71\u6570\u7a2e\u306a\u3069\uff09\u306b\u3088\u3063\u3066\u7d50\u679c\u304c\u5909\u308f\u308a\u3046\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u3002\u307e\u305f\u3001\u5909\u6570\u9593\u306e\u76f8\u95a2\u304c\u5f37\u3044\u5834\u5408\u3001MDI\u306e\u89e3\u91c8\u306f\u7279\u306b\u8907\u96d1\u306b\u306a\u308b\u3002</p>"},{"location":"research/note/variable-importance/#322-mda","title":"3.2.2 MDA\u306e\u7406\u8ad6\u7684\u6027\u8cea","text":"<p>MDA\u306b\u3064\u3044\u3066\u3082\u7406\u8ad6\u7684\u306a\u89e3\u6790\u304c\u3044\u304f\u3064\u304b\u884c\u308f\u308c\u3066\u3044\u308b\u3002Gregorutti et al. (2017) \u306f\u3001\u52a0\u6cd5\u30e2\u30c7\u30eb\uff08\\(Y = \\sum_{j=1}^p f_j(X_j) + \\varepsilon\\)\uff09\u304b\u3064\u7279\u5fb4\u91cf\u304c\u72ec\u7acb\u306e\u5834\u5408\u3001MDA\u91cd\u8981\u5ea6\u304c\u5404\u7279\u5fb4\u91cf\u306e\u5bc4\u4e0e\u306e\u5206\u6563\u306b\u6bd4\u4f8b\u3059\u308b\u3053\u3068\u3092\u793a\u3057\u305f\uff1a</p> \\[MDA(X_j) \\propto Var(f_j(X_j))\\] <p>\u3053\u306e\u7d50\u679c\u306f\u76f4\u89b3\u7684\u306b\u89e3\u91c8\u3057\u3084\u3059\u3044\u304c\u3001\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3084\u76f8\u4e92\u4f5c\u7528\u304c\u3042\u308b\u5834\u5408\u306b\u306f\u6210\u7acb\u3057\u306a\u3044\u3002</p> <p>Ishwaran (2007) \u306f\u3001\u5909\u5f62\u7248\u306ePermutation\u91cd\u8981\u5ea6\uff08Variable Hunting\uff09\u306b\u3064\u3044\u3066\u3001\u771f\u306b\u95a2\u9023\u306e\u3042\u308b\u5909\u6570\u306e\u91cd\u8981\u5ea6\u304c\u6f38\u8fd1\u7684\u306b\u6b63\u306e\u4e0b\u9650\u3092\u6301\u3064\u3053\u3068\u3092\u8a3c\u660e\u3057\u305f\uff1a</p> \\[\\liminf_{n \\to \\infty} P(VI(X_j) &gt; c) = 1\\] <p>\u3053\u3053\u3067 \\(c &gt; 0\\) \u306f\u5b9a\u6570\u3067\u3042\u308a\u3001\\(X_j\\) \u304c\u771f\u306b\u95a2\u9023\u3059\u308b\u5909\u6570\u3067\u3042\u308b\u5834\u5408\u306b\u6210\u7acb\u3059\u308b\u3002\u3053\u308c\u306f\u3001\u5341\u5206\u5927\u304d\u306a\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u3067\u306f\u3001\u95a2\u9023\u3059\u308b\u5909\u6570\u304c\u691c\u51fa\u3055\u308c\u308b\u78ba\u7387\u304c1\u306b\u53ce\u675f\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3059\u308b\u3002</p> <p>\u3057\u304b\u3057\u3001MDI\u3068\u540c\u69d8\u306b\u3001\u7279\u5fb4\u91cf\u9593\u306e\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3001MDA\u306e\u89e3\u91c8\u306f\u8907\u96d1\u306b\u306a\u308b\u3002\u76f8\u95a2\u3057\u305f\u5909\u6570\u306e\u7d44\u304c\u3042\u308b\u3068\u3001\u4e00\u65b9\u306e\u5909\u6570\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u3066\u3082\u3001\u3082\u3046\u4e00\u65b9\u304c\u4ee3\u66ff\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u305f\u3081\u3001\u500b\u3005\u306e\u5909\u6570\u306e\u91cd\u8981\u5ea6\u304c\u904e\u5c0f\u8a55\u4fa1\u3055\u308c\u308b\u50be\u5411\u304c\u3042\u308b\uff08Strobl et al., 2008\uff09\u3002</p>"},{"location":"research/note/variable-importance/#323-mdimda","title":"3.2.3 MDI\u3068MDA\u306e\u6bd4\u8f03","text":"<p>MDI\u3068MDA\u306e\u7406\u8ad6\u7684\u6027\u8cea\u3092\u6bd4\u8f03\u3059\u308b\u3068\u3001\u3044\u304f\u3064\u304b\u306e\u91cd\u8981\u306a\u9055\u3044\u304c\u660e\u3089\u304b\u306b\u306a\u308b\u3002\u8a08\u7b97\u52b9\u7387\u306e\u9762\u3067\u306f\u3001MDI\u306f\u30e2\u30c7\u30eb\u5b66\u7fd2\u6642\u306b\u8a08\u7b97\u3067\u304d\u308b\u4e00\u65b9\u3001MDA\u306f\u30e2\u30c7\u30eb\u5b66\u7fd2\u5f8c\u306b\u5404\u7279\u5fb4\u91cf\u306b\u3064\u3044\u3066\u30c7\u30fc\u30bf\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u3066\u518d\u8a55\u4fa1\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u3044\u3002\u30d0\u30a4\u30a2\u30b9\u7279\u6027\u3068\u3057\u3066\u306f\u3001MDI\u306f\u30ab\u30c6\u30b4\u30ea\u6570\u306e\u591a\u3044\u5909\u6570\u3084\u5206\u5e03\u306e\u504f\u3063\u305f\u5909\u6570\u3092\u904e\u5927\u8a55\u4fa1\u3059\u308b\u50be\u5411\u304c\u3042\u308b\u304c\u3001MDA\u306f\u3053\u306e\u7a2e\u306e\u30d0\u30a4\u30a2\u30b9\u304c\u6bd4\u8f03\u7684\u5c11\u306a\u3044\uff08Strobl et al., 2007\uff09\u3002\u76f8\u95a2\u3078\u306e\u5bfe\u5fdc\u306b\u3064\u3044\u3066\u306f\u3001\u4e21\u624b\u6cd5\u3068\u3082\u7279\u5fb4\u91cf\u9593\u306e\u76f8\u95a2\u306b\u5f71\u97ff\u3055\u308c\u308b\u304c\u3001\u5f71\u97ff\u306e\u4ed5\u65b9\u304c\u7570\u306a\u308a\u3001MDI\u3067\u306f\u76f8\u95a2\u5909\u6570\u306e\u4e00\u65b9\u306b\u91cd\u8981\u5ea6\u304c\u504f\u308a\u304c\u3061\u3060\u304c\u3001MDA\u3067\u306f\u76f8\u95a2\u5909\u6570\u306e\u4e21\u65b9\u306e\u91cd\u8981\u5ea6\u304c\u904e\u5c0f\u8a55\u4fa1\u3055\u308c\u308b\u50be\u5411\u304c\u3042\u308b\u3002\u7406\u8ad6\u7684\u88cf\u4ed8\u3051\u3068\u3057\u3066\u306f\u3001MDI\u306f\u60c5\u5831\u7406\u8ad6\u7684\u89e3\u91c8\u304c\u3042\u308a\u3001\u7406\u60f3\u7684\u306a\u6761\u4ef6\u4e0b\u3067\u306f\u4e0d\u7d14\u5ea6\u6e1b\u5c11\u304c\u60c5\u5831\u5229\u5f97\u306b\u5bfe\u5fdc\u3059\u308b\u4e00\u65b9\u3001MDA\u306f\u76f4\u63a5\u7684\u306b\u4e88\u6e2c\u6027\u80fd\u3078\u306e\u5bc4\u4e0e\u3092\u6e2c\u308b\u305f\u3081\u3001\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u8a55\u4fa1\u3068\u76f4\u7d50\u3057\u3066\u3044\u308b\u3002</p> <p>\u5b9f\u52d9\u4e0a\u306f\u3001\u3053\u308c\u3089\u306e\u7279\u6027\u3092\u7406\u89e3\u3057\u305f\u4e0a\u3067\u3001\u76ee\u7684\u3084\u72b6\u6cc1\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u624b\u6cd5\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#33","title":"3.3 \u30d0\u30a4\u30a2\u30b9\u306e\u554f\u984c\u3068\u5bfe\u7b56","text":""},{"location":"research/note/variable-importance/#331-mdi","title":"3.3.1 MDI\u306e\u30d0\u30a4\u30a2\u30b9","text":"<p>MDI\u306b\u306f\u8907\u6570\u306e\u30d0\u30a4\u30a2\u30b9\u304c\u5185\u5728\u3059\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u308b\u3002\u307e\u305a\u3001\u30ab\u30c6\u30b4\u30ea\u6570\u306e\u591a\u3044\u5909\u6570\u306e\u904e\u5927\u8a55\u4fa1\u3068\u3057\u3066\u3001\u5024\u306e\u3068\u308a\u3046\u308b\u7a2e\u985e\u304c\u591a\u3044\u5909\u6570\uff08\u30ab\u30c6\u30b4\u30ea\u6570\u304c\u591a\u3044\u96e2\u6563\u5909\u6570\u3084\u9023\u7d9a\u5909\u6570\uff09\u304c\u512a\u5148\u7684\u306b\u9078\u629e\u3055\u308c\u3001\u904e\u5927\u8a55\u4fa1\u3055\u308c\u308b\u50be\u5411\u304c\u3042\u308b\uff08Strobl et al., 2007\uff09\u3002\u3053\u308c\u306f\u6c7a\u5b9a\u6728\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u591a\u304f\u306e\u5206\u5272\u70b9\u5019\u88dc\u3092\u6301\u3064\u5909\u6570\u3092\u9078\u3073\u3084\u3059\u3044\u3053\u3068\u306b\u8d77\u56e0\u3059\u308b\u3002\u5f62\u5f0f\u7684\u306b\u306f\u3001\u30e9\u30f3\u30c0\u30e0\u306a\u5909\u6570 \\(X_j\\) \u3067\u3082\u3001\u30ab\u30c6\u30b4\u30ea\u6570\u304c\u591a\u3044\u307b\u3069\u4e0d\u7d14\u5ea6\u6e1b\u5c11\u306e\u671f\u5f85\u5024\u304c\u5927\u304d\u304f\u306a\u308b\u3053\u3068\u304c\u793a\u3055\u308c\u3066\u3044\u308b\uff08Louppe et al., 2013\uff09\u3002</p> <p>\u6b21\u306b\u3001\u504f\u3063\u305f\u5206\u5e03\u3092\u6301\u3064\u5909\u6570\u306e\u904e\u5927\u8a55\u4fa1\u3068\u3057\u3066\u3001\u504f\u308a\u306e\u3042\u308b\u5206\u5e03\uff08\u4e00\u90e8\u306e\u5024\u304c\u975e\u5e38\u306b\u983b\u7e41\u306b\u51fa\u73fe\u3059\u308b\u5206\u5e03\uff09\u3092\u6301\u3064\u5909\u6570\u3082\u3001\u4e0d\u7d14\u5ea6\u6e1b\u5c11\u304c\u751f\u3058\u3084\u3059\u304f\u91cd\u8981\u5ea6\u304c\u9ad8\u304f\u8a55\u4fa1\u3055\u308c\u308b\uff08Strobl et al., 2007\uff09\u3002\u4f8b\u3048\u3070\u3001\u4e00\u3064\u306e\u5024\u304c90%\u3092\u5360\u3081\u308b\u3088\u3046\u306a\u4e8c\u5024\u5909\u6570\u306f\u3001\u5b9f\u969b\u306e\u4e88\u6e2c\u529b\u304c\u306a\u304f\u3066\u3082MDI\u304c\u9ad8\u304f\u51fa\u308b\u50be\u5411\u304c\u3042\u308b\u3002</p> <p>\u307e\u305f\u3001\u76f8\u95a2\u3057\u305f\u7279\u5fb4\u91cf\u306e\u504f\u3063\u305f\u8a55\u4fa1\u3068\u3057\u3066\u3001\u5f37\u3044\u76f8\u95a2\u3092\u6301\u3064\u5909\u6570\u7fa4\u3067\u306f\u3001\u65e9\u3044\u6bb5\u968e\u3067\u9078\u3070\u308c\u305f\u5909\u6570\u304c\u9ad8\u3044MDI\u3092\u5f97\u3066\u3001\u6b8b\u308a\u306e\u5909\u6570\u306e\u91cd\u8981\u5ea6\u304c\u904e\u5c0f\u8a55\u4fa1\u3055\u308c\u308b\uff08Strobl et al., 2008\uff09\u3002\u3053\u308c\u306f\u3001\u4e00\u5ea6\u3042\u308b\u5909\u6570\u3067\u5206\u5272\u3059\u308b\u3068\u3001\u76f8\u95a2\u306e\u3042\u308b\u5909\u6570\u306e\u60c5\u5831\u304c\u90e8\u5206\u7684\u306b\u5229\u7528\u3055\u308c\u308b\u305f\u3081\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#332-mda","title":"3.3.2 MDA\u306e\u30d0\u30a4\u30a2\u30b9","text":"<p>MDA\u306f\u4e00\u822c\u306bMDI\u3088\u308a\u3082\u30d0\u30a4\u30a2\u30b9\u304c\u5c11\u306a\u3044\u3068\u3055\u308c\u308b\u304c\u3001\u3044\u304f\u3064\u304b\u306e\u554f\u984c\u304c\u5b58\u5728\u3059\u308b\u3002\u307e\u305a\u3001\u76f8\u95a2\u5909\u6570\u306e\u904e\u5c0f\u8a55\u4fa1\u3068\u3057\u3066\u3001\u76f8\u95a2\u306e\u5f37\u3044\u5909\u6570\u3067\u306f\u3001\u4e00\u65b9\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u3066\u3082\u4ed6\u65b9\u304c\u4ee3\u66ff\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u305f\u3081\u3001\u4e21\u65b9\u306e\u5909\u6570\u306e\u91cd\u8981\u5ea6\u304c\u904e\u5c0f\u8a55\u4fa1\u3055\u308c\u308b\u50be\u5411\u304c\u3042\u308b\uff08Gregorutti et al., 2017\uff09\u3002\u3053\u306e\u554f\u984c\u306f\u7279\u306b\u591a\u91cd\u5171\u7dda\u6027\u304c\u5f37\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u9855\u8457\u3068\u306a\u308b\u3002</p> <p>\u30b7\u30e3\u30c3\u30d5\u30eb\u306e\u4e0d\u78ba\u5b9f\u6027\u306e\u554f\u984c\u3082\u3042\u308a\u3001\u5358\u4e00\u306e\u30e9\u30f3\u30c0\u30e0\u30b7\u30e3\u30c3\u30d5\u30eb\u3067\u306f\u7d50\u679c\u304c\u5b89\u5b9a\u3057\u306a\u3044\u3053\u3068\u304c\u3042\u308a\u3001\u8907\u6570\u56de\u306e\u30b7\u30e3\u30c3\u30d5\u30eb\u3068\u5e73\u5747\u5316\u304c\u5fc5\u8981\u3068\u306a\u308b\uff08Altmann et al., 2010\uff09\u3002</p> <p>\u4e0d\u5747\u8861\u30c7\u30fc\u30bf\u3067\u306e\u504f\u308a\u3082\u8ab2\u984c\u3067\u3042\u308a\u3001\u30af\u30e9\u30b9\u4e0d\u5747\u8861\u304c\u5f37\u3044\u5206\u985e\u554f\u984c\u3067\u306f\u3001\u591a\u6570\u30af\u30e9\u30b9\u306e\u6b63\u78ba\u3055\u304c\u652f\u914d\u7684\u3068\u306a\u308a\u3001\u5c11\u6570\u30af\u30e9\u30b9\u306b\u91cd\u8981\u306a\u5909\u6570\u306e\u691c\u51fa\u304c\u96e3\u3057\u304f\u306a\u308b\u5834\u5408\u304c\u3042\u308b\uff08Janitza et al., 2013\uff09\u3002</p>"},{"location":"research/note/variable-importance/#333","title":"3.3.3 \u30d0\u30a4\u30a2\u30b9\u5bfe\u7b56\u624b\u6cd5","text":"<p>\u3053\u308c\u3089\u306e\u30d0\u30a4\u30a2\u30b9\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u3001\u3044\u304f\u3064\u304b\u306e\u6539\u826f\u624b\u6cd5\u304c\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u3002Strobl et al. (2008) \u306f\u76f8\u95a2\u306e\u5f71\u97ff\u3092\u7de9\u548c\u3059\u308b\u305f\u3081\u306b\u6761\u4ef6\u4ed8\u304d\u30d1\u30fc\u30df\u30e5\u30c6\u30fc\u30b7\u30e7\u30f3\u91cd\u8981\u5ea6\u3092\u63d0\u6848\u3057\u305f\u3002\u3053\u306e\u624b\u6cd5\u306f\u5909\u6570 \\(X_j\\) \u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3059\u308b\u969b\u306b\u3001\u76f8\u95a2\u306e\u3042\u308b\u4ed6\u306e\u5909\u6570\u3068\u306e\u6761\u4ef6\u4ed8\u304d\u5206\u5e03\u3092\u4fdd\u6301\u3059\u308b\u3088\u3046\u5de5\u592b\u3059\u308b\uff1a</p> <p>\\(MDA_{cond}(X_j) = \\frac{1}{N_T}\\sum_{T}\\left(err_T(\\tilde{X}_j | X_C) - err_T(X_j)\\right)\\)</p> <p>\u3053\u3053\u3067 \\(\\tilde{X}_j | X_C\\) \u306f\u5909\u6570\u30b0\u30eb\u30fc\u30d7 \\(X_C\\) \u306e\u5024\u3067\u6761\u4ef6\u4ed8\u3051\u305f\u4e0a\u3067\u306e \\(X_j\\) \u306e\u30d1\u30fc\u30df\u30e5\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u8868\u3059\u3002\u5b9f\u88c5\u3067\u306f\u3001\\(X_C\\) \u306e\u4f3c\u305f\u5024\u3092\u6301\u3064\u30b5\u30f3\u30d7\u30eb\u9593\u3067\u306e\u307f \\(X_j\\) \u306e\u5024\u3092\u5165\u308c\u66ff\u3048\u308b\u3053\u3068\u3067\u5b9f\u73fe\u3059\u308b\u3002</p> <p>\u307e\u305f\u3001Hothorn et al. (2006) \u306f\u5206\u5272\u5909\u6570\u306e\u9078\u629e\u306b\u7d71\u8a08\u7684\u691c\u5b9a\uff08\\(p\\)\u5024\u306b\u57fa\u3065\u304f\u9078\u629e\uff09\u3092\u7528\u3044\u308b\u6c7a\u5b9a\u6728\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u3042\u308b\u6761\u4ef6\u63a8\u8ad6\u6728\uff08Conditional Inference Trees\uff09\u3092\u63d0\u6848\u3057\u305f\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30ab\u30c6\u30b4\u30ea\u6570\u306e\u591a\u3044\u5909\u6570\u306b\u5bfe\u3059\u308b\u30d0\u30a4\u30a2\u30b9\u304c\u8efd\u6e1b\u3055\u308c\u308b\u3002\u3053\u306e\u624b\u6cd5\u3092\u62e1\u5f35\u3057\u305f\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\uff08cforest\uff09\u306f\u3001\u3088\u308a\u4fe1\u983c\u6027\u306e\u9ad8\u3044\u5909\u6570\u91cd\u8981\u5ea6\u3092\u63d0\u4f9b\u3059\u308b\u3068\u3055\u308c\u3066\u3044\u308b\uff08Strobl et al., 2007\uff09\u3002</p> <p>\u3055\u3089\u306b\u3001Loecher (2020) \u306f\u4e0d\u7d14\u5ea6\u6e1b\u5c11\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3067\u306f\u306a\u304fOOB\u30c7\u30fc\u30bf\u3067\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u3001MDI\u306e\u30d0\u30a4\u30a2\u30b9\u3092\u4f4e\u6e1b\u3059\u308b\u30a2\u30a6\u30c8\u30aa\u30d6\u30d0\u30c3\u30b0MDI\uff08OOB-MDI\uff09\u3092\u63d0\u6848\u3057\u305f\uff1a</p> <p>\\(MDI_{OOB}(X_j) = \\frac{1}{N_T}\\sum_{T}\\sum_{t \\in T: v(s_t)=j}p_{OOB}(t)\\Delta i_{OOB}(s_t, t)\\)</p> <p>\u3053\u3053\u3067 \\(p_{OOB}(t)\\) \u3068 \\(\\Delta i_{OOB}(s_t, t)\\) \u306fOOB\u30b5\u30f3\u30d7\u30eb\u306b\u57fa\u3065\u3044\u3066\u8a08\u7b97\u3055\u308c\u308b\u3002\u3053\u306e\u4fee\u6b63\u306b\u3088\u308a\u3001\u904e\u5b66\u7fd2\u306b\u3088\u308b\u30d0\u30a4\u30a2\u30b9\u304c\u4f4e\u6e1b\u3055\u308c\u3001\u3088\u308a\u4fe1\u983c\u6027\u306e\u9ad8\u3044\u91cd\u8981\u5ea6\u6307\u6a19\u304c\u5f97\u3089\u308c\u308b\u3002</p> <p>\u5206\u985e\u554f\u984c\u306b\u304a\u3051\u308b\u4ee3\u66ff\u8a55\u4fa1\u6307\u6a19\u3068\u3057\u3066\u3001Janitza et al. (2013) \u306f\u7cbe\u5ea6\uff08Accuracy\uff09\u306e\u4ee3\u308f\u308a\u306bAUC\uff08Area Under the ROC Curve\uff09\u306b\u57fa\u3065\u304f\u91cd\u8981\u5ea6\u6307\u6a19\u3092\u63d0\u6848\u3057\u305f\uff1a</p> <p>\\(MDA_{AUC}(X_j) = \\frac{1}{N_T}\\sum_{T}\\left(AUC_T(X_j) - AUC_T(\\tilde{X}_j)\\right)\\)</p> <p>AUC\u306f\u30af\u30e9\u30b9\u4e0d\u5747\u8861\u306b\u5bfe\u3057\u3066\u9811\u5065\u3067\u3042\u308a\u3001\u5c11\u6570\u30af\u30e9\u30b9\u306e\u4e88\u6e2c\u306b\u91cd\u8981\u306a\u5909\u6570\u3092\u3088\u308a\u9069\u5207\u306b\u8a55\u4fa1\u3067\u304d\u308b\u3002</p> <p>\u6700\u5f8c\u306b\u3001\u7d71\u8a08\u7684\u691c\u5b9a\u306e\u5c0e\u5165\u3068\u3057\u3066\u3001Altmann et al. (2010) \u306f\u91cd\u8981\u5ea6\u30b9\u30b3\u30a2\u306e\u7d71\u8a08\u7684\u6709\u610f\u6027\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u3001\u30e9\u30f3\u30c0\u30e0\u306a\u30ce\u30a4\u30ba\u5909\u6570\u3092\u8ffd\u52a0\u3057\u3001\u305d\u306e\u5206\u5e03\u304b\u3089\u6709\u610f\u6c34\u6e96\u3092\u6c7a\u5b9a\u3059\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3057\u305f\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5076\u7136\u9ad8\u3044\u91cd\u8981\u5ea6\u3092\u793a\u3059\u5909\u6570\u3092\u9664\u5916\u3067\u304d\u308b\u3002</p> <p>\u3053\u308c\u3089\u306e\u6539\u826f\u624b\u6cd5\u306f\u305d\u308c\u305e\u308c\u9577\u6240\u3068\u77ed\u6240\u304c\u3042\u308a\u3001\u30c7\u30fc\u30bf\u306e\u7279\u6027\u3084\u5206\u6790\u306e\u76ee\u7684\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u624b\u6cd5\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3042\u308b\u3002\u307e\u305f\u3001\u8907\u6570\u306e\u624b\u6cd5\u3092\u4f75\u7528\u3057\u3066\u7d50\u679c\u306e\u4e00\u8cab\u6027\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3082\u63a8\u5968\u3055\u308c\u308b\u3002</p>"},{"location":"research/note/variable-importance/#4","title":"4. \u5909\u6570\u9593\u306e\u76f8\u95a2\u554f\u984c\u3068\u89e3\u6c7a\u30a2\u30d7\u30ed\u30fc\u30c1","text":""},{"location":"research/note/variable-importance/#41","title":"4.1 \u76f8\u95a2\u554f\u984c\u306e\u7406\u8ad6\u7684\u5206\u6790","text":"<p>\u7279\u5fb4\u91cf\u9593\u306b\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u89e3\u91c8\u306f\u672c\u8cea\u7684\u306b\u96e3\u3057\u304f\u306a\u308b\u3002\u3053\u306e\u554f\u984c\u306f\u3001\u300c\u76f8\u95a2\u6b6a\u307f\uff08correlation bias\uff09\u300d\u3068\u3057\u3066\u77e5\u3089\u308c\u3066\u304a\u308a\u3001\u7279\u306b\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3084\u591a\u91cd\u5171\u7dda\u6027\u306e\u5f37\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u306f\u6df1\u523b\u3067\u3042\u308b\u3002</p> <p>\u76f8\u95a2\u554f\u984c\u306b\u95a2\u3059\u308b\u7406\u8ad6\u7684\u5206\u6790\u3068\u3057\u3066\u306f\u3001Gregorutti et al. (2017) \u306e\u7814\u7a76\u304c\u91cd\u8981\u3067\u3042\u308b\u3002\u5f7c\u3089\u306f\u7dda\u5f62\u30e2\u30c7\u30eb \\(Y = \\sum_{j=1}^p \\beta_j X_j + \\varepsilon\\) \u306b\u304a\u3044\u3066\u3001\u76f8\u95a2\u306e\u3042\u308b\u8aac\u660e\u5909\u6570\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306ePermutation\u91cd\u8981\u5ea6\u3092\u89e3\u6790\u3057\u305f\u30022\u3064\u306e\u5909\u6570 \\(X_1\\) \u3068 \\(X_2\\) \u306e\u76f8\u95a2\u4fc2\u6570\u304c \\(\\rho\\) \u3067\u3042\u308b\u5834\u5408\u3001\u305d\u308c\u305e\u308c\u306e\u91cd\u8981\u5ea6\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u308b\uff1a</p> <p>\\(MDA(X_1) \\approx \\beta_1^2 Var(X_1)(1 - \\rho^2)\\)</p> <p>\\(MDA(X_2) \\approx \\beta_2^2 Var(X_2)(1 - \\rho^2)\\)</p> <p>\u3053\u306e\u7d50\u679c\u304b\u3089\u3001\u76f8\u95a2\u4fc2\u6570 \\(\\rho\\) \u304c1\u306b\u8fd1\u3065\u304f\u307b\u3069\u4e21\u5909\u6570\u306e\u91cd\u8981\u5ea6\u304c0\u306b\u8fd1\u3065\u304f\u3053\u3068\u304c\u308f\u304b\u308b\u3002\u3064\u307e\u308a\u3001\u5f37\u3044\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3001\u672c\u6765\u91cd\u8981\u306a\u5909\u6570\u3067\u3082\u91cd\u8981\u5ea6\u304c\u904e\u5c0f\u8a55\u4fa1\u3055\u308c\u308b\u3002</p> <p>\u3055\u3089\u306b\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306eMDI\u306b\u3064\u3044\u3066\u3082\u3001\u76f8\u95a2\u306e\u3042\u308b\u5909\u6570\u9593\u3067\u300c\u7af6\u5408\u300d\u304c\u767a\u751f\u3057\u3001\u5148\u306b\u9078\u3070\u308c\u305f\u5909\u6570\u304c\u5f8c\u306e\u5909\u6570\u306e\u9078\u629e\u78ba\u7387\u3092\u4e0b\u3052\u308b\u52b9\u679c\u304c\u7406\u8ad6\u7684\u306b\u793a\u3055\u308c\u3066\u3044\u308b\uff08Louppe et al., 2013\uff09\u3002\u3053\u306e\u73fe\u8c61\u306b\u3088\u308a\u3001\u540c\u7b49\u306b\u91cd\u8981\u306a\u76f8\u95a2\u5909\u6570\u306e\u3046\u3061\u4e00\u90e8\u306e\u5909\u6570\u306e\u307f\u304c\u9ad8\u3044\u91cd\u8981\u5ea6\u3092\u793a\u3057\u3001\u6b8b\u308a\u306e\u5909\u6570\u306f\u904e\u5c0f\u8a55\u4fa1\u3055\u308c\u308b\u50be\u5411\u304c\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#42","title":"4.2 \u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u306b\u3088\u308b\u516c\u5e73\u306a\u91cd\u8981\u5ea6\u914d\u5206","text":"<p>\u76f8\u95a2\u554f\u984c\u306b\u5bfe\u51e6\u3059\u308b\u30a2\u30d7\u30ed\u30fc\u30c1\u306e\u4e00\u3064\u3068\u3057\u3066\u3001\u30b2\u30fc\u30e0\u7406\u8ad6\u306e\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u3092\u5fdc\u7528\u3059\u308b\u65b9\u6cd5\u304c\u6ce8\u76ee\u3055\u308c\u3066\u3044\u308b\u3002\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u306f\u5354\u529b\u30b2\u30fc\u30e0\u306b\u304a\u3044\u3066\u5404\u30d7\u30ec\u30a4\u30e4\u30fc\uff08\u7279\u5fb4\u91cf\uff09\u306e\u8ca2\u732e\u5ea6\u3092\u516c\u5e73\u306b\u914d\u5206\u3059\u308b\u305f\u3081\u306e\u6982\u5ff5\u3067\u3042\u308a\u3001\u52b9\u7387\u6027\u3001\u5bfe\u79f0\u6027\u3001\u30c0\u30df\u30fc\u30d7\u30ec\u30a4\u30e4\u30fc\u3001\u52a0\u6cd5\u6027\u3068\u3044\u3046\u516c\u7406\u7684\u6027\u8cea\u3092\u6e80\u305f\u3059\u552f\u4e00\u306e\u89e3\u3067\u3042\u308b\uff08Shapley, 1953\uff09\u3002\u52b9\u7387\u6027\u3068\u306f\u5168\u30d7\u30ec\u30a4\u30e4\u30fc\u306e\u4fa1\u5024\u306e\u5408\u8a08\u304c\u5168\u4f53\u306e\u4fa1\u5024\u306b\u7b49\u3057\u3044\u3053\u3068\u3001\u5bfe\u79f0\u6027\u3068\u306f\u540c\u7b49\u306e\u8ca2\u732e\u3092\u3059\u308b\u30d7\u30ec\u30a4\u30e4\u30fc\u306f\u540c\u7b49\u306e\u4fa1\u5024\u3092\u5f97\u308b\u3053\u3068\u3001\u30c0\u30df\u30fc\u30d7\u30ec\u30a4\u30e4\u30fc\u3068\u306f\u8ca2\u732e\u306e\u306a\u3044\u30d7\u30ec\u30a4\u30e4\u30fc\u306e\u4fa1\u5024\u306f0\u3067\u3042\u308b\u3053\u3068\u3001\u52a0\u6cd5\u6027\u3068\u306f\u8907\u6570\u306e\u30b2\u30fc\u30e0\u306e\u5408\u8a08\u306e\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u304c\u5404\u30b2\u30fc\u30e0\u306e\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u306e\u5408\u8a08\u306b\u7b49\u3057\u3044\u3053\u3068\u3092\u610f\u5473\u3059\u308b\u3002</p> <p>\u5909\u6570\u91cd\u8981\u5ea6\u306e\u6587\u8108\u3067\u306f\u3001\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff08Williamson et al., 2020\uff09\uff1a</p> <p>\\(\\phi_j = \\sum_{S \\subseteq \\{1,\\ldots,p\\} \\setminus \\{j\\}} \\frac{|S|! (p-|S|-1)!}{p!} [v(S \\cup \\{j\\}) - v(S)]\\)</p> <p>\u3053\u3053\u3067 \\(v(S)\\) \u306f\u5909\u6570\u96c6\u5408 \\(S\\) \u3092\u7528\u3044\u305f\u5834\u5408\u306e\u4e88\u6e2c\u6027\u80fd\u3092\u8868\u3059\u95a2\u6570\u3067\u3042\u308b\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u5168\u3066\u306e\u5909\u6570\u30b5\u30d6\u30bb\u30c3\u30c8\u306e\u7d44\u307f\u5408\u308f\u305b\u306b\u3064\u3044\u3066\u3001\u5909\u6570 \\(j\\) \u306e\u8ffd\u52a0\u306b\u3088\u308b\u6027\u80fd\u5411\u4e0a\u306e\u5e73\u5747\u3092\u91cd\u307f\u4ed8\u3051\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002</p> <p>Owen &amp; Prieur (2017) \u306f\u5206\u6563\u30d9\u30fc\u30b9\u306e\u91cd\u8981\u5ea6\u6307\u6a19\uff08Sobol\u6307\u6570\uff09\u306b\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u3092\u9069\u7528\u3057\u3001\u76f8\u95a2\u4e0b\u3067\u3082\u671b\u307e\u3057\u3044\u6027\u8cea\u3092\u6301\u3064\u5909\u6570\u91cd\u8981\u5ea6\u3092\u63d0\u6848\u3057\u305f\u3002\u3053\u306e\u65b9\u6cd5\u3067\u306f\u3001\u5168\u5909\u6570\u306e\u5206\u6563\u8aac\u660e\u91cf\u3092\u5404\u5909\u6570\u306b\u516c\u5e73\u306b\u5206\u914d\u3059\u308b\u305f\u3081\u3001\u76f8\u95a2\u69cb\u9020\u306b\u5f71\u97ff\u3055\u308c\u306b\u304f\u3044\u91cd\u8981\u5ea6\u6307\u6a19\u304c\u5f97\u3089\u308c\u308b\u3002</p> <p>Williamson et al. (2020) \u306f\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u306b\u57fa\u3065\u304f\u6bcd\u96c6\u56e3\u5909\u6570\u91cd\u8981\u5ea6\uff08SPVIM: Shapley Population Variable Importance Measure\uff09\u3092\u5b9a\u7fa9\u3057\u3001\u305d\u306e\u63a8\u5b9a\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u305f\uff1a</p> <p>\\(\\psi_{0,j}^{SP} = \\sum_{S \\subseteq \\{1,\\ldots,p\\} \\setminus \\{j\\}} \\frac{|S|! (p-|S|-1)!}{p!} [\\mathcal{R}_0(S) - \\mathcal{R}_0(S \\cup \\{j\\})]\\)</p> <p>\u3053\u3053\u3067 \\(\\mathcal{R}_0(S)\\) \u306f\u5909\u6570\u96c6\u5408 \\(S\\) \u3092\u7528\u3044\u305f\u5834\u5408\u306e\u6700\u826f\u306e\u4e88\u6e2c\u30ea\u30b9\u30af\u3067\u3042\u308b\u3002\u3053\u306e\u5b9a\u7fa9\u306b\u3088\u308a\u3001\u76f8\u95a2\u306e\u3042\u308b\u5909\u6570\u9593\u3067\u3082\u516c\u5e73\u306b\u91cd\u8981\u5ea6\u304c\u914d\u5206\u3055\u308c\u308b\u3002</p>"},{"location":"research/note/variable-importance/#43","title":"4.3 \u30b5\u30d6\u30bb\u30c3\u30c8\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3068\u8a08\u7b97\u52b9\u7387\u306e\u5411\u4e0a","text":"<p>\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u306e\u8a08\u7b97\u306f\u5168\u3066\u306e\u5909\u6570\u90e8\u5206\u96c6\u5408\uff08\\(2^p\\)\u500b\uff09\u3092\u8003\u616e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u5909\u6570\u6570 \\(p\\) \u304c\u5927\u304d\u3044\u5834\u5408\u306b\u8a08\u7b97\u91cf\u304c\u7206\u767a\u7684\u306b\u5897\u52a0\u3059\u308b\u3002\u3053\u306e\u554f\u984c\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u3001\u52b9\u7387\u7684\u306a\u8fd1\u4f3c\u624b\u6cd5\u304c\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u3002</p> <p>Williamson et al. (2020) \u306f\u30b5\u30d6\u30bb\u30c3\u30c8\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6226\u7565\u3092\u63d0\u6848\u3057\u305f\u3002\u5168\u3066\u306e\u90e8\u5206\u96c6\u5408\u3092\u8a55\u4fa1\u3059\u308b\u4ee3\u308f\u308a\u306b\u3001\u4e00\u90e8\u306e\u30b5\u30d6\u30bb\u30c3\u30c8\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3001\u305d\u306e\u7d50\u679c\u304b\u3089\u5168\u4f53\u306e\u91cd\u8981\u5ea6\u3092\u63a8\u5b9a\u3059\u308b\uff1a</p> <p>\\(\\hat{\\psi}_{j}^{SP} = \\frac{1}{M} \\sum_{m=1}^{M} \\frac{|S_m|! (p-|S_m|-1)!}{p!} [\\hat{\\mathcal{R}}(S_m) - \\hat{\\mathcal{R}}(S_m \\cup \\{j\\})]\\)</p> <p>\u3053\u3053\u3067 \\(M\\) \u306f\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u30b5\u30d6\u30bb\u30c3\u30c8\u6570\u3001\\(S_m\\) \u306f\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u3070\u308c\u305f\u5909\u6570\u30b5\u30d6\u30bb\u30c3\u30c8\u3067\u3042\u308b\u3002\u3053\u306e\u65b9\u6cd5\u306b\u3088\u308a\u3001\u8a08\u7b97\u91cf\u3092 \\(O(2^p)\\) \u304b\u3089 \\(O(M)\\) \u306b\u524a\u6e1b\u3067\u304d\u308b\u3002</p> <p>\u3055\u3089\u306b\u3001\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u7a4d\u5206\u306b\u57fa\u3065\u304f\u52b9\u7387\u7684\u306a\u63a8\u5b9a\u6cd5\u3082\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\uff08Castro et al., 2009; \u0160trumbelj &amp; Kononenko, 2014\uff09\u3002\u3053\u308c\u3089\u306e\u624b\u6cd5\u3067\u306f\u3001\u5909\u6570\u306e\u8ffd\u52a0\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u751f\u6210\u3057\u3001\u5404\u9806\u5e8f\u306b\u304a\u3051\u308b\u9650\u754c\u8ca2\u732e\u5ea6\u306e\u5e73\u5747\u304b\u3089\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u3092\u63a8\u5b9a\u3059\u308b\uff1a</p> <p>\\(\\hat{\\phi}_j = \\frac{1}{M} \\sum_{m=1}^{M} [v(Pre_{\\pi_m}(j) \\cup \\{j\\}) - v(Pre_{\\pi_m}(j))]\\)</p> <p>\u3053\u3053\u3067 \\(\\pi_m\\) \u306f\u5909\u6570\u306e\u9806\u5e8f\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u7f6e\u63db\u3001\\(Pre_{\\pi_m}(j)\\) \u306f\u9806\u5e8f \\(\\pi_m\\) \u306b\u304a\u3044\u3066\u5909\u6570 \\(j\\) \u3088\u308a\u524d\u306b\u6765\u308b\u5909\u6570\u306e\u96c6\u5408\u3067\u3042\u308b\u3002</p> <p>\u3053\u308c\u3089\u306e\u8a08\u7b97\u52b9\u7387\u5316\u624b\u6cd5\u306b\u3088\u308a\u3001\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u306b\u304a\u3044\u3066\u3082\u73fe\u5b9f\u7684\u306a\u6642\u9593\u3067\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u30d9\u30fc\u30b9\u306e\u5909\u6570\u91cd\u8981\u5ea6\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u3063\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#44","title":"4.4 \u6761\u4ef6\u4ed8\u304d\u91cd\u8981\u5ea6\u3068\u30b0\u30eb\u30fc\u30d7\u5909\u6570\u91cd\u8981\u5ea6","text":"<p>\u76f8\u95a2\u554f\u984c\u306b\u5bfe\u3059\u308b\u5225\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u3068\u3057\u3066\u3001\u6761\u4ef6\u4ed8\u304d\u91cd\u8981\u5ea6\u3068\u30b0\u30eb\u30fc\u30d7\u5909\u6570\u91cd\u8981\u5ea6\u304c\u3042\u308b\u3002</p> <p>\u6761\u4ef6\u4ed8\u304d\u30d1\u30fc\u30df\u30e5\u30c6\u30fc\u30b7\u30e7\u30f3\u91cd\u8981\u5ea6\uff08Conditional Permutation Importance\uff09\u306f\u3001Strobl et al. (2008) \u306b\u3088\u3063\u3066\u63d0\u6848\u3055\u308c\u305f\u624b\u6cd5\u3067\u3001\u5909\u6570 \\(X_j\\) \u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3059\u308b\u969b\u306b\u76f8\u95a2\u306e\u3042\u308b\u4ed6\u306e\u5909\u6570\u3068\u306e\u6761\u4ef6\u4ed8\u304d\u5206\u5e03\u3092\u4fdd\u6301\u3059\u308b\u3002\u3053\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u306f\u3001\u307e\u305a\u76f8\u95a2\u306e\u3042\u308b\u5909\u6570\u306e\u30b0\u30eb\u30fc\u30d7 \\(X_C\\) \u3092\u8b58\u5225\u3057\uff08\u4f8b\u3048\u3070\u76f8\u95a2\u4fc2\u6570\u306e\u95be\u5024\u306b\u57fa\u3065\u3044\u3066\uff09\u3001\u6b21\u306b \\(X_C\\) \u306e\u5024\u306b\u57fa\u3065\u3044\u3066\u30c7\u30fc\u30bf\u3092\u30b0\u30ea\u30c3\u30c9\u306b\u5206\u5272\u3057\u3001\u5404\u30b0\u30ea\u30c3\u30c9\u5185\u3067\u306e\u307f \\(X_j\\) \u306e\u5024\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u305f\u5f8c\u3001\u30b7\u30e3\u30c3\u30d5\u30eb\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u4f4e\u4e0b\u3092\u6e2c\u5b9a\u3059\u308b\u3002\u3053\u306e\u624b\u6cd5\u306b\u3088\u308a\u3001\u76f8\u95a2\u306e\u3042\u308b\u5909\u6570\u9593\u306e\u6761\u4ef6\u4ed8\u304d\u95a2\u4fc2\u3092\u8003\u616e\u3057\u305f\u91cd\u8981\u5ea6\u8a55\u4fa1\u304c\u53ef\u80fd\u306b\u306a\u308b\u3002</p> <p>\u3082\u3046\u4e00\u3064\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u3042\u308b\u30b0\u30eb\u30fc\u30d7\u5909\u6570\u91cd\u8981\u5ea6\uff08Grouped Variable Importance\uff09\u306f\u3001\u76f8\u95a2\u306e\u5f37\u3044\u5909\u6570\u3092\u307e\u3068\u3081\u3066\u4e00\u3064\u306e\u30b0\u30eb\u30fc\u30d7\u3068\u3057\u3066\u6271\u3044\u3001\u30b0\u30eb\u30fc\u30d7\u5358\u4f4d\u3067\u91cd\u8981\u5ea6\u3092\u8a55\u4fa1\u3059\u308b\u65b9\u6cd5\u3067\u3042\u308b\uff08Gregorutti et al., 2015\uff09\u3002\u30b0\u30eb\u30fc\u30d7 \\(G\\) \u306e\u91cd\u8981\u5ea6\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\uff1a</p> <p>\\(MDI(G) = \\sum_{j \\in G} MDI(X_j)\\)</p> <p>\\(MDA(G) = err(X_{\\{1,\\ldots,p\\} \\setminus G}) - err(X)\\)</p> <p>\u3053\u3053\u3067 \\(err(X_{\\{1,\\ldots,p\\} \\setminus G})\\) \u306f\u30b0\u30eb\u30fc\u30d7 \\(G\\) \u306e\u5168\u5909\u6570\u3092\u9664\u5916\u3057\u305f\u5834\u5408\u306e\u4e88\u6e2c\u8aa4\u5dee\u3067\u3042\u308b\u3002</p> <p>\u30b0\u30eb\u30fc\u30d7\u5909\u6570\u91cd\u8981\u5ea6\u306f\u3001\u7279\u306b\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u3084\u751f\u7269\u5b66\u7684\u306a\u7d4c\u8def\uff08\u30d1\u30b9\u30a6\u30a7\u30a4\uff09\u306e\u3088\u3046\u306a\u81ea\u7136\u306a\u30b0\u30eb\u30fc\u30d7\u69cb\u9020\u3092\u6301\u3064\u30c7\u30fc\u30bf\u3067\u6709\u52b9\u3067\u3042\u308b\u3002Williamson et al. (2021) \u306f\u3053\u306e\u6982\u5ff5\u3092\u3055\u3089\u306b\u767a\u5c55\u3055\u305b\u3001\u30b0\u30eb\u30fc\u30d7\u306b\u5bfe\u3059\u308bLOCO\u63a8\u8ad6\u306e\u67a0\u7d44\u307f\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#5","title":"5. \u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u306e\u6bd4\u8f03\u8a55\u4fa1\u3068\u9078\u629e\u6307\u91dd","text":""},{"location":"research/note/variable-importance/#51","title":"5.1 \u6027\u80fd\u7279\u6027\u306e\u7cfb\u7d71\u7684\u6bd4\u8f03","text":"<p>\u69d8\u3005\u306a\u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u306e\u7279\u6027\u3092\u4f53\u7cfb\u7684\u306b\u6bd4\u8f03\u3059\u308b\u3053\u3068\u306f\u3001\u5b9f\u52d9\u4e0a\u306e\u9078\u629e\u306b\u5f79\u7acb\u3064\u91cd\u8981\u306a\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u3002\u4ee3\u8868\u7684\u306a\u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u306e\u4e3b\u8981\u306a\u7279\u6027\u3092\u6bd4\u8f03\u3059\u308b\u3068\u3001\u8a08\u7b97\u30b3\u30b9\u30c8\u3001\u30ab\u30c6\u30b4\u30ea\u30d0\u30a4\u30a2\u30b9\u3001\u76f8\u95a2\u5bfe\u5fdc\u3001\u30e2\u30c7\u30eb\u4f9d\u5b58\u6027\u3001\u7406\u8ad6\u7684\u4fdd\u8a3c\u3001\u89e3\u91c8\u306e\u5bb9\u6613\u3055\u306a\u3069\u306e\u89b3\u70b9\u3067\u9055\u3044\u304c\u3042\u308b\u3002</p> <p>\u4f8b\u3048\u3070\u3001MDI\uff08\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\uff09\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u4f4e\u3044\u304c\u3001\u30ab\u30c6\u30b4\u30ea\u30d0\u30a4\u30a2\u30b9\u304c\u9ad8\u304f\u3001\u76f8\u95a2\u5bfe\u5fdc\u304c\u5f31\u3044\u3002\u4e00\u65b9\u3001MDA\uff08\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\uff09\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u4e2d\u301c\u9ad8\u7a0b\u5ea6\u3067\u3001\u30ab\u30c6\u30b4\u30ea\u30d0\u30a4\u30a2\u30b9\u304c\u4f4e\u304f\u3001\u76f8\u95a2\u5bfe\u5fdc\u304c\u4e2d\u7a0b\u5ea6\u3067\u3042\u308b\u3002\u6761\u4ef6\u4ed8\u304dMDA\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u3044\u304c\u3001\u76f8\u95a2\u5bfe\u5fdc\u304c\u9ad8\u3044\u7279\u5fb4\u3092\u6301\u3064\u3002LOCO\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u975e\u5e38\u306b\u9ad8\u3044\u304c\u3001\u30ab\u30c6\u30b4\u30ea\u30d0\u30a4\u30a2\u30b9\u304c\u306a\u304f\u3001\u89e3\u91c8\u304c\u5bb9\u6613\u3067\u3042\u308b\u3002Williamson\u6cd5\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u304f\u3001\u30ab\u30c6\u30b4\u30ea\u30d0\u30a4\u30a2\u30b9\u304c\u306a\u304f\u3001\u76f8\u95a2\u5bfe\u5fdc\u304c\u4e2d\u7a0b\u5ea6\u3001\u7406\u8ad6\u7684\u4fdd\u8a3c\u304c\u9ad8\u3044\u3002Floodgate\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u4e2d\u7a0b\u5ea6\u3067\u3001\u30ab\u30c6\u30b4\u30ea\u30d0\u30a4\u30a2\u30b9\u304c\u306a\u304f\u3001\u76f8\u95a2\u5bfe\u5fdc\u304c\u4e2d\u7a0b\u5ea6\u3067\u3042\u308b\u3002SPVIM\uff08\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\uff09\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u975e\u5e38\u306b\u9ad8\u3044\u304c\u3001\u30ab\u30c6\u30b4\u30ea\u30d0\u30a4\u30a2\u30b9\u304c\u306a\u304f\u3001\u76f8\u95a2\u5bfe\u5fdc\u304c\u9ad8\u3044\u3002\u30b0\u30eb\u30fc\u30d7\u91cd\u8981\u5ea6\u306f\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u3044\u304c\u3001\u30ab\u30c6\u30b4\u30ea\u30d0\u30a4\u30a2\u30b9\u304c\u4f4e\u304f\u3001\u76f8\u95a2\u5bfe\u5fdc\u304c\u9ad8\u304f\u3001\u89e3\u91c8\u304c\u5bb9\u6613\u3067\u3042\u308b\u3002</p> <p>\u3053\u308c\u3089\u306e\u624b\u6cd5\u306e\u5b9f\u8a3c\u7684\u306a\u6bd4\u8f03\u7814\u7a76\u3068\u3057\u3066\u306f\u3001Strobl et al. (2007) \u304cMDI\u3068\u6761\u4ef6\u4ed8\u304d\u30d1\u30fc\u30df\u30e5\u30c6\u30fc\u30b7\u30e7\u30f3\u91cd\u8981\u5ea6\u3092\u6bd4\u8f03\u3057\u3001\u5f8c\u8005\u304c\u30ab\u30c6\u30b4\u30ea\u30d0\u30a4\u30a2\u30b9\u306b\u5bfe\u3057\u3066\u9811\u5065\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u308b\u3002Gregorutti et al. (2017) \u306f\u76f8\u95a2\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308bMDA\u306e\u6027\u80fd\u3092\u5206\u6790\u3057\u3001\u76f8\u95a2\u69cb\u9020\u304c\u91cd\u8981\u5ea6\u306e\u89e3\u91c8\u306b\u5927\u304d\u304f\u5f71\u97ff\u3059\u308b\u3053\u3068\u3092\u5b9f\u8a3c\u3057\u305f\u3002Williamson et al. (2021) \u306f\u5f7c\u3089\u306e\u63a8\u5b9a\u91cf\u3068LOCO\u3092\u6bd4\u8f03\u3057\u3001\u8a08\u7b97\u52b9\u7387\u3068\u7d71\u8a08\u7684\u52b9\u7387\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3092\u793a\u3057\u3066\u3044\u308b\u3002\u307e\u305f\u3001Goldstein et al. (2015) \u306fLIME\u3001SHAP\u3001Permutation\u91cd\u8981\u5ea6\u306a\u3069\u3092\u6bd4\u8f03\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u8aac\u660e\u529b\u3068\u89e3\u91c8\u306e\u5bb9\u6613\u3055\u3092\u8a55\u4fa1\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#52","title":"5.2 \u5b9f\u52d9\u7684\u306a\u9078\u629e\u6307\u91dd","text":"<p>\u5b9f\u52d9\u306b\u304a\u3044\u3066\u9069\u5207\u306a\u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u3092\u9078\u629e\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u7528\u9014\u3068\u30c7\u30fc\u30bf\u7279\u6027\u3092\u8003\u616e\u3057\u305f\u6307\u91dd\u304c\u91cd\u8981\u3067\u3042\u308b\u3002</p> <p>\u307e\u305a\u3001\u7528\u9014\u306b\u3088\u308b\u9078\u629e\u3068\u3057\u3066\u306f\u3001\u30e2\u30c7\u30eb\u89e3\u91c8\u304c\u76ee\u7684\u306e\u5834\u5408\u306f\u30e2\u30c7\u30eb\u4f9d\u5b58\u578b\u306e\u91cd\u8981\u5ea6\uff08MDI\u3001MDA\uff09\u304c\u9069\u3057\u3066\u3044\u308b\u3002\u7279\u306b\u3001MDI\u306f\u30e2\u30c7\u30eb\u304c\u3069\u306e\u3088\u3046\u306b\u6c7a\u5b9a\u3092\u884c\u3063\u3066\u3044\u308b\u304b\u3092\u76f4\u63a5\u53cd\u6620\u3059\u308b\u3002\u5909\u6570\u9078\u629e\u304c\u76ee\u7684\u306a\u3089MDA\u3084LOCO\u304c\u9069\u3057\u3066\u304a\u308a\u3001\u3053\u308c\u3089\u306f\u4e88\u6e2c\u6027\u80fd\u3078\u306e\u5bc4\u4e0e\u3092\u76f4\u63a5\u8a55\u4fa1\u3059\u308b\u305f\u3081\u3001\u5909\u6570\u9078\u629e\u306e\u57fa\u6e96\u3068\u3057\u3066\u59a5\u5f53\u3067\u3042\u308b\u3002\u79d1\u5b66\u7684\u77e5\u898b\u306e\u7372\u5f97\u304c\u76ee\u7684\u306a\u3089\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u306e\u91cd\u8981\u5ea6\uff08Williamson\u6cd5\u3001Floodgate\u3001SPVIM\uff09\u304c\u9069\u3057\u3066\u304a\u308a\u3001\u3053\u308c\u3089\u306f\u7279\u5b9a\u306e\u30e2\u30c7\u30eb\u306b\u4f9d\u5b58\u305b\u305a\u3001\u30c7\u30fc\u30bf\u69cb\u9020\u81ea\u4f53\u306e\u7279\u6027\u3092\u8a55\u4fa1\u3059\u308b\u3002\u56e0\u679c\u63a8\u8ad6\u306e\u88dc\u52a9\u3092\u76ee\u7684\u3068\u3059\u308b\u5834\u5408\u306f\u3001\u4ea4\u7d61\u3092\u8003\u616e\u3057\u305f\u6761\u4ef6\u4ed8\u304d\u91cd\u8981\u5ea6\u3084\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u30d9\u30fc\u30b9\u306e\u624b\u6cd5\u304c\u6709\u7528\u3067\u3042\u308b\u3002</p> <p>\u6b21\u306b\u3001\u30c7\u30fc\u30bf\u7279\u6027\u306b\u3088\u308b\u9078\u629e\u3068\u3057\u3066\u306f\u3001\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u5834\u5408\u306f\u8a08\u7b97\u52b9\u7387\u304c\u91cd\u8981\u3068\u306a\u308b\u305f\u3081\u3001MDI\u3084Floodgate\u304c\u9069\u3057\u3066\u3044\u308b\u3002\u5909\u6570\u9593\u306e\u76f8\u95a2\u304c\u5f37\u3044\u5834\u5408\u306f\u3001\u6761\u4ef6\u4ed8\u304dMDA\u3001SPVIM\u3001\u30b0\u30eb\u30fc\u30d7\u91cd\u8981\u5ea6\u304c\u9069\u3057\u3066\u3044\u308b\u3002\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u304c\u591a\u3044\u5834\u5408\u3001MDI\u306f\u30d0\u30a4\u30a2\u30b9\u304c\u751f\u3058\u3084\u3059\u3044\u305f\u3081\u3001\u6761\u4ef6\u4ed8\u304dMDA\u3084\u6761\u4ef6\u63a8\u8ad6\u6728\u30d9\u30fc\u30b9\u306e\u624b\u6cd5\u304c\u63a8\u5968\u3055\u308c\u308b\u3002\u4e0d\u5747\u8861\u30c7\u30fc\u30bf\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u3001AUC\u30d9\u30fc\u30b9\u306ePermutation\u91cd\u8981\u5ea6\u3084\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u306e\u624b\u6cd5\u304c\u9069\u3057\u3066\u3044\u308b\u3002</p> <p>\u5b9f\u8df5\u7684\u306b\u306f\u3001\u7570\u306a\u308b\u539f\u7406\u306b\u57fa\u3065\u304f\u8907\u6570\u306e\u91cd\u8981\u5ea6\u624b\u6cd5\u3092\u4f75\u7528\u3057\u3001\u7d50\u679c\u306e\u4e00\u8cab\u6027\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u308b\u3002\u307e\u305f\u3001\u91cd\u8981\u5ea6\u30b9\u30b3\u30a2\u306e\u7d71\u8a08\u7684\u6709\u610f\u6027\u306e\u8a55\u4fa1\u3001\u30c9\u30e1\u30a4\u30f3\u77e5\u8b58\u3068\u306e\u7167\u5408\u3001\u53ef\u8996\u5316\u306e\u6d3b\u7528\uff08\u90e8\u5206\u4f9d\u5b58\u30d7\u30ed\u30c3\u30c8\u3084\u7d2f\u7a4d\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u306a\u3069\u3068\u306e\u4f75\u7528\uff09\u3001\u5b89\u5b9a\u6027\u306e\u78ba\u8a8d\uff08\u30c7\u30fc\u30bf\u306e\u90e8\u5206\u30b5\u30f3\u30d7\u30eb\u3084\u30e2\u30c7\u30eb\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5909\u3048\u3066\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u3092\u78ba\u8a8d\uff09\u306a\u3069\u306e\u65b9\u6cd5\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u4fe1\u983c\u6027\u306e\u9ad8\u3044\u89e3\u91c8\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002</p>"},{"location":"research/note/variable-importance/#6","title":"6. \u8a73\u7d30\u306a\u5fdc\u7528\u4e8b\u4f8b","text":""},{"location":"research/note/variable-importance/#61","title":"6.1 \u533b\u7642\u30fb\u30d0\u30a4\u30aa\u30a4\u30f3\u30d5\u30a9\u30de\u30c6\u30a3\u30af\u30b9\u5206\u91ce\u3067\u306e\u4e8b\u4f8b","text":""},{"location":"research/note/variable-importance/#611","title":"6.1.1 \u30b2\u30ce\u30e0\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u767a\u898b","text":"<p>\u30b2\u30ce\u30e0\u30c7\u30fc\u30bf\u304b\u3089\u75be\u60a3\u95a2\u9023\u907a\u4f1d\u5b50\u3092\u540c\u5b9a\u3059\u308b\u7814\u7a76\u3067\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u304c\u91cd\u8981\u306a\u30c4\u30fc\u30eb\u3068\u306a\u3063\u3066\u3044\u308b\u3002D\u00edaz-Uriarte &amp; de Andr\u00e9s (2006) \u306f\u3001\u30de\u30a4\u30af\u30ed\u30a2\u30ec\u30a4\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u907a\u4f1d\u5b50\u9078\u629e\u306b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u5909\u6570\u91cd\u8981\u5ea6\u3092\u9069\u7528\u3057\u3001\u5c11\u6570\u306e\u907a\u4f1d\u5b50\u3067\u9ad8\u3044\u5206\u985e\u7cbe\u5ea6\u3092\u9054\u6210\u3057\u305f\u3002\u5f7c\u3089\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u306f\u3001\u307e\u305a\u5168\u907a\u4f1d\u5b50\u3092\u7528\u3044\u3066RF\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u3001\u5909\u6570\u91cd\u8981\u5ea6\u304c\u4e0b\u4f4d20%\u306e\u907a\u4f1d\u5b50\u3092\u9664\u5916\u3057\u305f\u5f8c\u3001\u6b8b\u308a\u306e\u907a\u4f1d\u5b50\u3067\u65b0\u305f\u306aRF\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u3001OOB\u8aa4\u5dee\u304c\u5897\u52a0\u3059\u308b\u307e\u3067\u3053\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u7e70\u308a\u8fd4\u3059\u65b9\u6cd5\u3092\u63a1\u7528\u3057\u305f\u3002\u3053\u306e\u65b9\u6cd5\u306b\u3088\u308a\u3001\u4e73\u304c\u3093\u3084\u30ea\u30f3\u30d1\u816b\u306a\u3069\u306e\u8907\u6570\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u3001\u6570\u5343\u306e\u907a\u4f1d\u5b50\u304b\u3089\u6570\u5341\u306e\u91cd\u8981\u306a\u907a\u4f1d\u5b50\u3092\u7279\u5b9a\u3059\u308b\u3053\u3068\u306b\u6210\u529f\u3057\u305f\u3002</p> <p>\u4e00\u65b9\u3001Kursa &amp; Rudnicki (2010) \u306f\u300cBoruta\u300d\u3068\u3044\u3046\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u30d9\u30fc\u30b9\u306e\u7279\u5fb4\u9078\u629e\u6cd5\u3092\u958b\u767a\u3057\u3001\u907a\u4f1d\u5b50\u30c7\u30fc\u30bf\u3078\u306e\u5fdc\u7528\u3092\u793a\u3057\u305f\u3002Boruta\u306f\u30b7\u30e3\u30c9\u30a6\u5909\u6570\uff08\u30e9\u30f3\u30c0\u30e0\u306b\u751f\u6210\u3055\u308c\u305f\u30ce\u30a4\u30ba\u5909\u6570\uff09\u3068\u306e\u6bd4\u8f03\u306b\u3088\u308a\u3001\u7d71\u8a08\u7684\u306b\u6709\u610f\u306a\u5909\u6570\u3092\u8b58\u5225\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002</p> <p>\u5909\u6570\u9593\u306e\u76f8\u95a2\u304c\u5f37\u3044\u30b2\u30ce\u30e0\u30c7\u30fc\u30bf\u3067\u306f\u3001\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u3084\u6761\u4ef6\u4ed8\u304d\u91cd\u8981\u5ea6\u304c\u7279\u306b\u6709\u52b9\u3067\u3042\u308b\u3002Li et al. (2019) \u306f\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u30d9\u30fc\u30b9\u306e\u91cd\u8981\u5ea6\u3092\u4f7f\u3063\u3066\u3001\u76f8\u95a2\u306e\u3042\u308b\u907a\u4f1d\u5b50\u7fa4\u304b\u3089\u81b5\u81d3\u304c\u3093\u306e\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u3092\u7279\u5b9a\u3057\u3001\u5f93\u6765\u624b\u6cd5\u3088\u308a\u9ad8\u3044\u518d\u73fe\u6027\u3092\u793a\u3057\u305f\u3002</p>"},{"location":"research/note/variable-importance/#612","title":"6.1.2 \u96fb\u5b50\u30ab\u30eb\u30c6\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u75be\u60a3\u4e88\u6e2c\u30e2\u30c7\u30eb","text":"<p>\u96fb\u5b50\u30ab\u30eb\u30c6\uff08EHR\uff09\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u81e8\u5e8a\u4e88\u6e2c\u30e2\u30c7\u30eb\u306b\u304a\u3044\u3066\u3082\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306f\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3066\u3044\u308b\u3002Hsich et al. (2019) \u306f\u5fc3\u4e0d\u5168\u60a3\u8005\u306e\u4e88\u5f8c\u4e88\u6e2c\u306e\u305f\u3081\u306b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3092\u7528\u3044\u3001MDA\u306b\u57fa\u3065\u3044\u3066\u7d04100\u306e\u81e8\u5e8a\u5909\u6570\u304b\u3089\u91cd\u8981\u306a\u4e88\u5f8c\u56e0\u5b50\u3092\u7279\u5b9a\u3057\u305f\u3002\u305d\u306e\u7d50\u679c\u3001\u5f93\u6765\u304b\u3089\u77e5\u3089\u308c\u3066\u3044\u308b\u56e0\u5b50\uff08\u5e74\u9f62\u3001\u814e\u6a5f\u80fd\u3001\u5fc3\u6a5f\u80fd\u306a\u3069\uff09\u306b\u52a0\u3048\u3001\u65b0\u305f\u306a\u4e88\u5f8c\u56e0\u5b50\uff08\u7279\u5b9a\u306e\u691c\u67fb\u5024\u3084\u5408\u4f75\u75c7\uff09\u304c\u767a\u898b\u3055\u308c\u305f\u3002</p> <p>\u3053\u306e\u3088\u3046\u306a\u7814\u7a76\u3067\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u30d0\u30a4\u30a2\u30b9\u3068\u89e3\u91c8\u306e\u8ab2\u984c\u304c\u7279\u306b\u91cd\u8981\u3068\u306a\u308b\u3002EHR\u30c7\u30fc\u30bf\u306f\u6b20\u6e2c\u5024\u304c\u591a\u304f\u3001\u5909\u6570\u9593\u306e\u8907\u96d1\u306a\u76f8\u95a2\u69cb\u9020\u3092\u6301\u3064\u305f\u3081\u3001\u5358\u7d14\u306aMDI\u3084MDA\u3067\u306f\u8aa4\u3063\u305f\u89e3\u91c8\u3092\u62db\u304f\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002Steele et al. (2018) \u306f\u3053\u306e\u8ab2\u984c\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u3001\u8907\u6570\u306e\u91cd\u8981\u5ea6\u624b\u6cd5\uff08MDA\u3001\u6761\u4ef6\u4ed8\u304dMDA\u3001LOCO\uff09\u3092\u4f75\u7528\u3057\u3001\u7d50\u679c\u306e\u4e00\u8cab\u6027\u3092\u8a55\u4fa1\u3059\u308b\u67a0\u7d44\u307f\u3092\u63d0\u6848\u3057\u305f\u3002\u3055\u3089\u306b\u3001\u5404\u4e88\u6e2c\u56e0\u5b50\u306e\u90e8\u5206\u4f9d\u5b58\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3001\u4e88\u6e2c\u3078\u306e\u5bc4\u4e0e\u30d1\u30bf\u30fc\u30f3\u3092\u8996\u899a\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u81e8\u5e8a\u7684\u306a\u89e3\u91c8\u3092\u652f\u63f4\u3057\u3066\u3044\u308b\u3002</p> <p>\u5b9f\u969b\u306e\u5fdc\u7528\u4f8b\u3068\u3057\u3066\u3001Ribers &amp; Ullrich (2020) \u306f\u6297\u751f\u7269\u8cea\u51e6\u65b9\u306e\u9069\u5207\u6027\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u306b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3092\u9069\u7528\u3057\u3001Permutation\u91cd\u8981\u5ea6\u3068\u90e8\u5206\u4f9d\u5b58\u30d7\u30ed\u30c3\u30c8\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u533b\u5e2b\u306e\u8a3a\u65ad\u30d7\u30ed\u30bb\u30b9\u3092\u7406\u89e3\u3057\u3088\u3046\u3068\u3057\u305f\u3002\u305d\u306e\u7d50\u679c\u3001\u3044\u304f\u3064\u304b\u306e\u691c\u67fb\u5024\u304c\u91cd\u8981\u306a\u6c7a\u5b9a\u8981\u56e0\u3067\u3042\u308b\u3053\u3068\u304c\u660e\u3089\u304b\u306b\u306a\u308a\u3001\u4e0d\u5fc5\u8981\u306a\u6297\u751f\u7269\u8cea\u51e6\u65b9\u3092\u6e1b\u3089\u3059\u305f\u3081\u306e\u610f\u601d\u6c7a\u5b9a\u652f\u63f4\u30c4\u30fc\u30eb\u306e\u958b\u767a\u306b\u3064\u306a\u304c\u3063\u305f\u3002</p>"},{"location":"research/note/variable-importance/#613","title":"6.1.3 \u30d0\u30a4\u30aa\u30a4\u30f3\u30d5\u30a9\u30de\u30c6\u30a3\u30af\u30b9\u306b\u304a\u3051\u308b\u30c1\u30e3\u30ec\u30f3\u30b8\u3068\u89e3\u6c7a\u7b56","text":"<p>\u30d0\u30a4\u30aa\u30a4\u30f3\u30d5\u30a9\u30de\u30c6\u30a3\u30af\u30b9\u5206\u91ce\u3067\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5fdc\u7528\u306b\u306f\u3001\u3044\u304f\u3064\u304b\u306e\u7279\u6709\u306e\u30c1\u30e3\u30ec\u30f3\u30b8\u304c\u3042\u308b\u3002\u8d85\u9ad8\u6b21\u5143\u30c7\u30fc\u30bf\u306e\u554f\u984c\u3068\u3057\u3066\u3001\u907a\u4f1d\u5b50\u767a\u73fe\u3084SNP\u30c7\u30fc\u30bf\u306f\u6570\u5343\u304b\u3089\u6570\u767e\u4e07\u306e\u5909\u6570\u3092\u542b\u3080\u305f\u3081\u3001\u8a08\u7b97\u52b9\u7387\u304c\u91cd\u8981\u3068\u306a\u308b\u3002\u8907\u96d1\u306a\u76f8\u95a2\u69cb\u9020\u306e\u554f\u984c\u3068\u3057\u3066\u3001\u907a\u4f1d\u5b50\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3084\u4ee3\u8b1d\u7d4c\u8def\u306a\u3069\u3001\u751f\u7269\u5b66\u7684\u306b\u610f\u5473\u306e\u3042\u308b\u76f8\u95a2\u69cb\u9020\u304c\u5b58\u5728\u3059\u308b\u3002\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u306e\u5236\u7d04\u3082\u5927\u304d\u304f\u3001\u591a\u304f\u306e\u30d0\u30a4\u30aa\u30e1\u30c7\u30a3\u30ab\u30eb\u7814\u7a76\u3067\u306f\u3001\u5909\u6570\u6570\u306b\u6bd4\u3079\u3066\u30b5\u30f3\u30d7\u30eb\u6570\u304c\u8457\u3057\u304f\u5c11\u306a\u3044\u3002\u3055\u3089\u306b\u3001\u89e3\u91c8\u306e\u751f\u7269\u5b66\u7684\u59a5\u5f53\u6027\u306e\u554f\u984c\u3068\u3057\u3066\u3001\u7d71\u8a08\u7684\u306b\u91cd\u8981\u3068\u5224\u65ad\u3055\u308c\u305f\u5909\u6570\u304c\u751f\u7269\u5b66\u7684\u306b\u3082\u610f\u5473\u3092\u6301\u3064\u304b\u691c\u8a3c\u304c\u5fc5\u8981\u3067\u3042\u308b\u3002</p> <p>\u3053\u308c\u3089\u306e\u8ab2\u984c\u306b\u5bfe\u3059\u308b\u89e3\u6c7a\u7b56\u3068\u3057\u3066\u3001\u3044\u304f\u3064\u304b\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u63a1\u7528\u3055\u308c\u3066\u3044\u308b\u3002Le et al. (2020) \u306b\u3088\u308b\u968e\u5c64\u7684\u7279\u5fb4\u9078\u629e\u3067\u306f\u3001\u907a\u4f1d\u5b50\u3092\u307e\u305a\u751f\u7269\u5b66\u7684\u7d4c\u8def\u3067\u30b0\u30eb\u30fc\u30d7\u5316\u3057\u3001\u91cd\u8981\u306a\u7d4c\u8def\u3092\u7279\u5b9a\u3057\u305f\u5f8c\u3001\u305d\u306e\u4e2d\u306e\u91cd\u8981\u306a\u907a\u4f1d\u5b50\u3092\u540c\u5b9a\u3059\u308b\u4e8c\u6bb5\u968e\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u63d0\u6848\u3057\u305f\u3002Zhu et al. (2019) \u306b\u3088\u308b\u4e8b\u524d\u77e5\u8b58\u306e\u6d3b\u7528\u3067\u306f\u3001\u65e2\u77e5\u306e\u751f\u7269\u5b66\u7684\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u60c5\u5831\u3092\u53d6\u308a\u5165\u308c\u305f\u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u3092\u958b\u767a\u3057\u3001\u76f8\u95a2\u69cb\u9020\u3092\u8003\u616e\u3057\u306a\u304c\u3089\u751f\u7269\u5b66\u7684\u306b\u610f\u5473\u306e\u3042\u308b\u5909\u6570\u3092\u7279\u5b9a\u3057\u305f\u3002Seoane et al. (2014) \u306b\u3088\u308b\u7d71\u5408\u89e3\u6790\u3067\u306f\u3001\u8907\u6570\u306e\u30aa\u30df\u30af\u30b9\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\uff08\u907a\u4f1d\u5b50\u767a\u73fe\u3001\u30e1\u30c1\u30eb\u5316\u3001\u5909\u7570\u306a\u3069\uff09\u3092\u7d71\u5408\u3057\u3001\u5404\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u304b\u3089\u306e\u5909\u6570\u91cd\u8981\u5ea6\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u7dcf\u5408\u7684\u306a\u8a55\u4fa1\u3092\u884c\u3063\u305f\u3002Song et al. (2017) \u306b\u3088\u308b\u691c\u8a3c\u6226\u7565\u306e\u5f37\u5316\u3067\u306f\u3001\u4ea4\u5dee\u691c\u8a3c\u3068\u72ec\u7acb\u691c\u8a3c\u306e\u53b3\u683c\u306a\u67a0\u7d44\u307f\u3092\u69cb\u7bc9\u3057\u3001\u91cd\u8981\u5ea6\u306b\u3088\u308b\u9078\u629e\u30d0\u30a4\u30a2\u30b9\u3092\u6700\u5c0f\u5316\u3059\u308b\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u305f\u3002</p> <p>\u3053\u308c\u3089\u306e\u65b9\u6cd5\u306b\u3088\u308a\u3001\u30d0\u30a4\u30aa\u30a4\u30f3\u30d5\u30a9\u30de\u30c6\u30a3\u30af\u30b9\u5206\u91ce\u3067\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306e\u4fe1\u983c\u6027\u3068\u751f\u7269\u5b66\u7684\u59a5\u5f53\u6027\u304c\u5411\u4e0a\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#62","title":"6.2 \u91d1\u878d\u30fb\u30ea\u30b9\u30af\u8a55\u4fa1\u5206\u91ce\u3067\u306e\u4e8b\u4f8b","text":""},{"location":"research/note/variable-importance/#621","title":"6.2.1 \u4fe1\u7528\u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6","text":"<p>\u9280\u884c\u3084\u91d1\u878d\u6a5f\u95a2\u306f\u3001\u4fe1\u7528\u30ea\u30b9\u30af\u306e\u8a55\u4fa1\u3084\u4e0e\u4fe1\u5224\u65ad\u306e\u305f\u3081\u306b\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u6d3b\u7528\u3057\u3066\u3044\u308b\u3002\u3053\u3046\u3057\u305f\u30e2\u30c7\u30eb\u3067\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u304c\u89e3\u91c8\u53ef\u80fd\u6027\u3068\u898f\u5236\u5bfe\u5fdc\u306e\u9375\u3068\u306a\u308b\u3002</p> <p>Lessmann et al. (2015) \u306f\u3001\u8907\u6570\u306e\u4fe1\u7528\u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u30e2\u30c7\u30eb\u3092\u6bd4\u8f03\u3057\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306a\u3069\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u624b\u6cd5\u304c\u9ad8\u3044\u7cbe\u5ea6\u3092\u793a\u3059\u4e00\u65b9\u3001\u91cd\u8981\u5ea6\u6307\u6a19\u304c\u89e3\u91c8\u306e\u52a9\u3051\u3068\u306a\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u3002\u5178\u578b\u7684\u306a\u4fe1\u7528\u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u30e2\u30c7\u30eb\u3067\u306f\u3001\u8fd4\u6e08\u5c65\u6b74\uff08\u904e\u53bb\u306e\u5ef6\u6ede\u3084\u50b5\u52d9\u4e0d\u5c65\u884c\u306e\u8a18\u9332\uff09\u3001\u50b5\u52d9\u6bd4\u7387\uff08\u6240\u5f97\u306b\u5bfe\u3059\u308b\u50b5\u52d9\u306e\u5272\u5408\uff09\u3001\u4fe1\u7528\u5c65\u6b74\u306e\u9577\u3055\uff08\u4fe1\u7528\u8a18\u9332\u306e\u671f\u9593\uff09\u3001\u6700\u8fd1\u306e\u4fe1\u7528\u7167\u4f1a\u56de\u6570\uff08\u77ed\u671f\u9593\u306b\u591a\u6570\u306e\u7533\u8acb\u304c\u3042\u308b\u3068\u91cd\u8981\u5ea6\u304c\u9ad8\u304f\u306a\u308b\uff09\u306a\u3069\u306e\u5909\u6570\u304c\u9ad8\u3044\u91cd\u8981\u5ea6\u3092\u793a\u3059\u50be\u5411\u304c\u3042\u308b\u3002</p> <p>Xia et al. (2017) \u306fMDA\u306b\u57fa\u3065\u304f\u5909\u6570\u91cd\u8981\u5ea6\u3092\u7528\u3044\u3066\u4fe1\u7528\u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u3092\u884c\u3044\u3001\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u6839\u62e0\u3092\u660e\u78ba\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u627f\u8a8d\u30fb\u62d2\u7d76\u306e\u7406\u7531\u3092\u9867\u5ba2\u306b\u8aac\u660e\u3059\u308b\u4ed5\u7d44\u307f\u3092\u69cb\u7bc9\u3057\u305f\u3002</p> <p>\u4fe1\u7528\u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u306e\u6587\u8108\u3067\u306f\u3001\u7279\u306b\u300c\u516c\u5e73\u6027\u300d\u306e\u89b3\u70b9\u304b\u3089\u5909\u6570\u91cd\u8981\u5ea6\u304c\u91cd\u8981\u3068\u306a\u308b\u3002\u6027\u5225\u3084\u4eba\u7a2e\u306a\u3069\u306e\u4fdd\u8b77\u5c5e\u6027\u306b\u95a2\u9023\u3059\u308b\u5909\u6570\u304c\u9ad8\u3044\u91cd\u8981\u5ea6\u3092\u793a\u3059\u5834\u5408\u3001\u5dee\u5225\u7684\u306a\u5224\u65ad\u3092\u884c\u3063\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002Adler et al. (2018) \u306f\u5909\u6570\u91cd\u8981\u5ea6\u3092\u7528\u3044\u3066\u3001\u4fdd\u8b77\u5c5e\u6027\u3068\u76f8\u95a2\u306e\u9ad8\u3044\u4ee3\u7406\u5909\u6570\u3092\u7279\u5b9a\u3057\u3001\u30e2\u30c7\u30eb\u306e\u516c\u5e73\u6027\u3092\u8a55\u4fa1\u3059\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3057\u305f\u3002</p>"},{"location":"research/note/variable-importance/#622","title":"6.2.2 \u5e02\u5834\u30ea\u30b9\u30af\u3068\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u53d6\u5f15","text":"<p>\u91d1\u878d\u5e02\u5834\u306e\u30ea\u30b9\u30af\u8a55\u4fa1\u3084\u53d6\u5f15\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u958b\u767a\u306b\u3082\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306f\u91cd\u8981\u306a\u5f79\u5272\u3092\u679c\u305f\u3057\u3066\u3044\u308b\u3002</p> <p>Booth et al. (2014) \u306f\u682a\u4fa1\u4e88\u6e2c\u30e2\u30c7\u30eb\u306b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3092\u9069\u7528\u3057\u3001MDA\u306b\u57fa\u3065\u304f\u5909\u6570\u91cd\u8981\u5ea6\u3092\u7528\u3044\u3066\u4e88\u6e2c\u306b\u6700\u3082\u5bc4\u4e0e\u3059\u308b\u6307\u6a19\u3092\u7279\u5b9a\u3057\u305f\u3002\u5f7c\u3089\u306e\u7814\u7a76\u3067\u306f\u3001\u30de\u30af\u30ed\u7d4c\u6e08\u6307\u6a19\u3088\u308a\u3082\u4f01\u696d\u56fa\u6709\u306e\u8ca1\u52d9\u6307\u6a19\u3084\u5e02\u5834\u30c6\u30af\u30cb\u30ab\u30eb\u6307\u6a19\u306e\u91cd\u8981\u5ea6\u304c\u9ad8\u3044\u3053\u3068\u304c\u793a\u3055\u308c\u305f\u3002</p> <p>Krauss et al. (2017) \u306fS&amp;P 500\u69cb\u6210\u9298\u67c4\u306e\u65e5\u6b21\u30ea\u30bf\u30fc\u30f3\u4e88\u6e2c\u306b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306a\u3069\u306e\u6a5f\u68b0\u5b66\u7fd2\u624b\u6cd5\u3092\u9069\u7528\u3057\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306b\u57fa\u3065\u3044\u3066\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u53d6\u5f15\u6226\u7565\u3092\u69cb\u7bc9\u3057\u305f\u3002\u91cd\u8981\u5ea6\u306e\u9ad8\u3044\u7279\u5fb4\uff08\u904e\u53bb\u306e\u30ea\u30bf\u30fc\u30f3\u30d1\u30bf\u30fc\u30f3\u3084\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u6307\u6a19\u306a\u3069\uff09\u3092\u91cd\u8996\u3059\u308b\u3053\u3068\u3067\u53d6\u5f15\u6226\u7565\u306e\u7cbe\u5ea6\u3092\u5411\u4e0a\u3055\u305b\u305f\u3002</p> <p>\u5e02\u5834\u30ea\u30b9\u30af\u30e2\u30c7\u30ea\u30f3\u30b0\u3067\u306f\u3001\u5909\u6570\u9593\u306e\u8907\u96d1\u306a\u975e\u7dda\u5f62\u76f8\u4e92\u4f5c\u7528\u304c\u91cd\u8981\u3068\u306a\u308b\u3002Gu et al. (2020) \u306f\u8cc7\u7523\u4fa1\u683c\u4e88\u6e2c\u306b\u304a\u3044\u3066\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3068\u6df1\u5c64\u5b66\u7fd2\u306e\u5909\u6570\u91cd\u8981\u5ea6\u3092\u6bd4\u8f03\u3057\u3001\u4e21\u8005\u306e\u91cd\u8981\u5ea6\u30e9\u30f3\u30ad\u30f3\u30b0\u306e\u9055\u3044\u304b\u3089\u5e02\u5834\u306e\u975e\u7dda\u5f62\u69cb\u9020\u3092\u5206\u6790\u3057\u305f\u3002</p>"},{"location":"research/note/variable-importance/#623","title":"6.2.3 \u91d1\u878d\u898f\u5236\u3068\u30e2\u30c7\u30eb\u30ac\u30d0\u30ca\u30f3\u30b9\u306b\u304a\u3051\u308b\u6311\u6226","text":"<p>\u91d1\u878d\u5206\u91ce\u3067\u306f\u898f\u5236\u5bfe\u5fdc\u3068\u30e2\u30c7\u30eb\u30ac\u30d0\u30ca\u30f3\u30b9\u306e\u89b3\u70b9\u304b\u3089\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u4fe1\u983c\u6027\u3068\u8aac\u660e\u80fd\u529b\u304c\u7279\u306b\u91cd\u8981\u8996\u3055\u308c\u308b\u3002</p> <p>\u30a2\u30e1\u30ea\u30ab\u3067\u306fEqual Credit Opportunity Act (ECOA) \u3084Fair Credit Reporting Act (FCRA) \u306a\u3069\u306e\u6cd5\u898f\u5236\u306b\u3088\u308a\u3001\u4fe1\u7528\u5224\u65ad\u306e\u6839\u62e0\u3092\u8aac\u660e\u3059\u308b\u7fa9\u52d9\u304c\u3042\u308b\u3002\u540c\u69d8\u306b\u3001EU\u306e\u4e00\u822c\u30c7\u30fc\u30bf\u4fdd\u8b77\u898f\u5247\uff08GDPR\uff09\u3067\u306f\u300c\u8aac\u660e\u3092\u53d7\u3051\u308b\u6a29\u5229\uff08right to explanation\uff09\u300d\u304c\u898f\u5b9a\u3055\u308c\u3066\u3044\u308b\u3002\u3053\u3046\u3057\u305f\u898f\u5236\u74b0\u5883\u4e0b\u3067\u306f\u3001\u30e2\u30c7\u30eb\u306e\u5224\u65ad\u7406\u7531\u3092\u9867\u5ba2\u306b\u8aac\u660e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306f\u305d\u306e\u4e2d\u6838\u3068\u306a\u308b\u3002</p> <p>Bracke et al. (2019) \u306f\u82f1\u56fd\u91d1\u878d\u884c\u52d5\u76e3\u8996\u6a5f\u69cb\uff08FCA\uff09\u306e\u7814\u7a76\u3067\u3001\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u8aac\u660e\u53ef\u80fd\u6027\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5f79\u5272\u3068\u9650\u754c\u3092\u5206\u6790\u3057\u305f\u3002\u5f7c\u3089\u306f\u5b9f\u52d9\u7684\u8ab2\u984c\u3068\u3057\u3066\u3001\u30e2\u30c7\u30eb\u56fa\u6709\u306e\u91cd\u8981\u5ea6\u3068\u4eba\u53e3\u6bcd\u6570\u306e\u4e56\u96e2\uff08\u30e2\u30c7\u30eb\u4f9d\u5b58\u578b\u306e\u91cd\u8981\u5ea6\u304c\u30c7\u30fc\u30bf\u306e\u771f\u306e\u69cb\u9020\u3092\u53cd\u6620\u3057\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\uff09\u3001\u91cd\u8981\u5ea6\u306e\u4e0d\u5b89\u5b9a\u6027\uff08\u540c\u3058\u30c7\u30fc\u30bf\u3067\u3082\u7570\u306a\u308b\u30e2\u30c7\u30eb\u3067\u91cd\u8981\u5ea6\u30e9\u30f3\u30ad\u30f3\u30b0\u304c\u5927\u304d\u304f\u5909\u308f\u308b\u554f\u984c\uff09\u3001\u56e0\u679c\u95a2\u4fc2\u3068\u76f8\u95a2\u306e\u6df7\u540c\uff08\u91cd\u8981\u5ea6\u304c\u9ad8\u3044\u3053\u3068\u306f\u5fc5\u305a\u3057\u3082\u56e0\u679c\u95a2\u4fc2\u3092\u610f\u5473\u3057\u306a\u3044\uff09\u3001\u96c6\u7d04\u7684\u306a\u8aac\u660e\u3068\u500b\u5225\u8aac\u660e\u306e\u5dee\u7570\uff08\u5168\u4f53\u7684\u306a\u91cd\u8981\u5ea6\u3068\u500b\u5225\u4e8b\u4f8b\u306e\u8aac\u660e\u306e\u4e0d\u4e00\u81f4\uff09\u306a\u3069\u3092\u6307\u6458\u3057\u3066\u3044\u308b\u3002</p> <p>\u3053\u308c\u3089\u306e\u8ab2\u984c\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u3001\u91d1\u878d\u6a5f\u95a2\u3067\u306f\u3044\u304f\u3064\u304b\u306e\u5b9f\u8df5\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u63a1\u7528\u3055\u308c\u3066\u3044\u308b\u3002\u8907\u6570\u30e2\u30c7\u30eb\u306e\u4e00\u81f4\u5ea6\u8a55\u4fa1\u3067\u306f\u3001\u7570\u306a\u308b\u30e2\u30c7\u30eb\u9593\u3067\u91cd\u8981\u5ea6\u306e\u4e00\u81f4\u5ea6\u3092\u6e2c\u5b9a\u3057\u3001\u30e2\u30c7\u30eb\u306b\u4f9d\u5b58\u3057\u306a\u3044\u5b89\u5b9a\u3057\u305f\u8981\u56e0\u3092\u7279\u5b9a\u3059\u308b\u3002\u30ed\u30fc\u30ab\u30eb\u8aac\u660e\u3068\u306e\u4f75\u7528\u3067\u306f\u3001SHAP\u5024\u3084LIME\u306a\u3069\u306e\u500b\u5225\u4e88\u6e2c\u306e\u8aac\u660e\u624b\u6cd5\u3068\u3001\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u5909\u6570\u91cd\u8981\u5ea6\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u5229\u7528\u3059\u308b\u3002\u56e0\u679c\u63a8\u8ad6\u3068\u306e\u7d71\u5408\u3067\u306f\u3001\u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u30e2\u30c7\u30eb\u306a\u3069\u306e\u56e0\u679c\u63a8\u8ad6\u624b\u6cd5\u3068\u5909\u6570\u91cd\u8981\u5ea6\u3092\u7d44\u307f\u5408\u308f\u305b\u3001\u76f8\u95a2\u3068\u56e0\u679c\u3092\u533a\u5225\u3059\u308b\u3002\u30e2\u30c7\u30eb\u30ab\u30fc\u30c9\u3067\u306f\u3001\u30e2\u30c7\u30eb\u306e\u7279\u6027\u3084\u9650\u754c\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u89e3\u91c8\u65b9\u6cd5\u3092\u660e\u793a\u7684\u306b\u6587\u66f8\u5316\u3059\u308b\u3002</p> <p>\u3053\u308c\u3089\u306e\u5b9f\u52d9\u7684\u30a2\u30d7\u30ed\u30fc\u30c1\u306b\u3088\u308a\u3001\u91d1\u878d\u6a5f\u95a2\u306f\u898f\u5236\u5bfe\u5fdc\u3068\u9ad8\u5ea6\u306a\u30e2\u30c7\u30ea\u30f3\u30b0\u3092\u4e21\u7acb\u3055\u305b\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#7","title":"7. \u6700\u65b0\u306e\u7814\u7a76\u52d5\u5411\u3068\u65b0\u305f\u306a\u624b\u6cd5","text":""},{"location":"research/note/variable-importance/#71","title":"7.1 \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6","text":"<p>\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u30e2\u30c7\u30eb\u306f\u8907\u96d1\u306a\u975e\u7dda\u5f62\u95a2\u4fc2\u3092\u6349\u3048\u308b\u80fd\u529b\u306b\u512a\u308c\u3066\u3044\u308b\u304c\u3001\u89e3\u91c8\u304c\u96e3\u3057\u3044\u3068\u3044\u3046\u8ab2\u984c\u304c\u3042\u308b\u3002\u3053\u306e\u5206\u91ce\u3067\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306b\u95a2\u3059\u308b\u7814\u7a76\u304c\u8fd1\u5e74\u6025\u901f\u306b\u9032\u5c55\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#711","title":"7.1.1 \u52fe\u914d\u30d9\u30fc\u30b9\u624b\u6cd5","text":"<p>Sundararajan et al. (2017) \u306f\u300c\u7a4d\u5206\u52fe\u914d\uff08Integrated Gradients\uff09\u300d\u3068\u3044\u3046\u624b\u6cd5\u3092\u63d0\u6848\u3057\u305f\u3002\u3053\u308c\u306f\u5165\u529b\u304b\u3089\u51fa\u529b\u3078\u306e\u52fe\u914d\u3092\u7a4d\u5206\u3059\u308b\u3053\u3068\u3067\u3001\u5404\u7279\u5fb4\u306e\u5bc4\u4e0e\u5ea6\u3092\u5b9a\u91cf\u5316\u3059\u308b\uff1a</p> <p>\\(IG_i(x) = (x_i - x'_i) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha \\times (x-x'))}{\\partial x_i} d\\alpha\\)</p> <p>\u3053\u3053\u3067 \\(x\\) \u306f\u5165\u529b\u3001\\(x'\\) \u306f\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\uff08\u901a\u5e38\u306f\u30bc\u30ed\u30d9\u30af\u30c8\u30eb\uff09\u3001\\(F\\) \u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u95a2\u6570\u3067\u3042\u308b\u3002</p> <p>Shrikumar et al. (2017) \u306e\u300cDeepLIFT\u300d\u306f\u3001\u5404\u5165\u529b\u7279\u5fb4\u304c\u51fa\u529b\u306b\u3069\u308c\u3060\u3051\u8ca2\u732e\u3057\u305f\u304b\u3092\u3001\u53c2\u7167\u5165\u529b\u304b\u3089\u306e\u5909\u5316\u306b\u57fa\u3065\u3044\u3066\u8a08\u7b97\u3059\u308b\u3002\u3053\u306e\u624b\u6cd5\u306f\u30d0\u30c3\u30af\u30d7\u30ed\u30d1\u30b2\u30fc\u30b7\u30e7\u30f3\u306b\u4f3c\u305f\u52b9\u7387\u7684\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u5b9f\u88c5\u3055\u308c\u3001\u8907\u96d1\u306a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u3082\u8a08\u7b97\u304c\u53ef\u80fd\u3067\u3042\u308b\u3002</p> <p>\u3053\u308c\u3089\u306e\u52fe\u914d\u30d9\u30fc\u30b9\u624b\u6cd5\u306f\u3001\u7406\u8ad6\u7684\u306b\u306f\u5165\u529b\u306e\u5fae\u5c0f\u5909\u5316\u306b\u5bfe\u3059\u308b\u51fa\u529b\u306e\u611f\u5ea6\u3092\u6e2c\u5b9a\u3057\u3066\u304a\u308a\u3001\u5c40\u6240\u7684\u306a\u5909\u6570\u91cd\u8981\u5ea6\u3068\u89e3\u91c8\u3067\u304d\u308b\u3002</p>"},{"location":"research/note/variable-importance/#712","title":"7.1.2 \u6442\u52d5\u30d9\u30fc\u30b9\u624b\u6cd5","text":"<p>\u6442\u52d5\u30d9\u30fc\u30b9\u624b\u6cd5\u306f\u3001\u5165\u529b\u306e\u4e00\u90e8\u3092\u5909\u66f4\u307e\u305f\u306f\u524a\u9664\u3057\u305f\u5834\u5408\u306e\u51fa\u529b\u5909\u5316\u3092\u6e2c\u5b9a\u3059\u308b\u3002Zeiler &amp; Fergus (2014) \u306e\u300c\u30aa\u30af\u30eb\u30fc\u30b8\u30e7\u30f3\u611f\u5ea6\uff08Occlusion Sensitivity\uff09\u300d\u306f\u3001\u5165\u529b\u306e\u4e00\u90e8\u3092\u906e\u853d\uff08\u30aa\u30af\u30eb\u30fc\u30b8\u30e7\u30f3\uff09\u3057\u3001\u4e88\u6e2c\u3078\u306e\u5f71\u97ff\u3092\u53ef\u8996\u5316\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002</p> <p>\u6700\u8fd1\u3067\u306f\u3001Covert et al. (2020) \u306e\u300cSAGE\uff08Shapley Additive Global importancE\uff09\u300d\u304c\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u30e2\u30c7\u30eb\u306b\u3082\u9069\u7528\u53ef\u80fd\u306a\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u30d9\u30fc\u30b9\u306e\u30b0\u30ed\u30fc\u30d0\u30eb\u91cd\u8981\u5ea6\u6307\u6a19\u3068\u3057\u3066\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u3002SAGE\u306f\u60c5\u5831\u7406\u8ad6\u7684\u306a\u57fa\u790e\u3092\u6301\u3061\u3001\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u4e0d\u78ba\u5b9f\u6027\u306e\u524a\u6e1b\u306b\u5404\u7279\u5fb4\u304c\u3069\u308c\u3060\u3051\u8ca2\u732e\u3059\u308b\u304b\u3092\u6e2c\u5b9a\u3059\u308b\uff1a</p> <p>\\(SAGE(X_j) = \\sum_{S \\subseteq \\{1,\\ldots,p\\} \\setminus \\{j\\}} \\frac{|S|! (p-|S|-1)!}{p!} [I(Y; X_S, X_j) - I(Y; X_S)]\\)</p> <p>\u3053\u3053\u3067 \\(I(Y; X)\\) \u306f\u76f8\u4e92\u60c5\u5831\u91cf\u3092\u8868\u3059\u3002</p>"},{"location":"research/note/variable-importance/#713","title":"7.1.3 \u6ce8\u610f\u30d9\u30fc\u30b9\u624b\u6cd5","text":"<p>Transformer\u306a\u3069\u306e\u6ce8\u610f\uff08Attention\uff09\u6a5f\u69cb\u3092\u6301\u3064\u30e2\u30c7\u30eb\u3067\u306f\u3001\u6ce8\u610f\u91cd\u307f\u304c\u5909\u6570\u91cd\u8981\u5ea6\u3068\u3057\u3066\u89e3\u91c8\u3067\u304d\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002Wiegreffe &amp; Pinter (2019) \u306f\u6ce8\u610f\u91cd\u307f\u304c\u5e38\u306b\u89e3\u91c8\u53ef\u80fd\u3068\u306f\u9650\u3089\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u3064\u3064\u3082\u3001\u7279\u5b9a\u306e\u6761\u4ef6\u4e0b\u3067\u306f\u6709\u7528\u306a\u8aac\u660e\u3092\u63d0\u4f9b\u3067\u304d\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u3002</p> <p>Chefer et al. (2021) \u306f\u300cTransformer Interpretability Beyond Attention Visualization\u300d\u3068\u3044\u3046\u624b\u6cd5\u3092\u63d0\u6848\u3057\u3001\u52fe\u914d\u3068\u6ce8\u610f\u91cd\u307f\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u6b63\u78ba\u306a\u7279\u5fb4\u91cd\u8981\u5ea6\u3092\u8a08\u7b97\u3057\u305f\u3002</p> <p>\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u30e2\u30c7\u30eb\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306f\u767a\u5c55\u9014\u4e0a\u306e\u5206\u91ce\u3067\u3042\u308a\u3001\u30e2\u30c7\u30eb\u306e\u8907\u96d1\u3055\u306b\u8d77\u56e0\u3059\u308b\u7279\u6709\u306e\u8ab2\u984c\uff08\u52fe\u914d\u306e\u4e0d\u5b89\u5b9a\u6027\u3001\u904e\u30d1\u30e9\u30e1\u30fc\u30bf\u5316\u306a\u3069\uff09\u3078\u306e\u5bfe\u51e6\u65b9\u6cd5\u304c\u4eca\u5f8c\u3082\u7814\u7a76\u3055\u308c\u308b\u3068\u4e88\u60f3\u3055\u308c\u308b\u3002</p>"},{"location":"research/note/variable-importance/#72","title":"7.2 \u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306e\u65b0\u305f\u306a\u767a\u5c55","text":"<p>\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306b\u95a2\u3059\u308b\u7814\u7a76\u3082\u8fd1\u5e74\u5927\u304d\u304f\u9032\u5c55\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#721-model-x-knockoffs","title":"7.2.1 Model-X Knockoffs","text":"<p>Cand\u00e8s et al. (2018) \u304c\u63d0\u6848\u3057\u305f\u300cModel-X Knockoffs\u300d\u306f\u3001\u507d\u767a\u898b\u7387\uff08FDR\uff09\u3092\u53b3\u5bc6\u306b\u5236\u5fa1\u3057\u306a\u304c\u3089\u771f\u306b\u95a2\u9023\u3059\u308b\u7279\u5fb4\u3092\u540c\u5b9a\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002\u3053\u306e\u624b\u6cd5\u306e\u6838\u5fc3\u306f\u3001\u5404\u7279\u5fb4 \\(X_j\\) \u306b\u5bfe\u3057\u3066\u300c\u30ce\u30c3\u30af\u30aa\u30d5\u5909\u6570 \\(\\tilde{X}_j\\)\u300d\u3092\u751f\u6210\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u5909\u6570\u3068\u305d\u306e\u30ce\u30c3\u30af\u30aa\u30d5\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u306b\u3042\u308b\u3002</p> <p>\u30ce\u30c3\u30af\u30aa\u30d5\u5909\u6570\u306f\u3001\u30aa\u30ea\u30b8\u30ca\u30eb\u5909\u6570\u3068\u540c\u69d8\u306e\u5468\u8fba\u5206\u5e03\u3092\u6301\u3061\u3001\u30aa\u30ea\u30b8\u30ca\u30eb\u5909\u6570\u3068\u306e\u4ea4\u63db\u53ef\u80fd\u6027\u3092\u6301\u3061\uff08\u5143\u306e\u540c\u6642\u5206\u5e03\u3092\u4fdd\u5b58\uff09\u3001\u76ee\u7684\u5909\u6570 \\(Y\\) \u3068\u306f\u6761\u4ef6\u4ed8\u304d\u72ec\u7acb\u3068\u306a\u308b\u3088\u3046\u306b\u751f\u6210\u3055\u308c\u308b\u3002\u3053\u306e\u3088\u3046\u306a\u30ce\u30c3\u30af\u30aa\u30d5\u5909\u6570\u3092\u7528\u3044\u308b\u3068\u3001\u5404\u7279\u5fb4\u3068\u305d\u306e\u30ce\u30c3\u30af\u30aa\u30d5\u306b\u5bfe\u3059\u308b\u7d71\u8a08\u91cf \\(W_j\\) \u3092\u8a08\u7b97\u3067\u304d\u308b\uff1a</p> <p>\\(W_j = |Z_j| - |\\tilde{Z}_j|\\)</p> <p>\u3053\u3053\u3067 \\(Z_j\\) \u3068 \\(\\tilde{Z}_j\\) \u306f\u305d\u308c\u305e\u308c \\(X_j\\) \u3068 \\(\\tilde{X}_j\\) \u306b\u95a2\u9023\u3059\u308b\u7d71\u8a08\u91cf\u3067\u3042\u308b\u3002\u3053\u308c\u306b\u3088\u308a\u3001FDR\u3092\u5236\u5fa1\u3057\u306a\u304c\u3089\u91cd\u8981\u306a\u5909\u6570\u3092\u9078\u629e\u3067\u304d\u308b\u3002</p> <p>Lu et al. (2018) \u306f\u3053\u306e\u624b\u6cd5\u3092\u3055\u3089\u306b\u767a\u5c55\u3055\u305b\u3001\u9ad8\u6b21\u5143\u8a2d\u5b9a\u3067\u306e\u52b9\u7387\u7684\u306a\u5b9f\u88c5\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002Model-X Knockoffs\u306e\u7279\u5fb4\u306f\u3001\u4efb\u610f\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u3068\u7d44\u307f\u5408\u308f\u305b\u3089\u308c\u308b\u70b9\u3068\u3001FDR\u306b\u95a2\u3059\u308b\u7406\u8ad6\u4fdd\u8a3c\u304c\u3042\u308b\u70b9\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#722","title":"7.2.2 \u56e0\u679c\u7684\u5909\u6570\u91cd\u8981\u5ea6","text":"<p>\u8fd1\u5e74\u3001\u5909\u6570\u91cd\u8981\u5ea6\u3092\u56e0\u679c\u63a8\u8ad6\u306e\u67a0\u7d44\u307f\u3067\u6349\u3048\u308b\u7814\u7a76\u3082\u9032\u3093\u3067\u3044\u308b\u3002Fisher et al. (2019) \u306f\u300c\u56e0\u679c\u7684\u91cd\u8981\u5ea6\uff08Causal Importance\uff09\u300d\u3092\u63d0\u6848\u3057\u3001\u5909\u6570\u306e\u4ecb\u5165\u52b9\u679c\u306b\u57fa\u3065\u3044\u305f\u91cd\u8981\u5ea6\u6307\u6a19\u3092\u5b9a\u5f0f\u5316\u3057\u305f\uff1a</p> <p>\\(I_{causal}(X_j) = \\mathbb{E}_{X_{\\neg j}}[d(P(Y|X_j, X_{\\neg j}), P(Y|do(X_j=x'_j), X_{\\neg j}))]\\)</p> <p>\u3053\u3053\u3067 \\(do(X_j=x'_j)\\) \u306f\u5909\u6570 \\(X_j\\) \u3078\u306e\u4ecb\u5165\u3092\u8868\u3057\u3001\\(d\\) \u306f2\u3064\u306e\u78ba\u7387\u5206\u5e03\u9593\u306e\u8ddd\u96e2\u95a2\u6570\u3067\u3042\u308b\u3002\u3053\u306e\u5b9a\u7fa9\u306b\u3088\u308a\u3001\u5358\u306a\u308b\u76f8\u95a2\u95a2\u4fc2\u3067\u306f\u306a\u304f\u3001\u56e0\u679c\u7684\u306a\u5bc4\u4e0e\u3092\u6e2c\u5b9a\u3067\u304d\u308b\u3002</p> <p>Heskes et al. (2020) \u306f\u56e0\u679c\u7684\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\uff08Causal Shapley Values\uff09\u3092\u63d0\u6848\u3057\u3001\u69cb\u9020\u7684\u56e0\u679c\u30e2\u30c7\u30eb\uff08SCM\uff09\u306e\u67a0\u7d44\u307f\u3067\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u3092\u518d\u5b9a\u7fa9\u3057\u305f\u3002\u3053\u306e\u624b\u6cd5\u306f\u7279\u306b\u5909\u6570\u9593\u306b\u56e0\u679c\u69cb\u9020\u304c\u3042\u308b\u5834\u5408\u306b\u3001\u3088\u308a\u89e3\u91c8\u3057\u3084\u3059\u3044\u91cd\u8981\u5ea6\u3092\u63d0\u4f9b\u3059\u308b\u3002</p>"},{"location":"research/note/variable-importance/#723","title":"7.2.3 \u9ad8\u6b21\u5143\u30fb\u30b9\u30d1\u30fc\u30b9\u30c7\u30fc\u30bf\u5411\u3051\u624b\u6cd5","text":"<p>\u9ad8\u6b21\u5143\u30fb\u30b9\u30d1\u30fc\u30b9\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u52b9\u7387\u7684\u306a\u5909\u6570\u91cd\u8981\u5ea6\u63a8\u5b9a\u3082\u7814\u7a76\u3055\u308c\u3066\u3044\u308b\u3002Dai &amp; Yamada (2023) \u306f\u300cKernel Knockoffs\u300d\u3092\u63d0\u6848\u3057\u3001\u30ab\u30fc\u30cd\u30eb\u6cd5\u3068\u30ce\u30c3\u30af\u30aa\u30d5\u306e\u6982\u5ff5\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u975e\u7dda\u5f62\u306a\u9ad8\u6b21\u5143\u8a2d\u5b9a\u3067\u3082\u52b9\u7387\u7684\u306b\u5909\u6570\u91cd\u8981\u5ea6\u3092\u63a8\u5b9a\u3067\u304d\u308b\u624b\u6cd5\u3092\u958b\u767a\u3057\u305f\u3002</p> <p>Liu et al. (2022) \u306f\u30b9\u30d1\u30fc\u30b9\u30c7\u30fc\u30bf\u5411\u3051\u306e\u9811\u5065\u306a\u5909\u6570\u91cd\u8981\u5ea6\u63a8\u5b9a\u6cd5\u3092\u63d0\u6848\u3057\u3001\u6b20\u6e2c\u5024\u3084\u5916\u308c\u5024\u304c\u591a\u3044\u30c7\u30fc\u30bf\u3067\u3082\u5b89\u5b9a\u3057\u305f\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u3002</p> <p>\u3053\u308c\u3089\u306e\u65b0\u3057\u3044\u30a2\u30d7\u30ed\u30fc\u30c1\u306b\u3088\u308a\u3001\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306f\u3088\u308a\u5e83\u7bc4\u306a\u5fdc\u7528\u304c\u53ef\u80fd\u306b\u306a\u308a\u3064\u3064\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#73-ai","title":"7.3 \u8aac\u660e\u53ef\u80fdAI\u624b\u6cd5\u3068\u306e\u7d71\u5408","text":"<p>\u5909\u6570\u91cd\u8981\u5ea6\u306f\u3001\u3088\u308a\u5e83\u7bc4\u306a\u8aac\u660e\u53ef\u80fdAI\uff08XAI\uff09\u306e\u6587\u8108\u3067\u4ed6\u306e\u624b\u6cd5\u3068\u7d71\u5408\u3055\u308c\u3064\u3064\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#731-shap","title":"7.3.1 SHAP\u3068\u306e\u6bd4\u8f03\u3068\u7d71\u5408","text":"<p>SHAP\u306f\u30b2\u30fc\u30e0\u7406\u8ad6\u306e\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u306b\u57fa\u3065\u304f\u8aac\u660e\u624b\u6cd5\u3067\u3001Lundberg &amp; Lee (2017) \u306b\u3088\u3063\u3066\u63d0\u6848\u3055\u308c\u305f\u3002SHAP\u306f\u30ed\u30fc\u30ab\u30eb\u8aac\u660e\u3068\u30b0\u30ed\u30fc\u30d0\u30eb\u8aac\u660e\u306e\u4e21\u65b9\u3092\u63d0\u4f9b\u3057\u3001\u30e2\u30c7\u30eb\u306b\u4f9d\u5b58\u3057\u306a\u3044\u4e00\u822c\u7684\u306a\u624b\u6cd5\u3067\u3042\u308a\u3001\u52a0\u6cd5\u6027\u3084\u30ed\u30fc\u30ab\u30eb\u7cbe\u5ea6\u306a\u3069\u306e\u7406\u8ad6\u7684\u306a\u6027\u8cea\u3092\u6e80\u305f\u3059\u3068\u3044\u3046\u7279\u5fb4\u3092\u6301\u3064\u3002</p> <p>SHAP\u306e\u30b0\u30ed\u30fc\u30d0\u30eb\u91cd\u8981\u5ea6\uff08\u5404\u7279\u5fb4\u306eSHAP\u5024\u306e\u7d76\u5bfe\u5024\u306e\u5e73\u5747\uff09\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u4e00\u7a2e\u3068\u898b\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002Lundberg\u3089\u306fTreeSHAP\u3068\u3044\u3046\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u958b\u767a\u3057\u3001\u6c7a\u5b9a\u6728\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u306b\u5bfe\u3057\u3066\u52b9\u7387\u7684\u306bSHAP\u5024\u3092\u8a08\u7b97\u3059\u308b\u65b9\u6cd5\u3092\u793a\u3057\u305f\u3002</p> <p>SHAP\u3068\u5f93\u6765\u306e\u5909\u6570\u91cd\u8981\u5ea6\u3092\u6bd4\u8f03\u3057\u305f\u7814\u7a76\u306b\u3088\u308c\u3070\u3001\u4e21\u8005\u306f\u6982\u306d\u76f8\u95a2\u3059\u308b\u3082\u306e\u306e\u3001\u7279\u306b\u76f8\u95a2\u306e\u3042\u308b\u5909\u6570\u306b\u5bfe\u3057\u3066\u306f\u7570\u306a\u308b\u7d50\u679c\u3092\u793a\u3059\u3053\u3068\u304c\u3042\u308b\uff08Molnar et al., 2020\uff09\u3002SHAP\u306f\u76f8\u95a2\u5909\u6570\u306b\u5bfe\u3057\u3066\u3088\u308a\u300c\u516c\u5e73\u300d\u306b\u91cd\u8981\u5ea6\u3092\u914d\u5206\u3059\u308b\u50be\u5411\u304c\u3042\u308b\u3002</p> <p>\u6700\u8fd1\u3067\u306f\u3001Janizadeh et al. (2022) \u304cSHAP\u3068\u5f93\u6765\u306e\u91cd\u8981\u5ea6\u6307\u6a19\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u7d71\u5408\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u63d0\u6848\u3057\u3001\u4e21\u65b9\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306e\u9577\u6240\u3092\u6d3b\u304b\u3059\u65b9\u6cd5\u3092\u793a\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#732","title":"7.3.2 \u90e8\u5206\u4f9d\u5b58\u30d7\u30ed\u30c3\u30c8\u3068\u306e\u9023\u643a","text":"<p>\u5909\u6570\u91cd\u8981\u5ea6\u306f\u300c\u3069\u306e\u5909\u6570\u304c\u91cd\u8981\u304b\u300d\u3092\u793a\u3059\u304c\u3001\u300c\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3059\u308b\u304b\u300d\u306f\u793a\u3055\u306a\u3044\u3002\u3053\u306e\u9650\u754c\u3092\u88dc\u3046\u305f\u3081\u3001\u90e8\u5206\u4f9d\u5b58\u30d7\u30ed\u30c3\u30c8\uff08PDP\uff09\u3084\u500b\u5225\u6761\u4ef6\u671f\u5f85\u5024\uff08ICE\uff09\u30d7\u30ed\u30c3\u30c8\u306a\u3069\u306e\u53ef\u8996\u5316\u624b\u6cd5\u3068\u306e\u9023\u643a\u304c\u91cd\u8981\u3068\u306a\u308b\u3002</p> <p>Greenwell et al. (2018) \u306f\u5909\u6570\u91cd\u8981\u5ea6\u3068\u90e8\u5206\u4f9d\u5b58\u30d7\u30ed\u30c3\u30c8\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u4f53\u7cfb\u7684\u306a\u5206\u6790\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u63d0\u6848\u3057\u305f\u3002\u3053\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306b\u57fa\u3065\u3044\u3066\u4e0a\u4f4d\u5909\u6570\u3092\u7279\u5b9a\u3057\u3001\u5404\u91cd\u8981\u5909\u6570\u306ePDP\u3092\u4f5c\u6210\u3057\u3066\u4e88\u6e2c\u3078\u306e\u5f71\u97ff\u30d1\u30bf\u30fc\u30f3\u3092\u5206\u6790\u3057\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u5909\u6570\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u3092\u4e8c\u6b21\u5143PDP\u3067\u78ba\u8a8d\u3059\u308b\u3002\u3053\u306e\u7d44\u307f\u5408\u308f\u305b\u306b\u3088\u308a\u3001\u300c\u3069\u306e\u5909\u6570\u304c\u300d\u300c\u3069\u306e\u3088\u3046\u306b\u300d\u4e88\u6e2c\u306b\u5f71\u97ff\u3059\u308b\u304b\u3092\u7dcf\u5408\u7684\u306b\u7406\u89e3\u3067\u304d\u308b\u3002</p>"},{"location":"research/note/variable-importance/#733","title":"7.3.3 \u53cd\u5b9f\u4eee\u60f3\u8aac\u660e\u3068\u306e\u7d71\u5408","text":"<p>\u300c\u53cd\u5b9f\u4eee\u60f3\u8aac\u660e\uff08Counterfactual Explanations\uff09\u300d\u306f\u3001\u4e88\u6e2c\u7d50\u679c\u3092\u5909\u3048\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u6700\u5c0f\u9650\u306e\u5165\u529b\u5909\u66f4\u3092\u7279\u5b9a\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002Wachter et al. (2018) \u304c\u63d0\u6848\u3057\u305f\u3053\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u3001\u300c\u7570\u306a\u308b\u7d50\u679c\u3092\u5f97\u308b\u306b\u306f\u4f55\u3092\u5909\u3048\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u300d\u3068\u3044\u3046\u5b9f\u7528\u7684\u306a\u8cea\u554f\u306b\u7b54\u3048\u308b\u3002</p> <p>\u6700\u8fd1\u306e\u7814\u7a76\u3067\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u3068\u53cd\u5b9f\u4eee\u60f3\u8aac\u660e\u3092\u7d71\u5408\u3059\u308b\u8a66\u307f\u304c\u3042\u308b\u3002\u4f8b\u3048\u3070\u3001Virgolin et al. (2022) \u306f\u91cd\u8981\u5ea6\u306e\u9ad8\u3044\u5909\u6570\u306b\u7126\u70b9\u3092\u5f53\u3066\u305f\u52b9\u7387\u7684\u306a\u53cd\u5b9f\u4eee\u60f3\u63a2\u7d22\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u63d0\u6848\u3057\u305f\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5909\u5316\u3055\u305b\u308b\u5909\u6570\u306e\u6570\u3092\u524a\u6e1b\u3057\u3064\u3064\u3001\u5b9f\u7528\u7684\u306a\u53cd\u5b9f\u4eee\u60f3\u4f8b\u3092\u751f\u6210\u3067\u304d\u308b\u3002</p> <p>\u3053\u308c\u3089\u306e\u7d71\u5408\u7684\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u3001\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u3088\u308a\u6df1\u3044\u7406\u89e3\u3068\u52b9\u679c\u7684\u306a\u8aac\u660e\u3092\u53ef\u80fd\u306b\u3057\u3001\u5b9f\u52d9\u306b\u304a\u3051\u308b\u89e3\u91c8\u53ef\u80fd\u6027\u306e\u8ab2\u984c\u306b\u5bfe\u51e6\u3059\u308b\u91cd\u8981\u306a\u9032\u5c55\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#8","title":"8. \u7814\u7a76\u306e\u30ae\u30e3\u30c3\u30d7\u3068\u5c06\u6765\u306e\u65b9\u5411\u6027","text":""},{"location":"research/note/variable-importance/#81","title":"8.1 \u672a\u89e3\u6c7a\u306e\u7406\u8ad6\u7684\u554f\u984c","text":"<p>\u5909\u6570\u91cd\u8981\u5ea6\u306b\u95a2\u3059\u308b\u7814\u7a76\u306f\u5927\u304d\u304f\u9032\u5c55\u3057\u3066\u3044\u308b\u304c\u3001\u4f9d\u7136\u3068\u3057\u3066\u91cd\u8981\u306a\u7406\u8ad6\u7684\u554f\u984c\u304c\u672a\u89e3\u6c7a\u306e\u307e\u307e\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#811","title":"8.1.1 \u6f38\u8fd1\u7406\u8ad6\u306e\u62e1\u5f35","text":"<p>\u73fe\u5728\u306e\u6f38\u8fd1\u7406\u8ad6\u306f\u6bd4\u8f03\u7684\u5358\u7d14\u306a\u8a2d\u5b9a\uff08\u4f4e\u6b21\u5143\u3001\u5f31\u76f8\u95a2\u3001\u5358\u7d14\u306a\u4f9d\u5b58\u69cb\u9020\u306a\u3069\uff09\u306b\u9650\u5b9a\u3055\u308c\u3066\u3044\u308b\u3002\u3088\u308a\u73fe\u5b9f\u7684\u306a\u72b6\u6cc1\u306b\u5bfe\u5fdc\u3059\u308b\u305f\u3081\u306e\u62e1\u5f35\u304c\u5fc5\u8981\u3068\u3055\u308c\u3066\u3044\u308b\u3002\u8d85\u9ad8\u6b21\u5143\u8a2d\u5b9a\u3067\u306e\u7406\u8ad6\u3068\u3057\u3066\u3001\u5909\u6570\u6570 \\(p\\) \u304c\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba \\(n\\) \u3088\u308a\u5927\u304d\u3044\u30b1\u30fc\u30b9\u3001\u7279\u306b \\(p = O(e^{n^{\\alpha}})\\) \u306e\u3088\u3046\u306a\u8d85\u9ad8\u6b21\u5143\u8a2d\u5b9a\u3067\u306e\u6f38\u8fd1\u7279\u6027\u306e\u89e3\u6790\u304c\u6c42\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u5f37\u76f8\u95a2\u69cb\u9020\u306e\u3042\u308b\u5834\u5408\u306e\u6f38\u8fd1\u5206\u5e03\u306e\u554f\u984c\u3068\u3057\u3066\u3001\u591a\u91cd\u5171\u7dda\u6027\u304c\u5f37\u3044\u5834\u5408\u306e\u5909\u6570\u91cd\u8981\u5ea6\u63a8\u5b9a\u91cf\u306e\u4e00\u81f4\u6027\u3068\u6f38\u8fd1\u5206\u5e03\u306e\u7279\u5b9a\u304c\u8ab2\u984c\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u307e\u305f\u3001\u975e\u7dda\u5f62\u30fb\u76f8\u4e92\u4f5c\u7528\u306e\u5f37\u3044\u30e2\u30c7\u30eb\u3067\u306e\u7406\u8ad6\u3068\u3057\u3066\u3001\u8907\u96d1\u306a\u975e\u7dda\u5f62\u76f8\u4e92\u4f5c\u7528\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7684\u6027\u8cea\u306e\u89e3\u660e\u3082\u91cd\u8981\u3067\u3042\u308b\u3002</p> <p>\u3053\u306e\u5206\u91ce\u3067\u306e\u9032\u5c55\u306f\u3001\u3088\u308a\u4e00\u822c\u7684\u306a\u6761\u4ef6\u4e0b\u3067\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306e\u4fe1\u983c\u6027\u3068\u9069\u7528\u7bc4\u56f2\u3092\u62e1\u5927\u3059\u308b\u305f\u3081\u306b\u4e0d\u53ef\u6b20\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#812","title":"8.1.2 \u76f8\u95a2\u69cb\u9020\u306e\u7406\u8ad6\u7684\u89e3\u660e","text":"<p>\u76f8\u95a2\u306e\u3042\u308b\u5909\u6570\u306e\u91cd\u8981\u5ea6\u8a55\u4fa1\u306f\u4f9d\u7136\u3068\u3057\u3066\u96e3\u554f\u3067\u3042\u308b\u3002\u76f8\u95a2\u30d0\u30a4\u30a2\u30b9\u306e\u5b9a\u91cf\u5316\u3068\u3057\u3066\u3001\u69d8\u3005\u306a\u76f8\u95a2\u69cb\u9020\u4e0b\u3067\u306e\u91cd\u8981\u5ea6\u30d0\u30a4\u30a2\u30b9\u3092\u5b9a\u91cf\u7684\u306b\u7279\u5b9a\u3059\u308b\u7406\u8ad6\u306e\u69cb\u7bc9\u304c\u6c42\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u76f8\u95a2\u4e0b\u3067\u306e\u6700\u9069\u306a\u91cd\u8981\u5ea6\u5b9a\u7fa9\u3068\u3057\u3066\u3001\u76f8\u95a2\u69cb\u9020\u304c\u3042\u308b\u5834\u5408\u306b\u7406\u8ad6\u7684\u306b\u6700\u9069\u306a\uff08\u30d0\u30a4\u30a2\u30b9\u304c\u6700\u5c0f\u5316\u3055\u308c\u308b\uff09\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5b9a\u7fa9\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u3053\u3068\u3082\u91cd\u8981\u3067\u3042\u308b\u3002\u30b0\u30eb\u30fc\u30d7\u5909\u6570\u306e\u7406\u8ad6\u7684\u53d6\u308a\u6271\u3044\u3068\u3057\u3066\u3001\u5909\u6570\u7fa4\u3092\u30b0\u30eb\u30fc\u30d7\u3068\u3057\u3066\u6271\u3046\u5834\u5408\u306e\u7406\u8ad6\u7684\u57fa\u790e\u306e\u78ba\u7acb\u3001\u7279\u306b\u30b0\u30eb\u30fc\u30d7\u306e\u5b9a\u7fa9\u65b9\u6cd5\u3068\u30b0\u30eb\u30fc\u30d7\u5185\u30fb\u30b0\u30eb\u30fc\u30d7\u9593\u306e\u76f8\u95a2\u306e\u5f71\u97ff\u3092\u89e3\u660e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#813","title":"8.1.3 \u8a08\u7b97\u8907\u96d1\u6027\u3068\u8fd1\u4f3c\u7cbe\u5ea6\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5","text":"<p>\u591a\u304f\u306e\u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u3001\u7279\u306b\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u30d9\u30fc\u30b9\u306e\u624b\u6cd5\u306f\u8a08\u7b97\u91cf\u304c\u975e\u5e38\u306b\u591a\u3044\u3002\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6226\u7565\u306e\u7406\u8ad6\u7684\u4fdd\u8a3c\u3068\u3057\u3066\u3001\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u306e\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u8fd1\u4f3c\u306a\u3069\u306b\u304a\u3051\u308b\u8fd1\u4f3c\u8aa4\u5dee\u306e\u7406\u8ad6\u7684\u4e0a\u9650\u3068\u5fc5\u8981\u30b5\u30f3\u30d7\u30eb\u6570\u306e\u95a2\u4fc2\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3042\u308b\u3002\u8a08\u7b97\u52b9\u7387\u3068\u7d71\u8a08\u7684\u52b9\u7387\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3068\u3057\u3066\u3001\u8a08\u7b97\u30b3\u30b9\u30c8\u3092\u524a\u6e1b\u3059\u308b\u8fd1\u4f3c\u624b\u6cd5\u304c\u7d71\u8a08\u7684\u52b9\u7387\uff08\u5206\u6563\u306a\u3069\uff09\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u306e\u7406\u8ad6\u7684\u89e3\u660e\u3082\u6c42\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u6700\u9069\u306a\u5909\u6570\u30b5\u30d6\u30bb\u30c3\u30c8\u9078\u629e\u3068\u3057\u3066\u3001\u8a08\u7b97\u52b9\u7387\u5411\u4e0a\u306e\u305f\u3081\u306e\u5909\u6570\u30b5\u30d6\u30bb\u30c3\u30c8\u9078\u629e\u624b\u6cd5\u306e\u7406\u8ad6\u7684\u57fa\u790e\u306e\u78ba\u7acb\u3082\u8ab2\u984c\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#82","title":"8.2 \u5b9f\u52d9\u4e0a\u306e\u8ab2\u984c\u3068\u89e3\u6c7a\u3059\u3079\u304d\u554f\u984c","text":"<p>\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5b9f\u7528\u5316\u306b\u304a\u3044\u3066\u3001\u3044\u304f\u3064\u304b\u306e\u5b9f\u52d9\u7684\u8ab2\u984c\u304c\u6b8b\u3055\u308c\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#821","title":"8.2.1 \u5b89\u5b9a\u6027\u3068\u518d\u73fe\u6027","text":"<p>\u8907\u6570\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3084\u30e2\u30c7\u30eb\u306b\u308f\u305f\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5b89\u5b9a\u6027\u3068\u518d\u73fe\u6027\u306f\u3001\u5b9f\u52d9\u4e0a\u306e\u5927\u304d\u306a\u61f8\u5ff5\u3067\u3042\u308b\u3002\u5b89\u5b9a\u6027\u8a55\u4fa1\u306e\u6a19\u6e96\u624b\u6cd5\u3068\u3057\u3066\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5b89\u5b9a\u6027\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u6a19\u6e96\u7684\u306a\u624b\u6cd5\u3068\u30e1\u30c8\u30ea\u30af\u30b9\u306e\u78ba\u7acb\u304c\u5fc5\u8981\u3067\u3042\u308b\u3002\u4ea4\u5dee\u691c\u8a3c\u3068\u91cd\u8981\u5ea6\u306e\u95a2\u4fc2\u3068\u3057\u3066\u3001\u7570\u306a\u308b\u4ea4\u5dee\u691c\u8a3c\u5206\u5272\u3067\u306e\u91cd\u8981\u5ea6\u306e\u5909\u52d5\u3092\u6700\u5c0f\u5316\u3059\u308b\u624b\u6cd5\u306e\u958b\u767a\u3082\u91cd\u8981\u3067\u3042\u308b\u3002\u9577\u671f\u7684\u5b89\u5b9a\u6027\u306e\u554f\u984c\u3068\u3057\u3066\u3001\u6642\u9593\u7d4c\u904e\u306b\u4f34\u3046\u30c7\u30fc\u30bf\u5206\u5e03\u306e\u5909\u5316\u306b\u5bfe\u3059\u308b\u91cd\u8981\u5ea6\u306e\u5b89\u5b9a\u6027\u8a55\u4fa1\u3068\u5bfe\u7b56\u3082\u8ab2\u984c\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#822","title":"8.2.2 \u89e3\u91c8\u53ef\u80fd\u6027\u3068\u56e0\u679c\u6027","text":"<p>\u5909\u6570\u91cd\u8981\u5ea6\u306e\u89e3\u91c8\u306f\u3001\u7279\u306b\u56e0\u679c\u95a2\u4fc2\u3068\u306e\u6df7\u540c\u304c\u554f\u984c\u3068\u306a\u308b\u3002\u56e0\u679c\u7684\u89e3\u91c8\u306e\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u3068\u3057\u3066\u3001\u5909\u6570\u91cd\u8981\u5ea6\u3092\u56e0\u679c\u7684\u306b\u89e3\u91c8\u3067\u304d\u308b\u6761\u4ef6\u3068\u9650\u754c\u3092\u660e\u78ba\u5316\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u89b3\u5bdf\u7814\u7a76\u306b\u304a\u3051\u308b\u4ea4\u7d61\u306e\u5f71\u97ff\u3068\u3057\u3066\u3001\u89b3\u5bdf\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u304f\u5909\u6570\u91cd\u8981\u5ea6\u304c\u4ea4\u7d61\u306b\u3088\u308a\u3069\u306e\u7a0b\u5ea6\u6b6a\u3081\u3089\u308c\u308b\u304b\u306e\u5b9a\u91cf\u8a55\u4fa1\u3082\u91cd\u8981\u3067\u3042\u308b\u3002\u56e0\u679c\u30b0\u30e9\u30d5\u3092\u6d3b\u7528\u3057\u305f\u91cd\u8981\u5ea6\u3068\u3057\u3066\u3001\u56e0\u679c\u69cb\u9020\u60c5\u5831\u3092\u6d3b\u7528\u3057\u305f\u5909\u6570\u91cd\u8981\u5ea6\u306e\u6539\u826f\u3082\u6c42\u3081\u3089\u308c\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#823","title":"8.2.3 \u5b9f\u52d9\u5bb6\u5411\u3051\u306e\u9078\u629e\u6307\u91dd","text":"<p>\u5b9f\u52d9\u5bb6\u304c\u9069\u5207\u306a\u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u3092\u9078\u629e\u3059\u308b\u305f\u3081\u306e\u4f53\u7cfb\u7684\u306a\u6307\u91dd\u304c\u5fc5\u8981\u3067\u3042\u308b\u3002\u8a3a\u65ad\u30c4\u30fc\u30eb\u306e\u958b\u767a\u3068\u3057\u3066\u3001\u30c7\u30fc\u30bf\u306e\u7279\u6027\uff08\u76f8\u95a2\u69cb\u9020\u3001\u6b21\u5143\u306a\u3069\uff09\u306b\u57fa\u3065\u3044\u3066\u6700\u9069\u306a\u91cd\u8981\u5ea6\u624b\u6cd5\u3092\u63d0\u6848\u3059\u308b\u8a3a\u65ad\u30c4\u30fc\u30eb\u304c\u6c42\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u6574\u5099\u3068\u3057\u3066\u3001\u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u6a19\u6e96\u7684\u306a\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u78ba\u7acb\u3082\u91cd\u8981\u3067\u3042\u308b\u3002\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u56fa\u6709\u306e\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u3068\u3057\u3066\u3001\u533b\u7642\u3001\u91d1\u878d\u306a\u3069\u7279\u5b9a\u306e\u5fdc\u7528\u5206\u91ce\u306b\u7279\u5316\u3057\u305f\u5909\u6570\u91cd\u8981\u5ea6\u306e\u9078\u629e\u30fb\u89e3\u91c8\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u306e\u958b\u767a\u3082\u5fc5\u8981\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#83","title":"8.3 \u65b0\u305f\u306a\u5fdc\u7528\u9818\u57df\u3068\u5c06\u6765\u306e\u65b9\u5411\u6027","text":"<p>\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5c06\u6765\u7684\u306a\u767a\u5c55\u3068\u3057\u3066\u3001\u3044\u304f\u3064\u304b\u306e\u65b9\u5411\u6027\u304c\u8003\u3048\u3089\u308c\u308b\u3002</p>"},{"location":"research/note/variable-importance/#831","title":"8.3.1 \u6642\u7cfb\u5217\u30fb\u7e26\u65ad\u30c7\u30fc\u30bf\u3078\u306e\u62e1\u5f35","text":"<p>\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3084\u7e26\u65ad\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306f\u3001\u307e\u3060\u5341\u5206\u306b\u7814\u7a76\u3055\u308c\u3066\u3044\u306a\u3044\u9818\u57df\u3067\u3042\u308b\u3002\u6642\u5909\u3059\u308b\u91cd\u8981\u5ea6\u3068\u3057\u3066\u3001\u6642\u9593\u3068\u3068\u3082\u306b\u5909\u5316\u3059\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306e\u63a8\u5b9a\u3068\u53ef\u8996\u5316\u624b\u6cd5\u306e\u958b\u767a\u304c\u6c42\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u6642\u9593\u7684\u4f9d\u5b58\u69cb\u9020\u306e\u8003\u616e\u3068\u3057\u3066\u3001\u81ea\u5df1\u76f8\u95a2\u3084\u4ea4\u5dee\u9045\u5ef6\u52b9\u679c\u3092\u8003\u616e\u3057\u305f\u6642\u7cfb\u5217\u7279\u6709\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5b9a\u7fa9\u3092\u78ba\u7acb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u7e26\u65ad\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308b\u5909\u6570\u9593\u76f8\u4e92\u4f5c\u7528\u3068\u3057\u3066\u3001\u6642\u9593\u7d4c\u904e\u3068\u3068\u3082\u306b\u5909\u5316\u3059\u308b\u5909\u6570\u9593\u76f8\u4e92\u4f5c\u7528\u306e\u91cd\u8981\u5ea6\u8a55\u4fa1\u624b\u6cd5\u306e\u958b\u767a\u3082\u91cd\u8981\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#832","title":"8.3.2 \u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u30c7\u30fc\u30bf\u3068\u7570\u7a2e\u30c7\u30fc\u30bf\u306e\u7d71\u5408","text":"<p>\u7570\u306a\u308b\u7a2e\u985e\u306e\u30c7\u30fc\u30bf\uff08\u30c6\u30ad\u30b9\u30c8\u3001\u753b\u50cf\u3001\u6570\u5024\u306a\u3069\uff09\u3092\u7d71\u5408\u3057\u305f\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7814\u7a76\u3082\u91cd\u8981\u3067\u3042\u308b\u3002\u7570\u7a2e\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308b\u5171\u901a\u5c3a\u5ea6\u3068\u3057\u3066\u3001\u7570\u306a\u308b\u30c7\u30fc\u30bf\u30e2\u30c0\u30ea\u30c6\u30a3\u306b\u308f\u305f\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306e\u6bd4\u8f03\u3092\u53ef\u80fd\u306b\u3059\u308b\u5171\u901a\u5c3a\u5ea6\u306e\u78ba\u7acb\u304c\u6c42\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u76f8\u4e92\u4f5c\u7528\u306e\u91cd\u8981\u5ea6\u3068\u3057\u3066\u3001\u7570\u306a\u308b\u30e2\u30c0\u30ea\u30c6\u30a3\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306e\u5bc4\u4e0e\u3092\u8a55\u4fa1\u3059\u308b\u624b\u6cd5\u306e\u958b\u767a\u3082\u5fc5\u8981\u3067\u3042\u308b\u3002\u968e\u5c64\u7684\u91cd\u8981\u5ea6\u3068\u3057\u3066\u3001\u7570\u306a\u308b\u30ec\u30d9\u30eb\uff08\u7279\u5fb4\u3001\u30e2\u30c0\u30ea\u30c6\u30a3\u3001\u30e2\u30b8\u30e5\u30fc\u30eb\u306a\u3069\uff09\u3067\u306e\u91cd\u8981\u5ea6\u3092\u7d71\u5408\u7684\u306b\u8a55\u4fa1\u3059\u308b\u67a0\u7d44\u307f\u306e\u69cb\u7bc9\u3082\u8ab2\u984c\u3068\u306a\u3063\u3066\u3044\u308b\u3002</p>"},{"location":"research/note/variable-importance/#833-ai","title":"8.3.3 \u8aac\u660e\u53ef\u80fdAI\u3068\u81ea\u52d5\u610f\u601d\u6c7a\u5b9a\u30b7\u30b9\u30c6\u30e0","text":"<p>AI\u30b7\u30b9\u30c6\u30e0\u306e\u793e\u4f1a\u5b9f\u88c5\u304c\u9032\u3080\u4e2d\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306f\u8aac\u660e\u8cac\u4efb\u3068\u900f\u660e\u6027\u306e\u9375\u3068\u306a\u308b\u3002\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u5909\u6570\u91cd\u8981\u5ea6\u3068\u3057\u3066\u3001\u30aa\u30f3\u30e9\u30a4\u30f3\u5b66\u7fd2\u74b0\u5883\u306b\u304a\u3051\u308b\u52d5\u7684\u306a\u5909\u6570\u91cd\u8981\u5ea6\u306e\u66f4\u65b0\u3068\u53ef\u8996\u5316\u624b\u6cd5\u306e\u958b\u767a\u304c\u6c42\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u30e6\u30fc\u30b6\u30fc\u4e2d\u5fc3\u306e\u91cd\u8981\u5ea6\u8aac\u660e\u3068\u3057\u3066\u3001\u69d8\u3005\u306a\u30e6\u30fc\u30b6\u30fc\u5c64\uff08\u5c02\u9580\u5bb6\u3001\u4e00\u822c\u30e6\u30fc\u30b6\u30fc\u3001\u898f\u5236\u5f53\u5c40\u306a\u3069\uff09\u306b\u9069\u3057\u305f\u91cd\u8981\u5ea6\u306e\u8868\u73fe\u65b9\u6cd5\u306e\u78ba\u7acb\u3082\u91cd\u8981\u3067\u3042\u308b\u3002\u5909\u6570\u91cd\u8981\u5ea6\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u5faa\u74b0\u3068\u3057\u3066\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5206\u6790\u7d50\u679c\u306b\u57fa\u3065\u304f\u30e2\u30c7\u30eb\u6539\u5584\u3068\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u30b5\u30a4\u30af\u30eb\u306e\u78ba\u7acb\u3082\u8ab2\u984c\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#834","title":"8.3.4 \u5909\u6570\u91cd\u8981\u5ea6\u306e\u898f\u5236\u3068\u6a19\u6e96\u5316","text":"<p>AI\u30b7\u30b9\u30c6\u30e0\u306e\u898f\u5236\u304c\u9032\u3080\u4e2d\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u6a19\u6e96\u5316\u3082\u91cd\u8981\u306a\u8ab2\u984c\u3068\u306a\u308b\u3002\u898f\u5236\u5bfe\u5fdc\u306e\u6a19\u6e96\u624b\u6cd5\u3068\u3057\u3066\u3001\u91d1\u878d\u3084\u533b\u7642\u306a\u3069\u898f\u5236\u306e\u53b3\u3057\u3044\u5206\u91ce\u3067\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306e\u6a19\u6e96\u7684\u624b\u6cd5\u306e\u78ba\u7acb\u304c\u6c42\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u5909\u6570\u91cd\u8981\u5ea6\u306e\u76e3\u67fb\u624b\u6cd5\u3068\u3057\u3066\u3001\u7b2c\u4e09\u8005\u304c\u30e2\u30c7\u30eb\u306e\u5909\u6570\u91cd\u8981\u5ea6\u3092\u691c\u8a3c\u3059\u308b\u305f\u3081\u306e\u76e3\u67fb\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u958b\u767a\u3082\u5fc5\u8981\u3067\u3042\u308b\u3002\u5909\u6570\u91cd\u8981\u5ea6\u306e\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u3068\u3057\u3066\u3001\u696d\u754c\u3084\u5fdc\u7528\u5206\u91ce\u3054\u3068\u306e\u5909\u6570\u91cd\u8981\u5ea6\u306e\u5229\u7528\u306b\u95a2\u3059\u308b\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u306e\u78ba\u7acb\u3082\u91cd\u8981\u3067\u3042\u308b\u3002</p> <p>\u3053\u308c\u3089\u306e\u7814\u7a76\u65b9\u5411\u306f\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7684\u57fa\u76e4\u3092\u5f37\u5316\u3057\u3001\u5b9f\u8df5\u7684\u306a\u6709\u7528\u6027\u3092\u9ad8\u3081\u308b\u3068\u3068\u3082\u306b\u3001AI\u6280\u8853\u306e\u8cac\u4efb\u3042\u308b\u793e\u4f1a\u5b9f\u88c5\u3092\u652f\u63f4\u3059\u308b\u3082\u306e\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/variable-importance/#9","title":"9. \u7d50\u8ad6","text":"<p>\u672c\u30ec\u30d3\u30e5\u30fc\u8ad6\u6587\u3067\u306f\u3001\u6a5f\u68b0\u5b66\u7fd2\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u3068\u5fdc\u7528\u306b\u3064\u3044\u3066\u5305\u62ec\u7684\u306b\u5206\u6790\u3057\u305f\u3002\u5909\u6570\u91cd\u8981\u5ea6\u306f\u8907\u96d1\u306a\u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u3092\u9ad8\u3081\u3001\u4e88\u6e2c\u306b\u5bc4\u4e0e\u3059\u308b\u8981\u56e0\u3092\u7279\u5b9a\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u30c4\u30fc\u30eb\u3067\u3042\u308b\u3002\u3057\u304b\u3057\u3001\u305d\u306e\u5b9a\u7fa9\u3001\u63a8\u5b9a\u3001\u89e3\u91c8\u306b\u306f\u69d8\u3005\u306a\u8ab2\u984c\u304c\u4f34\u3046\u3053\u3068\u3082\u660e\u3089\u304b\u306b\u306a\u3063\u305f\u3002</p> <p>\u307e\u305a\u3001\u30e2\u30c7\u30eb\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u3068\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7684\u57fa\u76e4\u3092\u6574\u7406\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u6570\u5b66\u7684\u5b9a\u7fa9\u3068\u6f38\u8fd1\u7279\u6027\u306b\u3064\u3044\u3066\u8ad6\u3058\u305f\u3002\u7279\u306b\u3001LOCO\u3084Williamson\u306e\u52b9\u7387\u7684\u63a8\u5b9a\u91cf\u3001Floodgate\u306a\u3069\u306e\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u7684\u767a\u5c55\u304c\u3001\u3088\u308a\u4fe1\u983c\u6027\u306e\u9ad8\u3044\u5909\u6570\u8a55\u4fa1\u3092\u53ef\u80fd\u306b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u3002</p> <p>\u6b21\u306b\u3001\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u304a\u3051\u308bMDI\u3068MDA\u306e\u8a08\u7b97\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u7406\u8ad6\u7684\u6027\u8cea\u3092\u8a73\u8ff0\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u9577\u6240\u3068\u77ed\u6240\u3092\u5206\u6790\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u624b\u6cd5\u306b\u5185\u5728\u3059\u308b\u30d0\u30a4\u30a2\u30b9\u554f\u984c\u3068\u3001\u6761\u4ef6\u4ed8\u304d\u30d1\u30fc\u30df\u30e5\u30c6\u30fc\u30b7\u30e7\u30f3\u91cd\u8981\u5ea6\u3084\u4e0d\u504fMDI\u306a\u3069\u306e\u5bfe\u7b56\u624b\u6cd5\u306b\u3064\u3044\u3066\u3082\u8ad6\u3058\u305f\u3002</p> <p>\u5909\u6570\u9593\u306e\u76f8\u95a2\u554f\u984c\u306b\u3064\u3044\u3066\u306f\u3001\u30b7\u30e3\u30fc\u30d7\u30ec\u30fc\u5024\u306b\u57fa\u3065\u304f\u516c\u5e73\u306a\u91cd\u8981\u5ea6\u914d\u5206\u3084\u6761\u4ef6\u4ed8\u304d\u91cd\u8981\u5ea6\u3001\u30b0\u30eb\u30fc\u30d7\u5909\u6570\u91cd\u8981\u5ea6\u306a\u3069\u306e\u89e3\u6c7a\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u7d39\u4ecb\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u9ad8\u5ea6\u306a\u624b\u6cd5\u306b\u3088\u308a\u3001\u76f8\u95a2\u69cb\u9020\u4e0b\u3067\u3082\u3088\u308a\u4fe1\u983c\u6027\u306e\u9ad8\u3044\u5909\u6570\u8a55\u4fa1\u304c\u53ef\u80fd\u306b\u306a\u3063\u3066\u3044\u308b\u3002</p> <p>\u5b9f\u8df5\u7684\u306a\u89b3\u70b9\u304b\u3089\u306f\u3001\u69d8\u3005\u306a\u5909\u6570\u91cd\u8981\u5ea6\u624b\u6cd5\u306e\u6bd4\u8f03\u8a55\u4fa1\u3068\u9078\u629e\u6307\u91dd\u3092\u63d0\u793a\u3057\u3001\u30c7\u30fc\u30bf\u7279\u6027\u3084\u5206\u6790\u76ee\u7684\u306b\u5fdc\u3058\u305f\u9069\u5207\u306a\u624b\u6cd5\u9078\u629e\u3092\u652f\u63f4\u3057\u305f\u3002\u307e\u305f\u3001\u533b\u7642\u30fb\u30d0\u30a4\u30aa\u30a4\u30f3\u30d5\u30a9\u30de\u30c6\u30a3\u30af\u30b9\u5206\u91ce\u3067\u306e\u907a\u4f1d\u5b50\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u767a\u898b\u3084\u81e8\u5e8a\u4e88\u6e2c\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u5fdc\u7528\u3001\u91d1\u878d\u5206\u91ce\u3067\u306e\u4fe1\u7528\u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u3084\u5e02\u5834\u30ea\u30b9\u30af\u8a55\u4fa1\u306b\u304a\u3051\u308b\u5fdc\u7528\u4e8b\u4f8b\u3092\u8a73\u7d30\u306b\u5206\u6790\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u9818\u57df\u7279\u6709\u306e\u8ab2\u984c\u3068\u89e3\u6c7a\u7b56\u3092\u660e\u3089\u304b\u306b\u3057\u305f\u3002</p> <p>\u3055\u3089\u306b\u3001\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306b\u304a\u3051\u308b\u5909\u6570\u91cd\u8981\u5ea6\u306e\u767a\u5c55\u3084\u30e2\u30c7\u30eb\u975e\u4f9d\u5b58\u578b\u91cd\u8981\u5ea6\u306e\u65b0\u305f\u306a\u624b\u6cd5\u3001\u8aac\u660e\u53ef\u80fdAI\u6280\u8853\u3068\u306e\u7d71\u5408\u306a\u3069\u3001\u6700\u65b0\u306e\u7814\u7a76\u52d5\u5411\u3092\u6982\u89b3\u3057\u305f\u3002\u7279\u306b\u3001Model-X Knockoffs\u3084\u56e0\u679c\u7684\u5909\u6570\u91cd\u8981\u5ea6\u3001SHAP\u5024\u3068\u306e\u7d71\u5408\u3068\u3044\u3063\u305f\u5148\u9032\u7684\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7406\u8ad6\u3068\u5b9f\u8df5\u3092\u65b0\u305f\u306a\u6bb5\u968e\u3078\u3068\u9032\u3081\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u3002</p> <p>\u6700\u5f8c\u306b\u3001\u5909\u6570\u91cd\u8981\u5ea6\u7814\u7a76\u306b\u304a\u3051\u308b\u672a\u89e3\u6c7a\u306e\u7406\u8ad6\u7684\u554f\u984c\u3084\u5b9f\u52d9\u4e0a\u306e\u8ab2\u984c\u3001\u65b0\u305f\u306a\u5fdc\u7528\u9818\u57df\u3068\u5c06\u6765\u306e\u65b9\u5411\u6027\u306b\u3064\u3044\u3066\u8ad6\u3058\u305f\u3002\u6f38\u8fd1\u7406\u8ad6\u306e\u62e1\u5f35\u3084\u76f8\u95a2\u69cb\u9020\u306e\u7406\u8ad6\u7684\u89e3\u660e\u3001\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3084\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u30c7\u30fc\u30bf\u3078\u306e\u5fdc\u7528\u62e1\u5f35\u306a\u3069\u3001\u4eca\u5f8c\u53d6\u308a\u7d44\u3080\u3079\u304d\u91cd\u8981\u306a\u7814\u7a76\u8ab2\u984c\u3092\u7279\u5b9a\u3057\u305f\u3002</p> <p>\u5909\u6570\u91cd\u8981\u5ea6\u306e\u7814\u7a76\u306f\u3001\u7406\u8ad6\u7684\u306a\u53b3\u5bc6\u6027\u306e\u8ffd\u6c42\u3068\u5b9f\u8df5\u7684\u306a\u6709\u7528\u6027\u306e\u4e21\u9762\u3067\u5927\u304d\u304f\u767a\u5c55\u3057\u3066\u3044\u308b\u3002\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u89e3\u91c8\u53ef\u80fd\u6027\u3068\u4fe1\u983c\u6027\u304c\u91cd\u8996\u3055\u308c\u308b\u73fe\u4ee3\u306b\u304a\u3044\u3066\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306f\u5358\u306a\u308b\u6280\u8853\u7684\u30c4\u30fc\u30eb\u306b\u3068\u3069\u307e\u3089\u305a\u3001\u30e2\u30c7\u30eb\u306e\u900f\u660e\u6027\u3001\u516c\u5e73\u6027\u3001\u8aac\u660e\u8cac\u4efb\u3092\u652f\u3048\u308b\u57fa\u76e4\u3068\u3057\u3066\u3001\u305d\u306e\u91cd\u8981\u6027\u306f\u307e\u3059\u307e\u3059\u9ad8\u307e\u3063\u3066\u3044\u308b\u3002\u672c\u30ec\u30d3\u30e5\u30fc\u304c\u3001\u3053\u306e\u5206\u91ce\u306e\u7814\u7a76\u8005\u3068\u5b9f\u52d9\u5bb6\u306e\u53cc\u65b9\u306b\u3068\u3063\u3066\u6709\u76ca\u306a\u77e5\u898b\u3092\u63d0\u4f9b\u3057\u3001\u5909\u6570\u91cd\u8981\u5ea6\u306b\u95a2\u3059\u308b\u4eca\u5f8c\u306e\u7814\u7a76\u3068\u5fdc\u7528\u306e\u767a\u5c55\u306b\u5bc4\u4e0e\u3059\u308b\u3053\u3068\u3092\u671f\u5f85\u3059\u308b\u3002</p>"},{"location":"research/note/variable-importance/#_3","title":"\u8b1d\u8f9e","text":"<p>\u672c\u7814\u7a76\u306f\u25cb\u25cb\u7814\u7a76\u8cbb\uff08\u8ab2\u984c\u756a\u53f7\uff1axxxxx\uff09\u306e\u52a9\u6210\u3092\u53d7\u3051\u305f\u3082\u306e\u3067\u3042\u308b\u3002\u307e\u305f\u3001\u8cb4\u91cd\u306a\u3054\u610f\u898b\u3092\u3044\u305f\u3060\u3044\u305f\u533f\u540d\u306e\u67fb\u8aad\u8005\u306b\u6df1\u304f\u611f\u8b1d\u7533\u3057\u4e0a\u3052\u308b\u3002</p>"},{"location":"research/note/variable-importance/#_4","title":"\u53c2\u8003\u6587\u732e","text":"<p>Adler, P., Falk, C., Friedler, S. A., Nix, T., Rybeck, G., Scheidegger, C., Smith, B., &amp; Venkatasubramanian, S. (2018). Auditing black-box models for indirect influence. Knowledge and Information Systems, 54(1), 95-122.</p> <p>Altmann, A., Tolo\u015fi, L., Sander, O., &amp; Lengauer, T. (2010). Permutation importance: A corrected feature importance measure. Bioinformatics, 26(10), 1340-1347.</p> <p>Booth, A., Gerding, E., &amp; McGroarty, F. (2014). Automated trading with performance weighted random forests and seasonality. Expert Systems with Applications, 41(8), 3651-3661.</p> <p>Bracke, P., Datta, A., Jung, C., &amp; Sen, S. (2019). Machine learning explainability in finance: An application to default risk analysis. Bank of England Staff Working Paper No. 816.</p> <p>Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.</p> <p>Cand\u00e8s, E., Fan, Y., Janson, L., &amp; Lv, J. (2018). Panning for gold: Model-X knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B, 80(3), 551-577.</p> <p>Castro, J., G\u00f3mez, D., &amp; Tejada, J. (2009). Polynomial calculation of the Shapley value based on sampling. Computers &amp; Operations Research, 36(5), 1726-1730.</p> <p>Chefer, H., Gur, S., &amp; Wolf, L. (2021). Transformer interpretability beyond attention visualization. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 782-791.</p> <p>Covert, I., Lundberg, S. M., &amp; Lee, S. I. (2020). Understanding global feature contributions with additive importance measures. Advances in Neural Information Processing Systems, 33, 17212-17223.</p> <p>Dai, B., &amp; Yamada, M. (2023). Kernel knockoffs: Multiple hypothesis testing with high-dimensional covariates. Journal of Machine Learning Research, 24(1), 1-44.</p> <p>D\u00edaz-Uriarte, R., &amp; de Andr\u00e9s, S. A. (2006). Gene selection and classification of microarray data using random forest. BMC Bioinformatics, 7(1), 3.</p> <p>Fisher, A., Rudin, C., &amp; Dominici, F. (2019). All models are wrong, but many are useful: Learning a variable's importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1-81.</p> <p>Goldstein, A., Kapelner, A., Bleich, J., &amp; Pitkin, E. (2015). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics, 24(1), 44-65.</p> <p>Greenwell, B. M., Boehmke, B. C., &amp; McCarthy, A. J. (2018). A simple and effective model-based variable importance measure. arXiv preprint arXiv:1805.04755.</p> <p>Gregorutti, B., Michel, B., &amp; Saint-Pierre, P. (2015). Grouped variable importance with random forests and application to multiple functional data analysis. Computational Statistics &amp; Data Analysis, 90, 15-35.</p> <p>Gregorutti, B., Michel, B., &amp; Saint-Pierre, P. (2017). Correlation and variable importance in random forests. Statistics and Computing, 27(3), 659-678.</p> <p>Gu, S., Kelly, B., &amp; Xiu, D. (2020). Empirical asset pricing via machine learning. The Review of Financial Studies, 33(5), 2223-2273.</p> <p>Heskes, T., Sijben, E., Bucur, I. G., &amp; Claassen, T. (2020). Causal Shapley values: Exploiting causal knowledge to explain individual predictions of complex models. Advances in Neural Information Processing Systems, 33, 4778-4789.</p> <p>Hothorn, T., Hornik, K., &amp; Zeileis, A. (2006). Unbiased recursive partitioning: A conditional inference framework. Journal of Computational and Graphical Statistics, 15(3), 651-674.</p> <p>Hooker, G., &amp; Mentch, L. (2019). Please stop permuting features: An explanation and alternatives. arXiv preprint arXiv:1905.03151.</p> <p>Hsich, E., Gorodeski, E. Z., Blackstone, E. H., Ishwaran, H., &amp; Lauer, M. S. (2019). Identifying important risk factors for survival in patient with systolic heart failure using random survival forests. Circulation: Cardiovascular Quality and Outcomes, 4(1), 39-45.</p> <p>Ishwaran, H. (2007). Variable importance in binary regression trees and forests. Electronic Journal of Statistics, 1, 519-537.</p> <p>Janitza, S., Strobl, C., &amp; Boulesteix, A. L. (2013). An AUC-based permutation variable importance measure for random forests. BMC Bioinformatics, 14(1), 119.</p> <p>Janizadeh, S., Avand, M., Jaafari, A., &amp; Van Phong, T. (2022). Machine learning approaches for spatial prediction of wildfire susceptibility: A meta-analysis of method robustness and variable importance. Journal of Environmental Management, 313, 115014.</p> <p>Krauss, C., Do, X. A., &amp; Huck, N. (2017). Deep neural networks, gradient-boosted trees, random forests: Statistical arbitrage on the S&amp;P 500. European Journal of Operational Research, 259(2), 689-702.</p> <p>Kursa, M. B., &amp; Rudnicki, W. R. (2010). Feature selection with the Boruta package. Journal of Statistical Software, 36(11), 1-13.</p> <p>Le, T., Clarke, R., &amp; Gerszten, R. E. (2020). Pathway and network-based analysis of genome-wide association studies in multiple cohorts. Nature Communications, 11(1), 4489.</p> <p>Lei, J., G'Sell, M., Rinaldo, A., Tibshirani, R. J., &amp; Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094-1111.</p> <p>Lessmann, S., Baesens, B., Seow, H. V., &amp; Thomas, L. C. (2015). Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research. European Journal of Operational Research, 247(1), 124-136.</p> <p>Li, X., Dunn, J., Salins, D., Zhou, G., Zhou, W., Rose, S. M. S. F., Perelman, D., Colbert, E., Runge, R., Rego, S., &amp; Others. (2019). Digital health: Tracking physiomes and activity using wearable biosensors reveals useful health-related information. PLoS Biology, 15(1), e2001402.</p> <p>Liu, S., Fukumizu, K., &amp; Suzuki, T. (2022). Nearly optimal variable selection for random forests under minimal assumptions. arXiv preprint arXiv:2202.03794.</p> <p>Loecher, M. (2020). Unbiased variable importance for random forests. Communications in Statistics-Theory and Methods, 1-13.</p> <p>Louppe, G., Wehenkel, L., Sutera, A., &amp; Geurts, P. (2013). Understanding variable importances in forests of randomized trees. In Advances in Neural Information Processing Systems (pp. 431-439).</p> <p>Lu, Y., Fan, Y., Lv, J., &amp; Noble, W. S. (2018). DeepPINK: Reproducible feature selection in deep neural networks. Advances in Neural Information Processing Systems, 31, 8676-8686.</p> <p>Lundberg, S. M., &amp; Lee, S. I. (2017). A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems, 30, 4765-4774.</p> <p>Molnar, C., Casalicchio, G., &amp; Bischl, B. (2020). Interpretable machine learning\u2013a brief history, state-of-the-art and challenges. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 417-431). Springer.</p> <p>Owen, A. B., &amp; Prieur, C. (2017). On Shapley value for measuring importance of dependent inputs. SIAM/ASA Journal on Uncertainty Quantification, 5(1), 986-1002.</p> <p>Ribers, M. A., &amp; Ullrich, H. (2020). Machine prediction of antibiotic resistance: Advancing disease diagnosis and treatment. Scientific Reports, 10(1), 18457.</p> <p>Scornet, E. (2020). Trees, forests, and impurity-based variable importance. arXiv preprint arXiv:2001.04295.</p> <p>Seoane, J. A., Day, I. N., Gaunt, T. R., &amp; Campbell, C. (2014). A pathway-based data integration framework for prediction of disease progression. Bioinformatics, 30(6), 838-845.</p> <p>Shapley, L. S. (1953). A value for n-person games. Contributions to the Theory of Games, 2(28), 307-317.</p> <p>Shrikumar, A., Greenside, P., &amp; Kundaje, A. (2017). Learning important features through propagating activation differences. International Conference on Machine Learning, 3145-3153.</p> <p>Song, L., Langfelder, P., &amp; Horvath, S. (2017). Random generalized linear model: A highly accurate and interpretable ensemble predictor. BMC Bioinformatics, 14(1), 5.</p> <p>Steele, A. J., Denaxas, S. C., Shah, A. D., Hemingway, H., &amp; Luscombe, N. M. (2018). Machine learning models in electronic health records can outperform conventional survival models for predicting patient mortality in coronary artery disease. PloS ONE, 13(8), e0202344.</p> <p>Strobl, C., Boulesteix, A. L., Kneib, T., Augustin, T., &amp; Zeileis, A. (2008). Conditional variable importance for random forests. BMC Bioinformatics, 9(1), 307.</p> <p>Strobl, C., Boulesteix, A. L., Zeileis, A., &amp; Hothorn, T. (2007). Bias in random forest variable importance measures: Illustrations, sources and a solution. BMC Bioinformatics, 8(1), 25.</p> <p>\u0160trumbelj, E., &amp; Kononenko, I. (2014). Explaining prediction models and individual predictions with feature contributions. Knowledge and Information Systems, 41(3), 647-665.</p> <p>Sundararajan, M., Taly, A., &amp; Yan, Q. (2017). Axiomatic attribution for deep networks. International Conference on Machine Learning, 3319-3328.</p> <p>Virgolin, M., De Lorenzo, A., Medvet, E., &amp; Randone, F. (2022). Learning and visualizing region-based counterfactual explanations for pneumonia detection in chest x-rays. arXiv preprint arXiv:2206.03999.</p> <p>Wachter, S., Mittelstadt, B., &amp; Russell, C. (2018). Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harvard Journal of Law &amp; Technology, 31(2), 841-887.</p> <p>Wiegreffe, S., &amp; Pinter, Y. (2019). Attention is not not explanation. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, 11-20.</p> <p>Williamson, B. D., Gilbert, P. B., Carone, M., &amp; Simon, N. (2020). Nonparametric variable importance assessment using machine learning techniques. Biometrics, 77(1), 9-22.</p> <p>Williamson, B. D., Gilbert, P. B., Simon, N. R., &amp; Carone, M. (2021). A unified approach for inference on algorithm-agnostic variable importance. Journal of the American Statistical Association, 1-14.</p> <p>Xia, Y., Liu, C., Li, Y., &amp; Liu, N. (2017). A boosted decision tree approach using Bayesian hyper-parameter optimization for credit scoring. Expert Systems with Applications, 78, 225-241.</p> <p>Zeiler, M. D., &amp; Fergus, R. (2014). Visualizing and understanding convolutional networks. European Conference on Computer Vision, 818-833.</p> <p>Zhang, L., &amp; Janson, L. (2020). Floodgate: Inference for model-free variable importance. Journal of Machine Learning Research, 21(191), 1-32.</p> <p>Zhu, S., Patel, D., Shen, C., Lu, D., &amp; Jiang, D. (2019). Variable importance integrating network incorporating both direct and indirect relationships. Journal of Biomedical Informatics, 93, 103163.</p>"},{"location":"research/note/factor-analysis/","title":"Factor analysis","text":""},{"location":"research/note/factor-analysis/#_1","title":"\u524d\u66f8\u304d","text":"<p>\u56e0\u5b50\u5206\u6790\u306f\u300120\u4e16\u7d00\u521d\u982d\u306b\u5fc3\u7406\u5b66\u306e\u5206\u91ce\u3067\u30c1\u30e3\u30fc\u30eb\u30ba\u30fb\u30b9\u30da\u30a2\u30de\u30f3\u306b\u3088\u3063\u3066\u63d0\u5531\u3055\u308c\u305f\u300c\u4e00\u822c\u56e0\u5b50\uff08g\u56e0\u5b50\uff09\u300d\u306e\u6982\u5ff5\u304b\u3089\u767a\u5c55\u3057\u305f\u3001\u30c7\u30fc\u30bf\u89e3\u6790\u306e\u753b\u671f\u7684\u306a\u624b\u6cd5\u3067\u3059\u3002\u5f53\u521d\u306f\u3001\u77e5\u80fd\u3084\u6027\u683c\u3068\u3044\u3063\u305f\u3001\u76f4\u63a5\u6e2c\u5b9a\u3059\u308b\u3053\u3068\u304c\u96e3\u3057\u3044\u5fc3\u7406\u7684\u69cb\u9020\u306e\u80cc\u5f8c\u306b\u3042\u308b\u5171\u901a\u306e\u8981\u56e0\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u305f\u3081\u306b\u7528\u3044\u3089\u308c\u307e\u3057\u305f\u3002\u4ee5\u964d\u3001\u56e0\u5b50\u5206\u6790\u306f\u793e\u4f1a\u79d1\u5b66\u3001\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u3001\u533b\u7642\u3001\u6559\u80b2\u306a\u3069\u3001\u3055\u307e\u3056\u307e\u306a\u5206\u91ce\u306b\u304a\u3044\u3066\u3001\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u5727\u7e2e\u3084\u6f5c\u5728\u69cb\u9020\u306e\u89e3\u660e\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306e\u4e0d\u53ef\u6b20\u306a\u30c4\u30fc\u30eb\u3078\u3068\u9032\u5316\u3057\u3066\u304d\u307e\u3057\u305f\u3002</p> <p>\u672c\u8cc7\u6599\u3067\u306f\u3001\u56e0\u5b50\u5206\u6790\u306e\u6b74\u53f2\u7684\u80cc\u666f\u3068\u7406\u8ad6\u7684\u57fa\u76e4\u3092\u8e0f\u307e\u3048\u305f\u4e0a\u3067\u3001\u5f93\u6765\u306e\u9023\u7d9a\u5909\u6570\u306b\u57fa\u3065\u304f\u56e0\u5b50\u5206\u6790\u3060\u3051\u3067\u306a\u304f\u3001\u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306b\u5bfe\u3059\u308b\u65b0\u305f\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\uff08\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u3084\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u3092\u7528\u3044\u305f\u624b\u6cd5\uff09\u306b\u3064\u3044\u3066\u3082\u8a73\u3057\u304f\u89e3\u8aac\u3057\u3066\u3044\u307e\u3059\u3002\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3084\u5b9f\u969b\u306e\u533b\u7642\u30c7\u30fc\u30bf\u3001\u3055\u3089\u306b\u306f\u822a\u7a7a\u4f1a\u793e\u306e\u9867\u5ba2\u6e80\u8db3\u5ea6\u30c7\u30fc\u30bf\u3068\u3044\u3063\u305f\u591a\u69d8\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f8b\u306b\u3001\u5b9f\u8df5\u7684\u306a\u89e3\u6790\u30d7\u30ed\u30bb\u30b9\u3001\u56de\u8ee2\u624b\u6cd5\u3001\u611f\u5ea6\u5206\u6790\u306e\u65b9\u6cd5\u306a\u3069\u3001\u6bb5\u968e\u7684\u304b\u3064\u5177\u4f53\u7684\u306b\u793a\u3059\u3053\u3068\u3067\u3001\u8aad\u8005\u304c\u5b9f\u52d9\u306b\u304a\u3044\u3066\u56e0\u5b50\u5206\u6790\u3092\u6709\u52b9\u306b\u6d3b\u7528\u3067\u304d\u308b\u3088\u3046\u652f\u63f4\u3059\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u56e0\u5b50\u5206\u6790\u306f\u3001\u5358\u306b\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u3092\u524a\u6e1b\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u80cc\u5f8c\u306b\u6f5c\u3080\u300c\u898b\u3048\u306a\u3044\u300d\u69cb\u9020\u3084\u5171\u901a\u6027\u3092\u6d6e\u304b\u3073\u4e0a\u304c\u3089\u305b\u308b\u3053\u3068\u3067\u3001\u610f\u601d\u6c7a\u5b9a\u3084\u7406\u8ad6\u69cb\u7bc9\u306b\u5927\u304d\u306a\u793a\u5506\u3092\u4e0e\u3048\u307e\u3059\u3002\u73fe\u4ee3\u306e\u30d3\u30c3\u30b0\u30c7\u30fc\u30bf\u6642\u4ee3\u306b\u304a\u3044\u3066\u3082\u3001\u305d\u306e\u4fa1\u5024\u306f\u5909\u308f\u3089\u305a\u3001\u8907\u96d1\u306a\u73fe\u8c61\u3092\u30b7\u30f3\u30d7\u30eb\u304b\u3064\u76f4\u611f\u7684\u306b\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u5f37\u529b\u306a\u624b\u6cd5\u3068\u3057\u3066\u3001\u5e45\u5e83\u3044\u5206\u91ce\u3067\u306e\u5fdc\u7528\u304c\u9032\u3093\u3067\u3044\u307e\u3059\u3002</p> <p>\u3053\u306e\u8cc7\u6599\u304c\u3001\u56e0\u5b50\u5206\u6790\u306e\u7406\u8ad6\u3068\u5b9f\u8df5\u3092\u6df1\u304f\u7406\u89e3\u3057\u3001\u73fe\u5b9f\u306e\u30c7\u30fc\u30bf\u89e3\u6790\u306b\u304a\u3044\u3066\u65b0\u305f\u306a\u8996\u70b9\u3068\u89e3\u6c7a\u7b56\u3092\u898b\u51fa\u3059\u4e00\u52a9\u3068\u306a\u308b\u3053\u3068\u3092\u9858\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/#_2","title":"\u76ee\u6b21","text":"<ul> <li>\u306f\u3058\u3081\u306b</li> <li>\u56e0\u5b50\u3068\u306f\uff1f</li> <li>\u56e0\u5b50\u8ca0\u8377\u884c\u5217</li> <li>\u6f5c\u5728\u56e0\u5b50\u63a8\u5b9a\u6cd5</li> <li>\u56de\u8ee2\u3068\u7d50\u679c\u306e\u89e3\u91c8</li> <li>\u611f\u5ea6\u5206\u6790</li> <li>\u56e0\u5b50\u5206\u6790\u306e\u30b9\u30c6\u30c3\u30d7</li> <li>\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u5206\u6790</li> <li>\u7cd6\u5c3f\u75c5\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u5206\u6790</li> <li>\u9806\u5e8f\u4ed8\u304d\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u5206\u6790</li> <li>\u98db\u884c\u6a5f\u4e57\u5ba2\u306b\u5bfe\u3059\u308b\u6e80\u8db3\u5ea6\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u5206\u6790</li> <li>\u30c6\u30af\u30cb\u30ab\u30eb\u30ce\u30fc\u30c8\uff1a\u884c\u5217\u5206\u89e3\u3068\u56e0\u5b50\u5206\u6790</li> </ul>"},{"location":"research/note/factor-analysis/01-introduction/","title":"1. \u306f\u3058\u3081\u306b","text":""},{"location":"research/note/factor-analysis/01-introduction/#1","title":"\u7b2c1\u7ae0 \u306f\u3058\u3081\u306b","text":"<p>\u56e0\u5b50\u5206\u6790\u306f\u3001\u89b3\u6e2c\u3055\u308c\u305f\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u306e\u80cc\u5f8c\u306b\u3042\u308b\u6f5c\u5728\u7684\u306a\u69cb\u9020\u3092\u660e\u3089\u304b\u306b\u3059\u308b\u305f\u3081\u306e\u7d71\u8a08\u624b\u6cd5\u3067\u3059\u3002 \u3053\u306e\u624b\u6cd5\u306f\u3001\u8907\u6570\u306e\u89b3\u6e2c\u5909\u6570\u306b\u5171\u901a\u3057\u3066\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u6f5c\u5728\u56e0\u5b50\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3001\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u5727\u7e2e\u3084\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u3001\u89e3\u91c8\u306e\u5bb9\u6613\u5316\u306b\u5bc4\u4e0e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/01-introduction/#_1","title":"\u80cc\u666f\u3068\u6b74\u53f2","text":"<ul> <li> <p>\u5fc3\u7406\u5b66\u304b\u3089\u306e\u767a\u5c55:   20\u4e16\u7d00\u521d\u982d\u3001\u5fc3\u7406\u5b66\u8005\u30c1\u30e3\u30fc\u30eb\u30ba\u30fb\u30b9\u30da\u30a2\u30de\u30f3\u306f\u3001\u77e5\u80fd\u3092\u8aac\u660e\u3059\u308b\u305f\u3081\u306b\u300c\u4e00\u822c\u56e0\u5b50\uff08g\u56e0\u5b50\uff09\u300d\u3068\u3044\u3046\u6982\u5ff5\u3092\u63d0\u5531\u3057\u307e\u3057\u305f\u3002\u5f7c\u306f\u3001\u69d8\u3005\u306a\u77e5\u80fd\u691c\u67fb\u306e\u7d50\u679c\u306b\u5171\u901a\u3059\u308b\u57fa\u76e4\u304c\u5b58\u5728\u3059\u308b\u3068\u4eee\u5b9a\u3057\u3001\u56e0\u5b50\u5206\u6790\u3092\u7528\u3044\u3066\u305d\u306e\u80cc\u5f8c\u306b\u3042\u308b\u69cb\u9020\u3092\u63a2\u308a\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u793e\u4f1a\u79d1\u5b66\u30fb\u7d4c\u6e08\u5b66\u3067\u306e\u5fdc\u7528:   \u56e0\u5b50\u5206\u6790\u306f\u3001\u30a2\u30f3\u30b1\u30fc\u30c8\u8abf\u67fb\u3084\u7d4c\u6e08\u6307\u6a19\u306e\u5206\u6790\u306b\u3082\u5fdc\u7528\u3055\u308c\u3001\u6d88\u8cbb\u8005\u884c\u52d5\u306e\u7406\u89e3\u3084\u5e02\u5834\u30bb\u30b0\u30e1\u30f3\u30c8\u306e\u7279\u5b9a\u3001\u3055\u3089\u306b\u306f\u653f\u7b56\u8a55\u4fa1\u306a\u3069\u5e45\u5e83\u3044\u5206\u91ce\u3067\u5229\u7528\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> </li> <li> <p>\u73fe\u4ee3\u306e\u5fdc\u7528\u4f8b: </p> </li> <li>\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0: \u9867\u5ba2\u306e\u8cfc\u8cb7\u884c\u52d5\u3084\u597d\u307f\u3092\u89e3\u6790\u3057\u3001\u6f5c\u5728\u7684\u306a\u30bb\u30b0\u30e1\u30f3\u30c8\uff08\u4f8b\uff1a\u4fa1\u683c\u5fd7\u5411\u578b\u3001\u54c1\u8cea\u5fd7\u5411\u578b\u306a\u3069\uff09\u3092\u62bd\u51fa\u3059\u308b\u3002  </li> <li>\u533b\u7642\u30fb\u5065\u5eb7: \u60a3\u8005\u306e\u75c7\u72b6\u3084\u8a3a\u65ad\u30c7\u30fc\u30bf\u304b\u3089\u3001\u6f5c\u5728\u7684\u306a\u75be\u60a3\u30ea\u30b9\u30af\u8981\u56e0\u3092\u898b\u51fa\u3059\u3002  </li> <li>\u6559\u80b2: \u5b66\u751f\u306e\u30c6\u30b9\u30c8\u7d50\u679c\u304b\u3089\u3001\u5b66\u7fd2\u80fd\u529b\u3084\u7279\u5b9a\u306e\u77e5\u8b58\u9818\u57df\u306b\u5bfe\u3059\u308b\u7406\u89e3\u5ea6\u306e\u80cc\u5f8c\u306b\u3042\u308b\u8981\u56e0\u3092\u5206\u6790\u3059\u308b\u3002</li> </ul> <p>\u3053\u308c\u3089\u306e\u4e8b\u4f8b\u306f\u3001\u56e0\u5b50\u5206\u6790\u304c\u5358\u306a\u308b\u7d71\u8a08\u7684\u624b\u6cd5\u306b\u7559\u307e\u3089\u305a\u3001\u73fe\u5b9f\u4e16\u754c\u306e\u8907\u96d1\u306a\u73fe\u8c61\u3092\u30b7\u30f3\u30d7\u30eb\u306a\u30e2\u30c7\u30eb\u306b\u8981\u7d04\u3059\u308b\u305f\u3081\u306e\u6709\u529b\u306a\u30c4\u30fc\u30eb\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/02-factor/","title":"2. \u56e0\u5b50\u3068\u306f","text":""},{"location":"research/note/factor-analysis/02-factor/#2","title":"\u7b2c2\u7ae0 \u56e0\u5b50\u3068\u306f\u4f55\u304b","text":"<p>\u56e0\u5b50\u3068\u306f\u3001\u89b3\u6e2c\u3055\u308c\u305f\u8907\u6570\u306e\u5909\u6570\u306b\u5171\u901a\u3057\u3066\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u3001\u76f4\u63a5\u89b3\u6e2c\u3067\u304d\u306a\u3044\u6f5c\u5728\u7684\u306a\u5909\u6570\u306e\u3053\u3068\u3092\u6307\u3057\u307e\u3059\u3002 \u56e0\u5b50\u5206\u6790\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u6f5c\u5728\u56e0\u5b50\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u3001\u5909\u6570\u9593\u306e\u76f8\u95a2\u30d1\u30bf\u30fc\u30f3\u3092\u89e3\u660e\u3057\u3001\u80cc\u5f8c\u306b\u3042\u308b\u5171\u901a\u306e\u69cb\u9020\u3092\u660e\u3089\u304b\u306b\u3057\u3088\u3046\u3068\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/02-factor/#_1","title":"\u6f5c\u5728\u56e0\u5b50\u306e\u5f79\u5272","text":"<ul> <li> <p>\u6b21\u5143\u5727\u7e2e:   \u591a\u304f\u306e\u89b3\u6e2c\u5909\u6570\u304c\u3001\u5b9f\u969b\u306b\u306f\u5c11\u6570\u306e\u56e0\u5b50\u306b\u3088\u3063\u3066\u652f\u914d\u3055\u308c\u3066\u3044\u308b\u3068\u4eee\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u3092\u3088\u308a\u30b7\u30f3\u30d7\u30eb\u306a\u69cb\u9020\u306b\u8981\u7d04\u3067\u304d\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u5fc3\u7406\u5b66\u7684\u691c\u67fb\u3067\u306f\u3001\u8907\u6570\u306e\u30c6\u30b9\u30c8\u9805\u76ee\u304c\u300c\u8a18\u61b6\u529b\u300d\u3084\u300c\u6ce8\u610f\u529b\u300d\u3068\u3044\u3063\u305f\u5c11\u6570\u306e\u57fa\u790e\u7684\u306a\u56e0\u5b50\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u5171\u901a\u6027\u306e\u8aac\u660e:   \u3042\u308b\u56e0\u5b50\u304c\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064\u5909\u6570\u306f\u3001\u305d\u306e\u56e0\u5b50\u304c\u80cc\u5f8c\u306b\u3042\u308b\u5171\u901a\u306e\u8981\u56e0\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u5506\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u8868\u9762\u7684\u306b\u306f\u7570\u306a\u308b\u3088\u3046\u306b\u898b\u3048\u308b\u89b3\u6e2c\u5909\u6570\u306e\u9593\u306b\u3001\u5b9f\u306f\u5171\u901a\u306e\u8981\u56e0\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u7406\u89e3\u3067\u304d\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/02-factor/#_2","title":"\u5177\u4f53\u7684\u306a\u4e8b\u4f8b","text":"<ol> <li> <p>\u77e5\u80fd\u691c\u67fb\u306b\u304a\u3051\u308b\u4e00\u822c\u56e0\u5b50\uff08g\u56e0\u5b50\uff09:    \u30b9\u30da\u30a2\u30de\u30f3\u306f\u3001\u8907\u6570\u306e\u77e5\u80fd\u691c\u67fb\u306e\u7d50\u679c\u306b\u5171\u901a\u3059\u308b\u300c\u4e00\u822c\u77e5\u80fd\u300d\u3092\u793a\u3059\u56e0\u5b50\u304c\u5b58\u5728\u3059\u308b\u3068\u4eee\u5b9a\u3057\u307e\u3057\u305f\u3002\u5404\u691c\u67fb\u9805\u76ee\uff08\u4f8b\u3048\u3070\u3001\u8a00\u8a9e\u7406\u89e3\u3001\u6570\u7406\u80fd\u529b\u3001\u7a7a\u9593\u8a8d\u8b58\u306a\u3069\uff09\u306e\u6210\u7e3e\u306f\u3001\u3053\u306e\u5171\u901a\u56e0\u5b50\u306e\u5bc4\u4e0e\u3068\u3001\u500b\u5225\u306e\u80fd\u529b\u3084\u6e2c\u5b9a\u8aa4\u5dee\u306b\u3088\u3063\u3066\u8aac\u660e\u3055\u308c\u308b\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u30ea\u30b5\u30fc\u30c1\u306b\u304a\u3051\u308b\u6f5c\u5728\u30bb\u30b0\u30e1\u30f3\u30c8:    \u30a2\u30f3\u30b1\u30fc\u30c8\u8abf\u67fb\u306b\u3088\u308a\u53ce\u96c6\u3055\u308c\u305f\u6d88\u8cbb\u8005\u306e\u55dc\u597d\u3084\u8cfc\u8cb7\u884c\u52d5\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3059\u308b\u3068\u3001\u80cc\u5f8c\u306b\u300c\u54c1\u8cea\u91cd\u8996\u300d\u3084\u300c\u4fa1\u683c\u91cd\u8996\u300d\u306a\u3069\u306e\u6f5c\u5728\u56e0\u5b50\u304c\u62bd\u51fa\u3055\u308c\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u6d88\u8cbb\u8005\u306e\u8cfc\u8cb7\u6c7a\u5b9a\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u4e3b\u8981\u306a\u8981\u56e0\u3092\u628a\u63e1\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u5065\u5eb7\u8a3a\u65ad\u30c7\u30fc\u30bf\u306e\u89e3\u6790:    \u8907\u6570\u306e\u751f\u4f53\u6307\u6a19\uff08\u8840\u5727\u3001\u8840\u7cd6\u5024\u3001\u30b3\u30ec\u30b9\u30c6\u30ed\u30fc\u30eb\u5024\u306a\u3069\uff09\u306b\u5bfe\u3057\u3066\u56e0\u5b50\u5206\u6790\u3092\u884c\u3046\u3068\u3001\u3053\u308c\u3089\u306e\u6307\u6a19\u306b\u5171\u901a\u3057\u3066\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u300c\u751f\u6d3b\u7fd2\u6163\u300d\u3084\u300c\u907a\u4f1d\u7684\u7d20\u56e0\u300d\u3068\u3044\u3063\u305f\u56e0\u5b50\u304c\u62bd\u51fa\u3055\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5065\u5eb7\u30ea\u30b9\u30af\u306e\u80cc\u5f8c\u306b\u3042\u308b\u6f5c\u5728\u7684\u306a\u56e0\u5b50\u3092\u7279\u5b9a\u3057\u3001\u4e88\u9632\u7b56\u306e\u7b56\u5b9a\u306b\u5f79\u7acb\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> </li> </ol>"},{"location":"research/note/factor-analysis/02-factor/#_3","title":"\u56e0\u5b50\u306e\u6570\u5b66\u7684\u5b9a\u7fa9","text":"<p>\u6570\u5b66\u7684\u306b\u306f\u3001\u56e0\u5b50\u306f\u6b21\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\u3067\u8868\u3055\u308c\u307e\u3059\u3002</p> \\[ x_i = \\lambda_{i1} f_1 + \\lambda_{i2} f_2 + \\cdots + \\lambda_{im} f_m + \\epsilon_i,\\quad (i = 1, 2, \\dots, p) \\] <ul> <li>\\(x_i\\)\uff1a\u89b3\u6e2c\u5909\u6570  </li> <li>\\(f_j\\)\uff1a\u6f5c\u5728\u56e0\u5b50  </li> <li>\\(\\lambda_{ij}\\)\uff1a\u56e0\u5b50 \\(f_j\\) \u304c\u5909\u6570 \\(x_i\\) \u306b\u4e0e\u3048\u308b\u5f71\u97ff\uff08\u56e0\u5b50\u8ca0\u8377\u91cf\uff09  </li> <li>\\(\\epsilon_i\\)\uff1a\u305d\u306e\u5909\u6570\u56fa\u6709\u306e\u8aa4\u5dee\u307e\u305f\u306f\u7279\u6709\u56e0\u5b50</li> </ul> <p>\u3053\u306e\u30e2\u30c7\u30eb\u306f\u3001\u89b3\u6e2c\u5909\u6570\u306e\u5206\u6563\u304c\u5171\u901a\u56e0\u5b50\u306b\u3088\u308b\u90e8\u5206\u3068\u3001\u7279\u6709\u56e0\u5b50\u306b\u3088\u308b\u90e8\u5206\u306b\u5206\u89e3\u3067\u304d\u308b\u3068\u3044\u3046\u524d\u63d0\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u901a\u3057\u3066\u3001\u5404\u56e0\u5b50\u304c\u3069\u306e\u7a0b\u5ea6\u5404\u5909\u6570\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u304c\u6570\u5024\u7684\u306b\u793a\u3055\u308c\u308b\u305f\u3081\u3001\u56e0\u5b50\u5206\u6790\u306f\u30c7\u30fc\u30bf\u306e\u69cb\u9020\u7406\u89e3\u306b\u304a\u3044\u3066\u975e\u5e38\u306b\u5f37\u529b\u306a\u624b\u6cd5\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/03-factor-loading-matrix/","title":"3. \u56e0\u5b50\u8ca0\u8377\u884c\u5217","text":""},{"location":"research/note/factor-analysis/03-factor-loading-matrix/#3","title":"\u7b2c3\u7ae0 \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306b\u3064\u3044\u3066","text":"<p>\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306f\u3001\u56e0\u5b50\u5206\u6790\u306e\u7d50\u679c\u3092\u89e3\u91c8\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u30c4\u30fc\u30eb\u3067\u3059\u3002\u3053\u306e\u7ae0\u3067\u306f\u3001\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u5b9a\u7fa9\u3084\u5f79\u5272\u3001\u305d\u3057\u3066\u5177\u4f53\u4f8b\u3092\u901a\u3057\u3066\u305d\u306e\u610f\u5473\u3092\u6df1\u304f\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u8aac\u660e\u3092\u884c\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/03-factor-loading-matrix/#31","title":"3.1 \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u5b9a\u7fa9","text":"<p>\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Factor Loading Matrix\uff09\u3068\u306f\u3001\u5404\u89b3\u6e2c\u5909\u6570\u304c\u62bd\u51fa\u3055\u308c\u305f\u6f5c\u5728\u56e0\u5b50\u306b\u5bfe\u3057\u3066\u3069\u306e\u7a0b\u5ea6\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u793a\u3059\u6570\u5024\u306e\u884c\u5217\u3067\u3059\u3002\u6570\u7406\u7684\u306b\u306f\u3001\u89b3\u6e2c\u5909\u6570 $ x_i $ \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\u3002</p> \\[ x_i = \\lambda_{i1} f_1 + \\lambda_{i2} f_2 + \\cdots + \\lambda_{im} f_m + \\epsilon_i,\\quad (i = 1, 2, \\dots, p) \\] <p>\u3053\u3053\u3067\u3001</p> <ul> <li>$ f_j $ \u306f\u6f5c\u5728\u56e0\u5b50\u3001</li> <li>$ \\lambda_{ij} $ \u306f\u56e0\u5b50 $ f_j $ \u304c\u5909\u6570 $ x_i $ \u306b\u4e0e\u3048\u308b\u5f71\u97ff\uff08\u56e0\u5b50\u8ca0\u8377\u91cf\uff09\u3001</li> <li>$ \\epsilon_i $ \u306f\u7279\u6709\u56e0\u5b50\u307e\u305f\u306f\u8aa4\u5dee\u9805\u3067\u3059\u3002</li> </ul> <p>\u56e0\u5b50\u8ca0\u8377\u884c\u5217 \\(\\Lambda\\) \u306f\u3001\u884c\u304c\u89b3\u6e2c\u5909\u6570\u3001\u5217\u304c\u6f5c\u5728\u56e0\u5b50\u3068\u306a\u308b $ p \\times m $ \u306e\u884c\u5217\u3068\u3057\u3066\u8868\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/03-factor-loading-matrix/#32","title":"3.2 \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u5f79\u5272","text":"<ul> <li> <p>\u5909\u6570\u306e\u5bc4\u4e0e\u5ea6\u306e\u660e\u793a:   \u5404\u56e0\u5b50\u304c\u3069\u306e\u89b3\u6e2c\u5909\u6570\u306b\u5f37\u304f\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u6570\u5024\u3067\u793a\u3057\u307e\u3059\u3002\u9ad8\u3044\u8ca0\u8377\u91cf\u306f\u3001\u305d\u306e\u5909\u6570\u304c\u7279\u5b9a\u306e\u56e0\u5b50\u3068\u5f37\u3044\u95a2\u9023\u304c\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u5358\u7d14\u69cb\u9020\u306e\u5b9f\u73fe:   \u56e0\u5b50\u5206\u6790\u306e\u76ee\u7684\u306f\u3001\u5404\u5909\u6570\u304c\u3067\u304d\u308b\u3060\u30511\u3064\u306e\u56e0\u5b50\u306b\u5f37\u304f\u5bc4\u4e0e\u3057\u3001\u4ed6\u306e\u56e0\u5b50\u306b\u306f\u307b\u3068\u3093\u3069\u5bc4\u4e0e\u3057\u306a\u3044\u300c\u5358\u7d14\u69cb\u9020\u300d\u3092\u898b\u51fa\u3059\u3053\u3068\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5404\u56e0\u5b50\u306e\u610f\u5473\u3065\u3051\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u89e3\u91c8\u3068\u6b21\u5143\u5727\u7e2e:   \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u591a\u304f\u306e\u89b3\u6e2c\u5909\u6570\u3092\u5c11\u6570\u306e\u6f5c\u5728\u56e0\u5b50\u306b\u8981\u7d04\u3057\u3001\u30c7\u30fc\u30bf\u306e\u89e3\u91c8\u3092\u7c21\u6f54\u306b\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/03-factor-loading-matrix/#33","title":"3.3 \u5177\u4f53\u4f8b\u306b\u3088\u308b\u8aac\u660e","text":""},{"location":"research/note/factor-analysis/03-factor-loading-matrix/#331","title":"3.3.1 \u5fc3\u7406\u5b66\u7684\u691c\u67fb\u306e\u4f8b","text":"<p>\u305f\u3068\u3048\u3070\u3001\u3042\u308b\u5fc3\u7406\u5b66\u7684\u691c\u67fb\u3067\u306f\u300112\u500b\u306e\u9805\u76ee\u304b\u3089\u300c\u8a18\u61b6\u529b\u300d\u300c\u6ce8\u610f\u529b\u300d\u300c\u51e6\u7406\u901f\u5ea6\u300d\u3068\u3044\u30463\u3064\u306e\u6f5c\u5728\u56e0\u5b50\u3092\u62bd\u51fa\u3057\u305f\u3068\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u306f\u3001\u67b6\u7a7a\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u4f8b\u3067\u3059\u3002</p> \u89b3\u6e2c\u5909\u6570 \u8a18\u61b6\u529b (Factor 1) \u6ce8\u610f\u529b (Factor 2) \u51e6\u7406\u901f\u5ea6 (Factor 3) \u9805\u76ee1 (\u8a18\u61b6\u529b\u95a2\u9023) 0.85 0.20 0.05 \u9805\u76ee2 (\u8a18\u61b6\u529b\u95a2\u9023) 0.80 0.15 0.10 \u9805\u76ee3 (\u8a18\u61b6\u529b\u95a2\u9023) 0.78 0.10 0.12 \u9805\u76ee4 (\u6ce8\u610f\u529b\u95a2\u9023) 0.10 0.82 0.18 \u9805\u76ee5 (\u6ce8\u610f\u529b\u95a2\u9023) 0.12 0.80 0.15 \u9805\u76ee6 (\u6ce8\u610f\u529b\u95a2\u9023) 0.05 0.85 0.20 \u9805\u76ee7 (\u51e6\u7406\u901f\u5ea6\u95a2\u9023) 0.08 0.18 0.88 \u9805\u76ee8 (\u51e6\u7406\u901f\u5ea6\u95a2\u9023) 0.10 0.12 0.90 \u9805\u76ee9 (\u51e6\u7406\u901f\u5ea6\u95a2\u9023) 0.15 0.10 0.87 \u9805\u76ee10 (\u6df7\u5408\u8981\u7d20) 0.40 0.40 0.30 \u9805\u76ee11 (\u6df7\u5408\u8981\u7d20) 0.35 0.45 0.25 \u9805\u76ee12 (\u6df7\u5408\u8981\u7d20) 0.30 0.40 0.35 <p>\u89e3\u8aac:</p> <ul> <li> <p>\u660e\u77ad\u306a\u5bc4\u4e0e:   \u9805\u76ee1\uff5e3\u306f\u300c\u8a18\u61b6\u529b\u300d\u306b\u5bfe\u3057\u3066\u9ad8\u3044\u8ca0\u8377\u91cf\uff080.78\u301c0.85\uff09\u3092\u6301\u3063\u3066\u304a\u308a\u3001\u4ed6\u306e\u56e0\u5b50\u306e\u8ca0\u8377\u91cf\u306f\u4f4e\u3044\u305f\u3081\u3001\u3053\u308c\u3089\u306e\u9805\u76ee\u306f\u8a18\u61b6\u529b\u3068\u3044\u3046\u56e0\u5b50\u3092\u5f37\u304f\u53cd\u6620\u3057\u3066\u3044\u307e\u3059\u3002</p> </li> <li> <p>\u660e\u78ba\u306a\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0:   \u540c\u69d8\u306b\u3001\u9805\u76ee4\uff5e6\u306f\u300c\u6ce8\u610f\u529b\u300d\u306b\u5bfe\u3057\u3066\u3001\u9805\u76ee7\uff5e9\u306f\u300c\u51e6\u7406\u901f\u5ea6\u300d\u306b\u5bfe\u3057\u3066\u305d\u308c\u305e\u308c\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p> </li> <li> <p>\u6df7\u5408\u8981\u7d20:   \u9805\u76ee10\uff5e12\u306f\u8907\u6570\u306e\u56e0\u5b50\u306b\u4e2d\u7a0b\u5ea6\u306e\u8ca0\u8377\u91cf\u3092\u793a\u3057\u3066\u304a\u308a\u3001\u3053\u308c\u3089\u306e\u9805\u76ee\u306f\u3069\u306e\u56e0\u5b50\u306b\u5206\u985e\u3059\u308b\u304b\u304c\u660e\u78ba\u3067\u306a\u304f\u3001\u8ffd\u52a0\u306e\u691c\u8a0e\u304c\u5fc5\u8981\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/03-factor-loading-matrix/#332","title":"3.3.2 \u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u30ea\u30b5\u30fc\u30c1\u306e\u4f8b","text":"<p>\u6d88\u8cbb\u8005\u306e\u55dc\u597d\u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u8abf\u67fb\u3067\u300110\u500b\u306e\u9805\u76ee\u304b\u3089\u300c\u54c1\u8cea\u91cd\u8996\u300d\u300c\u4fa1\u683c\u91cd\u8996\u300d\u3068\u3044\u30462\u3064\u306e\u6f5c\u5728\u56e0\u5b50\u3092\u62bd\u51fa\u3057\u305f\u3068\u3057\u307e\u3059\u3002\u67b6\u7a7a\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p> \u89b3\u6e2c\u5909\u6570 \u54c1\u8cea\u91cd\u8996 (Factor 1) \u4fa1\u683c\u91cd\u8996 (Factor 2) \u88fd\u54c1\u306e\u8010\u4e45\u6027 0.88 0.10 \u88fd\u54c1\u306e\u30c7\u30b6\u30a4\u30f3 0.80 0.20 \u88fd\u54c1\u306e\u6027\u80fd 0.85 0.15 \u88fd\u54c1\u306e\u4fe1\u983c\u6027 0.90 0.05 \u4fa1\u683c\u306e\u59a5\u5f53\u6027 0.25 0.85 \u5272\u5f15\u3084\u30d7\u30ed\u30e2\u30fc\u30b7\u30e7\u30f3\u306e\u6709\u7121 0.20 0.80 \u8cfc\u5165\u5f8c\u306e\u30b5\u30fc\u30d3\u30b9 0.70 0.30 \u30d6\u30e9\u30f3\u30c9\u30a4\u30e1\u30fc\u30b8 0.75 0.35 \u88fd\u54c1\u306e\u6700\u65b0\u6027 0.60 0.40 \u7dcf\u5408\u7684\u306a\u6e80\u8db3\u5ea6 0.80 0.50 <p>\u89e3\u8aac:</p> <ul> <li> <p>\u54c1\u8cea\u91cd\u8996\u56e0\u5b50:   \u88fd\u54c1\u306e\u8010\u4e45\u6027\u3001\u6027\u80fd\u3001\u4fe1\u983c\u6027\u306a\u3069\u306e\u9805\u76ee\u306f\u3001\u54c1\u8cea\u91cd\u8996\u56e0\u5b50\u306b\u5bfe\u3057\u3066\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u54c1\u8cea\u306b\u5bfe\u3059\u308b\u6d88\u8cbb\u8005\u306e\u95a2\u5fc3\u304c\u660e\u78ba\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u4fa1\u683c\u91cd\u8996\u56e0\u5b50:   \u4fa1\u683c\u306e\u59a5\u5f53\u6027\u3084\u5272\u5f15\u306e\u6709\u7121\u306a\u3069\u306f\u3001\u4fa1\u683c\u91cd\u8996\u56e0\u5b50\u306b\u5bfe\u3057\u3066\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u793a\u3057\u3066\u304a\u308a\u3001\u6d88\u8cbb\u8005\u304c\u4fa1\u683c\u306b\u654f\u611f\u3067\u3042\u308b\u3053\u3068\u304c\u793a\u5506\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u6df7\u5408\u7684\u306a\u9805\u76ee:   \u8cfc\u5165\u5f8c\u306e\u30b5\u30fc\u30d3\u30b9\u3084\u7dcf\u5408\u7684\u306a\u6e80\u8db3\u5ea6\u306f\u3001\u4e21\u56e0\u5b50\u306b\u4e2d\u7a0b\u5ea6\u306e\u8ca0\u8377\u91cf\u3092\u793a\u3057\u3066\u304a\u308a\u3001\u54c1\u8cea\u3068\u4fa1\u683c\u306e\u4e21\u65b9\u304c\u5f71\u97ff\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u793a\u3055\u308c\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/03-factor-loading-matrix/#34","title":"3.4 \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u89e3\u91c8\u3068\u5fdc\u7528","text":"<ul> <li> <p>\u89e3\u91c8\u306e\u30dd\u30a4\u30f3\u30c8:   \u5206\u6790\u8005\u306f\u3001\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u5143\u306b\u5404\u56e0\u5b50\u306b\u540d\u524d\u3084\u610f\u5473\u3092\u4ed8\u4e0e\u3057\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u3042\u308b\u56e0\u5b50\u306b\u304a\u3044\u3066\u307b\u3068\u3093\u3069\u306e\u5909\u6570\u304c\u300c\u8a18\u61b6\u300d\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u305d\u306e\u56e0\u5b50\u306f\u300c\u8a18\u61b6\u529b\u56e0\u5b50\u300d\u3068\u89e3\u91c8\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u6b21\u5143\u5727\u7e2e:   \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u7528\u3044\u3066\u3001\u8907\u96d1\u306a\u591a\u5909\u91cf\u30c7\u30fc\u30bf\u3092\u5c11\u6570\u306e\u8981\u56e0\u3067\u8aac\u660e\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u3001\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u3092\u5727\u7e2e\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u5f8c\u7d9a\u306e\u89e3\u6790\uff08\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3001\u56de\u5e30\u5206\u6790\u306a\u3069\uff09\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u6539\u5584\u3068\u691c\u8a3c:   \u521d\u671f\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u3082\u3068\u306b\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u56de\u8ee2\uff08\u4f8b\uff1aVarimax\u56de\u8ee2\u3084Promax\u56de\u8ee2\uff09\u3092\u884c\u3044\u3001\u3088\u308a\u89e3\u91c8\u3057\u3084\u3059\u3044\u5358\u7d14\u69cb\u9020\u3092\u5f97\u308b\u3053\u3068\u304c\u4e00\u822c\u7684\u3067\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/","title":"4. \u6f5c\u5728\u56e0\u5b50\u63a8\u5b9a\u6cd5","text":""},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#4","title":"\u7b2c4\u7ae0 \u6f5c\u5728\u56e0\u5b50\u306e\u63a8\u5b9a\u65b9\u6cd5","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u56e0\u5b50\u5206\u6790\u306b\u304a\u3051\u308b\u6f5c\u5728\u56e0\u5b50\u306e\u63a8\u5b9a\u65b9\u6cd5\u3092\u6570\u7406\u7684\u306a\u5c55\u958b\u3068\u3068\u3082\u306b\u8a73\u7d30\u306b\u8aac\u660e\u3057\u307e\u3059\u3002\u7279\u306b\u3001\u9014\u4e2d\u5f0f\u306e\u5c55\u958b\u3084\u5404\u30b9\u30c6\u30c3\u30d7\u306e\u610f\u5473\u306b\u3064\u3044\u3066\u4e01\u5be7\u306b\u89e3\u8aac\u3057\u3001\u8aad\u8005\u304c\u63a8\u5b9a\u904e\u7a0b\u3092\u7406\u89e3\u3057\u3084\u3059\u3044\u3088\u3046\u306b\u5de5\u592b\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#41","title":"4.1 \u56e0\u5b50\u5206\u6790\u306e\u57fa\u672c\u30e2\u30c7\u30eb","text":"<p>\u56e0\u5b50\u5206\u6790\u3067\u306f\u3001\u89b3\u6e2c\u5909\u6570\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u3092\u6b21\u306e\u7dda\u5f62\u30e2\u30c7\u30eb\u3067\u8868\u3057\u307e\u3059\u3002</p> \\[ \\mathbf{x} = \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, \\] <ul> <li>\\(\\mathbf{x}\\): \\(p \\times 1\\) \u306e\u89b3\u6e2c\u5909\u6570\u30d9\u30af\u30c8\u30eb  </li> <li>\\(\\Lambda\\): \\(p \\times m\\) \u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217  </li> <li>\\(\\mathbf{f}\\): \\(m \\times 1\\) \u306e\u6f5c\u5728\u56e0\u5b50\u30d9\u30af\u30c8\u30eb  </li> <li>\\(\\boldsymbol{\\epsilon}\\): \\(p \\times 1\\) \u306e\u7279\u6709\u56e0\u5b50\uff08\u8aa4\u5dee\uff09\u30d9\u30af\u30c8\u30eb</li> </ul>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#411","title":"4.1.1 \u4eee\u5b9a","text":"<ol> <li>\u6f5c\u5728\u56e0\u5b50\u306e\u5206\u5e03    \u6f5c\u5728\u56e0\u5b50\u306f\u5e73\u57470\u3001\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u304c\u5358\u4f4d\u884c\u5217 $ \\mathbf{I}_m $ \u3068\u4eee\u5b9a\u3057\u307e\u3059\u3002</li> </ol> <p>$$    \\mathbf{f} \\sim N(\\mathbf{0}, \\mathbf{I}_m).    $$</p> <ol> <li>\u7279\u6709\u56e0\u5b50\u306e\u5206\u5e03    \u7279\u6709\u56e0\u5b50\u306f\u5e73\u57470\u3001\u5bfe\u89d2\u884c\u5217 \\(\\Psi\\)\uff08\u5404\u5909\u6570\u306e\u56fa\u6709\u5206\u6563\uff09\u3092\u6301\u3064\u3068\u4eee\u5b9a\u3057\u307e\u3059\u3002</li> </ol> <p>$$    \\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\Psi) \\quad \\text{\u3067\u3001} \\quad \\Psi = \\operatorname{diag}(\\psi_1, \\psi_2, \\dots, \\psi_p).    $$</p> <ol> <li>\u72ec\u7acb\u6027    \u6f5c\u5728\u56e0\u5b50\u3068\u7279\u6709\u56e0\u5b50\u306f\u4e92\u3044\u306b\u72ec\u7acb\u3067\u3042\u308b\u3068\u4eee\u5b9a\u3057\u307e\u3059\u3002</li> </ol> <p>\u3053\u308c\u3089\u306e\u4eee\u5b9a\u306e\u4e0b\u3001\u89b3\u6e2c\u5909\u6570 \\(\\mathbf{x}\\) \u306e\u5206\u6563\u5171\u5206\u6563\u884c\u5217 \\(\\Sigma\\) \u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5c0e\u304b\u308c\u307e\u3059\u3002</p> \\[ \\Sigma = \\operatorname{Cov}(\\mathbf{x}) = \\Lambda \\Lambda^\\top + \\Psi. \\]"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#42","title":"4.2 \u6700\u5c24\u6cd5\u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u63a8\u5b9a","text":"<p>\u6700\u5c24\u6cd5\u306f\u3001\u89b3\u6e2c\u30c7\u30fc\u30bf\u304c\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u4eee\u5b9a\u3057\u3001\u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\Lambda\\) \u3068 \\(\\Psi\\) \u3092\u63a8\u5b9a\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002</p>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#421","title":"4.2.1 \u5c24\u5ea6\u95a2\u6570\u306e\u5b9a\u5f0f\u5316","text":"<p>$ n $ \u500b\u306e\u72ec\u7acb\u306a\u89b3\u6e2c\u30c7\u30fc\u30bf \\(\\{ \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n \\}\\) \u306b\u5bfe\u3057\u3001\u5404 \\(\\mathbf{x}_i\\) \u306f\u6b21\u306e\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u3057\u307e\u3059\u3002</p> \\[ \\mathbf{x}_i \\sim N(\\mathbf{0}, \\Sigma), \\quad \\text{\u305f\u3060\u3057} \\quad \\Sigma = \\Lambda\\Lambda^\\top + \\Psi. \\] <p>\u5404\u89b3\u6e2c\u306e\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306f\u3001</p> \\[ p(\\mathbf{x}_i \\mid \\Lambda, \\Psi) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} \\mathbf{x}_i^\\top \\Sigma^{-1} \\mathbf{x}_i\\right). \\] <p>\u5168\u30b5\u30f3\u30d7\u30eb\u306e\u5c24\u5ea6\u95a2\u6570 \\(L(\\Lambda, \\Psi)\\) \u306f\u3001\u72ec\u7acb\u6027\u304b\u3089</p> \\[ L(\\Lambda, \\Psi) = \\prod_{i=1}^{n} \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} \\mathbf{x}_i^\\top \\Sigma^{-1} \\mathbf{x}_i\\right). \\]"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#422","title":"4.2.2 \u5bfe\u6570\u5c24\u5ea6\u306e\u5c0e\u51fa","text":"<p>\u5bfe\u6570\u3092\u3068\u308b\u3068\u3001\u8a08\u7b97\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002 \u307e\u305a\u3001\u5bfe\u6570\u3092\u53d6\u308b\u3068\u3001</p> \\[ \\begin{aligned} \\ell(\\Lambda, \\Psi) &amp;= \\ln L(\\Lambda, \\Psi) \\\\ &amp;= \\sum_{i=1}^{n} \\ln \\left[ \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} \\mathbf{x}_i^\\top \\Sigma^{-1} \\mathbf{x}_i\\right) \\right] \\\\ &amp;= -\\frac{np}{2} \\ln (2\\pi) - \\frac{n}{2} \\ln |\\Sigma| - \\frac{1}{2} \\sum_{i=1}^{n} \\mathbf{x}_i^\\top \\Sigma^{-1} \\mathbf{x}_i. \\end{aligned} \\] <p>\u3055\u3089\u306b\u3001\u30b5\u30f3\u30d7\u30eb\u5171\u5206\u6563\u884c\u5217 $ S $ \u3092</p> \\[ S = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{x}_i \\mathbf{x}_i^\\top \\] <p>\u3068\u5b9a\u7fa9\u3059\u308b\u3068\u3001\u6b21\u306e\u6027\u8cea\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002</p> \\[ \\sum_{i=1}^{n} \\mathbf{x}_i^\\top \\Sigma^{-1} \\mathbf{x}_i = n\\, \\operatorname{tr}(S \\Sigma^{-1}). \\] <p>\u3088\u3063\u3066\u3001\u5bfe\u6570\u5c24\u5ea6\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6574\u7406\u3055\u308c\u307e\u3059\u3002</p> \\[ \\ell(\\Lambda, \\Psi) = -\\frac{n}{2} \\left[ \\ln |\\Sigma| + \\operatorname{tr}(S \\Sigma^{-1}) \\right] + \\text{\u5b9a\u6570}. \\]"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#423","title":"4.2.3 \u30d1\u30e9\u30e1\u30fc\u30bf\u63a8\u5b9a\u306e\u76ee\u7684","text":"<p>\u6700\u5c24\u6cd5\u3067\u306f\u3001\u3053\u306e\u5bfe\u6570\u5c24\u5ea6 \\(\\ell(\\Lambda, \\Psi)\\) \u3092\u6700\u5927\u5316\u3059\u308b \\(\\Lambda\\) \u3068 \\(\\Psi\\) \u3092\u6c42\u3081\u307e\u3059\u3002 \u305f\u3060\u3057\u3001\u89e3\u6790\u7684\u306b\u9589\u3058\u305f\u89e3\u3092\u5f97\u308b\u306e\u306f\u56f0\u96e3\u306a\u305f\u3081\u3001\u6570\u5024\u7684\u306a\u6700\u9069\u5316\u624b\u6cd5\uff08EM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3084Newton-Raphson\u6cd5\uff09\u3092\u5229\u7528\u3057\u3066\u53cd\u5fa9\u7684\u306b\u63a8\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#43-em","title":"4.3 EM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3088\u308b\u63a8\u5b9a","text":"<p>EM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u3001\u6f5c\u5728\u5909\u6570 $ \\mathbf{f} $ \u3092\u300c\u6b20\u6e2c\u30c7\u30fc\u30bf\u300d\u3068\u307f\u306a\u3057\u3066\u63a8\u5b9a\u3092\u884c\u3046\u65b9\u6cd5\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u5404\u30b9\u30c6\u30c3\u30d7\u306e\u6982\u8981\u3068\u9014\u4e2d\u5f0f\u306e\u610f\u5473\u3092\u89e3\u8aac\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#431-e","title":"4.3.1 E\u30b9\u30c6\u30c3\u30d7\uff08\u671f\u5f85\u5024\u8a08\u7b97\uff09","text":"<p>\u73fe\u5728\u306e\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\Lambda^{(t)}\\) \u3068 \\(\\Psi^{(t)}\\) \u3092\u7528\u3044\u3066\u3001\u5404\u89b3\u6e2c \\(\\mathbf{x}_i\\) \u306b\u5bfe\u3059\u308b\u6f5c\u5728\u56e0\u5b50\u306e\u6761\u4ef6\u4ed8\u304d\u5206\u5e03\u3092\u6c42\u3081\u307e\u3059\u3002 \u6761\u4ef6\u4ed8\u304d\u5206\u5e03\u306f</p> \\[ \\mathbf{f}_i \\mid \\mathbf{x}_i \\sim N\\left( \\mathbf{M} \\mathbf{x}_i, \\, \\mathbf{V} \\right) \\] <p>\u3068\u8868\u3055\u308c\u3001\u3053\u3053\u3067</p> <ul> <li>\\(\\mathbf{M} = \\Lambda^\\top (\\Lambda\\Lambda^\\top + \\Psi)^{-1}\\)</li> <li>\\(\\mathbf{V} = \\mathbf{I}_m - \\Lambda^\\top (\\Lambda\\Lambda^\\top + \\Psi)^{-1}\\Lambda\\)</li> </ul> <p>\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u3053\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u306f\u3001\u4ee5\u4e0b\u306e2\u70b9\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> <ul> <li> <p>\u6761\u4ef6\u4ed8\u304d\u671f\u5f85\u5024:   $ E(\\mathbf{f}_i \\mid \\mathbf{x}_i) = \\mathbf{M} \\mathbf{x}_i $</p> </li> <li> <p>\u6761\u4ef6\u4ed8\u304d\u5171\u5206\u6563:   $ \\operatorname{Var}(\\mathbf{f}_i \\mid \\mathbf{x}_i) = \\mathbf{V} $</p> </li> </ul>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#432-m","title":"4.3.2 M\u30b9\u30c6\u30c3\u30d7\uff08\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\uff09","text":"<p>E\u30b9\u30c6\u30c3\u30d7\u3067\u5f97\u3089\u308c\u305f\u6761\u4ef6\u4ed8\u304d\u671f\u5f85\u5024\u3068\u5171\u5206\u6563\u3092\u7528\u3044\u3001\u5b8c\u5168\u30c7\u30fc\u30bf\uff08\u89b3\u6e2c\u5024\u3068\u6f5c\u5728\u5909\u6570\u3092\u542b\u3080\uff09\u306e\u5bfe\u6570\u5c24\u5ea6\u306e\u671f\u5f85\u5024\u3092\u6700\u5927\u5316\u3059\u308b\u5f62\u3067\u3001\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\Lambda\\) \u3068 \\(\\Psi\\) \u3092\u66f4\u65b0\u3057\u307e\u3059\u3002 \u66f4\u65b0\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p> \\[ \\Lambda^{(t+1)} = \\left[ \\sum_{i=1}^{n} \\mathbf{x}_i \\, E(\\mathbf{f}_i^\\top \\mid \\mathbf{x}_i) \\right] \\left[ \\sum_{i=1}^{n} E(\\mathbf{f}_i \\mathbf{f}_i^\\top \\mid \\mathbf{x}_i) \\right]^{-1}, \\] \\[ \\Psi^{(t+1)} = \\operatorname{diag}\\left\\{ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\mathbf{x}_i \\mathbf{x}_i^\\top - \\Lambda^{(t+1)} \\, E(\\mathbf{f}_i \\mathbf{x}_i^\\top \\mid \\mathbf{x}_i) \\right) \\right\\}. \\] <p>\u3053\u308c\u3089\u306e\u5f0f\u306f\u3001\u6700\u5c0f\u4e8c\u4e57\u63a8\u5b9a\u306e\u8003\u3048\u65b9\u306b\u57fa\u3065\u304d\u3001\u89b3\u6e2c\u5024 \\(\\mathbf{x}_i\\) \u3068\u6f5c\u5728\u56e0\u5b50\u306e\u6761\u4ef6\u4ed8\u304d\u671f\u5f85\u5024\u3068\u306e\u300c\u30ba\u30ec\u300d\u3092\u6700\u5c0f\u5316\u3059\u308b\u5f62\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 E\u30b9\u30c6\u30c3\u30d7\u3068M\u30b9\u30c6\u30c3\u30d7\u3092\u4ea4\u4e92\u306b\u7e70\u308a\u8fd4\u3059\u3053\u3068\u3067\u3001\u5bfe\u6570\u5c24\u5ea6\u304c\u53ce\u675f\u3059\u308b\u307e\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u66f4\u65b0\u304c\u884c\u308f\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#44","title":"4.4 \u305d\u306e\u4ed6\u306e\u63a8\u5b9a\u65b9\u6cd5","text":""},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#441-principal-axis-factoring","title":"4.4.1 \u4e3b\u8ef8\u56e0\u5b50\u6cd5\uff08Principal Axis Factoring\uff09","text":"<ol> <li> <p>\u5171\u901a\u6027\u306e\u63a8\u5b9a:    \u5404\u5909\u6570\u306e\u591a\u91cd\u76f8\u95a2\u4fc2\u6570\u306a\u3069\u3092\u7528\u3044\u3066\u3001\u521d\u671f\u306e\u5171\u901a\u6027 $ h_i^2 $\uff08\u5909\u6570 $ x_i $ \u306e\u5171\u901a\u90e8\u5206\u306e\u5206\u6563\uff09\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u56fa\u6709\u5024\u5206\u89e3:    \u30b5\u30f3\u30d7\u30eb\u306e\u76f8\u95a2\u884c\u5217 $ R $ \u306b\u5bfe\u3057\u3066\u56fa\u6709\u5024\u5206\u89e3\u3092\u884c\u3044\u3001\u56fa\u6709\u5024\u304c1\u4ee5\u4e0a\u306e\u56e0\u5b50\u6570\u3092\u9078\u5b9a\u3057\u307e\u3059\u3002    \u9078\u5b9a\u3055\u308c\u305f\u56e0\u5b50\u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u57fa\u306b\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u521d\u671f\u63a8\u5b9a\u5024\u3092\u6c7a\u5b9a\u3057\u307e\u3059\u3002</p> </li> </ol>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#442-minimum-residual-method","title":"4.4.2 \u6700\u5c0f\u6b8b\u5dee\u6cd5\uff08Minimum Residual Method\uff09","text":"<p>\u5b9f\u969b\u306e\u30b5\u30f3\u30d7\u30eb\u5171\u5206\u6563\u884c\u5217 $ S $ \u3068\u3001\u30e2\u30c7\u30eb\u3067\u518d\u73fe\u3055\u308c\u308b\u5171\u5206\u6563\u884c\u5217 \\(\\hat{\\Sigma} = \\Lambda \\Lambda^\\top + \\Psi\\) \u3068\u306e\u5dee\uff08\u6b8b\u5dee\uff09\u3092\u6700\u5c0f\u5316\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 \u76ee\u7684\u95a2\u6570\u3068\u3057\u3066\u3001\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u3092\u7528\u3044\u308b\u5834\u5408\u3001</p> \\[ \\min_{\\Lambda, \\Psi} \\| S - (\\Lambda \\Lambda^\\top + \\Psi) \\|_F^2, \\] <p>\u3092\u89e3\u304f\u5f62\u306b\u306a\u308a\u307e\u3059\u3002\u306a\u304a\u3001\\(\\Psi\\) \u306f\u5bfe\u89d2\u884c\u5217\u3067\u3042\u308b\u3068\u3044\u3046\u5236\u7d04\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#45","title":"4.5 \u6f5c\u5728\u56e0\u5b50\uff08\u56e0\u5b50\u5f97\u70b9\uff09\u306e\u63a8\u5b9a","text":"<p>\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\Lambda\\) \u3068 \\(\\Psi\\) \u306e\u63a8\u5b9a\u304c\u5b8c\u4e86\u3057\u305f\u5f8c\u3001\u5404\u89b3\u6e2c $ \\mathbf{x} $ \u306b\u5bfe\u3059\u308b\u6f5c\u5728\u56e0\u5b50\uff08\u56e0\u5b50\u5f97\u70b9\uff09\u3092\u63a8\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306b\u4ee3\u8868\u7684\u306a2\u3064\u306e\u65b9\u6cd5\u3092\u793a\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#451-bartletts-method","title":"4.5.1 \u30d0\u30fc\u30c8\u30ec\u30c3\u30c8\u6cd5\uff08Bartlett's Method\uff09","text":"<p>\u30d0\u30fc\u30c8\u30ec\u30c3\u30c8\u6cd5\u3067\u306f\u3001\u8aa4\u5dee\u306e\u5f71\u97ff\u3092\u6700\u5c0f\u5316\u3059\u308b\u91cd\u307f\u4ed8\u3051\u3092\u884c\u3044\u3001\u6b21\u306e\u5f0f\u3067\u56e0\u5b50\u5f97\u70b9\u3092\u6c42\u3081\u307e\u3059\u3002</p> \\[ \\hat{\\mathbf{f}} = \\left( \\Lambda^\\top \\Psi^{-1} \\Lambda \\right)^{-1} \\Lambda^\\top \\Psi^{-1} \\mathbf{x}. \\] <p>\u3053\u306e\u5f0f\u306f\u3001\u89b3\u6e2c\u5024 \\(\\mathbf{x}\\) \u304b\u3089\u6f5c\u5728\u56e0\u5b50\u306e\u5f71\u97ff\u3092\u9006\u7b97\u3059\u308b\u5f62\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#452-regression-method","title":"4.5.2 \u56de\u5e30\u6cd5\uff08Regression Method\uff09","text":"<p>\u56de\u5e30\u6cd5\u306f\u3001\u89b3\u6e2c\u5909\u6570\u304b\u3089\u56e0\u5b50\u7a7a\u9593\u3078\u306e\u5c04\u5f71\u3092\u6700\u5c0f\u4e8c\u4e57\u7684\u306b\u884c\u3046\u65b9\u6cd5\u3067\u3059\u3002 \u63a8\u5b9a\u5f0f\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> \\[ \\hat{\\mathbf{f}} = \\Lambda^\\top \\left( \\Lambda \\Lambda^\\top + \\Psi \\right)^{-1} \\mathbf{x}. \\] <p>\u3053\u306e\u65b9\u6cd5\u3067\u306f\u3001\u89b3\u6e2c\u5024\u306b\u5bfe\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\Lambda\\) \u3068 \\(\\Psi\\) \u306b\u57fa\u3065\u3044\u305f\u7dda\u5f62\u56de\u5e30\u3092\u884c\u3044\u3001\u6f5c\u5728\u56e0\u5b50\u306e\u5f97\u70b9\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/04-latent-factor-estimation/#46","title":"4.6 \u307e\u3068\u3081","text":"<ul> <li> <p>\u57fa\u672c\u30e2\u30c7\u30eb:   \u89b3\u6e2c\u5909\u6570\u306f\u3001\\(\\mathbf{x} = \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}\\) \u3068\u3044\u3046\u30e2\u30c7\u30eb\u306b\u57fa\u3065\u304d\u3001\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306f \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) \u3068\u8868\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u6700\u5c24\u6cd5:   \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u3092\u4eee\u5b9a\u3057\u3001\u5bfe\u6570\u5c24\u5ea6</p> </li> </ul> <p>$$   \\ell(\\Lambda, \\Psi) = -\\frac{n}{2} \\left[ \\ln |\\Sigma| + \\operatorname{tr}(S \\Sigma^{-1}) \\right] + \\text{\u5b9a\u6570}   $$</p> <p>\u3092\u6700\u5927\u5316\u3059\u308b\u3053\u3068\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002\u89e3\u6790\u7684\u89e3\u304c\u5f97\u306b\u304f\u3044\u305f\u3081\u3001EM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3084Newton-Raphson\u6cd5\u306a\u3069\u306e\u53cd\u5fa9\u6cd5\u304c\u7528\u3044\u3089\u308c\u307e\u3059\u3002</p> <ul> <li> <p>\u305d\u306e\u4ed6\u306e\u624b\u6cd5:   \u4e3b\u8ef8\u56e0\u5b50\u6cd5\u3084\u6700\u5c0f\u6b8b\u5dee\u6cd5\u306a\u3069\u3001\u30c7\u30fc\u30bf\u3084\u89e3\u6790\u76ee\u7684\u306b\u5fdc\u3058\u305f\u4ee3\u66ff\u624b\u6cd5\u3082\u5b58\u5728\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u56e0\u5b50\u5f97\u70b9\u306e\u63a8\u5b9a:   \u30d0\u30fc\u30c8\u30ec\u30c3\u30c8\u6cd5\u3084\u56de\u5e30\u6cd5\u3092\u7528\u3044\u3066\u3001\u63a8\u5b9a\u3055\u308c\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\u304b\u3089\u5404\u89b3\u6e2c\u306e\u6f5c\u5728\u56e0\u5b50\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/","title":"5. \u56de\u8ee2\u57fa\u6e96\u3068\u7d50\u679c\u306e\u89e3\u91c8","text":""},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#5","title":"\u7b2c5\u7ae0 \u56de\u8ee2\u3068\u5206\u6790\u7d50\u679c\u306e\u89e3\u91c8","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u56e0\u5b50\u5206\u6790\u306e\u7d50\u679c\u3092\u3088\u308a\u660e\u78ba\u304b\u3064\u89e3\u91c8\u3057\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u884c\u308f\u308c\u308b\u300c\u56de\u8ee2\u300d\u306e\u6982\u5ff5\u3068\u3001\u305d\u306e\u5f8c\u306e\u5206\u6790\u7d50\u679c\u306e\u89e3\u91c8\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u8aac\u660e\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u56de\u8ee2\u306e\u76ee\u7684\u3001\u76f4\u4ea4\u56de\u8ee2\u3068\u659c\u4ea4\u56de\u8ee2\u306e\u9055\u3044\u3001\u6570\u5b66\u7684\u306a\u5c55\u958b\u3001\u305d\u3057\u3066\u5177\u4f53\u4f8b\u3092\u901a\u3057\u3066\u3001\u5f97\u3089\u308c\u305f\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u8aad\u307f\u53d6\u308a\u65b9\u3084\u5b9f\u52d9\u4e0a\u306e\u89e3\u91c8\u306e\u30dd\u30a4\u30f3\u30c8\u3092\u89e3\u8aac\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#51","title":"5.1 \u306f\u3058\u3081\u306b","text":"<p>\u56e0\u5b50\u5206\u6790\u306e\u521d\u671f\u6bb5\u968e\u3067\u306f\u3001\u7d71\u8a08\u7684\u6700\u9069\u6027\u3092\u91cd\u8996\u3057\u305f\u63a8\u5b9a\u306b\u3088\u308a\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u5f97\u3089\u308c\u305f\u8ca0\u8377\u884c\u5217\u306f\u5fc5\u305a\u3057\u3082\u300c\u5358\u7d14\u69cb\u9020\uff08simple structure\uff09\u300d\u3092\u793a\u3057\u3066\u304a\u3089\u305a\u3001\u5404\u89b3\u6e2c\u5909\u6570\u304c\u8907\u6570\u306e\u56e0\u5b50\u306b\u5bfe\u3057\u3066\u4e2d\u7a0b\u5ea6\u306e\u8ca0\u8377\u91cf\u3092\u6301\u3064\u3088\u3046\u306a\u5834\u5408\u3082\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u72b6\u614b\u3067\u306f\u3001\u3069\u306e\u56e0\u5b50\u304c\u3069\u306e\u5909\u6570\u306b\u4e3b\u306b\u5f71\u97ff\u3092\u53ca\u307c\u3057\u3066\u3044\u308b\u306e\u304b\u306e\u89e3\u91c8\u304c\u56f0\u96e3\u306b\u306a\u308b\u305f\u3081\u3001\u56de\u8ee2\u3068\u3044\u3046\u624b\u6cd5\u3092\u7528\u3044\u3066\u3001\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u69cb\u9020\u3092\u89e3\u91c8\u3057\u3084\u3059\u3044\u5f62\u306b\u518d\u914d\u7f6e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#52","title":"5.2 \u56de\u8ee2\u306e\u76ee\u7684\u3068\u91cd\u8981\u6027","text":""},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#521","title":"5.2.1 \u56de\u8ee2\u306e\u76ee\u7684","text":"<ul> <li> <p>\u89e3\u91c8\u6027\u306e\u5411\u4e0a:   \u56de\u8ee2\u3092\u884c\u3046\u3053\u3068\u3067\u3001\u5404\u56e0\u5b50\u306b\u5bfe\u3057\u3066\u9ad8\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064\u5909\u6570\u3068\u4f4e\u3044\u8ca0\u8377\u91cf\u3092\u6301\u3064\u5909\u6570\u304c\u660e\u78ba\u306b\u306a\u308a\u3001\u5404\u56e0\u5b50\u306e\u610f\u5473\u3092\u628a\u63e1\u3057\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u5358\u7d14\u69cb\u9020\u306e\u5b9f\u73fe:   \u300c\u5358\u7d14\u69cb\u9020\u300d\u3068\u306f\u3001\u5404\u5909\u6570\u304c\u3067\u304d\u308b\u3060\u3051\u4e00\u3064\u306e\u56e0\u5b50\u306b\u5f37\u304f\u5bc4\u4e0e\u3057\u3001\u4ed6\u306e\u56e0\u5b50\u306b\u306f\u307b\u3068\u3093\u3069\u5bc4\u4e0e\u3057\u306a\u3044\u72b6\u614b\u3092\u6307\u3057\u307e\u3059\u3002\u56de\u8ee2\u306f\u3053\u306e\u5358\u7d14\u69cb\u9020\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u89e3\u91c8\u306e\u4e00\u81f4:   \u5206\u6790\u8005\u304c\u7570\u306a\u308b\u5834\u5408\u3067\u3082\u3001\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306b\u3088\u308a\u3001\u540c\u3058\u30c7\u30fc\u30bf\u304b\u3089\u4f3c\u305f\u89e3\u91c8\u304c\u5f97\u3089\u308c\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u307e\u308a\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#522","title":"5.2.2 \u306a\u305c\u521d\u671f\u306e\u89e3\u3060\u3051\u3067\u306f\u4e0d\u5341\u5206\u306a\u306e\u304b","text":"<ul> <li>\u521d\u671f\u89e3\u306e\u554f\u984c\u70b9:   \u521d\u671f\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306f\u3001\u6700\u5c24\u6cd5\u3084\u4e3b\u56e0\u5b50\u6cd5\u306a\u3069\u3067\u5f97\u3089\u308c\u308b\u305f\u3081\u3001\u7d71\u8a08\u7684\u6700\u9069\u6027\u306f\u3042\u308b\u3082\u306e\u306e\u3001\u89e3\u91c8\u306e\u5bb9\u6613\u3055\u3068\u3044\u3046\u70b9\u3067\u306f\u5fc5\u305a\u3057\u3082\u6700\u826f\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002  </li> <li>\u69cb\u9020\u306e\u66d6\u6627\u3055:   \u4f8b\u3048\u3070\u3001\u3042\u308b\u5909\u6570\u304c\u8907\u6570\u306e\u56e0\u5b50\u306b\u5bfe\u3057\u3066\u4e2d\u7a0b\u5ea6\u306e\u8ca0\u8377\u3092\u793a\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u305d\u306e\u5909\u6570\u304c\u3069\u306e\u56e0\u5b50\u306b\u4e3b\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u306e\u304b\u304c\u4e0d\u660e\u77ad\u3068\u306a\u308a\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#53","title":"5.3 \u76f4\u4ea4\u56de\u8ee2\u3068\u659c\u4ea4\u56de\u8ee2\u306e\u8a73\u7d30","text":"<p>\u56e0\u5b50\u5206\u6790\u3067\u306e\u56de\u8ee2\u306b\u306f\u5927\u304d\u304f\u5206\u3051\u3066\u300c\u76f4\u4ea4\u56de\u8ee2\u300d\u3068\u300c\u659c\u4ea4\u56de\u8ee2\u300d\u306e2\u7a2e\u985e\u304c\u3042\u308a\u3001\u305d\u308c\u305e\u308c\u306b\u30e1\u30ea\u30c3\u30c8\u30fb\u30c7\u30e1\u30ea\u30c3\u30c8\u304a\u3088\u3073\u5b9f\u969b\u306e\u5fdc\u7528\u3067\u306e\u5229\u70b9\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u5404\u624b\u6cd5\u306e\u4ee3\u8868\u7684\u306a\u57fa\u6e96\u306b\u3064\u3044\u3066\u3001Varimax\uff08\u76f4\u4ea4\u56de\u8ee2\uff09\u3068Promax\uff08\u659c\u4ea4\u56de\u8ee2\uff09\u306b\u52a0\u3048\u3001\u76f4\u4ea4\u56de\u8ee2\u306eQuartimax\u57fa\u6e96\u304a\u3088\u3073\u659c\u4ea4\u56de\u8ee2\u306eQuartimin\u57fa\u6e96\u3084Geomin\u57fa\u6e96\u306b\u3064\u3044\u3066\u3082\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#531-orthogonal-rotation","title":"5.3.1 \u76f4\u4ea4\u56de\u8ee2 (Orthogonal Rotation)","text":"<p>\u6982\u8981: \u76f4\u4ea4\u56de\u8ee2\u306f\u3001\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u304c\u4e92\u3044\u306b\u76f4\u4ea4\uff08\u3059\u306a\u308f\u3061\u76f8\u95a2\u304c\u30bc\u30ed\uff09\u3068\u306a\u308b\u3088\u3046\u306b\u56de\u8ee2\u3092\u884c\u3046\u65b9\u6cd5\u3067\u3059\u3002\u4ee3\u8868\u7684\u306a\u624b\u6cd5\u306b\u306f\u3001Varimax\u3001Quartimax\u3001Equamax \u306a\u3069\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>Varimax: - \u5404\u56e0\u5b50\u306b\u304a\u3051\u308b\u8ca0\u8377\u91cf\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u6975\u7aef\u306a\u9ad8\u5024\u3068\u4f4e\u5024\u3092\u5f37\u8abf\u3057\u3001\u5358\u7d14\u69cb\u9020\u3092\u5b9f\u73fe\u3057\u307e\u3059\u3002 - \u30e1\u30ea\u30c3\u30c8: \u89e3\u91c8\u304c\u30b7\u30f3\u30d7\u30eb\u3067\u3001\u5404\u5909\u6570\u304c\u3069\u306e\u56e0\u5b50\u306b\u5f37\u304f\u95a2\u9023\u3057\u3066\u3044\u308b\u304b\u304c\u660e\u78ba\u306b\u306a\u308a\u307e\u3059\u3002 - \u30c7\u30e1\u30ea\u30c3\u30c8: \u3082\u3057\u30c7\u30fc\u30bf\u306e\u80cc\u5f8c\u3067\u56e0\u5b50\u9593\u306b\u76f8\u95a2\u304c\u3042\u308b\u5834\u5408\u3001\u305d\u306e\u95a2\u4fc2\u3092\u7121\u8996\u3057\u3066\u3057\u307e\u3046\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>Quartimax: - Quartimax \u306f\u3001\u884c\u3054\u3068\u306e\u5358\u7d14\u6027\u3001\u3059\u306a\u308f\u3061\u5404\u89b3\u6e2c\u5909\u6570\uff08\u884c\uff09\u306e\u8ca0\u8377\u91cf\u306e\u5358\u7d14\u6027\u3092\u6700\u5927\u5316\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u305f\u56de\u8ee2\u65b9\u6cd5\u3067\u3059\u3002 - \u30e1\u30ea\u30c3\u30c8: \u5404\u5909\u6570\u304c\u4e00\u3064\u306e\u56e0\u5b50\u306b\u6975\u7aef\u306b\u5bc4\u4e0e\u3059\u308b\u3088\u3046\u306b\u5909\u63db\u3055\u308c\u308b\u305f\u3081\u3001\u5168\u4f53\u3068\u3057\u3066\u5358\u7d14\u306a\u69cb\u9020\u304c\u5f97\u3089\u308c\u3084\u3059\u304f\u3001\u4e00\u822c\u56e0\u5b50\u306e\u62bd\u51fa\u50be\u5411\u304c\u5f37\u304f\u51fa\u308b\u3053\u3068\u304c\u591a\u3044\u3067\u3059\u3002 - \u30c7\u30e1\u30ea\u30c3\u30c8: \u7d50\u679c\u3068\u3057\u3066\u3001\u4e00\u3064\u306e\u5927\u304d\u306a\u4e00\u822c\u56e0\u5b50\u304c\u652f\u914d\u7684\u306b\u306a\u308b\u50be\u5411\u304c\u3042\u308a\u3001\u8907\u6570\u306e\u660e\u78ba\u306a\u56e0\u5b50\u69cb\u9020\u304c\u898b\u3048\u306b\u304f\u304f\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>Equamax: - Varimax \u3068 Quartimax \u306e\u4e2d\u9593\u7684\u306a\u6027\u8cea\u3092\u6301\u3061\u3001\u5404\u56e0\u5b50\u3068\u5404\u5909\u6570\u306e\u5358\u7d14\u6027\u3092\u540c\u6642\u306b\u8ffd\u6c42\u3057\u307e\u3059\u3002</p> <p>\u5b9f\u969b\u306e\u5fdc\u7528\u4f8b: \u5fc3\u7406\u5b66\u7684\u691c\u67fb\u306a\u3069\u3001\u5909\u6570\u3054\u3068\u306b\u660e\u78ba\u306a\u7279\u5fb4\u304c\u5b58\u5728\u3057\u3001\u5404\u5909\u6570\u304c\u7279\u5b9a\u306e\u56e0\u5b50\u306b\u96c6\u4e2d\u3059\u308b\u3053\u3068\u304c\u671f\u5f85\u3055\u308c\u308b\u5834\u5408\u3001\u76f4\u4ea4\u56de\u8ee2\u306f\u9069\u7528\u3057\u3084\u3059\u3044\u3067\u3059\u3002Quartimax \u306f\u3001\u5168\u4f53\u3068\u3057\u3066\u5358\u7d14\u306a\u69cb\u9020\u3092\u5f97\u308b\u305f\u3081\u306b\u5229\u7528\u3055\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u4e00\u822c\u56e0\u5b50\u304c\u5f37\u8abf\u3055\u308c\u3059\u304e\u308b\u305f\u3081\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#532-oblique-rotation","title":"5.3.2 \u659c\u4ea4\u56de\u8ee2 (Oblique Rotation)","text":"<p>\u6982\u8981: \u659c\u4ea4\u56de\u8ee2\u306f\u3001\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u4ee3\u8868\u7684\u306a\u624b\u6cd5\u306b\u306f\u3001Promax\u3001Direct Oblimin\u3001Quartimin\u3001Geomin \u306a\u3069\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>Promax: - \u521d\u3081\u306b\u76f4\u4ea4\u56de\u8ee2\uff08\u901a\u5e38\u306f Varimax\uff09\u3092\u884c\u3044\u3001\u305d\u306e\u7d50\u679c\u306b\u5bfe\u3057\u3066\u30d1\u30ef\u30fc\u5909\u63db\u3092\u9069\u7528\u3057\u3066\u3001\u659c\u4ea4\u6027\u3092\u5c0e\u5165\u3057\u307e\u3059\u3002 - \u30e1\u30ea\u30c3\u30c8: \u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3067\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u3088\u308a\u73fe\u5b9f\u7684\u306a\u56e0\u5b50\u69cb\u9020\u3092\u53cd\u6620\u3067\u304d\u307e\u3059\u3002 - \u30c7\u30e1\u30ea\u30c3\u30c8: \u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u751f\u3058\u308b\u305f\u3081\u3001\u5404\u56e0\u5b50\u306e\u72ec\u7acb\u3057\u305f\u89e3\u91c8\u304c\u96e3\u3057\u304f\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>Direct Oblimin: - \u56de\u8ee2\u30d1\u30e9\u30e1\u30fc\u30bf\uff08delta\u5024\uff09\u3092\u8abf\u6574\u3059\u308b\u3053\u3068\u3067\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u306e\u5ea6\u5408\u3044\u3092\u5236\u5fa1\u3067\u304d\u308b\u67d4\u8edf\u306a\u65b9\u6cd5\u3067\u3059\u3002 - \u30e1\u30ea\u30c3\u30c8: \u56de\u8ee2\u306e\u81ea\u7531\u5ea6\u304c\u9ad8\u304f\u3001\u30c7\u30fc\u30bf\u306e\u7279\u6027\u306b\u5fdc\u3058\u305f\u8abf\u6574\u304c\u53ef\u80fd\u3067\u3059\u3002 - \u30c7\u30e1\u30ea\u30c3\u30c8: \u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u306b\u3088\u308a\u7d50\u679c\u304c\u5927\u304d\u304f\u5909\u5316\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u3001\u89e3\u91c8\u306b\u306f\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002</p> <p>Quartimin: - Quartimin\u306f\u3001\u659c\u4ea4\u56de\u8ee2\u306e\u4e00\u624b\u6cd5\u3067\u3001\u8ca0\u8377\u91cf\u884c\u5217\u306e\u300c\u8907\u96d1\u3055\u300d\u3092\u6700\u5c0f\u5316\u3059\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u307e\u3059\u3002 - \u30e1\u30ea\u30c3\u30c8: \u5909\u6570\u304c\u8907\u6570\u306e\u56e0\u5b50\u306b\u308f\u305f\u3063\u3066\u4e2d\u7a0b\u5ea6\u306e\u8ca0\u8377\u3092\u793a\u3059\u5834\u5408\u3001\u6bd4\u8f03\u7684\u30b7\u30f3\u30d7\u30eb\u306a\u69cb\u9020\u306b\u518d\u914d\u7f6e\u3055\u308c\u3001\u89e3\u91c8\u3057\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002 - \u30c7\u30e1\u30ea\u30c3\u30c8: \u76f4\u4ea4\u56de\u8ee2\u307b\u3069\u5358\u7d14\u306a\u72ec\u7acb\u6027\u306f\u5f97\u3089\u308c\u305a\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u6b8b\u308b\u305f\u3081\u3001\u89e3\u91c8\u304c\u3084\u3084\u8907\u96d1\u306b\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>Geomin: - Geomin\u56de\u8ee2\u306f\u3001\u8ca0\u8377\u91cf\u306e\u5c0f\u3055\u3044\u8981\u7d20\u306e\u5e7e\u4f55\u5e73\u5747\uff08\u307e\u305f\u306f\u305d\u306e\u985e\u4f3c\u6307\u6a19\uff09\u3092\u6700\u5c0f\u5316\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002 - \u30e1\u30ea\u30c3\u30c8: \u7d30\u304b\u306a\u8ca0\u8377\u91cf\u3092\u6291\u3048\u3001\u5927\u304d\u306a\u8ca0\u8377\u91cf\u3092\u969b\u7acb\u305f\u305b\u308b\u3053\u3068\u3067\u3001\u5909\u6570\u304c\u660e\u78ba\u306b\u3069\u306e\u56e0\u5b50\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u793a\u3059\u5358\u7d14\u69cb\u9020\u3092\u5c0e\u51fa\u3057\u307e\u3059\u3002 - \u30c7\u30e1\u30ea\u30c3\u30c8: \u56de\u8ee2\u57fa\u6e96\u304c\u8907\u96d1\u306a\u305f\u3081\u3001\u89e3\u91c8\u306e\u4e00\u8cab\u6027\u3092\u4fdd\u3064\u305f\u3081\u306b\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u56de\u8ee2\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u614e\u91cd\u306b\u9078\u3076\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u5b9f\u969b\u306e\u5fdc\u7528\u4f8b: \u5065\u5eb7\u8a3a\u65ad\u30c7\u30fc\u30bf\u3084\u6d88\u8cbb\u8005\u8abf\u67fb\u306a\u3069\u3001\u56e0\u5b50\u9593\u306b\u81ea\u7136\u306a\u76f8\u95a2\u304c\u5b58\u5728\u3059\u308b\u3068\u4e88\u60f3\u3055\u308c\u308b\u5834\u5408\u3001\u659c\u4ea4\u56de\u8ee2\u306f\u6709\u7528\u3067\u3059\u3002Quartimin \u3084 Geomin \u306f\u3001\u5909\u6570\u304c\u8907\u6570\u306e\u56e0\u5b50\u306b\u308f\u305f\u308b\u4e2d\u7a0b\u5ea6\u306e\u5bc4\u4e0e\u3092\u793a\u3059\u5834\u5408\u306b\u3001\u3088\u308a\u30b7\u30f3\u30d7\u30eb\u306a\u89e3\u91c8\u3092\u4fc3\u3059\u305f\u3081\u306b\u9078\u629e\u3055\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u307e\u3068\u3081: - \u76f4\u4ea4\u56de\u8ee2:   Varimax\u3001Quartimax\u3001Equamax \u306a\u3069\u304c\u3042\u308a\u3001\u56e0\u5b50\u9593\u306e\u72ec\u7acb\u6027\u3092\u4fdd\u6301\u3059\u308b\u305f\u3081\u3001\u89e3\u91c8\u304c\u76f4\u611f\u7684\u3067\u30b7\u30f3\u30d7\u30eb\u306a\u69cb\u9020\u304c\u5f97\u3089\u308c\u307e\u3059\u304c\u3001\u73fe\u5b9f\u306e\u76f8\u95a2\u95a2\u4fc2\u3092\u53cd\u6620\u3057\u306b\u304f\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002</p> <ul> <li>\u659c\u4ea4\u56de\u8ee2:   Promax\u3001Direct Oblimin\u3001Quartimin\u3001Geomin \u306a\u3069\u304c\u3042\u308a\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9\u3059\u308b\u3053\u3068\u3067\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u3088\u308a\u5fe0\u5b9f\u306b\u53cd\u6620\u3067\u304d\u307e\u3059\u304c\u3001\u89e3\u91c8\u304c\u8907\u96d1\u306b\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</li> </ul> <p>\u5b9f\u969b\u306e\u89e3\u6790\u3067\u306f\u3001\u7406\u8ad6\u7684\u306a\u524d\u63d0\u3084\u30c7\u30fc\u30bf\u306e\u7279\u6027\u306b\u5fdc\u3058\u3001\u3069\u3061\u3089\u306e\u56de\u8ee2\u624b\u6cd5\u304c\u9069\u5207\u304b\u3092\u5224\u65ad\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u5404\u624b\u6cd5\u306e\u30e1\u30ea\u30c3\u30c8\u3068\u30c7\u30e1\u30ea\u30c3\u30c8\u3092\u5341\u5206\u306b\u7406\u89e3\u3057\u305f\u4e0a\u3067\u3001\u76ee\u7684\u306b\u5408\u308f\u305b\u305f\u56de\u8ee2\u65b9\u6cd5\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3001\u56e0\u5b50\u5206\u6790\u306e\u7d50\u679c\u304c\u3088\u308a\u610f\u5473\u306e\u3042\u308b\u3082\u306e\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#54","title":"5.4 \u6570\u5b66\u7684\u5c55\u958b\u3068\u56de\u8ee2\u884c\u5217","text":"<p>\u672c\u7bc0\u3067\u306f\u3001\u56de\u8ee2\u884c\u5217\u306e\u6570\u5b66\u7684\u57fa\u790e\u3068\u3001\u3069\u306e\u3088\u3046\u306b\u3057\u3066\u521d\u671f\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217 $ \\Lambda $ \u3092\u3088\u308a\u89e3\u91c8\u3057\u3084\u3059\u3044\u5f62\u306b\u5909\u63db\u3059\u308b\u304b\u3001\u305d\u306e\u5177\u4f53\u7684\u306a\u624b\u9806\u3092\u8a73\u3057\u304f\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#541","title":"5.4.1 \u57fa\u672c\u6982\u5ff5","text":"<p>\u56e0\u5b50\u5206\u6790\u3067\u306f\u3001\u521d\u671f\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217 $ \\Lambda $ \u304c\u5f97\u3089\u308c\u305f\u5f8c\u3001\u305d\u306e\u89e3\u91c8\u3092\u5bb9\u6613\u306b\u3059\u308b\u305f\u3081\u306b\u300c\u56de\u8ee2\u300d\u3092\u65bd\u3057\u307e\u3059\u3002\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306f\u3001\u56de\u8ee2\u884c\u5217 $ T $ \u3092\u7528\u3044\u3066\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002</p> \\[ \\Lambda^* = \\Lambda T. \\] <ul> <li> <p>\u76f4\u4ea4\u56de\u8ee2\u306e\u5834\u5408:   $ T $ \u306f\u76f4\u4ea4\u884c\u5217\u3068\u306a\u308a\u3001\u3064\u307e\u308a $ T^\\top T = I $\uff08\u5358\u4f4d\u884c\u5217\uff09\u3092\u6e80\u305f\u3057\u307e\u3059\u3002\u3053\u306e\u5834\u5408\u3001\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u9593\u306f\u4e92\u3044\u306b\u72ec\u7acb\u3067\u3059\u3002</p> </li> <li> <p>\u659c\u4ea4\u56de\u8ee2\u306e\u5834\u5408:   $ T $ \u306f\u5fc5\u305a\u3057\u3082\u76f4\u4ea4\u3067\u306f\u306a\u304f\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9\u3057\u307e\u3059\u3002\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u884c\u5217\u306f\u3001   $$   \\Phi = T^\\top T   $$   \u3068\u3057\u3066\u6c42\u3081\u3089\u308c\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#542-varimax","title":"5.4.2 \u76f4\u4ea4\u56de\u8ee2\u306b\u304a\u3051\u308b\u6570\u5b66\u7684\u6700\u9069\u5316\uff08Varimax\u306e\u4f8b\uff09","text":"<p>\u76f4\u4ea4\u56de\u8ee2\u306f\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u300c\u5358\u7d14\u69cb\u9020\u300d\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306b\u3001\u5404\u56e0\u5b50\u306b\u5bfe\u3057\u3066\u9ad8\u3044\u8ca0\u8377\u91cf\u3068\u4f4e\u3044\u8ca0\u8377\u91cf\u306e\u5dee\u3092\u6700\u5927\u5316\u3057\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001Varimax\u56de\u8ee2\u3067\u306f\u4ee5\u4e0b\u306e\u76ee\u7684\u95a2\u6570\u3092\u6700\u5927\u5316\u3057\u307e\u3059\u3002</p> \\[ V = \\sum_{j=1}^m \\left\\{ \\frac{1}{p} \\sum_{i=1}^p (\\lambda^*_{ij})^4 - \\left(\\frac{1}{p} \\sum_{i=1}^p (\\lambda^*_{ij})^2\\right)^2 \\right\\}, \\] <p>\u3053\u3053\u3067\u3001 - $ \\lambda^*_{ij} $ \u306f\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u3001 - $ p $ \u306f\u89b3\u6e2c\u5909\u6570\u306e\u6570\u3001 - $ m $ \u306f\u62bd\u51fa\u3055\u308c\u305f\u56e0\u5b50\u306e\u6570\u3067\u3059\u3002</p> <p>\u3053\u306e\u76ee\u7684\u95a2\u6570\u306f\u3001\u5404\u56e0\u5b50\u306e\u8ca0\u8377\u91cf\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u6975\u7aef\u306a\u5024\uff08\u9ad8\u3044\u307e\u305f\u306f\u4f4e\u3044\uff09\u3092\u5f37\u8abf\u3057\u3001\u5358\u7d14\u69cb\u9020\u306b\u8fd1\u3065\u3051\u308b\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002\u76f4\u4ea4\u56de\u8ee2\u3067\u306f\u3001\u56de\u8ee2\u884c\u5217 $ T $ \u304c\u76f4\u4ea4\u6027 $ T^\\top T = I $ \u3092\u4fdd\u3064\u305f\u3081\u3001\u5909\u63db\u5f8c\u306e\u8ca0\u8377\u884c\u5217 $ \\Lambda^* $ \u306e\u5168\u4f53\u7684\u306a\u5206\u6563\u306f\u4fdd\u6301\u3055\u308c\u3001\u56e0\u5b50\u9593\u306e\u72ec\u7acb\u6027\u304c\u7dad\u6301\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#543-promax","title":"5.4.3 \u659c\u4ea4\u56de\u8ee2\u306b\u304a\u3051\u308bPromax\u56de\u8ee2\u306e\u5177\u4f53\u7684\u624b\u6cd5","text":"<p>\u659c\u4ea4\u56de\u8ee2\u306f\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3067\u56e0\u5b50\u9593\u306b\u5b58\u5728\u3059\u308b\u76f8\u95a2\u3092\u53cd\u6620\u3059\u308b\u305f\u3081\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002Promax\u56de\u8ee2\u306f\u3001\u305d\u306e\u4e00\u4f8b\u3067\u3042\u308a\u3001\u4ee5\u4e0b\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u884c\u308f\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#1","title":"\u30b9\u30c6\u30c3\u30d71: \u521d\u671f\u76f4\u4ea4\u56de\u8ee2","text":"<p>\u307e\u305a\u3001Varimax\u306a\u3069\u306e\u76f4\u4ea4\u56de\u8ee2\u3092\u7528\u3044\u3066\u521d\u671f\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217 $ \\Lambda $ \u3092\u5f97\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#2","title":"\u30b9\u30c6\u30c3\u30d72: \u30bf\u30fc\u30b2\u30c3\u30c8\u884c\u5217\u306e\u4f5c\u6210","text":"<p>\u5404\u8981\u7d20\u306b\u30d1\u30ef\u30fc\u5909\u63db\u3092\u9069\u7528\u3057\u3066\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u884c\u5217 $ \\Lambda_t $ \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u5404\u8981\u7d20\u306b\u3064\u3044\u3066</p> \\[ \\lambda_{ij}^{(t)} = \\operatorname{sgn}(\\lambda_{ij})\\,|\\lambda_{ij}|^k, \\] <p>\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u3001 - $ \\operatorname{sgn}(\\lambda_{ij}) $ \u306f $ \\lambda_{ij} $ \u306e\u7b26\u53f7\u3092\u4fdd\u6301\u3057\u3001 - $ k $ \u306f\u901a\u5e38 3 \u3084 4 \u3068\u3044\u3063\u305f\u6b63\u306e\u6574\u6570\u3067\u3059\u3002</p> <p>\u3053\u306e\u5909\u63db\u306b\u3088\u308a\u3001\u5927\u304d\u306a\u8ca0\u8377\u91cf\u306f\u3055\u3089\u306b\u5f37\u8abf\u3055\u308c\u3001\u5c0f\u3055\u306a\u8ca0\u8377\u91cf\u306f\u3088\u308a\u5c0f\u3055\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#3-t","title":"\u30b9\u30c6\u30c3\u30d73: \u5909\u63db\u884c\u5217 $ T $ \u306e\u63a8\u5b9a","text":"<p>\u6b21\u306b\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u884c\u5217 $ \\Lambda_t $ \u306b\u3067\u304d\u308b\u3060\u3051\u8fd1\u3065\u3051\u308b\u305f\u3081\u3001\u5909\u63db\u884c\u5217 $ T $ \u3092\u4ee5\u4e0b\u306e\u6700\u5c0f\u4e8c\u4e57\u554f\u984c\u3068\u3057\u3066\u6c42\u3081\u307e\u3059\u3002</p> \\[ \\min_{T} \\| \\Lambda T - \\Lambda_t \\|_F^2, \\] <p>\u3053\u3053\u3067\u3001$ | \\cdot |_F $ \u306f\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u3092\u793a\u3057\u307e\u3059\u3002\u6700\u5c0f\u4e8c\u4e57\u89e3\u306f\u3001\u89e3\u6790\u7684\u306b\u6b21\u306e\u5f62\u3067\u6c42\u3081\u3089\u308c\u307e\u3059\u3002</p> \\[ T = (\\Lambda^\\top \\Lambda)^{-1}\\Lambda^\\top \\Lambda_t. \\] <p>\u3053\u306e $ T $ \u306f\u76f4\u4ea4\u6027\u3092\u6e80\u305f\u3055\u306a\u3044\u975e\u76f4\u4ea4\u884c\u5217\u3068\u306a\u308a\u3001\u3053\u308c\u306b\u3088\u308a\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u5c0e\u5165\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#4","title":"\u30b9\u30c6\u30c3\u30d74: \u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u7b97\u51fa","text":"<p>\u6700\u7d42\u7684\u306b\u3001\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306f</p> \\[ \\Lambda^* = \\Lambda T, \\] <p>\u3068\u3057\u3066\u5f97\u3089\u308c\u3001\u3055\u3089\u306b\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u306f</p> \\[ \\Phi = T^\\top T, \\] <p>\u3068\u3057\u3066\u6c42\u3081\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#544","title":"5.4.4 \u307e\u3068\u3081\u3068\u91cd\u8981\u30dd\u30a4\u30f3\u30c8","text":"<ul> <li> <p>\u57fa\u672c\u5f0f:   \u56de\u8ee2\u64cd\u4f5c\u306f $ \\Lambda^* = \\Lambda T $ \u3068\u3044\u3046\u5358\u7d14\u306a\u7dda\u5f62\u5909\u63db\u3067\u8868\u73fe\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u76f4\u4ea4\u56de\u8ee2 vs \u659c\u4ea4\u56de\u8ee2: </p> </li> <li>\u76f4\u4ea4\u56de\u8ee2:     $ T $ \u306f\u76f4\u4ea4\u884c\u5217\uff08$ T^\\top T = I $\uff09\u3067\u3042\u308a\u3001\u56e0\u5b50\u9593\u306e\u72ec\u7acb\u6027\u3092\u4fdd\u6301\u3057\u307e\u3059\u3002Varimax\u306a\u3069\u304c\u4ee3\u8868\u7684\u3067\u3059\u3002</li> <li> <p>\u659c\u4ea4\u56de\u8ee2:     $ T $ \u306f\u975e\u76f4\u4ea4\u884c\u5217\u3068\u306a\u308a\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2 $ \\Phi = T^\\top T $ \u3092\u53cd\u6620\u3057\u307e\u3059\u3002Promax\u56de\u8ee2\u306f\u305d\u306e\u5178\u578b\u4f8b\u3067\u3059\u3002</p> </li> <li> <p>\u30d1\u30ef\u30fc\u5909\u63db\u306e\u5f79\u5272:   \u30bf\u30fc\u30b2\u30c3\u30c8\u884c\u5217 $ \\Lambda_t $ \u306e\u4f5c\u6210\u306b\u3088\u308a\u3001\u5927\u304d\u306a\u8ca0\u8377\u91cf\u306f\u3055\u3089\u306b\u5f37\u8abf\u3055\u308c\u3001\u5c0f\u3055\u306a\u8ca0\u8377\u91cf\u306f\u7e2e\u5c0f\u3055\u308c\u308b\u305f\u3081\u3001\u5404\u5909\u6570\u304c\u3069\u306e\u56e0\u5b50\u306b\u4e3b\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u304c\u660e\u78ba\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u6700\u9069\u5316\u554f\u984c\u3068\u3057\u3066\u306e\u56de\u8ee2\u884c\u5217:   \u56de\u8ee2\u884c\u5217 $ T $ \u306e\u63a8\u5b9a\u306f\u3001\u30d5\u30ed\u30d9\u30cb\u30a6\u30b9\u30ce\u30eb\u30e0\u306b\u3088\u308b\u6700\u5c0f\u4e8c\u4e57\u554f\u984c\u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3055\u308c\u3001\u305d\u306e\u89e3\u304c\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217 $ \\Lambda^* $ \u3092\u6c7a\u5b9a\u3057\u307e\u3059\u3002</p> </li> </ul> <p>\u3053\u306e\u3088\u3046\u306b\u3001\u6570\u5b66\u7684\u5c55\u958b\u3068\u56de\u8ee2\u884c\u5217\u306e\u8a73\u7d30\u306a\u7406\u89e3\u306f\u3001\u56e0\u5b50\u5206\u6790\u306e\u7d50\u679c\u3092\u3088\u308a\u660e\u77ad\u306b\u89e3\u91c8\u3059\u308b\u305f\u3081\u306e\u57fa\u76e4\u3068\u306a\u308a\u307e\u3059\u3002\u5404\u30b9\u30c6\u30c3\u30d7\u306e\u610f\u5473\u3068\u305d\u306e\u80cc\u666f\u3092\u628a\u63e1\u3059\u308b\u3053\u3068\u3067\u3001\u5f97\u3089\u308c\u305f\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u5909\u63db\u904e\u7a0b\u3068\u305d\u306e\u89e3\u91c8\u306b\u81ea\u4fe1\u3092\u6301\u3063\u3066\u81e8\u3080\u3053\u3068\u304c\u3067\u304d\u308b\u3067\u3057\u3087\u3046\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#55","title":"5.5 \u5206\u6790\u7d50\u679c\u306e\u89e3\u91c8","text":"<p>\u56e0\u5b50\u5206\u6790\u306e\u7d50\u679c\u3092\u6b63\u3057\u304f\u89e3\u91c8\u3059\u308b\u3053\u3068\u306f\u3001\u30c7\u30fc\u30bf\u304b\u3089\u610f\u5473\u306e\u3042\u308b\u77e5\u898b\u3092\u62bd\u51fa\u3059\u308b\u4e0a\u3067\u6975\u3081\u3066\u91cd\u8981\u3067\u3059\u3002\u3057\u304b\u3057\u3001\u5358\u306b\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u6570\u5024\u3092\u898b\u308b\u3060\u3051\u3067\u306f\u306a\u304f\u3001\u73fe\u5b9f\u7684\u306a\u61f8\u5ff5\u3084\u30c7\u30fc\u30bf\u306e\u7279\u6027\u3001\u7406\u8ad6\u7684\u80cc\u666f\u3092\u8003\u616e\u306b\u5165\u308c\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u89e3\u91c8\u306e\u5177\u4f53\u7684\u306a\u30dd\u30a4\u30f3\u30c8\u3001\u6f5c\u5728\u7684\u306a\u554f\u984c\u70b9\u3001\u304a\u3088\u3073\u5b9f\u4f8b\u3092\u4ea4\u3048\u306a\u304c\u3089\u8a73\u3057\u304f\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#551","title":"5.5.1 \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u89e3\u91c8","text":"<ul> <li>\u9ad8\u3044\u8ca0\u8377\u91cf\u3068\u4f4e\u3044\u8ca0\u8377\u91cf\u306e\u5224\u65ad:   \u4e00\u822c\u306b\u30010.70\u4ee5\u4e0a\u306e\u8ca0\u8377\u91cf\u306f\u300c\u9ad8\u3044\u300d\u3068\u898b\u306a\u3055\u308c\u30010.30\u672a\u6e80\u306e\u5024\u306f\u5f31\u3044\u3068\u5224\u65ad\u3055\u308c\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u3053\u308c\u3089\u306e\u95be\u5024\u306f\u30c7\u30fc\u30bf\u306e\u7a2e\u985e\u3084\u7814\u7a76\u5206\u91ce\u306b\u3088\u3063\u3066\u7570\u306a\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\u3002  </li> <li> <p>\u4f8b:     \u5fc3\u7406\u5b66\u7684\u691c\u67fb\u3067\u3001\u3042\u308b\u9805\u76ee\u304c\u300c\u8a18\u61b6\u529b\u56e0\u5b50\u300d\u306b\u5bfe\u3057\u30660.80\u306e\u8ca0\u8377\u91cf\u3092\u793a\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u305d\u306e\u9805\u76ee\u306f\u8a18\u61b6\u529b\u306b\u5927\u304d\u304f\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u3068\u5224\u65ad\u3055\u308c\u307e\u3059\u3002\u4e00\u65b9\u3001\u540c\u3058\u9805\u76ee\u304c\u300c\u6ce8\u610f\u529b\u56e0\u5b50\u300d\u306b\u5bfe\u3057\u30660.25\u306e\u8ca0\u8377\u91cf\u3057\u304b\u793a\u3055\u306a\u3044\u306a\u3089\u3001\u8a18\u61b6\u529b\u304c\u4e3b\u8981\u306a\u8981\u56e0\u3068\u89e3\u91c8\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u8907\u6570\u56e0\u5b50\u306b\u4e2d\u7a0b\u5ea6\u306e\u8ca0\u8377:   \u3042\u308b\u5909\u6570\u304c\u8907\u6570\u306e\u56e0\u5b50\u306b\u4e2d\u7a0b\u5ea6\u306e\u8ca0\u8377\u3092\u6301\u3064\u5834\u5408\u3001\u305d\u306e\u5909\u6570\u306f\u4e00\u3064\u306e\u660e\u78ba\u306a\u6982\u5ff5\u306b\u7d5e\u308a\u306b\u304f\u3044\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u3053\u3046\u3057\u305f\u5834\u5408\u3001\u9805\u76ee\u81ea\u4f53\u306e\u5185\u5bb9\u3084\u7406\u8ad6\u80cc\u666f\u3092\u518d\u8a55\u4fa1\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002  </p> </li> <li>\u61f8\u5ff5:     \u4f8b\u3048\u3070\u3001\u30a2\u30f3\u30b1\u30fc\u30c8\u9805\u76ee\u304c\u300c\u30b5\u30fc\u30d3\u30b9\u306e\u8cea\u300d\u3068\u300c\u4fa1\u683c\u300d\u306b\u540c\u7a0b\u5ea6\uff08\u4f8b\u3048\u3070\u305d\u308c\u305e\u308c0.45\uff09\u306e\u8ca0\u8377\u3092\u793a\u3059\u5834\u5408\u3001\u305d\u306e\u9805\u76ee\u304c\u3069\u3061\u3089\u306e\u5074\u9762\u3092\u3088\u308a\u53cd\u6620\u3057\u3066\u3044\u308b\u304b\u304c\u4e0d\u660e\u77ad\u3067\u3059\u3002\u7d50\u679c\u3068\u3057\u3066\u3001\u56e0\u5b50\u306e\u547d\u540d\u3084\u89e3\u91c8\u306b\u6df7\u4e71\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#552","title":"5.5.2 \u56e0\u5b50\u306e\u547d\u540d\u3068\u7406\u8ad6\u7684\u691c\u8a3c","text":"<ul> <li>\u56e0\u5b50\u306e\u547d\u540d:   \u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u304b\u3089\u3001\u5404\u56e0\u5b50\u306b\u6700\u3082\u95a2\u9023\u3059\u308b\u5909\u6570\u3092\u78ba\u8a8d\u3057\u3001\u7406\u8ad6\u3084\u5148\u884c\u7814\u7a76\u306b\u57fa\u3065\u3044\u3066\u540d\u79f0\u3092\u4ed8\u3051\u307e\u3059\u3002  </li> <li> <p>\u4f8b:     \u5fc3\u7406\u5b66\u7684\u691c\u67fb\u306e\u5834\u5408\u3001\u9805\u76ee1\uff5e3\u304c\u300c\u8a18\u61b6\u300d\u306b\u5f37\u304f\u5bc4\u4e0e\u3057\u3066\u3044\u308c\u3070\u3001\u305d\u306e\u56e0\u5b50\u306f\u300c\u8a18\u61b6\u529b\u56e0\u5b50\u300d\u3068\u547d\u540d\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u7406\u8ad6\u3068\u306e\u6574\u5408\u6027:   \u5f97\u3089\u308c\u305f\u56e0\u5b50\u69cb\u9020\u304c\u65e2\u5b58\u306e\u7406\u8ad6\u3084\u4eee\u8aac\u3068\u6574\u5408\u3057\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\u7406\u8ad6\u3068\u5927\u304d\u304f\u4e56\u96e2\u3059\u308b\u5834\u5408\u306f\u3001\u56e0\u5b50\u6570\u306e\u9078\u5b9a\u3084\u56de\u8ee2\u624b\u6cd5\u306e\u518d\u691c\u8a0e\u3001\u3042\u308b\u3044\u306f\u30c7\u30fc\u30bf\u81ea\u4f53\u306e\u8cea\u3084\u6e2c\u5b9a\u306e\u554f\u984c\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u5916\u90e8\u5909\u6570\u3068\u306e\u95a2\u9023:   \u56e0\u5b50\u5f97\u70b9\u3092\u7528\u3044\u3066\u3001\u5916\u90e8\u306e\u5909\u6570\uff08\u4f8b\uff1a\u696d\u7e3e\u3001\u5065\u5eb7\u6307\u6a19\u3001\u8cfc\u8cb7\u884c\u52d5\u306a\u3069\uff09\u3068\u306e\u76f8\u95a2\u3084\u56de\u5e30\u5206\u6790\u3092\u884c\u3044\u3001\u56e0\u5b50\u306e\u59a5\u5f53\u6027\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3082\u6709\u7528\u3067\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#553","title":"5.5.3 \u73fe\u5b9f\u7684\u306a\u61f8\u5ff5\u3068\u305d\u306e\u5bfe\u7b56","text":"<ul> <li>\u6e2c\u5b9a\u8aa4\u5dee\u3068\u7279\u6709\u56e0\u5b50:   \u5404\u89b3\u6e2c\u5909\u6570\u306b\u306f\u56fa\u6709\u306e\u8aa4\u5dee\uff08\u7279\u6709\u56e0\u5b50\uff09\u304c\u5b58\u5728\u3057\u307e\u3059\u3002\u8aa4\u5dee\u304c\u5927\u304d\u3044\u3068\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u304c\u5b9f\u969b\u306e\u69cb\u9020\u3092\u53cd\u6620\u3057\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002  </li> <li> <p>\u5bfe\u7b56:     \u6e2c\u5b9a\u306e\u4fe1\u983c\u6027\u3084\u59a5\u5f53\u6027\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306b\u3001\u4fe1\u983c\u6027\u4fc2\u6570\uff08\u4f8b\uff1aCronbach's \u03b1\uff09\u3084\u691c\u8a3c\u7684\u56e0\u5b50\u5206\u6790\uff08CFA\uff09\u3092\u4f75\u7528\u3059\u308b\u3002</p> </li> <li> <p>\u56e0\u5b50\u306e\u91cd\u8907\u3068\u89e3\u91c8\u306e\u4e0d\u660e\u77ad\u3055:   \u8907\u6570\u306e\u56e0\u5b50\u304c\u4f3c\u305f\u3088\u3046\u306a\u5909\u6570\u7fa4\u3092\u542b\u3080\u5834\u5408\u3001\u56e0\u5b50\u306e\u610f\u5473\u304c\u91cd\u8907\u3057\u3066\u3057\u307e\u3044\u3001\u3069\u3061\u3089\u304c\u4e3b\u8981\u306a\u69cb\u9020\u3092\u53cd\u6620\u3057\u3066\u3044\u308b\u304b\u5224\u65ad\u3057\u306b\u304f\u304f\u306a\u308a\u307e\u3059\u3002  </p> </li> <li> <p>\u5bfe\u7b56:     \u56de\u8ee2\u524d\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u6bd4\u8f03\u3057\u3001\u56e0\u5b50\u6570\u306e\u898b\u76f4\u3057\u3084\u3001\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u9805\u76ee\u306e\u524a\u9664\u30fb\u4fee\u6b63\u3092\u691c\u8a0e\u3059\u308b\u3002</p> </li> <li> <p>\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u306e\u554f\u984c:   \u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u304c\u5c0f\u3055\u3044\u5834\u5408\u3001\u56e0\u5b50\u5206\u6790\u306e\u7d50\u679c\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002  </p> </li> <li>\u5bfe\u7b56:     \u5341\u5206\u306a\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u3092\u78ba\u4fdd\u3059\u308b\u3001\u307e\u305f\u306f\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306a\u3069\u306e\u518d\u6a19\u672c\u62bd\u51fa\u6cd5\u3067\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u3092\u8a55\u4fa1\u3059\u308b\u3002</li> </ul>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#554","title":"5.5.4 \u5177\u4f53\u4f8b\u306b\u3088\u308b\u89e3\u91c8","text":""},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#1_1","title":"\u4f8b1: \u5fc3\u7406\u5b66\u7684\u691c\u67fb\u306e\u56e0\u5b50\u89e3\u91c8","text":"<p>\u3042\u308b\u77e5\u80fd\u691c\u67fb\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u304c\u5f97\u3089\u308c\u305f\u3068\u3057\u307e\u3059\u3002</p> \u9805\u76ee \u8a18\u61b6\u529b\u56e0\u5b50 \u6ce8\u610f\u529b\u56e0\u5b50 \u51e6\u7406\u901f\u5ea6\u56e0\u5b50 \u9805\u76ee1 0.82 0.25 0.10 \u9805\u76ee2 0.78 0.20 0.15 \u9805\u76ee3 0.80 0.30 0.05 \u9805\u76ee4 0.15 0.85 0.20 \u9805\u76ee5 0.10 0.80 0.25 \u9805\u76ee6 0.20 0.82 0.30 \u9805\u76ee7 0.05 0.10 0.88 \u9805\u76ee8 0.10 0.15 0.90 <ul> <li>\u89e3\u91c8: </li> <li>\u9805\u76ee1\uff5e3\u306f\u300c\u8a18\u61b6\u529b\u56e0\u5b50\u300d\u306b\u9ad8\u3044\u8ca0\u8377\u3092\u793a\u3057\u3066\u304a\u308a\u3001\u8a18\u61b6\u529b\u306e\u6e2c\u5b9a\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u3068\u89e3\u91c8\u3055\u308c\u308b\u3002  </li> <li>\u9805\u76ee4\uff5e6\u306f\u300c\u6ce8\u610f\u529b\u56e0\u5b50\u300d\u306b\u5f37\u304f\u95a2\u9023\u3057\u3066\u3044\u308b\u3002  </li> <li>\u9805\u76ee7\uff5e8\u306f\u300c\u51e6\u7406\u901f\u5ea6\u56e0\u5b50\u300d\u3092\u4e3b\u306b\u53cd\u6620\u3057\u3066\u3044\u308b\u3002  </li> <li>\u61f8\u5ff5:   \u4eee\u306b\u9805\u76ee3\u304c\u6ce8\u610f\u529b\u56e0\u5b50\u306b\u3082\u4e2d\u7a0b\u5ea6\u306e\u8ca0\u8377\u3092\u793a\u3057\u3066\u3044\u305f\u5834\u5408\u3001\u9805\u76ee3\u306f\u300c\u8a18\u61b6\u529b\u300d\u3068\u300c\u6ce8\u610f\u529b\u300d\u306e\u4e21\u65b9\u306b\u5f71\u97ff\u3055\u308c\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u3001\u9805\u76ee\u306e\u5185\u5bb9\u3084\u6e2c\u5b9a\u306e\u65b9\u6cd5\u3092\u518d\u691c\u8a0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002</li> </ul>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#2_1","title":"\u4f8b2: \u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u8abf\u67fb\u306b\u304a\u3051\u308b\u56e0\u5b50\u89e3\u91c8","text":"<p>\u6d88\u8cbb\u8005\u8abf\u67fb\u306e\u7d50\u679c\u3001\u4ee5\u4e0b\u306e\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u304c\u5f97\u3089\u308c\u305f\u3068\u3057\u307e\u3059\u3002</p> \u9805\u76ee \u54c1\u8cea\u91cd\u8996\u56e0\u5b50 \u4fa1\u683c\u91cd\u8996\u56e0\u5b50 \u88fd\u54c1\u306e\u8010\u4e45\u6027 0.90 0.20 \u88fd\u54c1\u306e\u30c7\u30b6\u30a4\u30f3 0.85 0.25 \u88fd\u54c1\u306e\u6027\u80fd 0.88 0.15 \u4fa1\u683c\u306e\u59a5\u5f53\u6027 0.30 0.82 \u5272\u5f15\u30fb\u30d7\u30ed\u30e2\u30fc\u30b7\u30e7\u30f3 0.20 0.85 \u8cfc\u5165\u5f8c\u306e\u30b5\u30fc\u30d3\u30b9 0.70 0.40 <ul> <li>\u89e3\u91c8: </li> <li>\u300c\u54c1\u8cea\u91cd\u8996\u56e0\u5b50\u300d\u306f\u3001\u88fd\u54c1\u306e\u8010\u4e45\u6027\u3001\u30c7\u30b6\u30a4\u30f3\u3001\u6027\u80fd\u306a\u3069\u306e\u9805\u76ee\u3067\u9ad8\u3044\u8ca0\u8377\u3092\u793a\u3057\u3001\u6d88\u8cbb\u8005\u304c\u54c1\u8cea\u306b\u5f37\u3044\u95a2\u5fc3\u3092\u6301\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u793a\u5506\u3059\u308b\u3002  </li> <li>\u300c\u4fa1\u683c\u91cd\u8996\u56e0\u5b50\u300d\u306f\u3001\u4fa1\u683c\u306e\u59a5\u5f53\u6027\u3084\u5272\u5f15\u306e\u6709\u7121\u306b\u5bfe\u3057\u3066\u9ad8\u3044\u8ca0\u8377\u3092\u793a\u3057\u3001\u6d88\u8cbb\u8005\u306e\u4fa1\u683c\u306b\u5bfe\u3059\u308b\u654f\u611f\u3055\u3092\u53cd\u6620\u3057\u3066\u3044\u308b\u3002  </li> <li>\u61f8\u5ff5:   \u8cfc\u5165\u5f8c\u306e\u30b5\u30fc\u30d3\u30b9\u304c\u4e21\u56e0\u5b50\u306b\u4e2d\u7a0b\u5ea6\u306e\u8ca0\u8377\u3092\u793a\u3059\u5834\u5408\u3001\u6d88\u8cbb\u8005\u304c\u54c1\u8cea\u3068\u4fa1\u683c\u306e\u4e21\u65b9\u3092\u8003\u616e\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u3001\u3053\u306e\u9805\u76ee\u306e\u89e3\u91c8\u306b\u306f\u614e\u91cd\u306a\u691c\u8a0e\u304c\u5fc5\u8981\u3068\u306a\u308b\u3002</li> </ul>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#555","title":"5.5.5 \u89e3\u91c8\u30d7\u30ed\u30bb\u30b9\u306e\u30dd\u30a4\u30f3\u30c8","text":"<ol> <li> <p>\u6570\u5024\u3060\u3051\u3067\u306a\u304f\u5185\u5bb9\u3092\u78ba\u8a8d\u3059\u308b:    \u5358\u306b\u8ca0\u8377\u91cf\u306e\u5927\u304d\u3055\u3060\u3051\u3067\u306a\u304f\u3001\u5404\u9805\u76ee\u306e\u5185\u5bb9\u3084\u80cc\u666f\u77e5\u8b58\u3082\u8003\u616e\u306b\u5165\u308c\u3066\u3001\u56e0\u5b50\u306e\u610f\u5473\u4ed8\u3051\u3092\u884c\u3044\u307e\u3059\u3002</p> </li> <li> <p>\u8907\u6570\u306e\u6307\u6a19\u3092\u7528\u3044\u308b:    \u56e0\u5b50\u8ca0\u8377\u91cf\u3001\u56e0\u5b50\u5f97\u70b9\u3001\u56e0\u5b50\u76f8\u95a2\u884c\u5217\u306a\u3069\u3001\u8907\u6570\u306e\u7d71\u8a08\u6307\u6a19\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u5805\u56fa\u306a\u89e3\u91c8\u3092\u8a66\u307f\u307e\u3059\u3002</p> </li> <li> <p>\u7406\u8ad6\u3068\u306e\u6574\u5408\u6027\u3092\u691c\u8a3c\u3059\u308b:    \u5f97\u3089\u308c\u305f\u56e0\u5b50\u304c\u5148\u884c\u7814\u7a76\u3084\u7406\u8ad6\u3068\u3069\u306e\u7a0b\u5ea6\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3057\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30e2\u30c7\u30eb\u306e\u898b\u76f4\u3057\u3092\u884c\u3044\u307e\u3059\u3002</p> </li> </ol>"},{"location":"research/note/factor-analysis/05-rotation-and-interpretation/#56","title":"5.6 \u307e\u3068\u3081","text":"<ul> <li> <p>\u56de\u8ee2\u306e\u610f\u7fa9:   \u56de\u8ee2\u306f\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u89e3\u91c8\u3057\u3084\u3059\u3044\u5f62\u306b\u518d\u914d\u7f6e\u3057\u3001\u5358\u7d14\u69cb\u9020\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u30b9\u30c6\u30c3\u30d7\u3067\u3059\u3002</p> </li> <li> <p>\u76f4\u4ea4\u56de\u8ee2 vs \u659c\u4ea4\u56de\u8ee2:   \u76f4\u4ea4\u56de\u8ee2\u306b\u306f Varimax\u3001Quartimax\u3001Equamax \u306a\u3069\u304c\u3042\u308a\u3001\u56e0\u5b50\u9593\u306e\u72ec\u7acb\u6027\u3092\u4fdd\u3061\u3064\u3064\u30b7\u30f3\u30d7\u30eb\u306a\u89e3\u91c8\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002   \u659c\u4ea4\u56de\u8ee2\u306b\u306f Promax\u3001Direct Oblimin\u3001Quartimin\u3001Geomin \u306a\u3069\u304c\u3042\u308a\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u53cd\u6620\u3057\u3001\u3088\u308a\u73fe\u5b9f\u7684\u306a\u69cb\u9020\u3092\u62bd\u51fa\u3059\u308b\u4e00\u65b9\u3001\u89e3\u91c8\u304c\u8907\u96d1\u306b\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u6570\u5b66\u7684\u80cc\u666f:   \u56de\u8ee2\u64cd\u4f5c\u306f \\( \\Lambda^* = \\Lambda T \\) \u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3055\u308c\u3001\u76f4\u4ea4\u56de\u8ee2\u306e\u5834\u5408\u306f \\( T^\\top T = I \\) \u3092\u3001\u659c\u4ea4\u56de\u8ee2\u306e\u5834\u5408\u306f \\( \\Phi = T^\\top T \\) \u3092\u6e80\u305f\u3059\u3053\u3068\u3067\u3001\u5404\u57fa\u6e96\u306b\u57fa\u3065\u3044\u305f\u56de\u8ee2\u304c\u5b9f\u73fe\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u89e3\u91c8:   \u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u57fa\u306b\u3001\u56e0\u5b50\u306e\u547d\u540d\u3001\u5909\u6570\u306e\u5bc4\u4e0e\u5ea6\u306e\u78ba\u8a8d\u3001\u8996\u899a\u7684\u8868\u793a\u306a\u3069\u3092\u901a\u3058\u3066\u3001\u5b9f\u52d9\u3084\u7406\u8ad6\u306b\u5373\u3057\u305f\u89e3\u91c8\u3092\u884c\u3044\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/","title":"6. \u611f\u5ea6\u5206\u6790","text":""},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#6","title":"\u7b2c6\u7ae0 \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a\u3068\u305d\u306e\u554f\u984c\u70b9\u304a\u3088\u3073\u3001\u611f\u5ea6\u5206\u6790","text":""},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#61","title":"6.1 \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a","text":""},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#611","title":"6.1.1 \u4eee\u5b9a\u306e\u610f\u5473\u3068\u80cc\u666f","text":"<p>\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a\u3068\u306f\u3001\u5404\u89b3\u6e2c\u30d9\u30af\u30c8\u30eb \\(\\mathbf{x}\\) \u304c\u3001\u5e73\u5747 \\(\\boldsymbol{\\mu}\\)\uff08\u591a\u304f\u306e\u5834\u5408\u3001\u5206\u6790\u524d\u306b\u4e2d\u5fc3\u5316\u3055\u308c\u30bc\u30ed\u3068\u4eee\u5b9a\u3055\u308c\u308b\uff09\u304a\u3088\u3073\u5206\u6563\u5171\u5206\u6563\u884c\u5217 \\(\\Sigma\\) \u3092\u6301\u3064\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u3044\u3046\u524d\u63d0\u3067\u3059\u3002 \u6570\u5b66\u7684\u306b\u306f\u3001\u6b21\u306e\u3088\u3046\u306b\u8868\u73fe\u3055\u308c\u307e\u3059\u3002</p> \\[ \\mathbf{x} \\sim N(\\boldsymbol{\\mu}, \\Sigma) \\] <p>\u3053\u306e\u4eee\u5b9a\u306e\u4e0b\u3067\u3001\u56e0\u5b50\u5206\u6790\u306e\u57fa\u672c\u30e2\u30c7\u30eb</p> \\[ \\Sigma = \\Lambda\\Lambda^\\top + \\Psi \\] <p>\u304c\u6210\u7acb\u3057\u3001\u6700\u5c24\u6cd5\u306a\u3069\u306e\u63a8\u5b9a\u624b\u6cd5\u306f\u3053\u306e\u524d\u63d0\u306b\u57fa\u3065\u3044\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u56e0\u5b50\u8ca0\u8377\u884c\u5217 \\(\\Lambda\\) \u3068\u7279\u6709\u5206\u6563 \\(\\Psi\\)\uff09\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#612","title":"6.1.2 \u975e\u6b63\u898f\u6027\u304c\u7591\u308f\u308c\u308b\u5177\u4f53\u7684\u306a\u30b1\u30fc\u30b9","text":"<p>\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5177\u4f53\u7684\u306a\u30b1\u30fc\u30b9\u3067\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a\u304c\u6210\u7acb\u3057\u306a\u3044\u3053\u3068\u304c\u3088\u304f\u3042\u308a\u307e\u3059\u3002</p> <ul> <li> <p>\u5fc3\u7406\u5b66\u7684\u691c\u67fb\u30c7\u30fc\u30bf:   \u4f8b\u3048\u3070\u3001\u6027\u683c\u3084\u611f\u60c5\u3092\u6e2c\u5b9a\u3059\u308b\u305f\u3081\u306e\u30ea\u30c3\u30ab\u30fc\u30c8\u5c3a\u5ea6\u3067\u306f\u3001\u56de\u7b54\u304c\u300c\u975e\u5e38\u306b\u5f53\u3066\u306f\u307e\u308b\u300d\u3084\u300c\u5168\u304f\u5f53\u3066\u306f\u307e\u3089\u306a\u3044\u300d\u306b\u504f\u308b\u3053\u3068\u304c\u591a\u304f\u3001\u5206\u5e03\u304c\u5de6\u53f3\u975e\u5bfe\u79f0\uff08\u6b6a\u5ea6\u304c\u5927\u304d\u3044\uff09\u3068\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u307b\u3068\u3093\u3069\u306e\u56de\u7b54\u304c\u4e2d\u592e\u4ed8\u8fd1\u306b\u96c6\u4e2d\u3057\u3001\u88fe\u304c\u8efd\u3044\u5834\u5408\u3082\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u8abf\u67fb\u30c7\u30fc\u30bf:   \u6d88\u8cbb\u8005\u306e\u8cfc\u8cb7\u983b\u5ea6\u3084\u88fd\u54c1\u8a55\u4fa1\uff08\u4f8b\u3048\u3070\u30010\uff5e10\u306e\u30b9\u30b1\u30fc\u30eb\uff09\u306b\u304a\u3044\u3066\u3001\u7279\u5b9a\u306e\u8a55\u4fa1\u306b\u6975\u7aef\u306b\u504f\u3063\u305f\u308a\u3001\u975e\u5e38\u306b\u9ad8\u3044\u307e\u305f\u306f\u4f4e\u3044\u8a55\u4fa1\u304c\u6563\u898b\u3055\u308c\u305f\u308a\u3059\u308b\u5834\u5408\u3001\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a\u304c\u7591\u308f\u308c\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u4fa1\u683c\u306b\u5bfe\u3057\u3066\u975e\u5e38\u306b\u654f\u611f\u306a\u6d88\u8cbb\u8005\u304c\u3054\u304f\u4e00\u90e8\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u30c7\u30fc\u30bf\u304c\u53f3\u5074\u306b\u9577\u3044\u5c3e\u3092\u6301\u3064\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u533b\u7642\u30fb\u751f\u7269\u5b66\u7684\u30c7\u30fc\u30bf:   \u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u3084\u8840\u6db2\u691c\u67fb\u306e\u5024\u306f\u3001\u6b63\u5e38\u5024\u306e\u7bc4\u56f2\u306b\u96c6\u4e2d\u3059\u308b\u4e00\u65b9\u3067\u3001\u4e00\u90e8\u306e\u7570\u5e38\u306a\u5024\u304c\u6975\u7aef\u306b\u9ad8\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u7279\u5b9a\u306e\u9175\u7d20\u306e\u5024\u304c\u307b\u3068\u3093\u3069\u306e\u88ab\u9a13\u8005\u3067\u4f4e\u3044\u5024\u306b\u96c6\u4e2d\u3057\u3001\u4e00\u90e8\u3067\u7570\u5e38\u306b\u9ad8\u3044\u5024\u304c\u89b3\u6e2c\u3055\u308c\u308b\u3068\u3001\u5206\u5e03\u304c\u53f3\u306b\u6b6a\u3080\u3053\u3068\u304c\u3042\u308a\u5f97\u307e\u3059\u3002</p> </li> <li> <p>\u9806\u5e8f\u5c3a\u5ea6\u3084\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u30c7\u30fc\u30bf:   \u30ea\u30c3\u30ab\u30fc\u30c8\u5c3a\u5ea6\u306e\u3088\u3046\u306a\u9806\u5e8f\u30c7\u30fc\u30bf\u306f\u3001\u96e2\u6563\u7684\u306a\u5024\u3057\u304b\u53d6\u308a\u5f97\u305a\u3001\u9023\u7d9a\u7684\u306a\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a\u304b\u3089\u5927\u304d\u304f\u9038\u8131\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#62","title":"6.2 \u975e\u6b63\u898f\u6027\u304c\u3082\u305f\u3089\u3059\u5f71\u97ff\u3068\u305d\u306e\u5bfe\u51e6\u6cd5","text":""},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#621","title":"6.2.1 \u63a8\u5b9a\u7d50\u679c\u3078\u306e\u5f71\u97ff","text":"<p>\u975e\u6b63\u898f\u6027\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5f71\u97ff\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u3002</p> <ul> <li> <p>\u30d1\u30e9\u30e1\u30fc\u30bf\u63a8\u5b9a\u306e\u504f\u308a:   \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a\u306b\u57fa\u3065\u304f\u6700\u5c24\u6cd5\u3067\u306f\u3001\u6b63\u898f\u6027\u304c\u6210\u7acb\u3057\u3066\u3044\u308b\u5834\u5408\u306b\u52b9\u7387\u7684\u3067\u4e00\u8cab\u3057\u305f\u63a8\u5b9a\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u975e\u6b63\u898f\u6027\u304c\u5b58\u5728\u3059\u308b\u3068\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u3084\u7279\u6709\u5206\u6563\u306e\u63a8\u5b9a\u306b\u30d0\u30a4\u30a2\u30b9\u304c\u751f\u3058\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 \u4f8b: \u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u30c7\u30fc\u30bf\u306b\u304a\u3044\u3066\u3001\u53f3\u306b\u9577\u3044\u5c3e\uff08\u30a2\u30a6\u30c8\u30e9\u30a4\u30e4\u30fc\uff09\u306e\u5b58\u5728\u304c\u5e73\u5747\u3084\u5206\u6563\u3092\u5f15\u304d\u4e0a\u3052\u3001\u56e0\u5b50\u69cb\u9020\u304c\u5b9f\u969b\u3088\u308a\u3082\u5f37\u8abf\u3055\u308c\u308b\u3053\u3068\u304c\u3042\u308b\u3002</p> </li> <li> <p>\u691c\u5b9a\u7d71\u8a08\u91cf\u306e\u4fe1\u983c\u6027\u4f4e\u4e0b:   \u30ab\u30a4\u4e8c\u4e57\u691c\u5b9a\u306a\u3069\u3001\u30e2\u30c7\u30eb\u9069\u5408\u5ea6\u306e\u691c\u5b9a\u306f\u6b63\u898f\u6027\u306e\u4eee\u5b9a\u306b\u4f9d\u5b58\u3057\u3066\u3044\u307e\u3059\u3002\u975e\u6b63\u898f\u6027\u304c\u5f37\u3044\u3068\u3001\u3053\u308c\u3089\u306e\u691c\u5b9a\u7d50\u679c\u304c\u5b9f\u969b\u306e\u9069\u5408\u5ea6\u3092\u6b63\u3057\u304f\u53cd\u6620\u3057\u306a\u304f\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#622","title":"6.2.2 \u975e\u6b63\u898f\u6027\u306b\u5bfe\u3059\u308b\u5bfe\u51e6\u6cd5","text":"<p>\u975e\u6b63\u898f\u6027\u306e\u5f71\u97ff\u3092\u8efd\u6e1b\u3059\u308b\u305f\u3081\u306b\u3001\u6b21\u306e\u3088\u3046\u306a\u65b9\u6cd5\u304c\u7528\u3044\u3089\u308c\u307e\u3059\u3002</p> <ol> <li>\u30ed\u30d0\u30b9\u30c8\u63a8\u5b9a\u6cd5\u306e\u63a1\u7528: </li> <li> <p>\u30ed\u30d0\u30b9\u30c8\u6700\u5c24\u6cd5 (Robust ML) \u3084 Satorra-Bentler \u88dc\u6b63:      \u3053\u308c\u3089\u306f\u6b63\u898f\u6027\u306e\u9038\u8131\u3092\u88dc\u6b63\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u3067\u3001\u6a19\u6e96\u8aa4\u5dee\u3084\u691c\u5b9a\u7d71\u8a08\u91cf\u3092\u88dc\u6b63\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u3001\u3088\u308a\u4fe1\u983c\u6027\u306e\u3042\u308b\u7d50\u679c\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002 \u5b9f\u4f8b: \u5fc3\u7406\u5b66\u306e\u5927\u898f\u6a21\u306a\u30a2\u30f3\u30b1\u30fc\u30c8\u8abf\u67fb\u3067\u3001\u56de\u7b54\u306e\u5206\u5e03\u304c\u8457\u3057\u304f\u6b6a\u3093\u3067\u3044\u308b\u5834\u5408\u3001Satorra-Bentler \u88dc\u6b63\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a\u306b\u7528\u3044\u3089\u308c\u308b\u30ab\u30a4\u4e8c\u4e57\u691c\u5b9a\u306e\u4fe1\u983c\u6027\u3092\u5411\u4e0a\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u5909\u63db: </p> </li> <li> <p>\u5bfe\u6570\u5909\u63db\u3001\u5e73\u65b9\u6839\u5909\u63db\u3001Box-Cox\u5909\u63db:      \u3053\u308c\u3089\u306e\u5909\u63db\u306f\u3001\u30c7\u30fc\u30bf\u306e\u6b6a\u5ea6\u3084\u88fe\u306e\u91cd\u3055\u3092\u6539\u5584\u3057\u3001\u5206\u5e03\u3092\u6b63\u898f\u5206\u5e03\u306b\u8fd1\u3065\u3051\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u307e\u3059\u3002 \u5b9f\u4f8b: \u533b\u7642\u30c7\u30fc\u30bf\u3067\u3001\u53f3\u306b\u5927\u304d\u304f\u6b6a\u3093\u3060\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u5024\u306b\u5bfe\u3057\u3066\u5bfe\u6570\u5909\u63db\u3092\u884c\u3046\u3068\u3001\u30c7\u30fc\u30bf\u306e\u5bfe\u79f0\u6027\u304c\u6539\u5584\u3055\u308c\u3001\u56e0\u5b50\u5206\u6790\u306e\u524d\u63d0\u6761\u4ef6\u306b\u8fd1\u3065\u3051\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u4ee3\u66ff\u76f8\u95a2\u884c\u5217\u306e\u4f7f\u7528: </p> </li> <li> <p>\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2:      \u9806\u5e8f\u5c3a\u5ea6\u30c7\u30fc\u30bf\u306e\u5834\u5408\u3001\u30d4\u30a2\u30bd\u30f3\u76f8\u95a2\u3067\u306f\u306a\u304f\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u5b9f\u969b\u306e\u5909\u6570\u9593\u306e\u95a2\u4fc2\u3092\u3088\u308a\u6b63\u78ba\u306b\u53cd\u6620\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u5b9f\u4f8b: \u30ea\u30c3\u30ab\u30fc\u30c8\u5c3a\u5ea6\u3067\u8a55\u4fa1\u3055\u308c\u305f\u6d88\u8cbb\u8005\u306e\u610f\u898b\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3092\u8a08\u7b97\u3057\u3001\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3059\u308b\u3053\u3068\u3067\u3001\u6e2c\u5b9a\u5c3a\u5ea6\u306e\u96e2\u6563\u6027\u3092\u8003\u616e\u3057\u305f\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u5916\u308c\u5024\u306e\u51e6\u7406: </p> </li> <li>\u5916\u308c\u5024\u691c\u51fa\u3068\u9664\u5916\u3001\u307e\u305f\u306f\u9811\u5065\u306a\u7d71\u8a08\u624b\u6cd5\u306e\u63a1\u7528:      \u5916\u308c\u5024\u304c\u63a8\u5b9a\u306b\u53ca\u307c\u3059\u5f71\u97ff\u3092\u4f4e\u6e1b\u3059\u308b\u305f\u3081\u3001\u5916\u308c\u5024\u3092\u7279\u5b9a\u3057\u9664\u5916\u3059\u308b\u304b\u3001\u5916\u308c\u5024\u306b\u5bfe\u3057\u3066\u9811\u5065\u306a\u63a8\u5b9a\u65b9\u6cd5\u3092\u7528\u3044\u308b\u3053\u3068\u304c\u6709\u52b9\u3067\u3059\u3002 \u5b9f\u4f8b: \u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u8abf\u67fb\u3067\u3001\u4e00\u90e8\u306e\u6975\u7aef\u306a\u9ad8\u8a55\u4fa1\u3084\u4f4e\u8a55\u4fa1\u304c\u5168\u4f53\u306e\u63a8\u5b9a\u306b\u5f37\u304f\u5f71\u97ff\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u70b9\u3092\u691c\u51fa\u3057\u3001\u9664\u5916\u3057\u305f\u4e0a\u3067\u56e0\u5b50\u5206\u6790\u3092\u518d\u5b9f\u65bd\u3057\u3001\u7d50\u679c\u306e\u5909\u5316\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> </ol>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#63","title":"6.3 \u611f\u5ea6\u5206\u6790","text":"<p>\u611f\u5ea6\u5206\u6790\u306f\u3001\u56e0\u5b50\u5206\u6790\u306b\u304a\u3044\u3066\u524d\u63d0\u6761\u4ef6\u3084\u30c7\u30fc\u30bf\u524d\u51e6\u7406\u306e\u9055\u3044\u304c\u63a8\u5b9a\u7d50\u679c\u306b\u3069\u306e\u7a0b\u5ea6\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u304b\u3092\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u624b\u6cd5\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u5b9f\u969b\u306e\u5177\u4f53\u4f8b\u3092\u4ea4\u3048\u3066\u3001\u611f\u5ea6\u5206\u6790\u306e\u624b\u6cd5\u3068\u305d\u306e\u52b9\u679c\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#631","title":"6.3.1 \u611f\u5ea6\u5206\u6790\u306e\u76ee\u7684","text":"<ul> <li> <p>\u30e2\u30c7\u30eb\u4eee\u5b9a\u306e\u691c\u8a3c:   \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a\u3084\u305d\u306e\u4ed6\u306e\u524d\u63d0\u6761\u4ef6\u304c\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u3084\u56e0\u5b50\u5f97\u70b9\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u63a8\u5b9a\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u8a55\u4fa1:   \u30b5\u30f3\u30d7\u30eb\u306e\u4e00\u90e8\u3092\u9664\u5916\u3001\u307e\u305f\u306f\u7570\u306a\u308b\u30c7\u30fc\u30bf\u524d\u51e6\u7406\u65b9\u6cd5\u3092\u9069\u7528\u3057\u305f\u5834\u5408\u306b\u3001\u63a8\u5b9a\u7d50\u679c\u304c\u3069\u306e\u7a0b\u5ea6\u5909\u52d5\u3059\u308b\u304b\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#632","title":"6.3.2 \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306b\u3088\u308b\u8a55\u4fa1","text":"<p>\u5177\u4f53\u4f8b\uff1a\u5fc3\u7406\u5b66\u7684\u691c\u67fb\u30c7\u30fc\u30bf\u306e\u5834\u5408 \u3042\u308b\u5fc3\u7406\u5b66\u7684\u691c\u67fb\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5bfe\u3057\u3066\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3057\u3001\u521d\u671f\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u304c\u5f97\u3089\u308c\u305f\u3068\u3057\u307e\u3059\u3002\u6b21\u306e\u624b\u9806\u3067\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3092\u9069\u7528\u3057\u307e\u3059\u3002</p> <ol> <li> <p>\u518d\u6a19\u672c\u62bd\u51fa:    \u30c7\u30fc\u30bf\u5168\u4f53\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u518d\u6a19\u672c\uff08\u4f8b\u3048\u30701000\u56de\uff09\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002\u5404\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u30b5\u30f3\u30d7\u30eb\u306f\u3001\u5143\u306e\u30c7\u30fc\u30bf\u3068\u540c\u3058\u30b5\u30a4\u30ba\u306b\u8a2d\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u65bd:    \u5404\u518d\u6a19\u672c\u306b\u3064\u3044\u3066\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3057\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u3084\u56e0\u5b50\u5f97\u70b9\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u4fe1\u983c\u533a\u9593\u306e\u7b97\u51fa:    \u5404\u5909\u6570\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u306b\u3064\u3044\u3066\u3001\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u30b5\u30f3\u30d7\u30eb\u304b\u3089\u5f97\u3089\u308c\u305f\u63a8\u5b9a\u5024\u306e95%\u4fe1\u983c\u533a\u9593\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> </li> </ol> <p>\u89e3\u91c8: - \u3082\u3057\u3001\u5143\u306e\u63a8\u5b9a\u5024\u304c\u3053\u308c\u3089\u306e\u4fe1\u983c\u533a\u9593\u5185\u306b\u53ce\u307e\u3063\u3066\u3044\u308c\u3070\u3001\u7d50\u679c\u306e\u30ed\u30d0\u30b9\u30c8\u6027\u304c\u9ad8\u3044\u3068\u5224\u65ad\u3067\u304d\u307e\u3059\u3002 - \u9006\u306b\u3001\u4fe1\u983c\u533a\u9593\u304c\u975e\u5e38\u306b\u5e83\u3044\u5834\u5408\u306f\u3001\u63a8\u5b9a\u7d50\u679c\u304c\u30b5\u30f3\u30d7\u30eb\u306e\u5909\u52d5\u306b\u654f\u611f\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u304a\u308a\u3001\u7d50\u679c\u306e\u4fe1\u983c\u6027\u306b\u7591\u554f\u304c\u751f\u3058\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#633","title":"6.3.3 \u30b5\u30f3\u30d7\u30eb\u5206\u5272\u6cd5\u306b\u3088\u308b\u691c\u8a3c","text":"<p>\u5177\u4f53\u4f8b\uff1a\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u8abf\u67fb\u30c7\u30fc\u30bf\u306e\u5834\u5408 \u6d88\u8cbb\u8005\u30a2\u30f3\u30b1\u30fc\u30c8\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3059\u308b\u969b\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u30b5\u30f3\u30d7\u30eb\u5206\u5272\u6cd5\u3092\u9069\u7528\u3057\u307e\u3059\u3002</p> <ol> <li> <p>\u30c7\u30fc\u30bf\u306e\u5206\u5272:    \u5168\u4f53\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u3001\u4f8b\u3048\u3070\u5730\u57df\u5225\u3084\u5e74\u4ee3\u5225\u306b\u5206\u5272\u3057\u307e\u3059\uff08\u4f8b\uff1a\u90fd\u5e02\u90e8\u3068\u5730\u65b9\u3001\u307e\u305f\u306f20\u4ee3\u306850\u4ee3\uff09\u3002</p> </li> <li> <p>\u500b\u5225\u306e\u56e0\u5b50\u5206\u6790:    \u5404\u30b5\u30d6\u30b5\u30f3\u30d7\u30eb\u306b\u5bfe\u3057\u3066\u72ec\u7acb\u306b\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3057\u3001\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3084\u56e0\u5b50\u5f97\u70b9\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u6bd4\u8f03:    \u5404\u30b5\u30d6\u30b5\u30f3\u30d7\u30eb\u3067\u5f97\u3089\u308c\u305f\u56e0\u5b50\u69cb\u9020\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u3001\u56e0\u5b50\u306e\u547d\u540d\u3084\u89e3\u91c8\u304c\u540c\u69d8\u3067\u3042\u308b\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> </li> </ol> <p>\u89e3\u91c8: - \u3082\u3057\u7570\u306a\u308b\u30b5\u30d6\u30b5\u30f3\u30d7\u30eb\u3067\u985e\u4f3c\u3057\u305f\u56e0\u5b50\u69cb\u9020\u304c\u78ba\u8a8d\u3067\u304d\u308c\u3070\u3001\u5168\u4f53\u306e\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u304c\u652f\u6301\u3055\u308c\u307e\u3059\u3002 - \u9006\u306b\u3001\u30b5\u30d6\u30b5\u30f3\u30d7\u30eb\u3054\u3068\u306b\u5927\u304d\u304f\u7570\u306a\u308b\u69cb\u9020\u304c\u73fe\u308c\u308b\u5834\u5408\u306f\u3001\u30b5\u30f3\u30d7\u30eb\u306e\u7279\u6027\u3084\u5916\u90e8\u8981\u56e0\u304c\u56e0\u5b50\u69cb\u9020\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u3001\u3055\u3089\u306a\u308b\u691c\u8a0e\u304c\u5fc5\u8981\u3067\u3059\u3002</p>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#634","title":"6.3.4 \u524d\u51e6\u7406\u306e\u5909\u66f4\u306b\u3088\u308b\u611f\u5ea6\u5206\u6790","text":"<p>\u5177\u4f53\u4f8b\uff1a\u533b\u7642\u30c7\u30fc\u30bf\u306b\u304a\u3051\u308b\u5909\u6570\u5909\u63db\u306e\u5f71\u97ff \u533b\u7642\u30c7\u30fc\u30bf\u306e\u4f8b\u3067\u306f\u3001\u3042\u308b\u9175\u7d20\u306e\u5024\u304c\u53f3\u306b\u6b6a\u3093\u3060\u5206\u5e03\u3092\u793a\u3057\u3066\u3044\u308b\u3068\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u624b\u6cd5\u3067\u524d\u51e6\u7406\u306e\u5f71\u97ff\u3092\u691c\u8a3c\u3057\u307e\u3059\u3002</p> <ol> <li> <p>\u5909\u6570\u5909\u63db\u524d\u306e\u56e0\u5b50\u5206\u6790:    \u751f\u306e\u9175\u7d20\u5024\u3092\u7528\u3044\u3066\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3057\u3001\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u304a\u3088\u3073\u56e0\u5b50\u5f97\u70b9\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u5909\u6570\u5909\u63db\u5f8c\u306e\u56e0\u5b50\u5206\u6790:    \u5bfe\u6570\u5909\u63db\u3084Box-Cox\u5909\u63db\u3092\u884c\u3063\u305f\u5f8c\u3001\u540c\u3058\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u6bd4\u8f03:    \u5909\u63db\u524d\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3084\u56e0\u5b50\u5f97\u70b9\u3001\u3055\u3089\u306b\u306f\u56e0\u5b50\u306e\u89e3\u91c8\u306b\u3069\u306e\u3088\u3046\u306a\u9055\u3044\u304c\u3042\u308b\u304b\u3092\u6bd4\u8f03\u3057\u307e\u3059\u3002</p> </li> </ol> <p>\u89e3\u91c8: - \u5909\u63db\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u304c\u3088\u308a\u6975\u6027\u3092\u660e\u78ba\u306b\u793a\u3057\u3001\u89e3\u91c8\u3057\u3084\u3059\u3044\u5358\u7d14\u69cb\u9020\u304c\u5f97\u3089\u308c\u305f\u5834\u5408\u3001\u5909\u6570\u5909\u63db\u304c\u6709\u52b9\u3067\u3042\u3063\u305f\u3068\u5224\u65ad\u3067\u304d\u307e\u3059\u3002 - \u9006\u306b\u3001\u5927\u304d\u306a\u9055\u3044\u304c\u751f\u3058\u305a\u3001\u7d50\u679c\u304c\u307b\u307c\u540c\u4e00\u3067\u3042\u308c\u3070\u3001\u5143\u306e\u30c7\u30fc\u30bf\u306e\u975e\u6b63\u898f\u6027\u304c\u56e0\u5b50\u5206\u6790\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u306f\u9650\u5b9a\u7684\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#635","title":"6.3.5 \u7570\u306a\u308b\u63a8\u5b9a\u6cd5\u306e\u6bd4\u8f03","text":"<p>\u5177\u4f53\u4f8b\uff1a\u540c\u4e00\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u6700\u5c24\u6cd5\u3068\u4e3b\u8ef8\u56e0\u5b50\u6cd5\u306e\u6bd4\u8f03 \u540c\u3058\u5fc3\u7406\u5b66\u7684\u691c\u67fb\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u6b21\u306e\u624b\u9806\u3067\u7570\u306a\u308b\u63a8\u5b9a\u6cd5\u306e\u7d50\u679c\u3092\u6bd4\u8f03\u3057\u307e\u3059\u3002</p> <ol> <li> <p>\u6700\u5c24\u6cd5\u306b\u3088\u308b\u56e0\u5b50\u5206\u6790:    \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a\u306b\u57fa\u3065\u304d\u3001\u6700\u5c24\u6cd5\u3092\u7528\u3044\u3066\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u4e3b\u8ef8\u56e0\u5b50\u6cd5\u306b\u3088\u308b\u56e0\u5b50\u5206\u6790:    \u5225\u306e\u624b\u6cd5\u3068\u3057\u3066\u3001\u4e3b\u8ef8\u56e0\u5b50\u6cd5\u3092\u7528\u3044\u3066\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u6bd4\u8f03:    \u4e21\u624b\u6cd5\u3067\u5f97\u3089\u308c\u305f\u56e0\u5b50\u69cb\u9020\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u3001\u304a\u3088\u3073\u56e0\u5b50\u5f97\u70b9\u306e\u985e\u4f3c\u6027\u3084\u9055\u3044\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p> </li> </ol> <p>\u89e3\u91c8: - \u4e21\u624b\u6cd5\u3067\u540c\u69d8\u306e\u56e0\u5b50\u69cb\u9020\u304c\u5f97\u3089\u308c\u308b\u5834\u5408\u3001\u6b63\u898f\u6027\u306e\u4eee\u5b9a\u306b\u4f9d\u5b58\u3057\u306a\u3044\u5805\u7262\u306a\u7d50\u679c\u3068\u5224\u65ad\u3067\u304d\u307e\u3059\u3002 - \u3082\u3057\u5927\u304d\u304f\u7570\u306a\u308b\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\u5834\u5408\u3001\u3069\u3061\u3089\u306e\u63a8\u5b9a\u6cd5\u304c\u30c7\u30fc\u30bf\u306e\u7279\u6027\u306b\u9069\u3057\u3066\u3044\u308b\u304b\u3001\u518d\u691c\u8a0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/06-sensitivity-analysis/#64","title":"6.4 \u307e\u3068\u3081","text":"<ul> <li> <p>\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u4eee\u5b9a:   \u56e0\u5b50\u5206\u6790\u3067\u306f\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u3044\u3046\u524d\u63d0\u304c\u57fa\u672c\u3067\u3059\u304c\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\uff08\u5fc3\u7406\u5b66\u3001\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u3001\u533b\u7642\u306a\u3069\uff09\u3067\u306f\u975e\u6b63\u898f\u6027\u304c\u983b\u7e41\u306b\u8a8d\u3081\u3089\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u975e\u6b63\u898f\u6027\u306e\u5f71\u97ff:   \u975e\u6b63\u898f\u6027\u306f\u3001\u63a8\u5b9a\u7d50\u679c\u306e\u30d0\u30a4\u30a2\u30b9\u3084\u691c\u5b9a\u7d71\u8a08\u91cf\u306e\u8aa4\u5dee\u3092\u62db\u304f\u305f\u3081\u3001\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002</p> </li> <li> <p>\u5bfe\u51e6\u6cd5:   \u30ed\u30d0\u30b9\u30c8\u63a8\u5b9a\u6cd5\u3001\u30c7\u30fc\u30bf\u5909\u63db\u3001\u4ee3\u66ff\u76f8\u95a2\u884c\u5217\u306e\u5229\u7528\u3001\u5916\u308c\u5024\u51e6\u7406\u306a\u3069\u306e\u65b9\u6cd5\u3067\u3001\u975e\u6b63\u898f\u6027\u306e\u5f71\u97ff\u3092\u8efd\u6e1b\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u611f\u5ea6\u5206\u6790:   \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3001\u30b5\u30f3\u30d7\u30eb\u5206\u5272\u6cd5\u3001\u524d\u51e6\u7406\u306e\u5909\u66f4\u3001\u7570\u306a\u308b\u63a8\u5b9a\u6cd5\u306e\u6bd4\u8f03\u306a\u3069\u3001\u5177\u4f53\u7684\u306a\u624b\u6cd5\u3092\u7528\u3044\u3066\u3001\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u3084\u30ed\u30d0\u30b9\u30c8\u6027\u3092\u8a55\u4fa1\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/","title":"7. \u56e0\u5b50\u5206\u6790\u306e\u624b\u9806","text":""},{"location":"research/note/factor-analysis/07-analysis-step/#7","title":"\u7b2c7\u7ae0 \u56e0\u5b50\u5206\u6790\u5b9f\u8df5\u306e\u30b9\u30c6\u30c3\u30d7\uff1a\u5168\u4f53\u306e\u6d41\u308c\u3068\u5404\u6bb5\u968e\u306e\u6ce8\u610f\u70b9","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u969b\u306b\u884c\u3046\u969b\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u3001\u30c7\u30fc\u30bf\u53ce\u96c6\u304b\u3089\u6700\u7d42\u7684\u306a\u89e3\u91c8\u306b\u81f3\u308b\u307e\u3067\u3001\u9806\u3092\u8ffd\u3063\u3066\u8a73\u7d30\u306b\u89e3\u8aac\u3057\u307e\u3059\u3002\u5404\u30b9\u30c6\u30c3\u30d7\u3067\u6ce8\u610f\u3059\u3079\u304d\u70b9\u3084\u3001\u524d\u63d0\u6761\u4ef6\u306e\u78ba\u8a8d\u3001\u89e3\u6790\u7d50\u679c\u306e\u30ed\u30d0\u30b9\u30c8\u6027\u3092\u78ba\u4fdd\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u306b\u3064\u3044\u3066\u3082\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/07-analysis-step/#71","title":"7.1 \u5168\u4f53\u306e\u6d41\u308c","text":"<p>\u56e0\u5b50\u5206\u6790\u306e\u30d7\u30ed\u30bb\u30b9\u306f\u5927\u304d\u304f\u4ee5\u4e0b\u306e\u6bb5\u968e\u306b\u5206\u3051\u3089\u308c\u307e\u3059\u3002</p> <ol> <li>\u30c7\u30fc\u30bf\u53ce\u96c6\u3068\u524d\u51e6\u7406 </li> <li>\u30c7\u30fc\u30bf\u306e\u53ce\u96c6\u65b9\u6cd5  </li> <li>\u6b20\u640d\u5024\u30fb\u5916\u308c\u5024\u306e\u51e6\u7406  </li> <li> <p>\u6a19\u6e96\u5316\u306a\u3069\u306e\u524d\u51e6\u7406</p> </li> <li> <p>\u56e0\u5b50\u62bd\u51fa\uff08\u56e0\u5b50\u5206\u6790\uff09 </p> </li> <li>\u30e2\u30c7\u30eb\u306e\u5b9a\u5f0f\u5316  </li> <li>\u9069\u5207\u306a\u56e0\u5b50\u62bd\u51fa\u6cd5\uff08\u6700\u5c24\u6cd5\u3001\u4e3b\u8ef8\u56e0\u5b50\u6cd5\u3001\u6700\u5c0f\u6b8b\u5dee\u6cd5\u306a\u3069\uff09\u306e\u9078\u629e  </li> <li> <p>\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a</p> </li> <li> <p>\u56de\u8ee2 </p> </li> <li>\u76f4\u4ea4\u56de\u8ee2\u3068\u659c\u4ea4\u56de\u8ee2\u306e\u9078\u629e  </li> <li>\u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u78ba\u8a8d  </li> <li> <p>\u56de\u8ee2\u884c\u5217\u306e\u6570\u5b66\u7684\u80cc\u666f\u306e\u7406\u89e3</p> </li> <li> <p>\u975e\u6b63\u898f\u6027\u3078\u306e\u5bfe\u51e6 </p> </li> <li>\u5206\u5e03\u306e\u691c\u8a0e\uff08\u6b6a\u5ea6\u3001\u88fe\u306e\u91cd\u3055\uff09  </li> <li> <p>\u30ed\u30d0\u30b9\u30c8\u63a8\u5b9a\u3084\u30c7\u30fc\u30bf\u5909\u63db\u306e\u5b9f\u65bd</p> </li> <li> <p>\u611f\u5ea6\u5206\u6790 </p> </li> <li> <p>\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u3001\u30b5\u30f3\u30d7\u30eb\u5206\u5272\u3001\u524d\u51e6\u7406\u5909\u66f4\u306a\u3069\u306b\u3088\u308b\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u306e\u8a55\u4fa1</p> </li> <li> <p>\u89e3\u91c8\u3068\u5831\u544a </p> </li> <li>\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u610f\u5473\u4ed8\u3051  </li> <li>\u56e0\u5b50\u306e\u547d\u540d\u3068\u7406\u8ad6\u3068\u306e\u6574\u5408\u6027\u306e\u691c\u8a3c  </li> <li>\u7d50\u679c\u306e\u8996\u899a\u5316\uff08\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3001\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306a\u3069\uff09</li> </ol>"},{"location":"research/note/factor-analysis/07-analysis-step/#72","title":"7.2 \u30c7\u30fc\u30bf\u53ce\u96c6\u3068\u524d\u51e6\u7406","text":""},{"location":"research/note/factor-analysis/07-analysis-step/#721","title":"7.2.1 \u30c7\u30fc\u30bf\u53ce\u96c6","text":"<ul> <li> <p>\u76ee\u7684\u306b\u5fdc\u3058\u305f\u30c7\u30fc\u30bf\u9078\u5b9a:   \u5206\u6790\u5bfe\u8c61\u3068\u306a\u308b\u5909\u6570\u306f\u3001\u7406\u8ad6\u7684\u80cc\u666f\u3084\u5148\u884c\u7814\u7a76\u306b\u57fa\u3065\u3044\u3066\u6c7a\u5b9a\u3057\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u5fc3\u7406\u5b66\u3067\u306f\u77e5\u80fd\u3084\u6027\u683c\u3001\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u3067\u306f\u6d88\u8cbb\u8005\u884c\u52d5\u3084\u88fd\u54c1\u8a55\u4fa1\u306a\u3069\u304c\u5bfe\u8c61\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> <li> <p>\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba:   \u5341\u5206\u306a\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u3092\u78ba\u4fdd\u3059\u308b\u3053\u3068\u3067\u3001\u56e0\u5b50\u5206\u6790\u306e\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u304c\u5411\u4e0a\u3057\u307e\u3059\u3002\u4e00\u822c\u306b\u3001\u89b3\u6e2c\u5909\u6570\u306e\u6570\u306b\u5bfe\u3057\u3066\u5341\u5206\u306a\u30b1\u30fc\u30b9\u6570\u304c\u5fc5\u8981\u3067\u3059\uff08\u4f8b\uff1a\u30b1\u30fc\u30b9\u6570\u304c\u5909\u6570\u6570\u306e10\u500d\u4ee5\u4e0a\u306a\u3069\uff09\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#722","title":"7.2.2 \u524d\u51e6\u7406","text":"<ul> <li> <p>\u6b20\u640d\u5024\u51e6\u7406:   \u6b20\u640d\u30c7\u30fc\u30bf\u304c\u3042\u308b\u5834\u5408\u3001\u30ea\u30b9\u30c8\u30ef\u30a4\u30ba\u524a\u9664\u3001\u5e73\u5747\u88dc\u5b8c\u3001\u307e\u305f\u306f\u9ad8\u5ea6\u306a\u591a\u91cd\u4ee3\u5165\u6cd5\u306a\u3069\u3092\u691c\u8a0e\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u5916\u308c\u5024\u306e\u691c\u51fa\u3068\u51e6\u7406:   \u5916\u308c\u5024\u306f\u56e0\u5b50\u62bd\u51fa\u7d50\u679c\u306b\u5927\u304d\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u305f\u3081\u3001\u691c\u51fa\u3057\u3066\u9664\u5916\u3059\u308b\u304b\u3001\u9811\u5065\u306a\u624b\u6cd5\u3067\u5bfe\u51e6\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u6a19\u6e96\u5316:   \u5404\u5909\u6570\u304c\u7570\u306a\u308b\u30b9\u30b1\u30fc\u30eb\u3067\u6e2c\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u3001Z\u5909\u63db\u306a\u3069\u3092\u7528\u3044\u3066\u6a19\u6e96\u5316\u3057\u3001\u5909\u6570\u9593\u306e\u6bd4\u8f03\u304c\u53ef\u80fd\u306a\u72b6\u614b\u306b\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u5206\u5e03\u306e\u691c\u67fb:   \u5404\u5909\u6570\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3001Q-Q\u30d7\u30ed\u30c3\u30c8\u3001\u6b6a\u5ea6\u3001\u88fe\u306e\u91cd\u3055\u306a\u3069\u3092\u78ba\u8a8d\u3057\u3001\u975e\u6b63\u898f\u6027\u306e\u6709\u7121\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#73","title":"7.3 \u56e0\u5b50\u62bd\u51fa\uff08\u56e0\u5b50\u5206\u6790\uff09\u306e\u5b9f\u65bd","text":""},{"location":"research/note/factor-analysis/07-analysis-step/#731","title":"7.3.1 \u30e2\u30c7\u30eb\u306e\u5b9a\u5f0f\u5316","text":"<ul> <li> <p>\u57fa\u672c\u30e2\u30c7\u30eb:   \u89b3\u6e2c\u5909\u6570 $ \\mathbf{x} $ \u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u7dda\u5f62\u30e2\u30c7\u30eb\u3067\u8868\u3055\u308c\u307e\u3059\u3002   $$   \\mathbf{x} = \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}   $$   \u3053\u3053\u3067\u3001\\(\\Lambda\\) \u306f\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3001\\(\\mathbf{f}\\) \u306f\u6f5c\u5728\u56e0\u5b50\u3001\\(\\boldsymbol{\\epsilon}\\) \u306f\u7279\u6709\u56e0\u5b50\u3067\u3059\u3002</p> </li> <li> <p>\u5206\u6563\u5171\u5206\u6563\u306e\u5206\u89e3: \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) \u3068\u3044\u3046\u5f62\u3067\u3001\u5171\u901a\u56e0\u5b50\u3068\u7279\u6709\u56e0\u5b50\u306e\u5bc4\u4e0e\u304c\u5206\u89e3\u3055\u308c\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#732","title":"7.3.2 \u56e0\u5b50\u62bd\u51fa\u6cd5\u306e\u9078\u629e","text":"<ul> <li>\u6700\u5c24\u6cd5 (Maximum Likelihood):   \u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u3092\u4eee\u5b9a\u3057\u3001\u5bfe\u6570\u5c24\u5ea6\u3092\u6700\u5927\u5316\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002  </li> <li>\u4e3b\u8ef8\u56e0\u5b50\u6cd5 (Principal Axis Factoring):   \u521d\u671f\u306e\u5171\u901a\u6027\u306e\u63a8\u5b9a\u306b\u57fa\u3065\u3044\u3066\u56e0\u5b50\u8ca0\u8377\u91cf\u3092\u5c0e\u51fa\u3057\u307e\u3059\u3002  </li> <li>\u6700\u5c0f\u6b8b\u5dee\u6cd5 (Minimum Residual):   \u30b5\u30f3\u30d7\u30eb\u5171\u5206\u6563\u884c\u5217\u3068\u30e2\u30c7\u30eb\u3067\u518d\u73fe\u3055\u308c\u308b\u5171\u5206\u6563\u884c\u5217\u306e\u5dee\u3092\u6700\u5c0f\u5316\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#733","title":"7.3.3 \u56e0\u5b50\u6570\u306e\u6c7a\u5b9a","text":"<ul> <li>\u56fa\u6709\u5024\u57fa\u6e96\uff08Kaiser\u57fa\u6e96\uff09\u3084\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8:   \u56fa\u6709\u5024\u304c1\u4ee5\u4e0a\u306e\u56e0\u5b50\u6570\u3084\u3001\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306e\u6298\u308c\u66f2\u304c\u308a\u70b9\u3092\u7528\u3044\u3066\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#74","title":"7.4 \u56de\u8ee2","text":""},{"location":"research/note/factor-analysis/07-analysis-step/#741","title":"7.4.1 \u56de\u8ee2\u306e\u76ee\u7684","text":"<ul> <li>\u89e3\u91c8\u306e\u5411\u4e0a:   \u521d\u671f\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306f\u3001\u7d71\u8a08\u7684\u306b\u6700\u9069\u306a\u89e3\u3067\u3042\u3063\u3066\u3082\u89e3\u91c8\u304c\u96e3\u3057\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u56de\u8ee2\u306b\u3088\u308a\u300c\u5358\u7d14\u69cb\u9020\u300d\u3092\u5b9f\u73fe\u3057\u3001\u5404\u5909\u6570\u304c\u3069\u306e\u56e0\u5b50\u306b\u5f37\u304f\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u660e\u78ba\u306b\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#742","title":"7.4.2 \u76f4\u4ea4\u56de\u8ee2\u3068\u659c\u4ea4\u56de\u8ee2","text":"<ul> <li> <p>\u76f4\u4ea4\u56de\u8ee2:   \u56e0\u5b50\u9593\u306e\u72ec\u7acb\u6027\u3092\u4fdd\u3064\uff08\u4f8b\uff1aVarimax\uff09\u3002 \u6ce8\u610f\u70b9: \u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u7406\u8ad6\u4e0a\u5fc5\u8981\u306a\u5834\u5408\u306b\u306f\u4e0d\u9069\u5207\u3002</p> </li> <li> <p>\u659c\u4ea4\u56de\u8ee2:   \u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9\u3059\u308b\uff08\u4f8b\uff1aPromax\uff09\u3002 \u6ce8\u610f\u70b9: \u56e0\u5b50\u76f8\u95a2\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u3001\u5404\u56e0\u5b50\u306e\u72ec\u7acb\u3057\u305f\u89e3\u91c8\u304c\u96e3\u3057\u304f\u306a\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u304c\u3001\u73fe\u5b9f\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u3088\u308a\u5fe0\u5b9f\u306b\u53cd\u6620\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#75","title":"7.5 \u975e\u6b63\u898f\u6027\u3078\u306e\u5bfe\u51e6","text":""},{"location":"research/note/factor-analysis/07-analysis-step/#751","title":"7.5.1 \u975e\u6b63\u898f\u6027\u306e\u691c\u8a3c","text":"<ul> <li>\u5206\u5e03\u306e\u30c1\u30a7\u30c3\u30af:   \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3001Q-Q\u30d7\u30ed\u30c3\u30c8\u3001\u6b6a\u5ea6\u30fb\u88fe\u306e\u91cd\u3055\u306e\u7d71\u8a08\u91cf\u3092\u78ba\u8a8d\u3057\u3001\u5404\u5909\u6570\u306e\u5206\u5e03\u304c\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3063\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#752","title":"7.5.2 \u5bfe\u51e6\u6cd5","text":"<ul> <li>\u30ed\u30d0\u30b9\u30c8\u63a8\u5b9a:   \u6b63\u898f\u6027\u306e\u9038\u8131\u3092\u88dc\u6b63\u3059\u308b\u305f\u3081\u306b\u3001Robust ML \u3084 Satorra-Bentler \u88dc\u6b63\u306a\u3069\u3092\u7528\u3044\u307e\u3059\u3002</li> <li>\u30c7\u30fc\u30bf\u5909\u63db:   \u5bfe\u6570\u5909\u63db\u3001\u5e73\u65b9\u6839\u5909\u63db\u3001Box-Cox\u5909\u63db\u3092\u9069\u7528\u3057\u3066\u3001\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u6b63\u898f\u306b\u8fd1\u3065\u3051\u307e\u3059\u3002</li> <li>\u4ee3\u66ff\u76f8\u95a2\u884c\u5217:   \u30ea\u30c3\u30ab\u30fc\u30c8\u5c3a\u5ea6\u306a\u3069\u306e\u5834\u5408\u3001\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u9069\u5207\u306a\u5171\u5206\u6563\u69cb\u9020\u3092\u53cd\u6620\u3055\u305b\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#76","title":"7.6 \u611f\u5ea6\u5206\u6790","text":""},{"location":"research/note/factor-analysis/07-analysis-step/#761","title":"7.6.1 \u611f\u5ea6\u5206\u6790\u306e\u76ee\u7684","text":"<ul> <li>\u30e2\u30c7\u30eb\u4eee\u5b9a\u306e\u691c\u8a3c:   \u524d\u63d0\u6761\u4ef6\uff08\u6b63\u898f\u6027\u3001\u5916\u308c\u5024\u306e\u6709\u7121\u306a\u3069\uff09\u304c\u56e0\u5b50\u5206\u6790\u7d50\u679c\u306b\u4e0e\u3048\u308b\u5f71\u97ff\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</li> <li>\u63a8\u5b9a\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u306e\u78ba\u8a8d:   \u524d\u51e6\u7406\u3084\u30b5\u30f3\u30d7\u30eb\u306e\u5909\u66f4\u304c\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u3084\u56e0\u5b50\u5f97\u70b9\u306b\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3059\u308b\u304b\u3092\u691c\u8a3c\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#762","title":"7.6.2 \u611f\u5ea6\u5206\u6790\u306e\u624b\u6cd5\u3068\u5177\u4f53\u4f8b","text":"<ul> <li>\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5: </li> <li>\u4f8b: \u5fc3\u7406\u5b66\u7684\u691c\u67fb\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u30661000\u56de\u306e\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u3092\u5b9f\u65bd\u3057\u3001\u5404\u5909\u6570\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u306e95%\u4fe1\u983c\u533a\u9593\u3092\u7b97\u51fa\u3002  </li> <li> <p>\u30dd\u30a4\u30f3\u30c8: \u4fe1\u983c\u533a\u9593\u304c\u72ed\u3044\u307b\u3069\u7d50\u679c\u304c\u5b89\u5b9a\u3057\u3066\u3044\u308b\u3068\u5224\u65ad\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u30b5\u30f3\u30d7\u30eb\u5206\u5272\u6cd5: </p> </li> <li>\u4f8b: \u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u8abf\u67fb\u30c7\u30fc\u30bf\u3092\u5730\u57df\u5225\u307e\u305f\u306f\u5e74\u4ee3\u5225\u306b\u5206\u5272\u3057\u3001\u5404\u30b0\u30eb\u30fc\u30d7\u3067\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3002  </li> <li> <p>\u30dd\u30a4\u30f3\u30c8: \u5404\u30b5\u30d6\u30b0\u30eb\u30fc\u30d7\u3067\u540c\u69d8\u306e\u56e0\u5b50\u69cb\u9020\u304c\u78ba\u8a8d\u3067\u304d\u308c\u3070\u3001\u5168\u4f53\u306e\u7d50\u679c\u306e\u4fe1\u983c\u6027\u304c\u62c5\u4fdd\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u524d\u51e6\u7406\u306e\u5909\u66f4: </p> </li> <li>\u4f8b: \u533b\u7642\u30c7\u30fc\u30bf\u306b\u304a\u3044\u3066\u3001\u5bfe\u6570\u5909\u63db\u524d\u5f8c\u3067\u56e0\u5b50\u5206\u6790\u3092\u884c\u3044\u3001\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u5909\u5316\u3092\u6bd4\u8f03\u3002  </li> <li> <p>\u30dd\u30a4\u30f3\u30c8: \u5909\u63db\u5f8c\u306b\u5358\u7d14\u69cb\u9020\u304c\u660e\u77ad\u306b\u306a\u308b\u5834\u5408\u3001\u5909\u63db\u304c\u6709\u52b9\u3067\u3042\u3063\u305f\u3068\u5224\u65ad\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u7570\u306a\u308b\u63a8\u5b9a\u6cd5\u306e\u6bd4\u8f03: </p> </li> <li>\u4f8b: \u540c\u4e00\u5fc3\u7406\u5b66\u7684\u691c\u67fb\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001\u6700\u5c24\u6cd5\u3068\u4e3b\u8ef8\u56e0\u5b50\u6cd5\u3067\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3057\u3001\u7d50\u679c\u306e\u4e00\u8cab\u6027\u3092\u8a55\u4fa1\u3002  </li> <li>\u30dd\u30a4\u30f3\u30c8: \u4f3c\u305f\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\u5834\u5408\u3001\u63a8\u5b9a\u306e\u30ed\u30d0\u30b9\u30c8\u6027\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#77","title":"7.7 \u7d50\u679c\u306e\u89e3\u91c8\u3068\u5831\u544a","text":""},{"location":"research/note/factor-analysis/07-analysis-step/#771","title":"7.7.1 \u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u89e3\u91c8","text":"<ul> <li> <p>\u9ad8\u3044\u8ca0\u8377\u91cf\u3068\u4f4e\u3044\u8ca0\u8377\u91cf\u306e\u5224\u65ad:   \u6570\u5024\u57fa\u6e96\uff08\u4f8b\uff1a0.70\u4ee5\u4e0a\u306f\u5f37\u3044\u5bc4\u4e0e\u30010.30\u672a\u6e80\u306f\u5f31\u3044\u5bc4\u4e0e\uff09\u3092\u57fa\u306b\u3001\u5404\u5909\u6570\u304c\u3069\u306e\u56e0\u5b50\u306b\u4e3b\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u304b\u3092\u5224\u65ad\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u56e0\u5b50\u306e\u547d\u540d\u3068\u7406\u8ad6\u7684\u691c\u8a3c:   \u56de\u8ee2\u5f8c\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u304b\u3089\u3001\u5404\u56e0\u5b50\u306b\u610f\u5473\u306e\u3042\u308b\u540d\u79f0\u3092\u4ed8\u3051\u3001\u5148\u884c\u7814\u7a76\u3084\u7406\u8ad6\u3068\u7167\u3089\u3057\u5408\u308f\u305b\u3066\u59a5\u5f53\u6027\u3092\u691c\u8a3c\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#772","title":"7.7.2 \u7d50\u679c\u306e\u8996\u899a\u5316\u3068\u5831\u544a","text":"<ul> <li> <p>\u8996\u899a\u7684\u30c4\u30fc\u30eb:   \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3001\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u68d2\u30b0\u30e9\u30d5\u306a\u3069\u3092\u7528\u3044\u3066\u3001\u7d50\u679c\u3092\u76f4\u611f\u7684\u306b\u7406\u89e3\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u5831\u544a\u306e\u30dd\u30a4\u30f3\u30c8: </p> </li> <li>\u4f7f\u7528\u3057\u305f\u524d\u51e6\u7406\u65b9\u6cd5\u3001\u56e0\u5b50\u62bd\u51fa\u6cd5\u3001\u56de\u8ee2\u624b\u6cd5\u3001\u975e\u6b63\u898f\u6027\u3078\u306e\u5bfe\u51e6\u6cd5\u3001\u611f\u5ea6\u5206\u6790\u306e\u7d50\u679c\u3092\u660e\u8a18\u3059\u308b\u3002  </li> <li>\u7d50\u679c\u306e\u89e3\u91c8\u306b\u304a\u3044\u3066\u3001\u73fe\u5b9f\u7684\u306a\u61f8\u5ff5\u70b9\uff08\u5916\u308c\u5024\u306e\u5f71\u97ff\u3001\u30b5\u30f3\u30d7\u30eb\u30b5\u30a4\u30ba\u3001\u5206\u5e03\u306e\u504f\u308a\uff09\u306b\u3064\u3044\u3066\u3082\u8b70\u8ad6\u3059\u308b\u3002</li> </ul>"},{"location":"research/note/factor-analysis/07-analysis-step/#78","title":"7.8 \u307e\u3068\u3081","text":"<p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u8df5\u7684\u306a\u6d41\u308c\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6574\u7406\u3057\u307e\u3057\u305f\u3002</p> <ol> <li> <p>\u30c7\u30fc\u30bf\u53ce\u96c6\u3068\u524d\u51e6\u7406:    \u6b20\u640d\u5024\u3001\u5916\u308c\u5024\u306e\u51e6\u7406\u3001\u6a19\u6e96\u5316\u3001\u5206\u5e03\u306e\u691c\u67fb\u306a\u3069\u3092\u9069\u5207\u306b\u884c\u3044\u3001\u89e3\u6790\u306b\u9069\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u56e0\u5b50\u62bd\u51fa:    \u9069\u5207\u306a\u62bd\u51fa\u6cd5\u3092\u9078\u3073\u3001\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3057\u3001\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u5f97\u307e\u3059\u3002\u62bd\u51fa\u6cd5\u306e\u9078\u629e\u3084\u56e0\u5b50\u6570\u306e\u5224\u65ad\u306b\u306f\u3001\u7406\u8ad6\u3068\u5b9f\u30c7\u30fc\u30bf\u306e\u4e21\u65b9\u3092\u8003\u616e\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u56de\u8ee2:    \u76f4\u4ea4\u56de\u8ee2\u307e\u305f\u306f\u659c\u4ea4\u56de\u8ee2\u3092\u7528\u3044\u3066\u3001\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u5358\u7d14\u69cb\u9020\u3092\u5b9f\u73fe\u3057\u3001\u89e3\u91c8\u3057\u3084\u3059\u3044\u5f62\u306b\u5909\u63db\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u975e\u6b63\u898f\u6027\u3078\u306e\u5bfe\u51e6:    \u30c7\u30fc\u30bf\u5909\u63db\u3084\u30ed\u30d0\u30b9\u30c8\u63a8\u5b9a\u3001\u4ee3\u66ff\u76f8\u95a2\u884c\u5217\u306e\u5229\u7528\u306b\u3088\u308a\u3001\u975e\u6b63\u898f\u6027\u306e\u5f71\u97ff\u3092\u8efd\u6e1b\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u611f\u5ea6\u5206\u6790:    \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u3001\u30b5\u30f3\u30d7\u30eb\u5206\u5272\u3001\u524d\u51e6\u7406\u306e\u5909\u66f4\u3001\u7570\u306a\u308b\u63a8\u5b9a\u6cd5\u306e\u6bd4\u8f03\u306a\u3069\u3067\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u3068\u30ed\u30d0\u30b9\u30c8\u6027\u3092\u691c\u8a3c\u3057\u3001\u89e3\u6790\u7d50\u679c\u306e\u4fe1\u983c\u6027\u3092\u78ba\u4fdd\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u89e3\u91c8\u3068\u5831\u544a:    \u5f97\u3089\u308c\u305f\u56e0\u5b50\u8ca0\u8377\u91cf\u3084\u56e0\u5b50\u5f97\u70b9\u306e\u610f\u5473\u3092\u5341\u5206\u306b\u691c\u8a0e\u3057\u3001\u7406\u8ad6\u7684\u306a\u80cc\u666f\u3084\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u306e\u7279\u6027\u3092\u8e0f\u307e\u3048\u305f\u4e0a\u3067\u3001\u7d50\u679c\u3092\u8996\u899a\u5316\u30fb\u5831\u544a\u3057\u307e\u3059\u3002</p> </li> </ol>"},{"location":"research/note/factor-analysis/08-simulation/","title":"8. \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3","text":""},{"location":"research/note/factor-analysis/08-simulation/#8-python","title":"\u7b2c8\u7ae0 \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305fPython\u306b\u3088\u308b\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u8df5","text":"<p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u7b2c7\u7ae0\u3067\u8aac\u660e\u3057\u305f\u56e0\u5b50\u5206\u6790\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u3001\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066Python\u3067\u5b9f\u969b\u306b\u5b9f\u884c\u3059\u308b\u65b9\u6cd5\u3092\u89e3\u8aac\u3057\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u8e0f\u307f\u307e\u3059\u3002</p> <ol> <li> <p>\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u751f\u6210:    \u6f5c\u5728\u56e0\u5b50\u3092\u751f\u6210\u3057\u3001\u5404\u56e0\u5b50\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u8ca0\u8377\u91cf\u3092\u5b9a\u3081\u305f\u4e0a\u3067\u89b3\u6e2c\u5909\u6570\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002\u5404\u89b3\u6e2c\u5909\u6570\u306f\u3001\u5171\u901a\u56e0\u5b50\u306e\u5bc4\u4e0e\u3068\u7279\u6709\u8aa4\u5dee\u306e\u7dda\u5f62\u7d50\u5408\u3068\u3057\u3066\u751f\u6210\u3055\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406:    \u6b20\u640d\u5024\u51e6\u7406\uff08\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3067\u306f\u5fc5\u8981\u306a\u3044\u5834\u5408\u304c\u591a\u3044\u3067\u3059\u304c\uff09\u3001\u6a19\u6e96\u5316\u3092\u884c\u3044\u3001\u5909\u6570\u9593\u306e\u30b9\u30b1\u30fc\u30eb\u3092\u7d71\u4e00\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u65bd:    \u9069\u5207\u306a\u56e0\u5b50\u62bd\u51fa\u6cd5\uff08\u3053\u3053\u3067\u306f\u6700\u5c24\u6cd5\uff09\u3092\u7528\u3044\u3066\u3001\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u63a8\u5b9a\u3057\u3001\u56fa\u6709\u5024\u3084\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u56de\u8ee2:    Varimax\uff08\u76f4\u4ea4\u56de\u8ee2\uff09\u3084Promax\uff08\u659c\u4ea4\u56de\u8ee2\uff09\u3092\u7528\u3044\u3066\u3001\u89e3\u91c8\u3057\u3084\u3059\u3044\u5358\u7d14\u69cb\u9020\u3092\u5b9f\u73fe\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u975e\u6b63\u898f\u6027\u30fb\u611f\u5ea6\u5206\u6790\u306e\u8003\u5bdf:    \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3067\u306f\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u307e\u3059\u304c\u3001\u30b3\u30fc\u30c9\u5185\u3067\u30c7\u30fc\u30bf\u5909\u63db\u3084\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u3092\u5b9f\u65bd\u3059\u308b\u624b\u6cd5\u306e\u6982\u8981\u3082\u7d39\u4ecb\u3057\u307e\u3059\u3002</p> </li> </ol> <p>\u4ee5\u4e0b\u306b\u3001\u5404\u30b9\u30c6\u30c3\u30d7\u306e\u8a73\u7d30\u306a\u30b3\u30fc\u30c9\u4f8b\u3068\u89e3\u8aac\u3092\u793a\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/08-simulation/#81","title":"8.1 \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u751f\u6210","text":"<p>\u307e\u305a\u30013\u3064\u306e\u6f5c\u5728\u56e0\u5b50\u306b\u57fa\u3065\u304f12\u500b\u306e\u89b3\u6e2c\u5909\u6570\u3092\u751f\u6210\u3057\u307e\u3059\u3002\u5404\u5909\u6570\u306f\u3001\u8a2d\u5b9a\u3057\u305f\u56e0\u5b50\u8ca0\u8377\u91cf\u3068\u72ec\u81ea\u306e\u8aa4\u5dee\u3092\u7528\u3044\u3066\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# \u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3057\u3066\u518d\u73fe\u6027\u3092\u78ba\u4fdd\nnp.random.seed(0)\nn_samples = 300\n\n# \u6f5c\u5728\u56e0\u5b50\u306e\u751f\u6210\uff083\u56e0\u5b50\u3092\u4eee\u5b9a\uff09\nlatent_factors = {\n    'F1': np.random.normal(0, 1, n_samples),\n    'F2': np.random.normal(0, 1, n_samples),\n    'F3': np.random.normal(0, 1, n_samples)\n}\n\n# \u56e0\u5b50\u8ca0\u8377\u91cf\u3092\u8f9e\u66f8\u3067\u5b9a\u7fa9\uff08\u4f8b: 12\u500b\u306e\u89b3\u6e2c\u5909\u6570\uff09\nloadings = {\n    'Var1': [0.8, 0.1, 0.0],\n    'Var2': [0.75, 0.15, 0.05],\n    'Var3': [0.7, 0.0, 0.1],\n    'Var4': [0.0, 0.8, 0.2],\n    'Var5': [0.1, 0.75, 0.0],\n    'Var6': [0.05, 0.7, 0.1],\n    'Var7': [0.0, 0.2, 0.8],\n    'Var8': [0.1, 0.0, 0.75],\n    'Var9': [0.05, 0.1, 0.7],\n    'Var10': [0.4, 0.4, 0.3],\n    'Var11': [0.35, 0.45, 0.25],\n    'Var12': [0.3, 0.4, 0.35]\n}\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210\ndata = pd.DataFrame()\nfor var, load in loadings.items():\n    # \u5404\u5909\u6570\u3092\u5171\u901a\u56e0\u5b50\u306e\u7dda\u5f62\u7d50\u5408\u3068\u3057\u3066\u751f\u6210\n    common_part = load[0] * latent_factors['F1'] + load[1] * latent_factors['F2'] + load[2] * latent_factors['F3']\n    # \u7279\u6709\u8aa4\u5dee\uff08\u5e73\u57470, \u5206\u6563=0.5\uff09\n    unique_part = np.random.normal(0, np.sqrt(0.5), n_samples)\n    data[var] = common_part + unique_part\n\n# \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u78ba\u8a8d\nprint(data.head())\n</code></pre>"},{"location":"research/note/factor-analysis/08-simulation/#82","title":"8.2 \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406","text":"<p>\u6b21\u306b\u3001\u5404\u5909\u6570\u306e\u30b9\u30b1\u30fc\u30eb\u3092\u7d71\u4e00\u3059\u308b\u305f\u3081\u306b\u6a19\u6e96\u5316\u3092\u884c\u3044\u307e\u3059\u3002</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndata_std = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n# \u6a19\u6e96\u5316\u5f8c\u306e\u30c7\u30fc\u30bf\u306e\u8981\u7d04\u7d71\u8a08\u91cf\u3092\u78ba\u8a8d\nprint(data_std.describe())\n</code></pre>"},{"location":"research/note/factor-analysis/08-simulation/#83","title":"8.3 \u56e0\u5b50\u62bd\u51fa\u306e\u5b9f\u65bd","text":""},{"location":"research/note/factor-analysis/08-simulation/#831","title":"8.3.1 \u9069\u5408\u6027\u306e\u691c\u5b9a","text":"<p>\u56e0\u5b50\u5206\u6790\u3092\u884c\u3046\u524d\u306b\u3001\u30d0\u30fc\u30c8\u30ec\u30c3\u30c8\u306e\u7403\u9762\u6027\u691c\u5b9a\u3084KMO\u691c\u5b9a\u3092\u5b9f\u65bd\u3057\u3066\u3001\u30c7\u30fc\u30bf\u304c\u56e0\u5b50\u5206\u6790\u306b\u9069\u3057\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> <pre><code>from factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n\nchi_square_value, p_value = calculate_bartlett_sphericity(data_std)\nkmo_all, kmo_model = calculate_kmo(data_std)\n\nprint(\"Bartlett\u306e\u7403\u9762\u6027\u691c\u5b9a: chi-square =\", chi_square_value, \", p-value =\", p_value)\nprint(\"KMO\u691c\u5b9a: KMO =\", kmo_model)\n</code></pre>"},{"location":"research/note/factor-analysis/08-simulation/#832","title":"8.3.2 \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3068\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a","text":"<p>\u56fa\u6709\u5024\u306e\u30d7\u30ed\u30c3\u30c8\u3092\u7528\u3044\u3066\u3001\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002</p> <pre><code>import matplotlib.pyplot as plt\nfrom factor_analyzer import FactorAnalyzer\n\nfa_temp = FactorAnalyzer(rotation=None)\nfa_temp.fit(data_std)\nev, v = fa_temp.get_eigenvalues()\n\nplt.figure(figsize=(8, 4))\nplt.scatter(range(1, data_std.shape[1]+1), ev)\nplt.plot(range(1, data_std.shape[1]+1), ev, 'b-')\nplt.title('\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8')\nplt.xlabel('\u56e0\u5b50\u6570')\nplt.ylabel('\u56fa\u6709\u5024')\nplt.grid(True)\nplt.show()\n</code></pre> <p>\u3053\u306e\u30d7\u30ed\u30c3\u30c8\u304b\u3089\u3001\u56fa\u6709\u5024\u304c1\u4ee5\u4e0a\u306e\u56e0\u5b50\u6570\u3084\u6298\u308c\u66f2\u304c\u308a\u70b9\u3092\u53c2\u8003\u306b\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u4f8b\u3068\u3057\u30663\u56e0\u5b50\u3092\u63a1\u7528\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/08-simulation/#833","title":"8.3.3 \u6700\u5c24\u6cd5\u306b\u3088\u308b\u56e0\u5b50\u62bd\u51fa\u3068\u56de\u8ee2","text":"<p>\u56e0\u5b50\u62bd\u51fa\u3092\u6700\u5c24\u6cd5\u3067\u884c\u3044\u3001Varimax\u56de\u8ee2\uff08\u76f4\u4ea4\u56de\u8ee2\uff09\u3092\u5b9f\u65bd\u3057\u307e\u3059\u3002</p> <pre><code># \u56e0\u5b50\u5206\u6790\u306e\u5b9f\u65bd\uff1a3\u56e0\u5b50\u3092\u62bd\u51fa\u3057\u3001Varimax\u56de\u8ee2\u3092\u9069\u7528\nfa = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml')\nfa.fit(data_std)\n\n# \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u8868\u793a\nloadings_df = pd.DataFrame(fa.loadings_, index=data_std.columns, columns=['Factor1', 'Factor2', 'Factor3'])\nprint(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Varimax\u56de\u8ee2\uff09:\")\nprint(loadings_df)\n</code></pre>"},{"location":"research/note/factor-analysis/08-simulation/#84","title":"8.4 \u975e\u6b63\u898f\u6027\u306e\u691c\u8a0e\u3068\u5bfe\u51e6\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09","text":"<p>\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306f\u6b63\u898f\u5206\u5e03\u306b\u57fa\u3065\u3044\u3066\u751f\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3067\u306f\u975e\u6b63\u898f\u6027\u304c\u554f\u984c\u3068\u306a\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u30c7\u30fc\u30bf\u5909\u63db\u306e\u4f8b\u3068\u3057\u3066\u5bfe\u6570\u5909\u63db\u306e\u624b\u6cd5\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code># \u4eee\u306b\u3001\u3059\u3079\u3066\u306e\u5909\u6570\u304c\u6b63\u306e\u5024\u3067\u3042\u308b\u3068\u4eee\u5b9a\u3057\u3066\u5bfe\u6570\u5909\u63db\u3092\u884c\u3046\u5834\u5408\u306e\u4f8b\ndata_log = data.copy()\nfor col in data_log.columns:\n    # \u3082\u3057\u30c7\u30fc\u30bf\u306b0\u4ee5\u4e0b\u306e\u5024\u304c\u3042\u308b\u5834\u5408\u306f\u3001\u9069\u5f53\u306a\u5b9a\u6570\uff08\u3053\u3053\u3067\u306f1\uff09\u3092\u8db3\u3057\u3066\u304b\u3089\u5909\u63db\n    data_log[col] = np.log(data_log[col] + 1)\n\n# \u6a19\u6e96\u5316\u3068\u56e0\u5b50\u5206\u6790\u306e\u518d\u5b9f\u65bd\uff08\u5909\u6570\u5909\u63db\u5f8c\uff09\ndata_log_std = pd.DataFrame(scaler.fit_transform(data_log), columns=data.columns)\n\nfa_log = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml')\nfa_log.fit(data_log_std)\nloadings_log_df = pd.DataFrame(fa_log.loadings_, index=data_log_std.columns, columns=['Factor1', 'Factor2', 'Factor3'])\nprint(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08\u5bfe\u6570\u5909\u63db\u5f8c\u30fbVarimax\u56de\u8ee2\uff09:\")\nprint(loadings_log_df)\n</code></pre>"},{"location":"research/note/factor-analysis/08-simulation/#85","title":"8.5 \u611f\u5ea6\u5206\u6790\u306e\u5b9f\u65bd","text":""},{"location":"research/note/factor-analysis/08-simulation/#851","title":"8.5.1 \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306e\u5b9f\u4f8b","text":"<p>\u3053\u3053\u3067\u306f\u3001\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u3092\u7528\u3044\u3066\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u4fe1\u983c\u533a\u9593\u3092\u8a55\u4fa1\u3059\u308b\u7c21\u5358\u306a\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002</p> <pre><code># \u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\nimport random\n\nn_boot = 500  # \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u56de\u6570\nboot_loadings = []\n\nfor i in range(n_boot):\n    # \u5143\u306e\u30c7\u30fc\u30bf\u304b\u3089\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u30b5\u30f3\u30d7\u30eb\u3092\u62bd\u51fa\n    boot_indices = np.random.choice(data_std.index, size=len(data_std), replace=True)\n    data_boot = data_std.loc[boot_indices]\n\n    # \u56e0\u5b50\u5206\u6790\u306e\u5b9f\u65bd\n    fa_boot = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml')\n    fa_boot.fit(data_boot)\n    boot_loadings.append(fa_boot.loadings_)\n\n# \u4f8b\u3068\u3057\u3066\u3001\u6700\u521d\u306e\u5909\u6570 (Var1) \u306eFactor1\u306e\u8ca0\u8377\u91cf\u306e\u4fe1\u983c\u533a\u9593\u3092\u8a08\u7b97\nvar1_factor1 = [boot[i][0, 0] for boot in boot_loadings]  # boot[i]\u306f\u914d\u5217\u5f62\u5f0f\nlower_bound = np.percentile(var1_factor1, 2.5)\nupper_bound = np.percentile(var1_factor1, 97.5)\nprint(\"Var1\u306eFactor1\u8ca0\u8377\u91cf\u306e95%\u4fe1\u983c\u533a\u9593: [{:.3f}, {:.3f}]\".format(lower_bound, upper_bound))\n</code></pre>"},{"location":"research/note/factor-analysis/08-simulation/#852","title":"8.5.2 \u30b5\u30f3\u30d7\u30eb\u5206\u5272\u6cd5\u306e\u5b9f\u4f8b","text":"<p>\u30b5\u30f3\u30d7\u30eb\u3092\u5730\u57df\u3084\u5e74\u4ee3\u306a\u3069\u3067\u5206\u5272\u3057\u3001\u5404\u30b0\u30eb\u30fc\u30d7\u3067\u56e0\u5b50\u5206\u6790\u3092\u884c\u3044\u7d50\u679c\u306e\u4e00\u8cab\u6027\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> <pre><code># \u3053\u3053\u3067\u306f\u3001\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u305f\u3081\u306b\u30e9\u30f3\u30c0\u30e0\u306b2\u30b0\u30eb\u30fc\u30d7\u306b\u5206\u5272\u3059\u308b\u4f8b\u3092\u793a\u3057\u307e\u3059\ngroup1 = data_std.sample(frac=0.5, random_state=1)\ngroup2 = data_std.drop(group1.index)\n\nfa_group1 = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml')\nfa_group1.fit(group1)\nloadings_group1 = pd.DataFrame(fa_group1.loadings_, index=group1.columns, columns=['Factor1', 'Factor2', 'Factor3'])\n\nfa_group2 = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml')\nfa_group2.fit(group2)\nloadings_group2 = pd.DataFrame(fa_group2.loadings_, index=group2.columns, columns=['Factor1', 'Factor2', 'Factor3'])\n\nprint(\"\u30b0\u30eb\u30fc\u30d71\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217:\")\nprint(loadings_group1)\nprint(\"\\n\u30b0\u30eb\u30fc\u30d72\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217:\")\nprint(loadings_group2)\n</code></pre>"},{"location":"research/note/factor-analysis/08-simulation/#86","title":"8.6 \u7d50\u679c\u306e\u89e3\u91c8\u3068\u307e\u3068\u3081","text":"<p>\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u304f\u56e0\u5b50\u5206\u6790\u306e\u5404\u30b9\u30c6\u30c3\u30d7\u3092\u901a\u3058\u3066\u3001\u4ee5\u4e0b\u306e\u70b9\u304c\u78ba\u8a8d\u3055\u308c\u307e\u3057\u305f\u3002</p> <ul> <li> <p>\u524d\u51e6\u7406:   \u6a19\u6e96\u5316\u306a\u3069\u306e\u524d\u51e6\u7406\u306b\u3088\u308a\u3001\u5909\u6570\u9593\u306e\u30b9\u30b1\u30fc\u30eb\u304c\u7d71\u4e00\u3055\u308c\u3001\u56e0\u5b50\u5206\u6790\u306b\u9069\u3057\u305f\u72b6\u614b\u3068\u306a\u308a\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u56e0\u5b50\u62bd\u51fa\u3068\u56de\u8ee2:   \u6700\u5c24\u6cd5\u3068Varimax\u56de\u8ee2\u3092\u7528\u3044\u3066\u3001\u89e3\u91c8\u3057\u3084\u3059\u3044\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u304c\u5f97\u3089\u308c\u3001\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3084\u9069\u5408\u6027\u691c\u5b9a\u306b\u3088\u308a\u56e0\u5b50\u6570\u306e\u59a5\u5f53\u6027\u3082\u78ba\u8a8d\u3055\u308c\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u975e\u6b63\u898f\u6027\u3078\u306e\u5bfe\u51e6:   \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306f\u6b63\u898f\u6027\u3092\u4eee\u5b9a\u3057\u3066\u751f\u6210\u3055\u308c\u307e\u3057\u305f\u304c\u3001\u5bfe\u6570\u5909\u63db\u306e\u4f8b\u3067\u3001\u5b9f\u30c7\u30fc\u30bf\u3067\u306e\u975e\u6b63\u898f\u6027\u5bfe\u7b56\u306e\u4e00\u7aef\u304c\u793a\u3055\u308c\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u611f\u5ea6\u5206\u6790:   \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3084\u30b5\u30f3\u30d7\u30eb\u5206\u5272\u6cd5\u306b\u3088\u308a\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u63a8\u5b9a\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u3092\u691c\u8a3c\u3059\u308b\u65b9\u6cd5\u304c\u793a\u3055\u308c\u3001\u7d50\u679c\u306e\u30ed\u30d0\u30b9\u30c8\u6027\u3092\u78ba\u8a8d\u3059\u308b\u624b\u6cd5\u3068\u3057\u3066\u6709\u52b9\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/","title":"9. \u7cd6\u5c3f\u75c5\u6f5c\u5728\u539f\u56e0\u5206\u6790","text":""},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#9-pima-indians-diabetes-database","title":"\u7b2c9\u7ae0 \u533b\u7642\u30c7\u30fc\u30bf\uff08Pima Indians Diabetes Database\uff09\u3092\u7528\u3044\u305f\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u8df5","text":"<p>\u3053\u306e\u7ae0\u3067\u306f\u3001Pima Indians Diabetes Database\uff08\u533b\u7642\u30c7\u30fc\u30bf\uff09\u3092\u7528\u3044\u3066\u3001\u7b2c8\u7ae0\u3067\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3067\u5b9f\u65bd\u3057\u305f\u56e0\u5b50\u5206\u6790\u306e\u4e00\u9023\u306e\u624b\u7d9a\u304d\uff08\u524d\u51e6\u7406\u3001\u56e0\u5b50\u62bd\u51fa\u3001\u56de\u8ee2\u3001\u975e\u6b63\u898f\u6027\u306e\u78ba\u8a8d\u3001\u611f\u5ea6\u5206\u6790\uff09\u3092\u5b9f\u969b\u306b\u884c\u3044\u307e\u3059\u3002\u307e\u305f\u3001\u76f4\u4ea4\u56de\u8ee2\uff08Varimax\uff09\u3068\u659c\u4ea4\u56de\u8ee2\uff08Promax\uff09\u306e\u7d50\u679c\u306e\u9055\u3044\u3084\u3001\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u306b\u3088\u308b\u611f\u5ea6\u5206\u6790\u3082\u5b9f\u4f8b\u3092\u901a\u3057\u3066\u89e3\u8aac\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#1","title":"1. \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\u3068\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom factor_analyzer import FactorAnalyzer, calculate_bartlett_sphericity, calculate_kmo\nfrom sklearn.preprocessing import StandardScaler\n\n# \u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f\n# Kaggle\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305fCSV\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u6307\u5b9a\uff08\u4f8b\uff1a\"diabetes.csv\"\uff09\ndata = pd.read_csv(\"diabetes.csv\")\n\n# \u30c7\u30fc\u30bf\u306e\u6700\u521d\u306e\u6570\u884c\u3092\u78ba\u8a8d\nprint(data.head())\n</code></pre>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#2","title":"2. \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406","text":""},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#21","title":"2.1 \u6b20\u640d\u5024\u306e\u78ba\u8a8d\u3068\u88dc\u6b63","text":"<p>Pima Indians Diabetes Database\u3067\u306f\u3001Glucose\u3001BloodPressure\u3001SkinThickness\u3001Insulin\u3001BMI \u306e\u5404\u5909\u6570\u306b\u300c0\u300d\u304c\u4e0d\u81ea\u7136\u306a\u5024\u3068\u3057\u3066\u542b\u307e\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u3092\u6b20\u640d\u5024\u3068\u307f\u306a\u3057\u3001\u4e2d\u592e\u5024\u3067\u88dc\u5b8c\u3057\u307e\u3059\u3002</p> <pre><code># \u88dc\u6b63\u5bfe\u8c61\u306e\u30ab\u30e9\u30e0\ncols_with_zero = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n\n# \u5404\u5bfe\u8c61\u30ab\u30e9\u30e0\u30670\u3092NaN\u306b\u5909\u63db\nfor col in cols_with_zero:\n    data[col] = data[col].replace(0, np.nan)\n\n# \u5404\u30ab\u30e9\u30e0\u306e\u6b20\u640d\u5024\u6570\u3092\u78ba\u8a8d\nprint(\"\u6b20\u640d\u5024\u306e\u6570:\")\nprint(data.isnull().sum())\n\n# \u5404\u30ab\u30e9\u30e0\u306e\u4e2d\u592e\u5024\u3067\u88dc\u5b8c\nfor col in cols_with_zero:\n    median_val = data[col].median()\n    data[col].fillna(median_val, inplace=True)\n\n# \u88dc\u5b8c\u5f8c\u306e\u6b20\u640d\u5024\u78ba\u8a8d\nprint(\"\\n\u88dc\u5b8c\u5f8c\u306e\u6b20\u640d\u5024\u6570:\")\nprint(data.isnull().sum())\n</code></pre>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#22","title":"2.2 \u30c7\u30fc\u30bf\u306e\u5206\u5e03\u78ba\u8a8d\uff08\u975e\u6b63\u898f\u6027\u306e\u691c\u8a0e\uff09","text":"<p>\u5404\u5909\u6570\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3068Q-Q\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3001\u5206\u5e03\u306e\u6b6a\u307f\u3084\u88fe\u306e\u91cd\u3055\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p> <pre><code># \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u63cf\u753b\ndata.hist(bins=20, figsize=(12, 10))\nplt.suptitle(\"\u5404\u5909\u6570\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\")\nplt.show()\n\n# Q-Q\u30d7\u30ed\u30c3\u30c8\u306e\u4f5c\u6210\uff08\u4f8b\u3068\u3057\u3066Glucose\u5909\u6570\uff09\nplt.figure(figsize=(6,4))\nstats.probplot(data['Glucose'], dist=\"norm\", plot=plt)\nplt.title(\"Glucose\u306eQ-Q\u30d7\u30ed\u30c3\u30c8\")\nplt.show()\n</code></pre>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#3","title":"3. \u30c7\u30fc\u30bf\u306e\u6a19\u6e96\u5316","text":"<p>\u56e0\u5b50\u5206\u6790\u3067\u306f\u5404\u5909\u6570\u306e\u30b9\u30b1\u30fc\u30eb\u304c\u7d71\u4e00\u3055\u308c\u308b\u3053\u3068\u304c\u671b\u307e\u3057\u3044\u305f\u3081\u3001\u6a19\u6e96\u5316\u3092\u884c\u3044\u307e\u3059\u3002</p> <pre><code>scaler = StandardScaler()\ndata_std = pd.DataFrame(scaler.fit_transform(data.drop('Outcome', axis=1)), \n                        columns=data.columns[:-1])\n# Outcome\u5909\u6570\u306f\u89e3\u6790\u306e\u5bfe\u8c61\u5916\uff08\u30af\u30e9\u30b9\u30e9\u30d9\u30eb\uff09\u3068\u3057\u3066\u6271\u3046\nprint(data_std.head())\n</code></pre>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#4","title":"4. \u56e0\u5b50\u62bd\u51fa\u306e\u5b9f\u65bd","text":""},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#41","title":"4.1 \u9069\u5408\u6027\u306e\u691c\u5b9a","text":"<p>\u56e0\u5b50\u5206\u6790\u306b\u9069\u3057\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u3001\u30d0\u30fc\u30c8\u30ec\u30c3\u30c8\u306e\u7403\u9762\u6027\u691c\u5b9a\u3068KMO\u691c\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002</p> <pre><code>chi_square_value, p_value = calculate_bartlett_sphericity(data_std)\nkmo_all, kmo_model = calculate_kmo(data_std)\n\nprint(\"Bartlett\u306e\u7403\u9762\u6027\u691c\u5b9a: chi-square =\", chi_square_value, \", p-value =\", p_value)\nprint(\"KMO\u691c\u5b9a: KMO =\", kmo_model)\n</code></pre>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#42","title":"4.2 \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u3068\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a","text":"<p>\u56fa\u6709\u5024\u3092\u78ba\u8a8d\u3057\u3001\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3057\u307e\u3059\uff08\u3053\u306e\u4f8b\u3067\u306f\u30013\u56e0\u5b50\u3092\u4eee\u5b9a\uff09\u3002</p> <pre><code>fa_temp = FactorAnalyzer(rotation=None)\nfa_temp.fit(data_std)\nev, v = fa_temp.get_eigenvalues()\n\nplt.figure(figsize=(8,4))\nplt.scatter(range(1, data_std.shape[1]+1), ev)\nplt.plot(range(1, data_std.shape[1]+1), ev, 'b-')\nplt.title('\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8')\nplt.xlabel('\u56e0\u5b50\u6570')\nplt.ylabel('\u56fa\u6709\u5024')\nplt.axhline(1, color='r', linestyle='--')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#5","title":"5. \u56e0\u5b50\u5206\u6790\u3068\u56de\u8ee2\u306e\u5b9f\u65bd","text":""},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#51-varimax","title":"5.1 Varimax\u56de\u8ee2\uff08\u76f4\u4ea4\u56de\u8ee2\uff09","text":"<pre><code># 3\u56e0\u5b50\u3067\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\uff08\u6700\u5c24\u6cd5\u3001Varimax\u56de\u8ee2\uff09\nfa_varimax = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml')\nfa_varimax.fit(data_std)\n\n# \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u8868\u793a\nloadings_varimax = pd.DataFrame(fa_varimax.loadings_, index=data_std.columns,\n                                  columns=['Factor1', 'Factor2', 'Factor3'])\nprint(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Varimax\u56de\u8ee2\uff09:\")\nprint(loadings_varimax)\n\n# \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u306b\u3088\u308b\u53ef\u8996\u5316\nplt.figure(figsize=(8,6))\nsns.heatmap(loadings_varimax, annot=True, cmap='coolwarm')\nplt.title(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Varimax\u56de\u8ee2\uff09\")\nplt.show()\n</code></pre>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#52-promax","title":"5.2 Promax\u56de\u8ee2\uff08\u659c\u4ea4\u56de\u8ee2\uff09","text":"<pre><code># \u540c\u3058\u304f3\u56e0\u5b50\u3067Promax\u56de\u8ee2\u3092\u5b9f\u65bd\nfa_promax = FactorAnalyzer(n_factors=3, rotation='promax', method='ml')\nfa_promax.fit(data_std)\n\n# \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u8868\u793a\nloadings_promax = pd.DataFrame(fa_promax.loadings_, index=data_std.columns,\n                                 columns=['Factor1', 'Factor2', 'Factor3'])\nprint(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Promax\u56de\u8ee2\uff09:\")\nprint(loadings_promax)\n\n# \u56e0\u5b50\u9593\u306e\u76f8\u95a2\uff08Promax\u306e\u5834\u5408\uff09\nphi = fa_promax.get_factor_correlation()\nprint(\"\\n\u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217\uff08Promax\u56de\u8ee2\uff09:\")\nprint(pd.DataFrame(phi, columns=['Factor1', 'Factor2', 'Factor3'], index=['Factor1', 'Factor2', 'Factor3']))\n\n# \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u306b\u3088\u308b\u53ef\u8996\u5316\nplt.figure(figsize=(8,6))\nsns.heatmap(loadings_promax, annot=True, cmap='coolwarm')\nplt.title(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Promax\u56de\u8ee2\uff09\")\nplt.show()\n</code></pre> <p>\u3010\u89e3\u8aac\u3011 - Varimax\u56de\u8ee2\u3067\u306f\u56e0\u5b50\u9593\u304c\u72ec\u7acb\u3068\u306a\u308b\u305f\u3081\u3001\u5404\u5909\u6570\u304c\u3069\u306e\u56e0\u5b50\u306b\u4e3b\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u304c\u660e\u78ba\u306b\u306a\u308a\u307e\u3059\u3002 - Promax\u56de\u8ee2\u3067\u306f\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9\u3057\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u306b\u3088\u308a\u67d4\u8edf\u306b\u5bfe\u5fdc\u3067\u304d\u307e\u3059\u3002\u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3067\u3001\u56e0\u5b50\u540c\u58eb\u306e\u95a2\u9023\u6027\u3082\u7406\u89e3\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#6","title":"6. \u611f\u5ea6\u5206\u6790\uff08\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\uff09","text":"<p>\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3092\u7528\u3044\u3066\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u63a8\u5b9a\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u4f8b\u3068\u3057\u3066\u300cGlucose\u300d\u306eFactor1\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u91cf\u306e95%\u4fe1\u983c\u533a\u9593\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> <pre><code>n_boot = 500  # \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u56de\u6570\nboot_loadings = []\n\nfor i in range(n_boot):\n    # \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u30b5\u30f3\u30d7\u30eb\u3092\u62bd\u51fa\uff08\u7f6e\u63db\u3042\u308a\uff09\n    boot_indices = np.random.choice(data_std.index, size=len(data_std), replace=True)\n    data_boot = data_std.loc[boot_indices]\n\n    # \u56e0\u5b50\u5206\u6790\uff08Varimax\u56de\u8ee2\uff09\u3092\u5b9f\u65bd\n    fa_boot = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml')\n    fa_boot.fit(data_boot)\n    boot_loadings.append(fa_boot.loadings_)\n\n# \u4f8b: 'Glucose'\uff08\u30c7\u30fc\u30bf_std\u306e1\u5217\u76ee\u3068\u4eee\u5b9a\uff09\u306eFactor1\u306e\u8ca0\u8377\u91cf\u306e\u4fe1\u983c\u533a\u9593\n# \u5217\u540d\u304c 'Glucose' \u3067\u3042\u308c\u3070\u3001\u305d\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u5229\u7528\nglucose_idx = data_std.columns.get_loc('Glucose')\nfactor1_loadings = [boot[glucose_idx, 0] for boot in boot_loadings]\n\nlower_bound = np.percentile(factor1_loadings, 2.5)\nupper_bound = np.percentile(factor1_loadings, 97.5)\nprint(\"Glucose\u306eFactor1\u8ca0\u8377\u91cf\u306e95%\u4fe1\u983c\u533a\u9593: [{:.3f}, {:.3f}]\".format(lower_bound, upper_bound))\n</code></pre>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#7","title":"7. \u7d50\u679c\u306e\u89e3\u91c8\u3068\u53ef\u8996\u5316","text":""},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#71","title":"7.1 \u7d50\u679c\u306e\u6bd4\u8f03","text":"<p>Varimax\u3068Promax\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u6bd4\u8f03\u3057\u3066\u3001\u5404\u5909\u6570\u304c\u3069\u306e\u56e0\u5b50\u306b\u3069\u306e\u7a0b\u5ea6\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u3001\u89e3\u91c8\u4e0a\u3069\u3061\u3089\u306e\u56de\u8ee2\u6cd5\u304c\u7406\u8ad6\u3084\u76ee\u7684\u306b\u5408\u81f4\u3057\u3066\u3044\u308b\u304b\u3092\u691c\u8a0e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#72","title":"7.2 \u53ef\u8996\u5316","text":"<ul> <li>\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8: \u65e2\u306b\u56e0\u5b50\u6570\u6c7a\u5b9a\u306e\u305f\u3081\u306b\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002</li> <li>\u30d2\u30fc\u30c8\u30de\u30c3\u30d7: Varimax\u304a\u3088\u3073Promax\u306e\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u8868\u793a\u3057\u307e\u3057\u305f\u3002</li> <li>\u56e0\u5b50\u9593\u76f8\u95a2: Promax\u56de\u8ee2\u306e\u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217\u3082\u8868\u793a\u3057\u3001\u56e0\u5b50\u306e\u95a2\u9023\u6027\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#73","title":"7.3 \u89e3\u91c8\u306e\u30dd\u30a4\u30f3\u30c8","text":"<ul> <li>\u4f8b\u3048\u3070\u3001\u3082\u3057\u300cGlucose\u300d\u3084\u300cBMI\u300d\u306a\u3069\u304c\u3001\u3042\u308b\u56e0\u5b50\u306b\u9ad8\u3044\u8ca0\u8377\u3092\u793a\u3059\u5834\u5408\u3001\u305d\u306e\u56e0\u5b50\u306f\u300c\u4ee3\u8b1d\u300d\u3084\u300c\u30a4\u30f3\u30b9\u30ea\u30f3\u611f\u53d7\u6027\u300d\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u3068\u89e3\u91c8\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</li> <li>\u5404\u5909\u6570\u306e\u7406\u8ad6\u7684\u80cc\u666f\u3068\u3001\u5f97\u3089\u308c\u305f\u56e0\u5b50\u69cb\u9020\u3068\u306e\u6574\u5408\u6027\u3092\u691c\u8a0e\u3057\u3001\u56e0\u5b50\u306e\u547d\u540d\u3092\u884c\u3044\u307e\u3059\u3002</li> </ul>"},{"location":"research/note/factor-analysis/09-pima-indians-diabetes-analysis/#8","title":"8. \u307e\u3068\u3081","text":"<p>\u672c\u7ae0\u3067\u306f\u3001Pima Indians Diabetes Database\u3092\u7528\u3044\u3066\u5b9f\u969b\u306e\u533b\u7642\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u5206\u6790\u306e\u4e00\u9023\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u3001\u4ee5\u4e0b\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u5b9f\u65bd\u3057\u307e\u3057\u305f\u3002</p> <ol> <li> <p>\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068\u524d\u51e6\u7406:    \u6b20\u640d\u5024\u306e\u88dc\u5b8c\u3001\u4e0d\u8981\u306a\u5024\u306e\u51e6\u7406\u3001\u6a19\u6e96\u5316\u3092\u5b9f\u65bd\u3057\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u5206\u5e03\u306e\u78ba\u8a8d:    \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3068Q-Q\u30d7\u30ed\u30c3\u30c8\u3092\u7528\u3044\u3066\u3001\u5404\u5909\u6570\u306e\u5206\u5e03\uff08\u975e\u6b63\u898f\u6027\u306e\u53ef\u80fd\u6027\uff09\u3092\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u56e0\u5b50\u62bd\u51fa\u3068\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a:    \u30d0\u30fc\u30c8\u30ec\u30c3\u30c8\u691c\u5b9a\u3001KMO\u691c\u5b9a\u3001\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u306b\u3088\u308a\u89e3\u6790\u306e\u9069\u5408\u6027\u3068\u56e0\u5b50\u6570\u3092\u691c\u8a3c\u3057\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u56de\u8ee2:    Varimax\uff08\u76f4\u4ea4\u56de\u8ee2\uff09\u3068Promax\uff08\u659c\u4ea4\u56de\u8ee2\uff09\u306e\u4e21\u624b\u6cd5\u3067\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u63a8\u5b9a\u3057\u3001\u305d\u306e\u9055\u3044\u3092\u6bd4\u8f03\u3057\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u611f\u5ea6\u5206\u6790:    \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3092\u7528\u3044\u3066\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u4fe1\u983c\u533a\u9593\u3092\u8a55\u4fa1\u3057\u3001\u63a8\u5b9a\u7d50\u679c\u306e\u30ed\u30d0\u30b9\u30c8\u6027\u3092\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u7d50\u679c\u306e\u89e3\u91c8\u3068\u53ef\u8996\u5316:    \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3084\u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217\u306a\u3069\u3092\u7528\u3044\u3066\u3001\u5f97\u3089\u308c\u305f\u56e0\u5b50\u69cb\u9020\u3092\u8996\u899a\u7684\u306b\u89e3\u91c8\u3057\u307e\u3057\u305f\u3002</p> </li> </ol>"},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/","title":"10. \u9806\u5e8f\u3042\u308a\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u6271\u3044","text":""},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#10","title":"\u7b2c10\u7ae0 \u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u5206\u6790\u3068\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u30fb\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u306e\u7406\u8ad6","text":"<p>\u5f93\u6765\u306e\u56e0\u5b50\u5206\u6790\u306f\u3001\u9023\u7d9a\u5909\u6570\u304b\u3064\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u3092\u4eee\u5b9a\u3057\u305f\u30c7\u30fc\u30bf\u3092\u524d\u63d0\u3068\u3057\u3066\u89e3\u6790\u3092\u884c\u3044\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u5b9f\u52d9\u3067\u306f\u3057\u3070\u3057\u3070\u30ea\u30c3\u30ab\u30fc\u30c8\u5c3a\u5ea6\u306a\u3069\u306e\u300c\u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u300d\u304c\u7528\u3044\u3089\u308c\u308b\u3053\u3068\u304c\u591a\u304f\u3001\u3053\u308c\u3089\u306e\u5909\u6570\u306b\u5bfe\u3057\u3066\u306f\u5f93\u6765\u306e\u624b\u6cd5\u3067\u306f\u9069\u5207\u306a\u76f8\u95a2\u69cb\u9020\u304c\u5f97\u3089\u308c\u306a\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002\u672c\u7ae0\u3067\u306f\u3001\u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u80cc\u5f8c\u306b\u5b58\u5728\u3059\u308b\u6f5c\u5728\u7684\u306a\u9023\u7d9a\u5909\u6570\u306e\u6982\u5ff5\u306b\u57fa\u3065\u304d\u3001\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u304a\u3088\u3073\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u3092\u7528\u3044\u305f\u56e0\u5b50\u5206\u6790\u306e\u65b9\u6cd5\u306b\u3064\u3044\u3066\u3001\u6570\u7406\u7684\u306a\u80cc\u666f\u3001\u8a08\u7b97\u65b9\u6cd5\u3001\u305d\u3057\u3066\u5177\u4f53\u7684\u306a\u5fdc\u7528\u4f8b\u3092\u4ea4\u3048\u3066\u8a73\u3057\u304f\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#101","title":"10.1 \u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u3068\u6f5c\u5728\u9023\u7d9a\u5909\u6570\u30e2\u30c7\u30eb","text":"<p>\u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\uff08\u4f8b\uff1a\u30ea\u30c3\u30ab\u30fc\u30c8\u5c3a\u5ea6\u306e\u56de\u7b54\u306a\u3069\uff09\u306f\u3001\u76f4\u63a5\u7684\u306b\u306f\u9023\u7d9a\u5909\u6570\u3068\u3057\u3066\u6271\u3048\u307e\u305b\u3093\u304c\u3001\u3053\u308c\u3089\u306e\u5909\u6570\u306f\u80cc\u5f8c\u306b\u9023\u7d9a\u7684\u306a\u6f5c\u5728\u5909\u6570\u304c\u5b58\u5728\u3059\u308b\u3068\u4eee\u5b9a\u3059\u308b\u3053\u3068\u3067\u89e3\u6790\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002 \u5177\u4f53\u7684\u306b\u306f\u3001\u89b3\u6e2c\u3055\u308c\u305f\u9806\u5e8f\u30c7\u30fc\u30bf \\(X\\) \u306f\u3001\u6f5c\u5728\u9023\u7d9a\u5909\u6570 \\(X^*\\) \u304c\u7279\u5b9a\u306e\u95be\u5024\u306b\u3088\u3063\u3066\u30ab\u30c6\u30b4\u30ea\u306b\u533a\u5206\u3055\u308c\u3066\u3044\u308b\u3068\u30e2\u30c7\u30eb\u5316\u3055\u308c\u307e\u3059\u3002</p> <p>\u4f8b\u3048\u3070\u3001\\(X\\) \u304c \\(k\\) \u500b\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u3092\u6301\u3064\u5834\u5408\u3001\u95be\u5024 \\(\\tau_0 = -\\infty, \\tau_1, \\tau_2, \\dots, \\tau_{k-1}, \\tau_k = \\infty\\) \u3092\u7528\u3044\u3066\u3001 $$ X = i \\quad \\text{if} \\quad \\tau_{i-1} &lt; X^* \\le \\tau_i, \\quad i=1,\\dots,k. $$</p> <p>\u3053\u306e\u3088\u3046\u306a\u6f5c\u5728\u5909\u6570\u30e2\u30c7\u30eb\u3092\u524d\u63d0\u306b\u3001\u9806\u5e8f\u30c7\u30fc\u30bf\u540c\u58eb\u306e\u76f8\u95a2\u3092\u63a8\u5b9a\u3059\u308b\u624b\u6cd5\u304c\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u3001\u307e\u305f\u9806\u5e8f\u30c7\u30fc\u30bf\u3068\u9023\u7d9a\u30c7\u30fc\u30bf\u9593\u306e\u76f8\u95a2\u3092\u63a8\u5b9a\u3059\u308b\u624b\u6cd5\u304c\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u3067\u3059\u3002</p>"},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#102","title":"10.2 \u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u306e\u6570\u7406\u7684\u80cc\u666f\u3068\u8a08\u7b97\u65b9\u6cd5","text":""},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#1021","title":"10.2.1 \u6570\u7406\u7684\u80cc\u666f","text":"<p>\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u306f\u30012\u3064\u306e\u9806\u5e8f\u5909\u6570 \\(X\\) \u3068 \\(Y\\) \u306e\u80cc\u5f8c\u306b\u3042\u308b\u9023\u7d9a\u6f5c\u5728\u5909\u6570 \\(X^*\\) \u3068 \\(Y^*\\) \u304c\u3001\u4e8c\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u4eee\u5b9a\u3057\u307e\u3059\u3002 \u3059\u306a\u308f\u3061\u3001 $$ \\begin{pmatrix} X^ \\ Y^ \\end{pmatrix} \\sim N\\left(\\begin{pmatrix} 0 \\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 &amp; \\rho \\ \\rho &amp; 1 \\end{pmatrix}\\right). $$</p> <p>\u305d\u308c\u305e\u308c\u306e\u89b3\u6e2c\u5024 \\(X\\) \u3068 \\(Y\\) \u306f\u3001\u4ee5\u4e0b\u306e\u95be\u5024\u306b\u3088\u3063\u3066\u533a\u5206\u3055\u308c\u307e\u3059\u3002 - \\(X = i\\) if \\(\\tau_{i-1}^X &lt; X^* \\le \\tau_i^X\\) - \\(Y = j\\) if \\(\\tau_{j-1}^Y &lt; Y^* \\le \\tau_j^Y\\)</p> <p>\u3053\u306e\u30e2\u30c7\u30eb\u306b\u57fa\u3065\u304f\u3068\u3001\\(X\\) \u306e\u30ab\u30c6\u30b4\u30ea\u30fc \\(i\\) \u3068 \\(Y\\) \u306e\u30ab\u30c6\u30b4\u30ea\u30fc \\(j\\) \u304c\u540c\u6642\u306b\u89b3\u6e2c\u3055\u308c\u308b\u78ba\u7387\u306f\u3001\u4e8c\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306e\u7d2f\u7a4d\u5206\u5e03\u95a2\u6570\uff08CDF\uff09\u3092\u7528\u3044\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u73fe\u3055\u308c\u307e\u3059\u3002 $$ P(X=i, Y=j) = \\Phi_2(\\tau_i^X, \\tau_j^Y; \\rho) - \\Phi_2(\\tau_{i-1}^X, \\tau_j^Y; \\rho) - \\Phi_2(\\tau_i^X, \\tau_{j-1}^Y; \\rho) + \\Phi_2(\\tau_{i-1}^X, \\tau_{j-1}^Y; \\rho) $$ \u3053\u3053\u3067\u3001\\(\\Phi_2(a, b; \\rho)\\) \u306f\u3001\u76f8\u95a2\u4fc2\u6570 \\(\\rho\\) \u3092\u6301\u3064\u4e8c\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306eCDF\u3092\u8868\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#1022","title":"10.2.2 \u63a8\u5b9a\u65b9\u6cd5","text":"<p>\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u306f\u3001\u89b3\u6e2c\u3055\u308c\u305f\u30af\u30ed\u30b9\u96c6\u8a08\u8868\u306e\u983b\u5ea6\u3092\u7528\u3044\u3001\u4e0a\u8a18\u306e\u5f0f\u304b\u3089\u95be\u5024\u3068\u76f8\u95a2 \\(\\rho\\) \u3092\u6700\u5c24\u6cd5\u306b\u3088\u308a\u63a8\u5b9a\u3057\u307e\u3059\u3002\u5b9f\u969b\u306e\u8a08\u7b97\u306f\u3001\u53cd\u5fa9\u7684\u306a\u6700\u5c24\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff08\u4f8b\u3048\u3070\u3001Newton-Raphson\u6cd5\uff09\u3092\u7528\u3044\u3066\u884c\u308f\u308c\u3001\u591a\u304f\u306e\u7d71\u8a08\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\uff08R\u306e<code>polycor</code>\u30d1\u30c3\u30b1\u30fc\u30b8\u3001Python\u306e<code>statsmodels</code>\u306a\u3069\uff09\u3067\u5b9f\u88c5\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#103","title":"10.3 \u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u306e\u6570\u7406\u7684\u80cc\u666f\u3068\u8a08\u7b97\u65b9\u6cd5","text":""},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#1031","title":"10.3.1 \u6570\u7406\u7684\u80cc\u666f","text":"<p>\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u306f\u30011\u3064\u306e\u9806\u5e8f\u5909\u6570 \\(X\\) \u30681\u3064\u306e\u9023\u7d9a\u5909\u6570 \\(Y\\) \u306e\u9593\u306e\u76f8\u95a2\u3092\u63a8\u5b9a\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001\\(X\\) \u306e\u80cc\u5f8c\u306b\u3042\u308b\u9023\u7d9a\u6f5c\u5728\u5909\u6570 \\(X^*\\) \u304c\u3001\u95be\u5024\u306b\u3088\u308a\u9806\u5e8f\u30c7\u30fc\u30bf\u3068\u3057\u3066\u89b3\u6e2c\u3055\u308c\u3001\\(Y\\) \u306f\u76f4\u63a5\u89b3\u6e2c\u3055\u308c\u305f\u9023\u7d9a\u5909\u6570\u3068\u3057\u307e\u3059\u3002</p> <p>\\(X^*\\) \u3068 \\(Y\\) \u306e\u9593\u3082\u307e\u305f\u4e8c\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u4eee\u5b9a\u3057\u3001 $$ \\begin{pmatrix} X^* \\ Y \\end{pmatrix} \\sim N\\left(\\begin{pmatrix} 0 \\ \\mu_Y \\end{pmatrix}, \\begin{pmatrix} 1 &amp; \\rho \\ \\rho &amp; \\sigma_Y^2 \\end{pmatrix}\\right). $$</p> <p>\\(X\\) \u306f\u4ee5\u4e0b\u306e\u95be\u5024\u306b\u3088\u3063\u3066\u533a\u5206\u3055\u308c\u307e\u3059\u3002 $$ X = i \\quad \\text{if} \\quad \\tau_{i-1} &lt; X^* \\le \\tau_i. $$</p>"},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#1032","title":"10.3.2 \u63a8\u5b9a\u65b9\u6cd5","text":"<p>\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u306e\u63a8\u5b9a\u3082\u3001\\(X\\) \u306e\u89b3\u6e2c\u3055\u308c\u305f\u30ab\u30c6\u30b4\u30ea\u30fc\u3068 \\(Y\\) \u306e\u9023\u7d9a\u5024\u306b\u57fa\u3065\u304d\u3001\u6700\u5c24\u6cd5\u3067\u76f8\u95a2\u30d1\u30e9\u30e1\u30fc\u30bf \\(\\rho\\) \u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002 \u5177\u4f53\u7684\u306b\u306f\u3001\u3042\u308b\u9023\u7d9a\u5024 \\(Y=y\\) \u306b\u5bfe\u3057\u3066\u3001\\(X\\) \u304c\u30ab\u30c6\u30b4\u30ea\u30fc \\(i\\) \u3068\u306a\u308b\u6761\u4ef6\u4ed8\u304d\u78ba\u7387\u306f\u3001 $$ P(X = i \\mid Y = y) = \\Phi\\left(\\frac{\\tau_i - \\rho \\frac{y - \\mu_Y}{\\sigma_Y}}{\\sqrt{1-\\rho^2}}\\right) - \\Phi\\left(\\frac{\\tau_{i-1} - \\rho \\frac{y - \\mu_Y}{\\sigma_Y}}{\\sqrt{1-\\rho^2}}\\right) $$ \u3068\u306a\u308a\u3001\u3053\u306e\u78ba\u7387\u306b\u57fa\u3065\u3044\u3066\u5168\u30c7\u30fc\u30bf\u306e\u5c24\u5ea6\u3092\u69cb\u7bc9\u3057\u3001\u6700\u5c24\u63a8\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#104","title":"10.4 \u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u5206\u6790\u3078\u306e\u5fdc\u7528","text":"<p>\u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u9593\u306e\u76f8\u95a2\u3092\u3001\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3068\u3057\u3066\u7b97\u51fa\u3059\u308b\u3053\u3068\u3067\u3001\u5f93\u6765\u306e\u56e0\u5b50\u5206\u6790\u306e\u5165\u529b\u3068\u3057\u3066\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 - \u30b9\u30c6\u30c3\u30d71: \u5404\u9806\u5e8f\u5909\u6570\u9593\u306e\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u3092\u63a8\u5b9a\u3057\u3001\u76f8\u95a2\u884c\u5217 \\(R_{poly}\\) \u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002 - \u30b9\u30c6\u30c3\u30d72: \\(R_{poly}\\) \u3092\u57fa\u306b\u3001\u901a\u5e38\u306e\u56e0\u5b50\u5206\u6790\uff08\u4f8b\uff1a\u6700\u5c0f\u4e8c\u4e57\u6cd5\u3001\u6700\u5c24\u6cd5\uff09\u3092\u5b9f\u65bd\u3057\u307e\u3059\u3002   \u3053\u306e\u3068\u304d\u3001\u5143\u306e\u89b3\u6e2c\u30c7\u30fc\u30bf\u306f\u9806\u5e8f\u30c7\u30fc\u30bf\u3067\u3042\u3063\u3066\u3082\u3001\u80cc\u5f8c\u306b\u6f5c\u3080\u9023\u7d9a\u7684\u306a\u76f8\u95a2\u69cb\u9020\u306b\u57fa\u3065\u304f\u56e0\u5b50\u304c\u62bd\u51fa\u3055\u308c\u307e\u3059\u3002</p> <p>\u540c\u69d8\u306b\u3001\u9023\u7d9a\u5909\u6570\u3068\u9806\u5e8f\u5909\u6570\u304c\u6df7\u5728\u3059\u308b\u5834\u5408\u306f\u3001\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u3092\u5229\u7528\u3057\u3066\u76f8\u95a2\u884c\u5217\u3092\u4f5c\u6210\u3057\u3001\u56e0\u5b50\u5206\u6790\u3092\u884c\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/10-ordered-categorical-factor-analysis/#105","title":"10.5 \u307e\u3068\u3081","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u9023\u7d9a\u5909\u6570\u3092\u524d\u63d0\u3068\u3059\u308b\u5f93\u6765\u306e\u56e0\u5b50\u5206\u6790\u3067\u306f\u5bfe\u5fdc\u304c\u96e3\u3057\u3044\u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306b\u5bfe\u3057\u3066\u3001\u6f5c\u5728\u9023\u7d9a\u5909\u6570\u30e2\u30c7\u30eb\u3092\u4eee\u5b9a\u3057\u3001\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u304a\u3088\u3073\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u3092\u7528\u3044\u308b\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3057\u305f\u3002</p> <ul> <li> <p>\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2:   2\u3064\u306e\u9806\u5e8f\u5909\u6570\u306e\u80cc\u5f8c\u306b\u3042\u308b\u9023\u7d9a\u6f5c\u5728\u5909\u6570\u304c\u4e8c\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u3044\u3046\u4eee\u5b9a\u306e\u4e0b\u3001\u6700\u5c24\u6cd5\u306b\u3088\u308a\u76f8\u95a2\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u95be\u5024\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2:   \u9806\u5e8f\u5909\u6570\u3068\u9023\u7d9a\u5909\u6570\u306e\u7d44\u307f\u5408\u308f\u305b\u306b\u5bfe\u3057\u3066\u3001\u540c\u69d8\u306e\u6f5c\u5728\u5909\u6570\u30e2\u30c7\u30eb\u3092\u7528\u3044\u3066\u76f8\u95a2\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u5fdc\u7528:   \u63a8\u5b9a\u3055\u308c\u305f\u76f8\u95a2\u884c\u5217\u3092\u56e0\u5b50\u5206\u6790\u306e\u5165\u529b\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u9806\u5e8f\u30c7\u30fc\u30bf\u306e\u56e0\u5b50\u5206\u6790\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/","title":"11. \u98db\u884c\u6a5f\u4e57\u5ba2\u6e80\u8db3\u5ea6\u5206\u6790","text":""},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#11","title":"\u7b2c11\u7ae0 \u9806\u5e8f\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u3092\u5bfe\u8c61\u3068\u3057\u305f\u56e0\u5b50\u5206\u6790\u306e\u5b9f\u8df5","text":"<p>\uff08Airline Passenger Satisfaction \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u305f\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u30fb\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u30d9\u30fc\u30b9\u306e\u89e3\u6790\uff09</p> <p>\u672c\u7ae0\u3067\u306f\u3001Airline Passenger Satisfaction \u306b\u95a2\u3059\u308b\u5b9f\u30c7\u30fc\u30bf\uff08Kaggle\u63d0\u4f9b\uff09\u3092\u7528\u3044\u3066\u3001\u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\uff088\u5217\u301c24\u5217\uff1a\u6e80\u8db3\u5ea6\u306b\u95a2\u3059\u308b\u9805\u76ee\uff09\u306b\u5bfe\u3057\u3066\u3001\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\uff08\u304a\u3088\u3073\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\uff09\u3092\u5229\u7528\u3057\u305f\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3057\u307e\u3059\u3002 \u89e3\u6790\u306e\u6d41\u308c\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> <ol> <li>\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068\u524d\u51e6\u7406 </li> <li>\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8  </li> <li>\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f  </li> <li> <p>\u5bfe\u8c61\u3068\u306a\u308b\u9806\u5e8f\u30c7\u30fc\u30bf\uff08\u52178\u301c24\uff09\u306e\u62bd\u51fa\u3068\u524d\u51e6\u7406</p> </li> <li> <p>\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u306e\u7b97\u51fa </p> </li> <li> <p>Python \u306e <code>semopy</code> \u30e2\u30b8\u30e5\u30fc\u30eb\u306e <code>polychoric_corr</code> \u95a2\u6570\u3092\u7528\u3044\u3066\u3001\u9806\u5e8f\u5909\u6570\u540c\u58eb\u306e\u76f8\u95a2\u884c\u5217\u3092\u63a8\u5b9a</p> </li> <li> <p>\u56e0\u5b50\u62bd\u51fa\u3068\u56de\u8ee2 </p> </li> <li>\u5f97\u3089\u308c\u305f\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3092\u5165\u529b\u3068\u3057\u3066\u3001\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\uff08\u3053\u3053\u3067\u306f\u6700\u5c24\u6cd5\u3092\u4f7f\u7528\uff09</li> <li>\u76f4\u4ea4\u56de\u8ee2\uff08Varimax\uff09\u3068\u659c\u4ea4\u56de\u8ee2\uff08Promax\uff09\u306e\u4e21\u65b9\u306e\u7d50\u679c\u3092\u6bd4\u8f03</li> <li> <p>\u56fa\u6709\u5024\u30d7\u30ed\u30c3\u30c8\uff08\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\uff09\u306b\u3088\u308b\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a\u3082\u884c\u3046</p> </li> <li> <p>\u611f\u5ea6\u5206\u6790\uff08\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\uff09 </p> </li> <li> <p>\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3092\u7528\u3044\u3066\u3001\u63a8\u5b9a\u3055\u308c\u305f\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u5b89\u5b9a\u6027\uff08\u4fe1\u983c\u533a\u9593\uff09\u3092\u8a55\u4fa1</p> </li> <li> <p>\u7d50\u679c\u306e\u53ef\u8996\u5316\u3068\u89e3\u91c8 </p> </li> <li>\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3084\u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217\u306e\u30d7\u30ed\u30c3\u30c8\u306a\u3069\u3092\u7528\u3044\u3066\u3001\u7d50\u679c\u3092\u8996\u899a\u5316  </li> <li>\u5f97\u3089\u308c\u305f\u56e0\u5b50\u69cb\u9020\u306e\u89e3\u91c8\u304a\u3088\u3073\u3001Varimax \u3068 Promax \u306e\u9055\u3044\u306b\u3064\u3044\u3066\u8b70\u8ad6</li> </ol> <p>\u4ee5\u4e0b\u3001\u5404\u30b9\u30c6\u30c3\u30d7\u306e Python \u30b3\u30fc\u30c9\u3068\u89e3\u8aac\u3092\u793a\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#111","title":"11.1. \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068\u524d\u51e6\u7406","text":"<p>\u307e\u305a\u3001\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002 Airline Passenger Satisfaction \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f Kaggle \u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u6e80\u8db3\u5ea6\u306b\u95a2\u3059\u308b\u30c7\u30fc\u30bf\u306f 8 \u5217\u76ee\u304b\u3089 24 \u5217\u76ee\u306b\u8a72\u5f53\u3057\u307e\u3059\uff08\u3053\u3053\u3067\u306f\u4eee\u306b\u5217\u540d\u304c <code>Q1</code> \uff5e <code>Q17</code> \u3068\u3057\u3066\u6271\u3044\u307e\u3059\uff09\u3002</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom semopy import polychoric_corr\nfrom factor_analyzer import FactorAnalyzer\nfrom sklearn.preprocessing import StandardScaler\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\uff08\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u9069\u5b9c\u8abf\u6574\uff09\ndata = pd.read_csv(\"Airline_Passenger_Satisfaction.csv\")\n\n# \u30c7\u30fc\u30bf\u306e\u6982\u8981\u3092\u78ba\u8a8d\nprint(data.head())\nprint(data.columns)\n\n# \u6e80\u8db3\u5ea6\u306b\u95a2\u3059\u308b\u5217\uff08\u4f8b: 8\u5217\u76ee\u304b\u308924\u5217\u76ee\uff09\n# \u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5217\u540d\u306b\u5408\u308f\u305b\u3066\u8abf\u6574\u3057\u3066\u304f\u3060\u3055\u3044\nordinal_cols = data.columns[7:24]  # Python\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306f0\u59cb\u307e\u308a\u306e\u305f\u3081\u30017\u301c23\u5217\u76ee\ndata_ord = data[ordinal_cols].copy()\n\n# \u5404\u5217\u306e\u30c7\u30fc\u30bf\u578b\u3084\u5206\u5e03\u306e\u78ba\u8a8d\uff08\u9806\u5e8f\u5c3a\u5ea6\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\uff09\nprint(data_ord.dtypes)\ndata_ord.describe()\n</code></pre> <p>\u3010\u89e3\u8aac\u3011 - \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u6e80\u8db3\u5ea6\u306b\u95a2\u3059\u308b\u5217\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002 - \u3053\u308c\u3089\u306e\u5909\u6570\u306f\u9806\u5e8f\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u30c7\u30fc\u30bf\uff08\u4f8b\uff1a\u975e\u5e38\u306b\u4e0d\u6e80\u301c\u975e\u5e38\u306b\u6e80\u8db3\uff09\u3068\u3057\u3066\u6271\u308f\u308c\u308b\u524d\u63d0\u3067\u3059\u3002</p>"},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#112","title":"11.2. \u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u306e\u7b97\u51fa","text":"<p><code>semopy</code> \u306e <code>polychoric_corr</code> \u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u3001\u9806\u5e8f\u5909\u6570\u9593\u306e\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002</p> <pre><code># semopy \u306e polychoric_corr \u3092\u7528\u3044\u3066\u3001\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3092\u7b97\u51fa\n# polychoric_corr \u306f\u3001\u9806\u5e8f\u30c7\u30fc\u30bf\u306e\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3068\u95be\u5024\u306e\u63a8\u5b9a\u3082\u884c\u3044\u307e\u3059\u3002\npoly_corr_matrix, thresholds = polychoric_corr(data_ord)\n\n# \u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u306e\u78ba\u8a8d\npoly_corr_df = pd.DataFrame(poly_corr_matrix, index=ordinal_cols, columns=ordinal_cols)\nprint(\"\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217:\")\nprint(poly_corr_df)\n\n# \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u306b\u3088\u308b\u53ef\u8996\u5316\nplt.figure(figsize=(10,8))\nsns.heatmap(poly_corr_df, annot=True, cmap='coolwarm')\nplt.title(\"\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\")\nplt.show()\n</code></pre> <p>\u3010\u89e3\u8aac\u3011 - <code>polychoric_corr</code> \u306f\u3001\u9806\u5e8f\u5909\u6570\u540c\u58eb\u306e\u80cc\u5f8c\u306b\u3042\u308b\u9023\u7d9a\u6f5c\u5728\u5909\u6570\u306e\u76f8\u95a2\u3092\u6700\u5c24\u6cd5\u306a\u3069\u3067\u63a8\u5b9a\u3057\u3001\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3092\u8fd4\u3057\u307e\u3059\u3002 - \u3053\u306e\u884c\u5217\u306f\u3001\u56e0\u5b50\u5206\u6790\u306e\u5165\u529b\u3068\u3057\u3066\u5229\u7528\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#113","title":"11.3. \u56e0\u5b50\u62bd\u51fa\u3068\u56de\u8ee2","text":""},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#1131","title":"11.3.1 \u56e0\u5b50\u6570\u306e\u6c7a\u5b9a\uff08\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\uff09","text":"<p>\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3092\u7528\u3044\u3066\u3001\u56e0\u5b50\u6570\u306e\u6c7a\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002 \u56e0\u5b50\u5206\u6790\u306b\u306f\u3001<code>FactorAnalyzer</code> \u3092\u7528\u3044\u3001<code>is_corr_matrix=True</code> \u3092\u6307\u5b9a\u3057\u3066\u76f8\u95a2\u884c\u5217\u3092\u5165\u529b\u3057\u307e\u3059\u3002</p> <pre><code># FactorAnalyzer \u3092\u7528\u3044\u3066\u3001\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u304b\u3089\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3059\u308b\u305f\u3081\u306b\u3001\n# is_corr_matrix \u30d1\u30e9\u30e1\u30fc\u30bf\u3092 True \u306b\u8a2d\u5b9a\nfa_temp = FactorAnalyzer(rotation=None, is_corr_matrix=True)\nfa_temp.fit(poly_corr_matrix)\n\n# \u56fa\u6709\u5024\u3092\u53d6\u5f97\nev, v = fa_temp.get_eigenvalues()\n\nplt.figure(figsize=(8,4))\nplt.scatter(range(1, len(ev)+1), ev)\nplt.plot(range(1, len(ev)+1), ev, 'b-')\nplt.title('\u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\uff08\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\uff09')\nplt.xlabel('\u56e0\u5b50\u6570')\nplt.ylabel('\u56fa\u6709\u5024')\nplt.axhline(1, color='red', linestyle='--')\nplt.grid(True)\nplt.show()\n</code></pre> <p>\u3010\u89e3\u8aac\u3011 - \u30b9\u30af\u30ea\u30fc\u30d7\u30ed\u30c3\u30c8\u304b\u3089\u3001\u56fa\u6709\u5024\u304c1\u4ee5\u4e0a\u3068\u306a\u308b\u56e0\u5b50\u6570\u3001\u307e\u305f\u306f\u6298\u308c\u66f2\u304c\u308a\u70b9\u3092\u57fa\u306b\u56e0\u5b50\u6570\u3092\u6c7a\u5b9a\u3057\u307e\u3059\u3002 - \u3053\u306e\u4f8b\u3067\u306f\u4eee\u306b 3 \u56e0\u5b50\u3092\u63a1\u7528\u3057\u307e\u3059\uff08\u30c7\u30fc\u30bf\u306b\u5fdc\u3058\u3066\u8abf\u6574\u3057\u3066\u304f\u3060\u3055\u3044\uff09\u3002</p>"},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#1132-varimax-promax","title":"11.3.2 \u56e0\u5b50\u5206\u6790\u306e\u5b9f\u65bd\uff08Varimax \u3068 Promax \u306e\u6bd4\u8f03\uff09","text":""},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#11321-varimax","title":"11.3.2.1 Varimax \u56de\u8ee2\uff08\u76f4\u4ea4\u56de\u8ee2\uff09\u306e\u5834\u5408","text":"<pre><code># Varimax\u56de\u8ee2\u3092\u7528\u3044\u3066\u56e0\u5b50\u5206\u6790\uff083\u56e0\u5b50\u3001\u6700\u5c24\u6cd5\u3001\u76f8\u95a2\u884c\u5217\u5165\u529b\uff09\nfa_varimax = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml', is_corr_matrix=True)\nfa_varimax.fit(poly_corr_matrix)\n\n# \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u53d6\u5f97\nloadings_varimax = pd.DataFrame(fa_varimax.loadings_, index=ordinal_cols,\n                                  columns=['Factor1', 'Factor2', 'Factor3'])\nprint(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Varimax\u56de\u8ee2\uff09:\")\nprint(loadings_varimax)\n\n# \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3067\u53ef\u8996\u5316\nplt.figure(figsize=(10,8))\nsns.heatmap(loadings_varimax, annot=True, cmap='coolwarm')\nplt.title(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Varimax\u56de\u8ee2\uff09\")\nplt.show()\n</code></pre>"},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#11322-promax","title":"11.3.2.2 Promax \u56de\u8ee2\uff08\u659c\u4ea4\u56de\u8ee2\uff09\u306e\u5834\u5408","text":"<pre><code># Promax\u56de\u8ee2\u3092\u7528\u3044\u3066\u56e0\u5b50\u5206\u6790\uff083\u56e0\u5b50\u3001\u6700\u5c24\u6cd5\u3001\u76f8\u95a2\u884c\u5217\u5165\u529b\uff09\nfa_promax = FactorAnalyzer(n_factors=3, rotation='promax', method='ml', is_corr_matrix=True)\nfa_promax.fit(poly_corr_matrix)\n\n# \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u53d6\u5f97\nloadings_promax = pd.DataFrame(fa_promax.loadings_, index=ordinal_cols,\n                                 columns=['Factor1', 'Factor2', 'Factor3'])\nprint(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Promax\u56de\u8ee2\uff09:\")\nprint(loadings_promax)\n\n# \u56e0\u5b50\u9593\u306e\u76f8\u95a2\u884c\u5217\uff08Promax\u56de\u8ee2\u306e\u5834\u5408\uff09\nphi = fa_promax.get_factor_correlation()\nphi_df = pd.DataFrame(phi, index=['Factor1', 'Factor2', 'Factor3'], columns=['Factor1', 'Factor2', 'Factor3'])\nprint(\"\\n\u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217\uff08Promax\u56de\u8ee2\uff09:\")\nprint(phi_df)\n\n# \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3067\u53ef\u8996\u5316\nplt.figure(figsize=(10,8))\nsns.heatmap(loadings_promax, annot=True, cmap='coolwarm')\nplt.title(\"\u56e0\u5b50\u8ca0\u8377\u884c\u5217\uff08Promax\u56de\u8ee2\uff09\")\nplt.show()\n</code></pre> <p>\u3010\u89e3\u8aac\u3011 - Varimax \u56de\u8ee2\u306f\u56e0\u5b50\u9593\u306e\u72ec\u7acb\u6027\u3092\u4fdd\u6301\u3057\u3001\u5404\u5909\u6570\u304c\u3069\u306e\u56e0\u5b50\u306b\u5f37\u304f\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u304c\u660e\u78ba\u3067\u3059\u3002 - Promax \u56de\u8ee2\u306f\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u3092\u8a31\u5bb9\u3059\u308b\u305f\u3081\u3001\u3088\u308a\u67d4\u8edf\u306a\u30e2\u30c7\u30eb\u304c\u5f97\u3089\u308c\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u53cd\u6620\u3057\u3084\u3059\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002 - \u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217\uff08phi_df\uff09\u3092\u78ba\u8a8d\u3057\u3001\u5404\u56e0\u5b50\u306e\u95a2\u9023\u6027\u3092\u691c\u8a0e\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#114","title":"11.4. \u611f\u5ea6\u5206\u6790\uff08\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\uff09","text":"<p>\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3092\u7528\u3044\u3066\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u63a8\u5b9a\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u4f8b\u3068\u3057\u3066\u300cQ1\u300d\uff08ordinal_cols \u306e\u5148\u982d\u306e\u5909\u6570\uff09\u306e Factor1 \u306e\u8ca0\u8377\u91cf\u306b\u5bfe\u3059\u308b 95% \u4fe1\u983c\u533a\u9593\u3092\u6c42\u3081\u307e\u3059\u3002</p> <pre><code>n_boot = 500  # \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u56de\u6570\nboot_loadings = []\n\nfor i in range(n_boot):\n    # \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u30b5\u30f3\u30d7\u30eb\uff08\u76f8\u95a2\u884c\u5217\u306e\u518d\u8a08\u7b97\u306f\u5fc5\u8981\u306a\u5834\u5408\u304c\u3042\u308b\u304c\u3001\u3053\u3053\u3067\u306f\u30c7\u30fc\u30bf\u306e\u518d\u62bd\u51fa\u3068\u518d\u8a08\u7b97\u306e\u6d41\u308c\u3092\u793a\u3059\uff09\n    boot_indices = np.random.choice(data_ord.index, size=len(data_ord), replace=True)\n    data_boot = data_ord.loc[boot_indices]\n\n    # \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u30b5\u30f3\u30d7\u30eb\u306b\u5bfe\u3057\u3066\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u306e\u7b97\u51fa\n    boot_poly_corr, _ = polychoric_corr(data_boot)\n\n    # \u56e0\u5b50\u5206\u6790\uff08Varimax\u56de\u8ee2\uff09\u3092\u5b9f\u65bd\uff08\u540c\u30583\u56e0\u5b50\u3068\u3057\u3066\uff09\n    fa_boot = FactorAnalyzer(n_factors=3, rotation='varimax', method='ml', is_corr_matrix=True)\n    fa_boot.fit(boot_poly_corr)\n\n    boot_loadings.append(fa_boot.loadings_)\n\n# \u5bfe\u8c61\u5909\u6570: ordinal_cols[0]\uff08\u4f8b: Q1\uff09\u304a\u3088\u3073 Factor1\nq1_idx = 0  # ordinal_cols \u306e\u6700\u521d\u306e\u5909\u6570\nfactor1_values = [boot[q1_idx, 0] for boot in boot_loadings]\n\nlower_bound = np.percentile(factor1_values, 2.5)\nupper_bound = np.percentile(factor1_values, 97.5)\nprint(\"Q1\u306eFactor1\u8ca0\u8377\u91cf\u306e95%\u4fe1\u983c\u533a\u9593: [{:.3f}, {:.3f}]\".format(lower_bound, upper_bound))\n</code></pre> <p>\u3010\u89e3\u8aac\u3011 - \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306b\u3088\u308a\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u5b89\u5b9a\u6027\u3084\u4fe1\u983c\u533a\u9593\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002 - \u7d50\u679c\u304c\u72ed\u3044\u4fe1\u983c\u533a\u9593\u306b\u53ce\u307e\u308c\u3070\u3001\u63a8\u5b9a\u7d50\u679c\u304c\u5b89\u5b9a\u3057\u3066\u3044\u308b\u3068\u5224\u65ad\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#115","title":"11.5. \u7d50\u679c\u306e\u89e3\u91c8","text":"<ul> <li> <p>\u56e0\u5b50\u8ca0\u8377\u884c\u5217:   \u305d\u308c\u305e\u308c\u306e\u56de\u8ee2\u6cd5\uff08Varimax, Promax\uff09\u3067\u5f97\u3089\u308c\u305f\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3092\u5143\u306b\u3001\u5404\u56e0\u5b50\u304c\u3069\u306e\u6e80\u8db3\u5ea6\u9805\u76ee\u306b\u5f37\u304f\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3092\u691c\u8a0e\u3057\u307e\u3059\u3002   \u4f8b\u3048\u3070\u3001\u3082\u3057\u3042\u308b\u56e0\u5b50\u304c\u300c\u30c1\u30a7\u30c3\u30af\u30a4\u30f3\u30b5\u30fc\u30d3\u30b9\u300d\u3084\u300c\u6a5f\u5185\u30b5\u30fc\u30d3\u30b9\u300d\u306a\u3069\u306e\u9805\u76ee\u306b\u9ad8\u3044\u8ca0\u8377\u3092\u793a\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u305d\u306e\u56e0\u5b50\u306f\u300c\u30b5\u30fc\u30d3\u30b9\u54c1\u8cea\u300d\u3092\u53cd\u6620\u3057\u3066\u3044\u308b\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p> </li> <li> <p>\u56e0\u5b50\u9593\u76f8\u95a2:   Promax \u56de\u8ee2\u306e\u5834\u5408\u3001\u56e0\u5b50\u9593\u306e\u76f8\u95a2\u304c\u793a\u3055\u308c\u308b\u305f\u3081\u3001\u95a2\u9023\u6027\u306e\u5f37\u3044\u56e0\u5b50\u540c\u58eb\u306f\u7d71\u5408\u7684\u306b\u89e3\u91c8\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p> </li> <li> <p>\u611f\u5ea6\u5206\u6790:   \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u306e\u7d50\u679c\u3001\u5404\u5909\u6570\u306e\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u4fe1\u983c\u533a\u9593\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3067\u3001\u63a8\u5b9a\u7d50\u679c\u306e\u5b89\u5b9a\u6027\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002\u5b89\u5b9a\u3057\u305f\u7d50\u679c\u304c\u5f97\u3089\u308c\u3066\u3044\u308c\u3070\u3001\u30e2\u30c7\u30eb\u306e\u4fe1\u983c\u6027\u304c\u9ad8\u3044\u3068\u5224\u65ad\u3067\u304d\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/11-airline-passenger-satisfaction-analysis/#116","title":"11.6. \u307e\u3068\u3081","text":"<p>\u672c\u7ae0\u3067\u306f\u3001Airline Passenger Satisfaction \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\uff08\u6e80\u8db3\u5ea6\u306b\u95a2\u3059\u308b8\u301c24\u5217\uff09\u306b\u5bfe\u3057\u3001\u4ee5\u4e0b\u306e\u30d7\u30ed\u30bb\u30b9\u3067\u56e0\u5b50\u5206\u6790\u3092\u5b9f\u65bd\u3057\u307e\u3057\u305f\u3002</p> <ol> <li> <p>\u30c7\u30fc\u30bf\u524d\u51e6\u7406:    \u5bfe\u8c61\u5909\u6570\u306e\u62bd\u51fa\u3068\u524d\u51e6\u7406\uff08\u5fc5\u8981\u306b\u5fdc\u3058\u305f\u6b20\u640d\u5024\u51e6\u7406\u306a\u3069\uff09</p> </li> <li> <p>\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u306e\u7b97\u51fa: <code>semopy.polychoric_corr</code> \u3092\u7528\u3044\u3066\u3001\u9806\u5e8f\u30c7\u30fc\u30bf\u9593\u306e\u76f8\u95a2\u884c\u5217\u3092\u63a8\u5b9a</p> </li> <li> <p>\u56e0\u5b50\u62bd\u51fa\u3068\u56de\u8ee2:    \u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u76f8\u95a2\u884c\u5217\u3092\u5165\u529b\u3068\u3057\u3001\u56e0\u5b50\u5206\u6790\u3092\u6700\u5c24\u6cd5\u3067\u5b9f\u65bd\u3002Varimax\uff08\u76f4\u4ea4\u56de\u8ee2\uff09\u3068 Promax\uff08\u659c\u4ea4\u56de\u8ee2\uff09\u306e\u7d50\u679c\u3092\u6bd4\u8f03</p> </li> <li> <p>\u611f\u5ea6\u5206\u6790:    \u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3092\u7528\u3044\u3066\u3001\u56e0\u5b50\u8ca0\u8377\u91cf\u306e\u5b89\u5b9a\u6027\uff08\u4fe1\u983c\u533a\u9593\uff09\u3092\u8a55\u4fa1</p> </li> <li> <p>\u7d50\u679c\u306e\u89e3\u91c8:    \u5f97\u3089\u308c\u305f\u56e0\u5b50\u69cb\u9020\u3092\u7406\u8ad6\u7684\u80cc\u666f\u3068\u7167\u3089\u3057\u5408\u308f\u305b\u3001\u5404\u56e0\u5b50\u306e\u610f\u5473\u4ed8\u3051\u3092\u884c\u3044\u3001\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3084\u56e0\u5b50\u9593\u76f8\u95a2\u884c\u5217\u3092\u7528\u3044\u3066\u8996\u899a\u5316</p> </li> </ol> <p>\u3053\u308c\u3089\u306e\u624b\u9806\u3092\u901a\u3058\u3066\u3001\u9806\u5e8f\u306e\u3042\u308b\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306b\u5bfe\u3059\u308b\u56e0\u5b50\u5206\u6790\u306e\u624b\u6cd5\u3068\u3001\u305d\u306e\u7d50\u679c\u306e\u89e3\u91c8\u65b9\u6cd5\u3092\u5b9f\u8df5\u7684\u306b\u5b66\u3076\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002\u8aad\u8005\u306f\u3053\u306e Demo \u3092\u53c2\u8003\u306b\u3001\u81ea\u8eab\u306e\u9806\u5e8f\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3082\u30dd\u30ea\u30b3\u30ea\u30c3\u30af\u30fb\u30dd\u30ea\u30bb\u30ea\u30c3\u30af\u76f8\u95a2\u3092\u5229\u7528\u3057\u305f\u56e0\u5b50\u5206\u6790\u3092\u9069\u7528\u3057\u3001\u5b9f\u52d9\u306b\u5f79\u7acb\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3067\u3057\u3087\u3046\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/","title":"\u7b2c12\u7ae0 \u884c\u5217\u5206\u89e3\u3068\u56e0\u5b50\u5206\u6790\u306e\u3064\u306a\u304c\u308a","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u884c\u5217\u5206\u89e3\u306e\u6280\u6cd5\u3068\u56e0\u5b50\u5206\u6790\u3068\u306e\u5bc6\u63a5\u306a\u95a2\u4fc2\u306b\u3064\u3044\u3066\u3001\u6570\u7406\u7684\u306a\u89b3\u70b9\u304b\u3089\u8a73\u7d30\u306b\u8b70\u8ad6\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u7279\u306b\u4ee5\u4e0b\u306e\u70b9\u306b\u7126\u70b9\u3092\u5f53\u3066\u307e\u3059\u3002</p> <ul> <li>\u884c\u5217\u5206\u89e3\u306e\u57fa\u672c\u6982\u5ff5\u3068\u305d\u306e\u4ee3\u8868\u7684\u306a\u624b\u6cd5\uff08\u7279\u306b\u56fa\u6709\u5024\u5206\u89e3\u304a\u3088\u3073\u7279\u7570\u5024\u5206\u89e3\uff09</li> <li>\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3068\u56e0\u5b50\u5206\u6790\u306e\u6570\u5b66\u7684\u95a2\u4fc2</li> <li>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u884c\u5217\u5206\u89e3\u306e\u5f79\u5272\u3068\u3001\u305d\u306e\u63a8\u5b9a\u65b9\u6cd5\u306e\u80cc\u666f</li> <li>\u30e2\u30c7\u30eb\u306e\u540c\u5b9a\u6027\u3001\u56de\u8ee2\u3001\u81ea\u7531\u5ea6\u306b\u95a2\u3059\u308b\u7406\u8ad6\u7684\u8b70\u8ad6</li> </ul> <p>\u672c\u7ae0\u306f\u3001\u7814\u7a76\u8005\u3084\u5927\u5b66\u9662\u751f\u5411\u3051\u306e\u30ec\u30af\u30c1\u30e3\u30fc\u30ce\u30fc\u30c8\u3068\u3057\u3066\u3001\u30c6\u30af\u30cb\u30ab\u30eb\u306a\u5185\u5bb9\u3092\u4e2d\u5fc3\u306b\u89e3\u8aac\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#121","title":"12.1 \u884c\u5217\u5206\u89e3\u306e\u57fa\u790e","text":""},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1211-eigenvalue-decomposition","title":"12.1.1 \u56fa\u6709\u5024\u5206\u89e3\uff08Eigenvalue Decomposition\uff09","text":"<p>\u4efb\u610f\u306e $ p \\times p $ \u306e\u5bfe\u79f0\u884c\u5217 $ \\mathbf{A} $\uff08\u4f8b\u3048\u3070\u5206\u6563\u5171\u5206\u6563\u884c\u5217 $ \\Sigma $\uff09\u306f\u3001\u56fa\u6709\u5024\u5206\u89e3\u306b\u3088\u308a\u6b21\u306e\u3088\u3046\u306b\u8868\u3055\u308c\u307e\u3059\u3002</p> \\[ \\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^\\top, \\] <ul> <li>$ \\mathbf{Q} $ \u306f\u76f4\u4ea4\u884c\u5217\u3067\u3042\u308a\u3001\u305d\u306e\u5217\u306f $ \\mathbf{A} $ \u306e\u56fa\u6709\u30d9\u30af\u30c8\u30eb</li> <li>$ \\mathbf{\\Lambda} $ \u306f\u5bfe\u89d2\u884c\u5217\u3067\u3001\u5bfe\u89d2\u8981\u7d20\u306f\u56fa\u6709\u5024 \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_p\\)</li> </ul> <p>\u3053\u306e\u5206\u89e3\u306f\u3001\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3084\u56e0\u5b50\u5206\u6790\u306b\u304a\u3051\u308b\u6b21\u5143\u524a\u6e1b\u306e\u57fa\u790e\u3068\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1212-singular-value-decomposition-svd","title":"12.1.2 \u7279\u7570\u5024\u5206\u89e3\uff08Singular Value Decomposition, SVD\uff09","text":"<p>\u4efb\u610f\u306e $ m \\times n $ \u884c\u5217 $ \\mathbf{X} $ \u306b\u5bfe\u3057\u3066\u3001SVD \u306f\u6b21\u306e\u3088\u3046\u306b\u5206\u89e3\u3055\u308c\u307e\u3059\u3002</p> \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{S} \\mathbf{V}^\\top, \\] <ul> <li>$ \\mathbf{U} $ \u306f $ m \\times m $ \u306e\u76f4\u4ea4\u884c\u5217</li> <li>$ \\mathbf{S} $ \u306f $ m \\times n $ \u306e\u5bfe\u89d2\u884c\u5217\uff08\u305f\u3060\u3057\u3001\u5bfe\u89d2\u8981\u7d20\u306f\u975e\u8ca0\u306e\u7279\u7570\u5024\uff09</li> <li>$ \\mathbf{V} $ \u306f $ n \\times n $ \u306e\u76f4\u4ea4\u884c\u5217</li> </ul> <p>SVD\u306f\u3001\u30c7\u30fc\u30bf\u306e\u69cb\u9020\u3092\u660e\u793a\u3059\u308b\u305f\u3081\u306e\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u3067\u3042\u308a\u3001PCA \u306e\u8a08\u7b97\u306b\u304a\u3044\u3066\u3082\u5e83\u304f\u7528\u3044\u3089\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#122-pca","title":"12.2 \u4e3b\u6210\u5206\u5206\u6790 (PCA) \u3068\u56e0\u5b50\u5206\u6790\u306e\u6570\u5b66\u7684\u95a2\u4fc2","text":""},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1221-pca","title":"12.2.1 PCA\u306e\u6570\u5b66\u7684\u67a0\u7d44\u307f","text":"<p>PCA\u306f\u3001\u30c7\u30fc\u30bf\u884c\u5217 \\(\\mathbf{X}\\) \u306e\u5206\u6563\u5171\u5206\u6563\u69cb\u9020\u3092\u6700\u5927\u9650\u306b\u8aac\u660e\u3059\u308b\u76f4\u4ea4\u57fa\u5e95\u3092\u898b\u3064\u3051\u308b\u624b\u6cd5\u3067\u3059\u3002 \u4e2d\u5fc3\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\u884c\u5217 \\(\\mathbf{X}\\)\uff08\u5404\u5217\u306e\u5e73\u5747\u304c0\uff09\u306f\u3001SVD\u306b\u3088\u308a\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5206\u89e3\u3055\u308c\u307e\u3059\u3002</p> \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{S} \\mathbf{V}^\\top. \\] <p>\u4e3b\u6210\u5206\u306f\u3001\u53f3\u5074\u306e\u76f4\u4ea4\u884c\u5217 $ \\mathbf{V} $ \u306e\u5217\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308a\u3001\u56fa\u6709\u5024\u306f $ \\mathbf{S}^2/(n-1) $ \u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002 \u3053\u306e\u67a0\u7d44\u307f\u3067\u3001\u5206\u6563\u306e\u5927\u90e8\u5206\u3092\u5360\u3081\u308b\u6210\u5206\u3092\u62bd\u51fa\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u6b21\u5143\u524a\u6e1b\u304c\u5b9f\u73fe\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1222","title":"12.2.2 \u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u3068\u306e\u6bd4\u8f03","text":"<p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u306f\u3001\u89b3\u6e2c\u5909\u6570 $ \\mathbf{x} $ \u3092\u6f5c\u5728\u56e0\u5b50 $ \\mathbf{f} $ \u3068\u7279\u6709\u8aa4\u5dee $ \\boldsymbol{\\epsilon} $ \u306e\u7dda\u5f62\u7d50\u5408\u3068\u3057\u3066\u30e2\u30c7\u30eb\u5316\u3057\u307e\u3059\u3002</p> \\[ \\mathbf{x} = \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, $$ $$ \\operatorname{Cov}(\\mathbf{x}) = \\Sigma = \\Lambda\\Lambda^\\top + \\Psi, \\] <p>\u3053\u3053\u3067\u3001\\(\\Lambda\\) \u306f\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u3001\\(\\Psi\\) \u306f\u7279\u6709\u5206\u6563\uff08\u5bfe\u89d2\u884c\u5217\uff09\u3067\u3059\u3002 PCA\u306e\u5834\u5408\u3001\u5168\u5206\u6563\u306e\u69cb\u9020\u306b\u57fa\u3065\u304d\u76f4\u4ea4\u5909\u63db\u3092\u884c\u3046\u306e\u306b\u5bfe\u3057\u3001\u56e0\u5b50\u5206\u6790\u306f\u5171\u901a\u56e0\u5b50\uff08\\(\\Lambda\\Lambda^\\top\\)\uff09\u3068\u7279\u6709\u56e0\u5b50\uff08\\(\\Psi\\)\uff09\u306b\u5206\u89e3\u3059\u308b\u70b9\u3067\u7570\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#_1","title":"\u540c\u5b9a\u6027\u306e\u554f\u984c","text":"<p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u306f\u3001\u56de\u8ee2\u4e0d\u5909\u6027\uff08\u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306f\u4efb\u610f\u306e\u76f4\u4ea4\u5909\u63db\u306b\u3088\u3063\u3066\u8868\u73fe\u3067\u304d\u308b\uff09\u306a\u3069\u306e\u7406\u7531\u304b\u3089\u3001\u540c\u5b9a\u6027\u306b\u95a2\u3059\u308b\u554f\u984c\u304c\u751f\u3058\u307e\u3059\u3002 \u3053\u308c\u3092\u6570\u5b66\u7684\u306b\u6271\u3046\u305f\u3081\u306b\u3001\u5236\u7d04\u6761\u4ef6\uff08\u4f8b\u3048\u3070\u3001\u6f5c\u5728\u56e0\u5b50\u306e\u5206\u6563\u3092\u5358\u4f4d\u884c\u5217\u306b\u56fa\u5b9a\u3059\u308b\u306a\u3069\uff09\u3092\u5c0e\u5165\u3057\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#123","title":"12.3 \u56e0\u5b50\u5206\u6790\u306b\u304a\u3051\u308b\u884c\u5217\u5206\u89e3\u306e\u8996\u70b9","text":""},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1231","title":"12.3.1 \u884c\u5217\u5206\u89e3\u3068\u3057\u3066\u306e\u56e0\u5b50\u5206\u6790","text":"<p>\u56e0\u5b50\u5206\u6790\u306f\u3001\u89b3\u6e2c\u5909\u6570\u306e\u5171\u5206\u6563\u884c\u5217 \\(\\Sigma\\) \u3092\u3001\u4f4e\u30e9\u30f3\u30af\u884c\u5217 \\(\\Lambda\\Lambda^\\top\\) \u3068\u5bfe\u89d2\u884c\u5217 \\(\\Psi\\) \u306b\u5206\u89e3\u3059\u308b\u554f\u984c\u3068\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3053\u308c\u306f\u4ee5\u4e0b\u306e\u6700\u9069\u5316\u554f\u984c\u306b\u5e30\u7740\u3055\u308c\u307e\u3059\u3002</p> \\[ \\min_{\\Lambda,\\Psi} \\|\\Sigma - (\\Lambda\\Lambda^\\top + \\Psi)\\|_F^2, \\] <p>\u305f\u3060\u3057\u3001\\(\\Psi\\) \u306f\u5bfe\u89d2\u884c\u5217\u3067\u3042\u308b\u3068\u3044\u3046\u5236\u7d04\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u554f\u984c\u306f\u3001\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u306e\u4e00\u7a2e\u3068\u3057\u3066\u3001\u884c\u5217\u5206\u89e3\u306e\u67a0\u7d44\u307f\u3067\u7406\u89e3\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1232","title":"12.3.2 \u56de\u8ee2\u3068\u975e\u4e00\u610f\u6027","text":"<p>\u884c\u5217 \\(\\Lambda\\) \u306e\u89e3\u306f\u3001\u4efb\u610f\u306e\u76f4\u4ea4\u884c\u5217 $ \\mathbf{T} $ \u306b\u5bfe\u3057\u3066\u3001</p> \\[ \\Lambda^* = \\Lambda \\mathbf{T} $$ $$ \\Lambda^*(\\Lambda^*)^\\top = \\Lambda \\mathbf{T} \\mathbf{T}^\\top \\Lambda^\\top = \\Lambda\\Lambda^\\top, \\] <p>\u3068\u306a\u308b\u305f\u3081\u3001\u56e0\u5b50\u5206\u6790\u306e\u89e3\u306f\u975e\u4e00\u610f\u3067\u3059\u3002 \u3053\u306e\u6027\u8cea\u306f\u3001\u56e0\u5b50\u56de\u8ee2\uff08Varimax, Promax \u306a\u3069\uff09\u306e\u7406\u8ad6\u7684\u6839\u62e0\u3068\u3082\u306a\u308a\u3001\u89e3\u91c8\u3092\u5bb9\u6613\u306b\u3059\u308b\u305f\u3081\u306e\u300c\u5358\u7d14\u69cb\u9020\u300d\u3092\u76ee\u6307\u3059\u969b\u306e\u81ea\u7531\u5ea6\u3068\u3057\u3066\u5229\u7528\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#124","title":"12.4 \u6570\u7406\u7684\u306a\u63a8\u5b9a\u3068\u6700\u5c24\u6cd5","text":""},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1241","title":"12.4.1 \u6700\u5c24\u6cd5\u306e\u5b9a\u5f0f\u5316","text":"<p>\u56e0\u5b50\u5206\u6790\u306e\u6700\u5c24\u6cd5\u3067\u306f\u3001\u89b3\u6e2c\u30c7\u30fc\u30bf $ \\mathbf{x}_i $ \u304c\u591a\u5909\u91cf\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u4eee\u5b9a\u3057\u307e\u3059\u3002</p> \\[ \\mathbf{x}_i \\sim N(\\mathbf{0}, \\Sigma) \\quad \\text{\u3067} \\quad \\Sigma = \\Lambda\\Lambda^\\top + \\Psi. \\] <p>\u5bfe\u6570\u5c24\u5ea6\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8868\u73fe\u3055\u308c\u307e\u3059\u3002</p> \\[ \\ell(\\Lambda, \\Psi) = -\\frac{n}{2} \\left[ \\ln |\\Sigma| + \\operatorname{tr}(S \\Sigma^{-1}) \\right] + \\text{\u5b9a\u6570}, \\] <p>\u3053\u3053\u3067\u3001$ S $ \u306f\u30b5\u30f3\u30d7\u30eb\u5171\u5206\u6563\u884c\u5217\u3067\u3059\u3002 \u3053\u306e\u6700\u5c24\u95a2\u6570\u3092\u6700\u5927\u5316\u3059\u308b\u554f\u984c\u306f\u3001\u975e\u7dda\u5f62\u6700\u9069\u5316\u554f\u984c\u3068\u3057\u3066\u89e3\u304b\u308c\u3001\u6570\u5024\u7684\u624b\u6cd5\uff08\u4f8b\uff1aEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3001Newton-Raphson \u6cd5\uff09\u306b\u3088\u3063\u3066\u63a8\u5b9a\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1242-em","title":"12.4.2 EM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u6f5c\u5728\u5909\u6570\u306e\u6271\u3044","text":"<p>EM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u3001\u6f5c\u5728\u5909\u6570 $ \\mathbf{f} $ \u3092\u300c\u6b20\u6e2c\u30c7\u30fc\u30bf\u300d\u3068\u307f\u306a\u3057\u3066\u3001E\u30b9\u30c6\u30c3\u30d7\u3067\u305d\u306e\u6761\u4ef6\u4ed8\u304d\u671f\u5f85\u5024\u3068\u5171\u5206\u6563\u3092\u8a08\u7b97\u3057\u3001M\u30b9\u30c6\u30c3\u30d7\u3067\u30d1\u30e9\u30e1\u30fc\u30bf $ \\Lambda $ \u3068 $ \\Psi $ \u3092\u66f4\u65b0\u3059\u308b\u53cd\u5fa9\u7684\u624b\u6cd5\u3067\u3059\u3002 \u3053\u306e\u624b\u6cd5\u306f\u3001\u884c\u5217\u5206\u89e3\u306e\u8996\u70b9\u304b\u3089\u3082\u3001\u30c7\u30fc\u30bf\u306e\u5b8c\u5168\u306a\u30e2\u30c7\u30eb\u3092\u300c\u5206\u89e3\u300d\u3057\u3066\u3044\u304f\u904e\u7a0b\u3068\u89e3\u91c8\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#125","title":"12.5 \u56e0\u5b50\u5206\u6790\u3068\u305d\u306e\u4ed6\u306e\u884c\u5217\u5206\u89e3\u624b\u6cd5\u3068\u306e\u6bd4\u8f03","text":""},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1251-pca","title":"12.5.1 \u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3068\u306e\u6bd4\u8f03","text":"<ul> <li> <p>PCA:   PCA\u306f\u3001\u30c7\u30fc\u30bf\u884c\u5217\u306eSVD\u306b\u57fa\u3065\u3044\u3066\u3001\u30c7\u30fc\u30bf\u306e\u5206\u6563\u3092\u6700\u5927\u9650\u306b\u8aac\u660e\u3059\u308b\u76f4\u4ea4\u57fa\u5e95\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002   \u6570\u5b66\u7684\u306b\u306f\u3001\u4e2d\u5fc3\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf \\(\\mathbf{X}\\) \u306b\u5bfe\u3057\u3001   $$   \\mathbf{X} = \\mathbf{U} \\mathbf{S} \\mathbf{V}^\\top   $$   \u3068\u5206\u89e3\u3057\u3001\u4e3b\u6210\u5206\u306f $ \\mathbf{V} $ \u306e\u5217\u30d9\u30af\u30c8\u30eb\u3067\u3059\u3002</p> </li> <li> <p>\u56e0\u5b50\u5206\u6790:   \u56e0\u5b50\u5206\u6790\u306f\u3001\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u3092\u3001\u5171\u901a\u56e0\u5b50\u3068\u7279\u6709\u56e0\u5b50\u306e\u5bc4\u4e0e\u306b\u5206\u89e3\u3059\u308b\u70b9\u3067\u3001PCA\u3068\u306f\u76ee\u7684\u304c\u7570\u306a\u308a\u307e\u3059\u3002   PCA\u306f\u5168\u5206\u6563\u3092\u5bfe\u8c61\u3068\u3059\u308b\u306e\u306b\u5bfe\u3057\u3001\u56e0\u5b50\u5206\u6790\u306f\u5171\u901a\u5206\u6563\u306b\u7126\u70b9\u3092\u5f53\u3066\u3001\u7279\u6709\u5206\u6563\u3092\u30e2\u30c7\u30eb\u5916\u306e\u96d1\u97f3\u3068\u3057\u3066\u6271\u3044\u307e\u3059\u3002</p> </li> </ul>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#1252-nmf","title":"12.5.2 \u975e\u8ca0\u5024\u884c\u5217\u5206\u89e3\uff08NMF\uff09\u3068\u306e\u95a2\u9023","text":"<p>\u975e\u8ca0\u5024\u884c\u5217\u5206\u89e3\uff08NMF\uff09\u306f\u3001\u30c7\u30fc\u30bf\u304c\u975e\u8ca0\u5024\u3092\u6301\u3064\u5834\u5408\u306b\u3001\u8981\u7d20\u304c\u975e\u8ca0\u5024\u3068\u306a\u308b\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\u3092\u6c42\u3081\u308b\u624b\u6cd5\u3067\u3059\u3002 \u56e0\u5b50\u5206\u6790\u3068\u306f\u7570\u306a\u308a\u3001NMF\u306f\u7269\u7406\u7684\u30fb\u610f\u5473\u7684\u306a\u5236\u7d04\uff08\u975e\u8ca0\u6027\uff09\u3092\u52a0\u3048\u308b\u3053\u3068\u3067\u89e3\u91c8\u6027\u3092\u5411\u4e0a\u3055\u305b\u308b\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u3059\u304c\u3001\u4e21\u8005\u3068\u3082\u4f4e\u30e9\u30f3\u30af\u884c\u5217\u5206\u89e3\u306e\u4e00\u5f62\u614b\u3068\u3057\u3066\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"research/note/factor-analysis/12-technical-note-matrix-factorization/#126","title":"12.6 \u307e\u3068\u3081","text":"<p>\u672c\u7ae0\u3067\u306f\u3001\u56e0\u5b50\u5206\u6790\u3092\u884c\u5217\u5206\u89e3\u306e\u4e00\u7a2e\u3068\u3057\u3066\u4f4d\u7f6e\u3065\u3051\u3001\u4ee5\u4e0b\u306e\u70b9\u3092\u4e2d\u5fc3\u306b\u89e3\u8aac\u3057\u307e\u3057\u305f\u3002</p> <ol> <li> <p>\u884c\u5217\u5206\u89e3\u306e\u57fa\u672c\u6982\u5ff5:    \u56fa\u6709\u5024\u5206\u89e3\u3084\u7279\u7570\u5024\u5206\u89e3\u306a\u3069\u3001\u884c\u5217\u5206\u89e3\u306e\u57fa\u790e\u7684\u624b\u6cd5\u3068\u305d\u306e\u6570\u5b66\u7684\u6027\u8cea\u3092\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002</p> </li> <li> <p>PCA \u3068\u56e0\u5b50\u5206\u6790\u306e\u6bd4\u8f03:    PCA\u306f\u30c7\u30fc\u30bf\u5168\u4f53\u306e\u5206\u6563\u3092\u6700\u5927\u5316\u3059\u308b\u76f4\u4ea4\u5909\u63db\u3067\u3042\u308b\u306e\u306b\u5bfe\u3057\u3001\u56e0\u5b50\u5206\u6790\u306f\u5171\u901a\u56e0\u5b50\u3068\u7279\u6709\u56e0\u5b50\u306b\u5206\u89e3\u3059\u308b\u70b9\u3067\u7570\u306a\u308b\u3053\u3068\u3092\u793a\u3057\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u56e0\u5b50\u5206\u6790\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u5206\u89e3:    \u56e0\u5b50\u5206\u6790\u306f\u3001\u5171\u5206\u6563\u884c\u5217 $ \\Sigma $ \u3092 $ \\Lambda\\Lambda^\\top + \\Psi $ \u306b\u5206\u89e3\u3059\u308b\u554f\u984c\u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3055\u308c\u3001\u305d\u306e\u63a8\u5b9a\u306f\u6700\u5c24\u6cd5\u3084EM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u901a\u3058\u3066\u884c\u308f\u308c\u307e\u3059\u3002</p> </li> <li> <p>\u56de\u8ee2\u3068\u975e\u4e00\u610f\u6027:    \u56e0\u5b50\u8ca0\u8377\u884c\u5217\u306e\u56de\u8ee2\u306b\u3088\u308a\u3001\u89e3\u91c8\u306e\u5bb9\u6613\u3055\u3092\u8ffd\u6c42\u3059\u308b\u4e00\u65b9\u3001\u975e\u4e00\u610f\u6027\u306e\u554f\u984c\uff08\u4efb\u610f\u306e\u76f4\u4ea4\u5909\u63db\u306b\u3088\u308b\u4e0d\u5909\u6027\uff09\u306b\u3064\u3044\u3066\u3082\u8003\u5bdf\u3057\u307e\u3057\u305f\u3002</p> </li> <li> <p>\u4ed6\u306e\u884c\u5217\u5206\u89e3\u624b\u6cd5\u3068\u306e\u95a2\u9023:    PCA\u3084NMF\u306a\u3069\u3001\u4ed6\u306e\u884c\u5217\u5206\u89e3\u6280\u6cd5\u3068\u306e\u6bd4\u8f03\u3092\u901a\u3058\u3066\u3001\u56e0\u5b50\u5206\u6790\u306e\u4f4d\u7f6e\u3065\u3051\u3068\u305d\u306e\u72ec\u81ea\u6027\u3092\u660e\u78ba\u306b\u3057\u307e\u3057\u305f\u3002</p> </li> </ol> <p>\u3053\u306e\u5185\u5bb9\u306f\u3001\u7814\u7a76\u8005\u3084\u4e0a\u7d1a\u5b66\u751f\u3092\u5bfe\u8c61\u3068\u3057\u305f\u30c6\u30af\u30cb\u30ab\u30eb\u306a\u30ec\u30af\u30c1\u30e3\u30fc\u30ce\u30fc\u30c8\u3068\u3057\u3066\u3001\u56e0\u5b50\u5206\u6790\u306e\u6570\u7406\u7684\u80cc\u666f\u3068\u884c\u5217\u5206\u89e3\u3068\u306e\u95a2\u4fc2\u3092\u6df1\u304f\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u57fa\u76e4\u3068\u306a\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u307e\u3059\u3002\u6700\u65b0\u306e\u6587\u732e\u3084\u65e2\u5b58\u306e\u7406\u8ad6\uff08J\u00f6reskog\u3001S\u00f6rbom\u3001Browne\u306a\u3069\uff09\u306e\u77e5\u898b\u3092\u8e0f\u307e\u3048\u3001\u6b63\u78ba\u304b\u3064\u7db2\u7f85\u7684\u306a\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u3088\u3046\u52aa\u3081\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/","title":"\u8b1b\u7fa9\u8cc7\u6599\u3068\u767a\u8868\u8cc7\u6599\u3092\u81ea\u52d5\u4f5c\u6210\u3059\u308b\u30b7\u30b9\u30c6\u30e0","text":"<p>\u8b1b\u7fa9\u8cc7\u6599\u3068\u767a\u8868\u8cc7\u6599\u306e\u4f5c\u6210\u306f\u3001\u6559\u54e1\u306b\u3068\u3063\u3066\u975e\u5e38\u306b\u6642\u9593\u3092\u304b\u3051\u308b\u91cd\u3044\u4f5c\u696d\u3067\u3059\u3002\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u306f\u3001\u3053\u306e\u4f5c\u696d\u3092AI\u306b\u3088\u3063\u3066\u81ea\u52d5\u5316\u3059\u308b\u305f\u3081\u306e\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u3092\u307e\u3068\u3081\u3066\u3044\u304d\u307e\u3059\u3002\u5b66\u751f\u5411\u3051\u306e\u6388\u696d\u652f\u63f4\u30b7\u30b9\u30c6\u30e0\u3068\u306f\u7570\u306a\u308a\u3001\u3053\u308c\u306f\u6559\u54e1\u306e\u6388\u696d\u306b\u304b\u3051\u308b\u52b4\u529b\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3053\u3068\u3092\u5927\u304d\u306a\u76ee\u7684\u3068\u3057\u3001\u7814\u7a76\u6d3b\u52d5\u306b\u30a8\u30d5\u30a9\u30fc\u30c8\u3092\u5272\u3051\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>\u3053\u306e\u65b9\u6cd5\u3092\u69cb\u7bc9\u3059\u308b\u306b\u3042\u305f\u3063\u3066\u3001\u5229\u7528\u3059\u308bAI\u3084\u30b5\u30fc\u30d3\u30b9\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059\u3002 - Mistral OCR - Claude 3.7 Sonnet - GitHub - VS Code or Google Colab</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/#11","title":"1.1 \u3069\u3093\u306a\u4eba\u306b\u5411\u3044\u3066\u3044\u306a\u3044\u65b9\u6cd5\u304b","text":"<p>\u3053\u306e\u65b9\u6cd5\u3092\u5229\u7528\u3059\u308b\u306b\u3042\u305f\u3063\u3066\u3001\u307e\u305a\u5927\u304d\u306a\u52d8\u9055\u3044\u3092\u3057\u306a\u3044\u3088\u3046\u306b\u91d8\u3092\u523a\u3057\u3066\u304a\u304d\u305f\u3044\u306e\u3067\u3059\u304c\u3001\u5168\u3066\u3092\u81ea\u52d5\u5316\u3057\u3066\u7406\u60f3\u306e\u8b1b\u7fa9\u8cc7\u6599\u3092AI\u5358\u4f53\u3067\u4f5c\u308b\u3068\u3044\u3046\u306e\u306f\u7121\u7406\u304c\u3042\u308a\u307e\u3059\u3002\u5bfe\u8c61\u3068\u306a\u308b\u5b66\u90e8\u3084\u5b66\u751f\u304c\u7570\u306a\u308c\u3070\u3001\u81ea\u305a\u3068\u6700\u9069\u306a\u8b1b\u7fa9\u3068\u8b1b\u7fa9\u8cc7\u6599\u3068\u3044\u3046\u306e\u306f\u7570\u306a\u3063\u3066\u304f\u308b\u304b\u3089\u3067\u3059\u3002\u306a\u306e\u3067\u3001\u3059\u3079\u3066\u3092AI\u306b\u4e38\u6295\u3052\u3059\u308b\u3068\u3044\u3046\u306e\u306f\u304b\u306a\u308a\u96e3\u3057\u3044\u3067\u3059\u3002\u307e\u305f\u3001\u4e00\u767a\u306e\u751f\u6210\u3067\u7406\u60f3\u306e\u3082\u306e\u304c\u51fa\u6765\u4e0a\u304c\u3089\u306a\u3044\u3053\u3068\u306b\u30a4\u30e9\u30a4\u30e9\u3057\u305f\u308a\u3057\u3066\u3057\u307e\u3046\u4eba\u306b\u3082\u5411\u3044\u3066\u3044\u307e\u305b\u3093\uff08\u3080\u3057\u308d\u6307\u5c0e\u8005\u306b\u3082\u5411\u3044\u3066\u3044\u306a\u3044\u306e\u3067\u306f\uff1f\uff09\u3002</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/#12","title":"1.2 \u3069\u3093\u306a\u4eba\u306b\u5411\u3044\u3066\u3044\u308b\u304b","text":"<p>\u3053\u306e\u65b9\u6cd5\u304c\u5411\u3044\u3066\u3044\u308b\u306e\u306f\u3001\u3068\u306b\u304b\u304f\u8a66\u884c\u932f\u8aa4\u3092\u3059\u308b\u306e\u304c\u597d\u304d\u306a\u4eba\u3067\u3059\u3002\u3084\u3063\u3066\u307f\u3066\u3046\u307e\u304f\u3044\u304b\u306a\u3044\u304b\u3089\u3001\u6b21\u306f\u3053\u3046\u3057\u3066\u307f\u3088\u3046\u3068\u3044\u3046\u4f55\u5ea6\u3082\u8a66\u3059\u5fc3\u610f\u6c17\u306e\u3042\u308b\u4eba\u306b\u5411\u3044\u3066\u3044\u307e\u3059\u3002\u7c21\u5358\u306b\u8a00\u3048\u3070\u3001\u3059\u3054\u304f\u512a\u79c0\u306a\u3093\u3060\u3051\u3069\u305d\u306e\u80fd\u529b\u3092\u3069\u3053\u306b\u5411\u3051\u308c\u3070\u3044\u3044\u306e\u304b\u304c\u308f\u304b\u3089\u306a\u3044\u5927\u5b66\u9662\u751f\u3092\u76f8\u624b\u306b\u3057\u3066\u3044\u308b\u3068\u601d\u3063\u3066\u304f\u3060\u3055\u3044\u3002\u305d\u3057\u3066\u3001\u305d\u306e\u4eba\u306b\u6307\u793a\u3092\u51fa\u3057\u3066\u4f5c\u6210\u3057\u3066\u304d\u305f\u3082\u306e\u304c\u610f\u306b\u305d\u3050\u308f\u306a\u3044\u5834\u5408\u3001\u81ea\u5206\u306e\u6307\u793a\u304c\u60aa\u304b\u3063\u305f\u304b\u3089\u3001\u3069\u3046\u6307\u793a\u3092\u6539\u5584\u3059\u308c\u3070\u826f\u3044\u304b\u3068\u81ea\u7701\u3067\u304d\u308b\u4eba\u304c\u6700\u3082\u5411\u3044\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/#13","title":"1.3 \u4f55\u3092\u4f5c\u308b\u306e\u304b?\u305d\u306e\u76ee\u7684\u306f\u4f55\u304b?","text":"<p>\u4eca\u56de\u306e\u76ee\u6a19\u306f\u3001\u6559\u79d1\u66f8\u3092Markdown \u306b\u5909\u63db\u3057\u300115\u56de\u5206\u306e\u8b1b\u7fa9\u306e\u76ee\u6b21\u3092\u4f5c\u6210\u3057\u3001\u305d\u308c\u3068\u4e26\u884c\u3057\u3066\u8b1b\u7fa9\u30ce\u30fc\u30c8\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u6c7a\u3081\u3001\u76ee\u6b21\u3068\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u3082\u3068\u306b\u6559\u79d1\u66f8\u3092\u53c2\u7167\u3057\u306a\u304c\u3089\u8b1b\u7fa9\u30ce\u30fc\u30c8\u3092\u4f5c\u6210\u3057\u3001\u6700\u5f8c\u306b\u4f5c\u6210\u3057\u305f\u8b1b\u7fa9\u30ce\u30fc\u30c8\u3092\u30b9\u30e9\u30a4\u30c9\u5316\u3059\u308b\u3068\u3044\u3046\u4ed5\u7d44\u307f\u3092AI-powered \u306b\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067\u3042\u308b\u3002\u3055\u3089\u306b\u3001\u4f5c\u6210\u3057\u305f\u8b1b\u7fa9\u30ce\u30fc\u30c8\u3001\u30b9\u30e9\u30a4\u30c9\u3092\u8aad\u307f\u8fbc\u3093\u3060AI\u3092\u4f5c\u308a\u3001\u5b66\u751f\u306e\u8cea\u7591\u5fdc\u7b54\u304c\u3067\u304d\u308b\u4ed5\u7d44\u307f\u3092\u4f5c\u6210\u3059\u308b\u3002</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/#2","title":"2. \u8b1b\u7fa9\u8cc7\u6599\u3068\u30b9\u30e9\u30a4\u30c9\u4f5c\u6210\u306e\u6d41\u308c","text":"<p>\u8b1b\u7fa9\u8cc7\u6599\u3068\u30b9\u30e9\u30a4\u30c9\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u6b21\u306e\u3082\u306e\u3092\u7528\u610f\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u7279\u306b\u3001\u8b1b\u7fa9\u306f\u5358\u4f53\u3067\u306f\u306a\u304f15\u56de\u300130\u56de\u306a\u3069\u306e\u901a\u3057\u3067\u5b66\u671f\u3092\u901a\u3057\u3066\u5b9f\u65bd\u3055\u308c\u308b\u306f\u305a\u306a\u306e\u3067\u3001\u307e\u305a\u306f15\u56de\u307e\u305f\u306f30\u56de\u5206\u306e\u8b1b\u7fa9\u306e\u5185\u5bb9\u306e\u30ea\u30b9\u30c8\u304c\u5fc5\u8981\u3067\u3059\u3002\u3053\u3053\u306f\u6559\u54e1\u304c\u4f5c\u6210\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u304c\u3001\u3082\u3057\u6559\u79d1\u66f8\u304c\u6c7a\u307e\u3063\u3066\u3044\u308b\u5834\u5408\u306b\u306f\u3001\u81ea\u708a\uff0bAI\u3067\u3082\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/#21-mistral-ocr-claude-37-sonnet15","title":"2.1 Mistral OCR + Claude 3.7 Sonnet\u3067\u8b1b\u7fa9\u8cc7\u6599\u306e15\u56de\u306e\u76ee\u6b21\u3092\u4f5c\u6210\u3059\u308b\u3002","text":"<p>Mistral OCR\u3092\u5229\u7528\u3057\u305f\u8b1b\u7fa9\u8cc7\u6599\u306e\u76ee\u6b21\u4f5c\u6210\u306b\u3064\u3044\u3066\u8a18\u8ff0\u3057\u307e\u3059\u3002\u66f8\u7c4d\u3092PDF\u5316\u3059\u308b\u3053\u3068\u306f\u3088\u304f\u884c\u3044\u307e\u3059\u304c\u3001\u3053\u3053\u304b\u3089\u624b\u52d5\u3067\u8b1b\u7fa9\u8cc7\u6599\u306e\u76ee\u6b21\u3092\u4f5c\u308b\u306e\u306f\u5b9f\u306f\u9aa8\u304c\u6298\u308c\u308b\u4f5c\u696d\u3067\u3059\u3002\u305d\u3053\u3067\u3001\u672c\u3092\u81ea\u708a\u3057\u3066PDF\u306b\u5909\u63db\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u524d\u63d0\u3067\u3001Mistral OCR \u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u3053\u306e\u60c5\u5831\u3092Markdown\u3078\u3068\u5909\u63db\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u4f7f\u3044\u65b9\u306f\u7c21\u5358\u3067\u3059\u3002\u307e\u305a\u306f\u3001\u3053\u3061\u3089 \u304b\u3089 API Key\u3092\u767a\u884c\u3057\u307e\u3059\u3002\u30aa\u30f3\u30e9\u30a4\u30f3\u74b0\u5883\u3067\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u3053\u3061\u3089\u304b\u3089 \u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067mistral AI\u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3057\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306b\u3059\u308b\u3068\u3001PDF\u304c <code>result</code> \u30d5\u30a9\u30eb\u30c0\u30fc\u306b Markdown\u5f62\u5f0f\u3067\u66f8\u304d\u51fa\u3055\u308c\u307e\u3059\u3002 </p> <p>\u3053\u306e\u65b9\u6cd5\u306f\u3001\u533b\u5e2b\u56fd\u5bb6\u8a66\u9a13\u3084\u85ac\u5264\u5e2b\u8a66\u9a13\u306a\u3069\u306e\u8cc7\u683c\u8a66\u9a13\u306b\u3064\u3044\u3066\u306f\u3001\u3053\u308c\u3089\u306e\u65b9\u6cd5\u3092\u7528\u3044\u3066\u554f\u984c\u3092markdown\u5f62\u5f0f\u3067\u51fa\u529b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3001GPT\u3084Claude 3.7 Sonnet\u306a\u3069\u3067\u8aad\u307f\u8fbc\u307e\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/#211","title":"2.1.1 \u305d\u306e\u307b\u304b\u306e\u30a2\u30d7\u30ed\u30fc\u30c1","text":"<p>\u305d\u306e\u307b\u304b\u306e\u65b9\u6cd5\u3068\u3057\u3066\u306f\u3001Microsoft \u304c\u63d0\u4f9b\u3057\u3066\u3044\u308b markitdown \u3092\u7528\u3044\u308b\u65b9\u6cd5\u3082\u3042\u308a\u307e\u3059\u3002\u3053\u3061\u3089\u306e\u5834\u5408\u306f\u3001\u305d\u306e\u304b\u306b\u3082\u8272\u3005\u3068\u3067\u304d\u308b\u3053\u3068\u304c\u3042\u308b\u306e\u3067\u3001\u3053\u308c\u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u3001Markdown \u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u8b1b\u6f14\u306e .mp4 \u30d5\u30a1\u30a4\u30eb\u306a\u3069\u3092\u3053\u3061\u3089\u306b\u8aad\u307f\u8fbc\u3093\u3067\u3001\u305d\u308c\u3092Claude 3.7 Sonnet \u3067\u8a18\u4e8b\u306b\u3057\u3066\u516c\u958b\u3059\u308b\u306a\u3069\u30821\u3064\u306e\u30a2\u30a4\u30c7\u30a2\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/#22","title":"2.2 \u76ee\u6b21\u306e\u4f5c\u6210\uff08\u8d85\u5927\u4e8b\uff09","text":"<p>\u76ee\u6b21\u306e\u4f5c\u6210\u306f\u3001\u3069\u306e\u30ec\u30d9\u30eb\u307e\u3067\u30d6\u30ec\u30a4\u30af\u30c0\u30a6\u30f3\u3059\u308b\u306e\u304b\u304c\u975e\u5e38\u306b\u91cd\u8981\u3068\u306a\u308b\u3002\u5927\u9805\u76ee\u3057\u304b\u4f5c\u3063\u3066\u3044\u306a\u3044\u306a\u3089\u3001\u305d\u306e\u72b6\u614b\u3067\u8b1b\u7fa9\u8cc7\u6599\u3092\u4f5c\u6210\u3059\u308b\u3068\u304b\u306a\u308a\u8352\u3044\u3082\u306e\u3068\u306a\u308b\u306e\u3067\u3001\u305d\u306e\u70b9\u306f\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3042\u308b\u3002\u3053\u3053\u306f\u4eba\u9593\u304c\u672c\u6c17\u3067\u8003\u3048\u308b\u5fc5\u8981\u306e\u3042\u308b\u5834\u6240\u3067\u3001\u81ea\u5206\u306e\u5b66\u90e8\u3084\u5b66\u751f\u306b\u5411\u3051\u3066\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3059\u308b\u5fc5\u8981\u306e\u3042\u308b\u3082\u306e\u3067\u3042\u308b\u3002</p> <p>\u4f8b\u3048\u3070\u3001\u7dda\u5f62\u4ee3\u6570\u5b66\u306e\u6388\u696d\u30921\u3064\u3068\u3063\u3066\u3082\u3001\u305d\u306e\u6388\u696d\u304c\u7406\u5b66\u90e8\u3067\u5b9f\u65bd\u3055\u308c\u308b\u306e\u304b\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u5b66\u90e8\u3067\u5b9f\u65bd\u3055\u308c\u308b\u306e\u304b\u3001\u7d4c\u6e08\u5b66\u90e8\u3067\u5b9f\u65bd\u3055\u308c\u308b\u306e\u304b\u3067\u5927\u304d\u304f\u5185\u5bb9\u306f\u7570\u306a\u308b\u3057\u3001\u7406\u8ad6\u3092\u3069\u306e\u7a0b\u5ea6\u8aac\u660e\u3059\u308b\u304b\u3082\u5909\u308f\u3063\u3066\u304f\u308b\u3002\u307e\u305f\u3001\u3082\u30461\u3064\u306f\u5b66\u751f\u306e\u30ec\u30d9\u30eb\u611f\u304c\u308f\u304b\u308b\u306e\u306f\u6559\u54e1\u3057\u304b\u3044\u306a\u3044\u306e\u3067\u3001\u305d\u306e\u5b66\u90e8\u306b\u5165\u5b66\u3059\u308b\u5b66\u751f\u306b\u5411\u3051\u3066\u9069\u5207\u306b\u30a2\u30ec\u30f3\u30b8\u3092\u884c\u3046\u306e\u3082\u3001\u3053\u306e\u5de5\u7a0b\u3067\u3042\u308b\u3002</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/#23","title":"2.3 \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u4f5c\u308b","text":"<p>\u4e00\u822c\u7684\u306b\u8b1b\u7fa9\u306b\u304a\u3044\u3066\u30d1\u30ef\u30fc\u30dd\u30a4\u30f3\u30c8\u3092\u4f5c\u308b\u3053\u3068\u306f\u307e\u307e\u3042\u308b\u3053\u3068\u3060\u304c\u3001\u3053\u3053\u3067\u306f Markdown \u3068\u3044\u3046\u5f62\u5f0f\u3092\u63a1\u7528\u3059\u308b\u3002\u3053\u308c\u3092\u63a1\u7528\u3059\u308b\u7406\u7531\u306f\u3001\u3068\u306b\u304b\u304fAI\u3068\u306e\u76f8\u6027\u304c\u826f\u3044\u3053\u3068\u3068\u3001Markdown\u3055\u3048\u3042\u308c\u3070\u30b9\u30e9\u30a4\u30c9\u3082\u3067\u304d\u308b\u3057\u3001\u305d\u306e\u307b\u304b\u306e\u6d41\u7528\u3082\u52b9\u304f\u3068\u3044\u3046\u3053\u3068\u304c\u3042\u308b\u3002\u57fa\u672c\u7684\u306b\u751f\u6210AI\u3092\u5b66\u5185\u3067\u6d3b\u7528\u3059\u308b\u306a\u3089\u3001\u3059\u3079\u3066\u306e\u8cc7\u6599\u306f markdown \u3067\u4f5c\u6210\u3055\u308c\u308b\u3079\u304d\u3067\u3042\u308b\u3002\u307e\u305f\u3001\u81ea\u5206\u306e\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u3082 markdown \u3067\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u6b63\u76f4 markdown \u4ee5\u5916\u3067\u4f5c\u308b\u30e1\u30ea\u30c3\u30c8\u304c\u4f55\u3082\u306a\u3044\u3002\u6b21\u306b\u3001markdown \u5f62\u5f0f\u306f Marp \u3084\u3001sli.dev \u3092\u7528\u3044\u3066\u767a\u8868\u8cc7\u6599\u304c\u7c21\u5358\u306b\u4f5c\u6210\u3067\u304d\u308b\u3002</p>"},{"location":"research/note/lecture-note-automation/Lecture-note-slide-automation/#3-zoomgoogle-meet","title":"3. Zoom\u3092\u3084\u3081\u308d\uff01\u4f7f\u3046\u306a\u3089Google Meet\u3060\u3002","text":"<p>\u6b63\u76f4zoom\u306e\u6b8b\u5ff5\u306a\u753b\u8cea\u3068\u97f3\u58f0\u306b\u306f\u3046\u3093\u3056\u308a\u3057\u3066\u3044\u308b\u3002\u6587\u5b57\u8d77\u3053\u3057\u304c\u65e5\u672c\u8a9e\u5bfe\u5fdc\u3057\u305f\u3044\u307e\u3001\u3082\u3046zoom\u3092\u5229\u7528\u3059\u308b\u4fa1\u5024\u306f\u306a\u3044\u3002\u3053\u3061\u3089\u3002</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/","title":"Robust Statistics","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#robust-statistics_1","title":"Robust Statistics","text":"<p>PETER J. HUBERProfessor of StatisticsHarvard UniversityCambridge, Massachusetts</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#john-wiley-sons","title":"John Wiley \\&amp; Sons","text":"<p>New York \u30fb Chichester \u30fb Brisbane \\(\\cdot\\) Toronto \u30fb Singapore</p> <p>A NOTE TO THE READER This book has been electronically reproduced from digital information stored at John Wiley \\&amp; Sons, Inc. We are pleased that the use of this new technology will enable us to keep works of enduring scholarly value in print as long as there is a reasonable demand for them. The content of this book is identical to previous printings.</p> <p>Copyright (c) 1981 by John Wiley \\&amp; Sons, Inc. All rights reserved. Published simultaneously in Canada. Reproduction or translation of any part of this work beyond that permitted by Sections 107 or 108 of the 1976 United States Copyright Act without the permission of the copyright owner is unlawful. Requests for permission or further information should be addressed to the Permissions Department, John Wiley \\&amp; Sons, Inc.</p> <p>Library of Congress Cataloging in Publication Data: Huber, Peter J Robust statistics. (Wiley series in probability and mathematical statistics) \"A Wiley-Interscience publication.\" Includes index.</p> <ol> <li>Robust statistics. I. Title.</li> </ol> <p>QA276.H785 519.5 80-18627 ISBN 0-471-41805-6 Printed in the United States of America</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#preface","title":"Preface","text":"<p>The present monograph is the first systematic, book-length exposition of robust statistics. The technical term \"robust\" was coined only in 1953 (by G. E. P. Box), and the subject matter acquired recognition as a legitimate topic for investigation only in the mid-sixties, but it certainly never was a revolutionary new concept. Among the leading scientists of the late nineteenth and early twentieth century, there were several practicing statisticians (to name but a few: the astronomer S. Newcomb, the astrophysicist A. Eddington, and the geophysicist H. Jeffreys), who had a perfectly clear, operational understanding of the idea; they knew the dangers of long-tailed error distributions, they proposed probability models for gross errors, and they even invented excellent robust alternatives to the standard estimates, which were rediscovered only recently. But for a long time theoretical statisticians tended to shun the subject as being inexact and \"dirty.\" My 1964 paper may have helped to dispel such prejudices. Amusingly (and disturbingly), it seems that lately a kind of bandwagon effect has evolved, that the pendulum has swung to the other extreme, and that \"robust\" has now become a magic word, which is invoked in order to add respectability.</p> <p>This book gives a solid foundation in robustness to both the theoretical and the applied statistician. The treatment is theoretical, but the stress is on concepts, rather than on mathematical completeness. The level of presentation is deliberately uneven: in some chapters simple cases are treated with mathematical rigor; in others the results obtained in the simple cases are transferred by analogy to more complicated situations (like multiparameter regression and covariance matrix estimation), where proofs are not always available (or are available only under unrealistically severe assumptions). Also selected numerical algorithms for computing robust estimates are described and, where possible, convergence proofs are given.</p> <p>Chapter 1 gives a general introduction and overview; it is a must for every reader. Chapter 2 contains an account of the formal mathematical background behind qualitative and quantitative robustness, which can be skipped (or skimmed) if the reader is willing to accept certain results on faith. Chapter 3 introduces and discusses the three basic types of estimates ( \\(M\\)-, \\(L\\)-, and \\(R\\)-estimates), and Chapter 4 treats the asymptotic minimax</p> <p>theory for location estimates; both chapters again are musts. The remaining chapters branch out in different directions and are fairly independent and self-contained; they can be read or taught in more or less any order.</p> <p>The book does not contain exercises-I found it hard to invent a sufficient number of problems in this area that were neither trivial nor too hard-so it does not satisfy some of the formal criteria for a textbook. Nevertheless I have successfully used various stages of the manuscript as such in graduate courses.</p> <p>The book also has no pretensions of being encyclopedic. I wanted to cover only those aspects and tools that I personally considered to be the most important ones. Some omissions and gaps are simply due to the fact that I currently lack time to fill them in, but do not want to procrastinate any longer (the first draft for this book goes back to 1972). Others are intentional. For instance, adaptive estimates were excluded because I would now prefer to classify them with nonparametric rather than with robust statistics, under the heading of nonparametric efficient estimation. The so-called Bayesian approach to robustness confounds the subject with admissible estimation in an ad hoc parametric supermodel, and still lacks reliable guidelines on how to select the supermodel and the prior so that we end up with something robust. The coverage of \\(L\\) - and \\(R\\)-estimates was cut back from earlier plans because they do not generalize well and get awkward to compute and to handle in multiparameter situations.</p> <p>A large part of the final draft was written when I was visiting Harvard University in the fall of 1977; my thanks go to the students, in particular to P. Rosenbaum and Y. Yoshizoe, who then sat in my seminar course and provided many helpful comments.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#p-j-huber","title":"P. J. Huber","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#contents","title":"Contents","text":"<p>1 GENERALITIES ..... 1 1.1 Why Robust Procedures? ..... 1 1.2 What Should a Robust Procedure Achieve? ..... 5 1.3 Qualitative Robustness, ..... 7 1.4 Quantitative Robustness, ..... 10 1.5 Infinitesimal Aspects, ..... 13 1.6 Optimal Robustness, ..... 16 1.7 Computation of Robust Estimates, ..... 17 2 THE WEAK TOPOLOGY AND ITS METRIZATION ..... 20 2.1 General Remarks, ..... 20 2.2 The Weak Topology, ..... 20 2.3 L\u00e9vy and Prohorov Metrics, ..... 25 2.4 The Bounded Lipschitz Metric, ..... 29 2.5 Fr\u00e9chet and G\u00e2teaux Derivatives, ..... 34 2.6 Hampel's Theorem, ..... 40 3 THE BASIC TYPES OF ESTIMATES ..... 43 3.1 General Remarks, ..... 43 3.2 Maximum Likelihood Type Estimates ( \\(M\\)-Estimates), ..... 43 3.3 Linear Combinations of Order Statistics ( \\(L\\)-Estimates), ..... 55 3.4 Estimates Derived from Rank Tests ( \\(R\\)-Estimates), ..... 61 3.5 Asymptotically Efficient \\(M\\)-, \\(L\\)-, and \\(R\\)-Estimates, ..... 68 4 ASYMPTOTIC MINIMAX THEORY FOR ESTIMATING A LOCATION PARAMETER ..... 73 4.1 General Remarks, ..... 73 4.2 Minimax Bias, ..... 74 4.3 Minimax Variance: Preliminaries, ..... 76 4.4 Distributions Minimizing Fisher Information, ..... 77</p> <p>4.5 Determination of \\(F_{0}\\) by Variational Methods, ..... 82 4.6 Asymptotically Minimax \\(M\\)-Estimates, ..... 94 4.7 On the Minimax Property for \\(L\\) - and \\(R\\)-Estimates, ..... 97 4.8 Descending \\(M\\)-Estimates, ..... 100 4.9 Questions of Asymmetric Contamination, ..... 104 5 SCALE ESTIMATES ..... 107 5.1 General Remarks, ..... 107 5.2 \\(M\\)-Estimates of Scale, ..... 109 5.3 L-Estimates of Scale, ..... 110 5.4 \\(R\\)-Estimates of Scale, ..... 114 5.5 Asymptotically Efficient Scale Estimates, ..... 116 5.6 Distributions Minimizing Fisher Information for Scale, ..... 118 5.7 Minimax Properties, ..... 122 6 MULTIPARAMETER PROBLEMS, IN PARTICULAR JOINT ESTIMATION OF LOCATION AND SCALE ..... 127 6.1 General Remarks, ..... 127 6.2 Consistency of \\(M\\)-Estimates, ..... 127 6.3 Asymptotic Normality of \\(M\\)-Estimates, ..... 132 6.4 Simultaneous \\(M\\)-Estimates of Location and Scale, ..... 135 6.5 M-Estimates with Preliminary Estimates of Scale, ..... 140 6.6 Quantitative Robustness Properties of Simultaneous Estimates for Location and Scale, ..... 141 6.7 The Computation of \\(M\\)-Estimates, ..... 146 6.8 Studentizing, ..... 148 7 REGRESSION ..... 153 7.1 General Remarks, ..... 153 7.2 The Classical Linear Least Squares Case, ..... 155 7.3 Robustizing the Least Squares Approach, ..... 162 7.4 Asymptotics of Robust Regression Estimates, ..... 164 7.5 Conjectures and Empirical Results, ..... 170 7.6 Asymptotic Covariances and Their Estimation, ..... 172 7.7 Concomitant Scale Estimates, ..... 175 7.8 Computation of Regression \\(M\\)-Estimates, ..... 179 7.9 Moderate Leverage Points, ..... 192 7.10 Analysis of Variance, ..... 195</p> <p>8 ROBUST COVARIANCE AND CORRELATION MATRICES ..... 199 8.1 General Remarks, ..... 199 8.2 Estimation of Matrix Elements through Robust Variances, ..... 202 8.3 Estimation of Matrix Elements through Robust Correlation, ..... 204 8.4 An Affinely Invariant Approach, ..... 211 8.5 Estimates Determined by Implicit Equations, ..... 213 8.6 Existence and Uniqueness of Solutions, ..... 215 8.7 Influence Functions and Qualitative Robustness, ..... 223 8.8 Consistency and Asymptotic Normality, ..... 226 8.9 Breakdown Point, ..... 227 8.10 Least Informative Distributions, ..... 229 8.11 Some Notes on Computation, ..... 237 9 ROBUSTNESS OF DESIGN ..... 243 9.1 General Remarks, ..... 243 9.2 Minimax Global Fit, ..... 243 9.3 Minimax Slope, ..... 251 10 EXACT FINITE SAMPLE RESULTS ..... 253 10.1 General Remarks, ..... 253 10.2 Lower and Upper Probabilities and Capacities, ..... 254 10.3 Robust Tests, ..... 264 10.4 Sequential Tests, ..... 273 10.5 The Neyman-Pearson Lemma for 2-Alternating Capacities, ..... 275 10.6 Estimates Derived from Tests, ..... 278 10.7 Minimax Interval Estimates, ..... 282 11 MISCELLANEOUS TOPICS ..... 286 11.1 Hampel's Extremal Problem, ..... 286 11.2 Shrinking Neighborhoods, ..... 290 REFERENCES ..... 294 INDEX ..... 301</p> <p>Robust Statistics</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-1","title":"CHAPTER 1","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#generalities","title":"Generalities","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#11-why-robust-procedures","title":"1.1 WHY ROBUST PROCEDURES?","text":"<p>Statistical inferences are based only in part upon the observations. An equally important base is formed by prior assumptions about the underlying situation. Even in the simplest cases, there are explicit or implicit assumptions about randomness and independence, about distributional models, perhaps prior distributions for some unknown parameters, and so on.</p> <p>These assumptions are not supposed to be exactly true-they are mathematically convenient rationalizations of an often fuzzy knowledge or belief. As in every other branch of applied mathematics, such rationalizations or simplifications are vital, and one justifies their use by appealing to a vague continuity or stability principle: a minor error in the mathematical model should cause only a small error in the final conclusions.</p> <p>Unfortunately, this does not always hold. During the past decades one has become increasingly aware that some of the most common statistical procedures (in particular, those optimized for an underlying normal distribution) are excessively sensitive to seemingly minor deviations from the assumptions, and a plethora of alternative \"robust\" procedures have been proposed.</p> <p>The word \"robust\" is loaded with many-sometimes inconsistentconnotations. We use it in a relatively narrow sense: for our purposes, robustness signifies insensitivity to small deviations from the assumptions.</p> <p>Primarily, we are concerned with distributional robustness: the shape of the true underlying distribution deviates slightly from the assumed model (usually the Gaussian law). This is both the most important case and the best understood one. Much less is known about what happens when the other standard assumptions of statistics are not quite satisfied and about the appropriate safeguards in these other cases.</p> <p>The following example, due to Tukey (1960), shows the dramatic lack of distributional robustness of some of the classical procedures.</p> <p>Example 1.1 Assume that we have a large, randomly mixed batch of \\(n\\) \"good\" and \"bad\" observations \\(x_{i}\\) of the same quantity \\(\\mu\\). Each single observation with probability \\(1-\\varepsilon\\) is a \"good\" one, with probability \\(\\varepsilon\\) a \"bad\" one, where \\(\\varepsilon\\) is a small number. In the former case \\(x_{i}\\) is \\(\\mathfrak{R}\\left(\\mu, \\sigma^{2}\\right)\\), in the latter \\(\\mathfrak{R}\\left(\\mu, 9 \\sigma^{2}\\right)\\). In other words all observations have the same mean, but the errors of some are increased by a factor of 3 .</p> <p>Equivalently, we could say that the \\(x_{i}\\) are independent, identically distributed with the common underlying distribution</p> \\[ F(x)=(1-\\varepsilon) \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)+\\varepsilon \\Phi\\left(\\frac{x-\\mu}{3 \\sigma}\\right) \\] <p>where</p> \\[ \\Phi(x)=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{x} e^{-y^{2} / 2} d y \\] <p>is the standard normal cumulative. Two time-honored measures of scatter are the mean absolute deviation</p> \\[ d_{n}=\\frac{1}{n} \\sum\\left|x_{i}-\\bar{x}\\right| \\] <p>and the mean square deviation</p> \\[ s_{n}=\\left[\\frac{1}{n} \\sum\\left(x_{i}-\\bar{x}\\right)^{2}\\right]^{1 / 2} \\] <p>There was a dispute between Eddington (1914, p. 147) and Fisher (1920, footnote on p. 762) about the relative merits of \\(d_{n}\\) and \\(s_{n}\\). Eddington advocated the use of the former: \"This is contrary to the advice of most textbooks; but it can be shown to be true.\" Fisher seemingly settled the matter by pointing out that for normal observations \\(s_{n}\\) is about \\(12 \\%\\) more efficient than \\(d_{n}\\).</p> <p>Of course, the two statistics measure different characteristics of the error distribution. For instance, if the errors are exactly normal, \\(s_{n}\\) converges to \\(\\sigma\\), while \\(d_{n}\\) converges to \\(\\sqrt{2 / \\pi} \\sigma \\approx 0.80 \\sigma\\). So we must be precise about how their performances are to be compared; we use the asymptotic relative</p> <p>efficiency (ARE) of \\(d_{n}\\) relative to \\(s_{n}\\), defined as follows:</p> \\[ \\begin{aligned} \\operatorname{ARE}(\\varepsilon) &amp; =\\lim _{n \\rightarrow \\infty} \\frac{\\operatorname{var}\\left(s_{n}\\right) /\\left(E s_{n}\\right)^{2}}{\\operatorname{var}\\left(d_{n}\\right) /\\left(E d_{n}\\right)^{2}} \\\\ &amp; =\\frac{\\left[\\frac{3(1+80 \\varepsilon)}{(1+8 \\varepsilon)^{2}}-1\\right] / 4}{\\frac{\\pi(1+8 \\varepsilon)}{2(1+2 \\varepsilon)^{2}}-1} \\end{aligned} \\] <p>The results are summarized in the Exhibit 1.1.1. The result is disquieting: just 2 bad observations in 1000 suffice to offset the \\(12 \\%\\) advantage of the mean square error, and \\(\\operatorname{ARE}(\\varepsilon)\\) reaches a maximum value greater than 2 at about \\(\\varepsilon=0.05\\).</p> <p>This is particularly unfortunate since in the physical sciences typical \"good data\" samples appear to be well modeled by an error law of the form (1.1) with \\(\\varepsilon\\) in the range between 0.01 and 0.1 . (This does not imply that these samples contain between \\(1 \\%\\) and \\(10 \\%\\) gross errors, although this is very of ten true; the above law (1.1) may just be a convenient description of a slightly longer-tailed than normal distribution.) Thus it becomes painfully clear that the naturally occurring deviations from the idealized model are large enough to render meaningless the traditional asymptotic optimality theory: in practice we should certainly prefer \\(d_{n}\\) to \\(s_{n}\\), since it is better for all \\(\\varepsilon\\) between 0.002 and 0.5 .</p> \\(\\varepsilon\\) \\(\\operatorname{ARE}(\\varepsilon)\\) 0 0.876 0.001 0.948 0.002 1.016 0.005 1.198 0.01 1.439 0.02 1.752 0.05 2.035 0.10 1.903 0.15 1.689 0.25 1.371 0.5 1.017 1.0 0.876 <p>Exhibit 1.1.1 Asymptotic efficiency of mean absolute relative to mean square deviation. From Huber (1977b), with permission of the publisher.</p> <p>To avoid misunderstandings, we should hasten to emphasize what is not implied here. First, the above does not imply that we advocate the use of the mean absolute deviation (there are still better estimates of scale). Second, some people have argued that the example is unrealistic insofar as the \"bad\" observations will stick out as outliers, so any conscientious statistician will do something about them before calculating the mean square error. This is beside the point; outlier rejection followed by the mean square error might very well beat the performance of the mean absolute error, but we are concerned here with the behavior of the unmodified classical estimates.</p> <p>The example clearly has to do with longtailedness: lengthening the tails of the underlying distribution explodes the variance of \\(s_{n}\\) ( \\(d_{n}\\) is much less affected). Shortening the tails, on the other hand, produces quite negligible effects on the distributions of the estimates. (It may impair the absolute efficiency by decreasing the asymptotic Cram\u00e9r-Rao bound, but the latter is so unstable under small changes of the distribution that this effect cannot be taken very seriously.)</p> <p>The sensitivity of classical procedures to longtailedness is typical and not limited to this example. As a consequence \"distributionally robust\" and \"outlier resistant,\" although conceptually distinct, are practically synonymous notions. Any reasonable, formal or informal, procedure for rejecting outliers will prevent the worst.</p> <p>We might therefore ask whether robust procedures are needed at all; perhaps a two-step approach would suffice: (1) First clean the data by applying some rule for outlier rejection. (2) Then use classical estimation and testing procedures on the remainder.</p> <p>Would these steps do the same job in a simpler way? Unfortunately they will not, for the following reasons: (1) It is rarely possible to separate the two steps cleanly; for instance, in multiparameter regression problems outliers are difficult to recognize unless we have reliable, robust estimates for the parameters. (2) Even if the original batch of observations consists of normal observations interspersed with some gross errors, the cleaned data will not be normal (there will be statistical errors of both kinds, false rejections and false retentions), and the situation is even worse when the original batch derives from a genuine nonnormal distribution, instead of from a gross-error framework. Therefore the classical normal theory is not applicable to cleaned samples, and the</p> <p>actual performance of such a two-step procedure may be more difficult to work out than that of a straight robust procedure. (3) It is an empirical fact that the best rejection procedures do not quite reach the performance of the best robust procedures. The latter apparently are superior because they can make a smooth transition between full acceptance and full rejection of an observation. See Hampel (1974a, 1976).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#12-what-should-a-robust-procedure-achieve","title":"1.2 WHAT SHOULD A ROBUST PROCEDURE ACHIEVE?","text":"<p>We are adopting what might be called an \"applied parametric viewpoint\": we have a parametric model, which hopefully is a good approximation to the true underlying situation, but we cannot and do not assume that it is exactly correct. Therefore any statistical procedure should possess the following desirable features: (1) It should have a reasonably good (optimal or nearly optimal) efficiency at the assumed model. (2) It should be robust in the sense that small deviations from the model assumptions should impair the performance only slightly, that is, the latter (described, say, in terms of the asymptotic variance of an estimate, or of the level and power of a test) should be close to the nominal value calculated at the model. (3) Somewhat larger deviations from the model should not cause a catastrophe.</p> <p>If asymptotic performance criteria are used, some care is needed. In particular, the convergence should be uniform over a neighborhood of the model, or there should be at least a one-sided uniform bound, because otherwise we cannot guarantee robustness for any finite \\(n\\), no matter how large \\(n\\) is. This point has often been overlooked in the past.</p> <p>It should be emphasized once more that the occurrence of gross errors in a small fraction of the observations is to be regarded as a small deviation, and that, in view of the extreme sensitivity of some classical procedures, a primary goal of robust procedures is to safeguard against gross errors.</p> <p>The literature contains many other explicit and implicit goals for robust procedures, for example, high asymptotic relative efficiency (relative to some classical reference procedures), or high absolute efficiency, and this either for completely arbitrary (sufficiently smooth) underlying distributions, or for a specific parametric family.</p> <p>However, it seems to me that these goals are secondary in importance, and they should never be allowed to take precedence over the abovementioned three.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#robust-nonparametric-and-distribution-free","title":"Robust, Nonparametric, and Distribution-Free","text":"<p>Traditionally, robust procedures have been classified together with nonparametric and distribution-free ones. In our view the three notions have very little overlap.</p> <p>A procedure is called nonparametric if it is supposed to be used for a broad, not-parametrized set of underlying distributions. For instance, the sample mean and the sample median are the nonparametric estimates of the population mean and median, respectively. Although nonparametric the sample mean is highly sensitive to outliers and therefore very nonrobust. In the relatively rare cases where one is specifically interested in estimating the true population mean, there is little choice except to pray and use the sample mean.</p> <p>A test is called distribution-free if the probability of falsely rejecting the null hypothesis is the same for all possible underlying continuous distributions (optimal robustness of validity). The typical examples are the two-sample rank tests for testing equality between distributions. Most distribution-free tests happen to have a reasonably stable power and thus also a good robustness of total performance. But this seems to be a fortunate accident, since distribution-freeness does not imply anything about the behavior of the power function.</p> <p>Estimates derived from a distribution-free test are sometimes also called distribution-free, but this is a misnomer: the stochastic behavior of point estimates is intimately connected with the power (not the level) of the parent tests and depends on the underlying distribution. The only exceptions are interval estimates derived from rank tests: for example the interval between two specified sample quantiles catches the true median with a fixed probability (but still the distribution of the length of this interval depends on the underlying distribution).</p> <p>Robust methods, as conceived in this book, are much closer to the classical parametric ideas than to nonparametric or distribution-free ones. They are destined to work with parametric models; the only differences are that the latter are no longer supposed to be literally true, and that one is also trying to take this into account in a formal way.</p> <p>In accordance with these ideas, we intend to standardize robust estimates such that they are consistent estimates of the unknown parameters at the idealized model. Because of robustness they will not drift too far away if the model is only approximately true. Outside of the model we then may</p> <p>define the parameter to be estimated in terms of the limiting value of the estimate-for example, if we use the sample median, then the natural estimand is the population median, and so on.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#adaptive-procedures","title":"Adaptive Procedures","text":"<p>Stein (1956) discovered the possibility of devising nonparametric efficient tests and estimates. Later, several authors, in particular Takeuchi (1971), Beran (1974), Sacks (1975), and Stone (1975), described specific location estimates that are asymptotically efficient for all sufficiently smooth symmetric densities. Since we may say that these estimates adapt themselves to the underlying distribution, they have become known under the name of adaptive procedures. See also the review article by Hogg (1974).</p> <p>At one time, it almost seemed as if the ultimate goal of robust estimation were to construct fully efficient adaptive estimates.</p> <p>However, the connection between adaptivity and robustness is far from clear. The crucial point is that in robustness the emphasis rests much more on safety than on efficiency. The behavior of adaptive procedures under asymmetry is practically unexplored. For extremely large samples, where at first blush adaptive estimates look particularly attractive, the statistical variability of the estimate falls below its potential bias (caused by asymmetric contamination and the like), and robustness would therefore suggest to move toward a less efficient estimate, namely the sample median, that minimizes bias (see Section 4.2). We therefore prefer to follow Stein's original terminology and to classify adaptive estimates not under robustness, but under the heading of efficient nonparametric procedures.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#resistant-procedures","title":"Resistant Procedures","text":"<p>A statistical procedure is called resistant (see Mosteller and Tukey, 1977, p. 203) if the value of the estimate (or test statistic) is insensitive to small changes in the underlying sample (small changes in all, or large changes in a few of the values). The underlying distribution does not enter at all. This notion is particularly appropriate for (exploratory) data analysis and is of course conceptually distinct from robustness. However, in view of Hampel's theorem (Section 2.6), the two notions are for all practical purposes synonymous.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#13-qualitative-robustness","title":"1.3 QUALITATIVE ROBUSTNESS","text":"<p>In this section we motivate and give a formal definition of qualitative asymptotic robustness. For statistics representable as a functional \\(T\\) of the</p> <p>empirical distribution, qualitative robustness is essentially equivalent to weak(-star) continuity of \\(T\\), and for the sake of clarity we first discuss this particular case.</p> <p>Many of the most common test statistics and estimators depend on the sample \\(\\left(x_{1}, \\ldots, x_{n}\\right)\\) only through the empirical distribution function</p> \\[ F_{n}(x)=n^{-1} \\sum 1_{\\left\\{x_{i}&lt;x\\right\\}} \\] <p>or, for more general sample spaces, through the empirical measure</p> \\[ F_{n}=n^{-1} \\sum \\delta_{x_{i}} \\] <p>where \\(\\delta_{x}\\) stands for the pointmass 1 at \\(x\\), that is, we can write</p> \\[ T_{n}\\left(x_{1}, \\ldots, x_{n}\\right)=T\\left(F_{n}\\right) \\] <p>for some functional \\(T\\) defined (at least) on the space of empirical measures. Often \\(T\\) has a natural extension to (a subspace of) the space \\(\\mathscr{R}\\) of all probability measures on the sample space. For instance, if the limit in probability exists, put</p> \\[ T(F)=\\lim _{n \\rightarrow \\infty} T\\left(F_{n}\\right) \\] <p>where \\(F\\) is the true underlying common distribution of the observations. If a functional \\(T\\) satisfies (3.4), it is called consistent at \\(F\\).</p> <p>Example 3.1 The Test Statistic of the Neyman-Pearson Lemma The most powerful tests between two densities \\(p_{0}\\) and \\(p_{1}\\) are based on a statistic of the form</p> \\[ \\int \\psi(x) F_{n}(d x)=\\frac{1}{n} \\sum \\psi\\left(x_{i}\\right) \\] <p>with</p> \\[ \\psi(x)=\\log \\frac{p_{1}(x)}{p_{0}(x)} \\] <p>Example 3.2 The maximum likelihood estimate of \\(\\theta\\) for an assumed underlying family of densities \\(f(x, \\theta)\\) is a solution of</p> \\[ \\int \\psi(x, \\theta) F_{n}(d x)=0 \\] <p>with</p> \\[ \\psi(x, \\theta)=\\frac{\\partial}{\\partial \\theta} \\log f(x, \\theta) \\] <p>Example 3.3 The \\(\\alpha\\)-trimmed mean can be written as</p> \\[ \\bar{X}_{\\alpha}=\\frac{1}{1-2 \\alpha} \\int_{\\alpha}^{1-\\alpha} F_{n}^{-1}(t) d t \\] <p>Example 3.4 The so-called Hodges-Lehmann estimate is one-half of the median of the convolution square</p> \\[ \\frac{1}{2} \\operatorname{med}\\left(F_{n} * F_{n}\\right) \\] <p>(NOTE: this is the median of all \\(n^{2}\\) pairwise means \\(\\left(x_{i}+x_{j}\\right) / 2\\); the more customary versions use only the pairs \\(i&lt;j\\) or \\(i&lt;j\\), but are asymptotically equivalent.)</p> <p>Assume now that the sample space is Euclidean, or more generally, a complete, separable metrizable space. We claim that, in this case, the natural robustness (more precisely, resistance) requirement for a statistic of the form (3.3) is that \\(T\\) should be continuous with respect to the weak(-star) topology. By definition this is the weakest topology in the space \\(\\mathscr{M}\\) of all probability measures such that the map</p> \\[ F \\rightarrow \\int \\psi d F \\] <p>from \\(\\mathscr{M}\\) into \\(\\mathbb{R}\\) is continuous whenever \\(\\psi\\) is bounded and continuous. The converse is also true: if a linear functional of the form (3.11) is weakly continuous, then \\(\\psi\\) must be bounded and continuous; see Chapter 2 for details.</p> <p>The motivation behind this claim is the following basic resistance requirement. Take a linear statistic of the form (3.5) and make a small change in the sample, that is, make either small changes in all of the observations \\(x_{i}\\) (rounding, grouping) or large changes in a few of them (gross errors, blunders). If \\(\\psi\\) is bounded and continuous, then this will result in a small change of \\(T\\left(F_{n}\\right)=\\int \\psi d F_{n}\\). But if \\(\\psi\\) is not bounded, then a single, strategically placed gross error can completely upset \\(T\\left(F_{n}\\right)\\). If \\(\\psi\\) is not continuous, and if \\(F_{n}\\) happens to put mass onto discontinuity points, then small changes in many of the \\(x_{i}\\) may produce a large change in \\(T\\left(F_{n}\\right)\\).</p> <p>We conclude from this that our vague, intuitive notion of resistance or robustness should be made precise as follows: a linear functional \\(T\\) is robust everywhere if and only if (iff) the corresponding \\(\\psi\\) is bounded and continuous, that is, iff \\(T\\) is weakly continuous.</p> <p>We could take this last property as our definition and call a (not necessarily linear) statistical functional \\(T\\) robust if it is weakly continuous.</p> <p>But, following Hampel (1971), we prefer to adopt a slightly more general definition.</p> <p>Let the observations \\(x_{i}\\) be independent identically distributed, with common distribution \\(F\\), and let \\(\\left(T_{n}\\right)\\) be a sequence of estimates or test statistics \\(T_{n}=T_{n}\\left(x_{1}, \\ldots, x_{n}\\right)\\). Then this sequence is called robust at \\(F=F_{0}\\) if the sequence of maps of distributions</p> \\[ F \\rightarrow \\hat{\\mathbb{C}}_{F}\\left(T_{n}\\right) \\] <p>is equicontinuous at \\(F_{0}\\), that is, if we take a suitable distance function \\(d_{*}\\) in the space \\(\\mathscr{M}\\) of probability measures, metrizing the weak topology, and assume that, for each \\(\\varepsilon&gt;0\\), there is a \\(\\delta&gt;0\\) and an \\(n_{0}&gt;0\\) such that, for all \\(F\\) and all \\(n \\geqslant n_{0}\\),</p> \\[ d_{*}\\left(F_{0}, F\\right) \\leqslant \\delta \\Rightarrow d_{*}\\left(\\hat{\\mathbb{C}}_{F_{0}}\\left(T_{n}\\right), \\hat{\\mathbb{C}}_{F}\\left(T_{n}\\right)\\right) \\leqslant \\varepsilon \\] <p>If the sequence \\(\\left(T_{n}\\right)\\) derives from a functional \\(T_{n}=T\\left(F_{n}\\right)\\), then it is shown in Section 2.6 that this definition is essentially equivalent to weak continuity of \\(T\\).</p> <p>Note the close formal analogy between this definition of robustness and stability of ordinary differential equations; let \\(y_{x}(\\cdot)\\) be the solution with initial value \\(y(0)=x\\) of the differential equation</p> \\[ \\frac{d y}{d t}=f(t, y) \\] <p>Then we have stability at \\(x=x_{0}\\) if, for all \\(\\varepsilon&gt;0\\), there is a \\(\\delta&gt;0\\) such that, for all \\(x\\) and all \\(t \\geqslant 0\\),</p> \\[ d\\left(x_{0}, x\\right) \\leqslant \\delta \\Rightarrow d\\left(y_{x_{0}}(t), y_{x}(t)\\right) \\leqslant \\varepsilon \\]"},{"location":"research/note/lecture-note-automation/result/ocr_result/#14-quantitative-robustness","title":"1.4 QUANTITATIVE ROBUSTNESS","text":"<p>For several reasons it may be useful to describe quantitatively how greatly a small change in the underlying distribution \\(F\\) changes the distribution</p> <p>\\(\\mathcal{E}_{F}\\left(T_{n}\\right)\\) of an estimate or test statistic \\(T_{n}=T_{n}\\left(x_{1}, \\ldots, x_{n}\\right)\\). A few crude and simple numerical quantifiers might be more effective than a very detailed description.</p> <p>To fix the idea assume that \\(T_{n}=T\\left(F_{n}\\right)\\) derives from a functional \\(T\\). In most cases of practical, interest, \\(T_{n}\\) is then consistent:</p> \\[ T_{n} \\rightarrow T(F), \\quad \\text { in probability } \\] <p>and asymptotically normal</p> \\[ \\mathcal{E}_{F}\\left\\{\\sqrt{n}\\left[T_{n}-T(F)\\right]\\right\\} \\rightarrow \\mathscr{R}(0, A(F, T)) \\] <p>Then it is convenient to discuss the quantitative large sample robustness of \\(T\\) in terms of the behavior of its asymptotic bias \\(T(F)-T\\left(F_{0}\\right)\\) and asymptotic variance \\(A(F, T)\\) in some neighborhood \\(\\mathscr{P}_{\\varepsilon}\\left(F_{0}\\right)\\) of the model distribution \\(F_{0}\\).</p> <p>For instance, \\(\\mathscr{P}_{\\varepsilon}\\) might be a L\u00e9vy neighborhood</p> \\[ \\mathscr{P}_{\\varepsilon}\\left(F_{0}\\right)=\\left\\{F \\mid \\forall t, F_{0}(t-\\varepsilon)-\\varepsilon \\leqslant F(t) \\leqslant F_{0}(t+\\varepsilon)+\\varepsilon\\right\\} \\] <p>or a contamination \"neighborhood\"</p> \\[ \\mathscr{P}_{\\varepsilon}\\left(F_{0}\\right)=\\left\\{F \\mid F=(1-\\varepsilon) F_{0}+\\varepsilon H, H \\in \\mathscr{R} \\mathbb{\\}}\\right. \\] <p>(the latter is not a neighborhood in the sense of the weak topology). Equation 4.4 is also called the gross error model.</p> <p>The two most important characteristics then are the maximum bias</p> \\[ b_{1}(\\varepsilon)=\\sup _{F \\in \\mathscr{P}_{\\varepsilon}}\\left|T(F)-T\\left(F_{0}\\right)\\right| \\] <p>and the maximum variance</p> \\[ v_{1}(\\varepsilon)=\\sup _{F \\in \\mathscr{P}_{\\varepsilon}} A(F, T) \\] <p>We often consider a restricted supremum of \\(A(F, T)\\) also, assuming that \\(F\\) varies only over some slice of \\(\\mathscr{P}_{\\varepsilon}\\) where \\(T(F)\\) stays constant, for example, only over the set of symmetric distributions.</p> <p>Unfortunately, the above approach to the problem is conceptually inadequate; we should like to establish that, for sufficiently large \\(n\\), our estimate \\(T_{n}\\) behaves well for all \\(F \\in \\mathscr{P}_{\\varepsilon}\\). A description in terms of \\(b_{1}\\) and \\(v_{1}\\) would allow us to show that, for each fixed \\(F \\in \\mathscr{P}_{\\varepsilon}, T_{n}\\) behaves well for</p> <p>sufficiently large \\(n\\). The distinction involves an interchange in the order of quantifiers and is fundamental, but has been largely ignored in the literature.</p> <p>A better approach is as follows. Let \\(M\\left(F, T_{n}\\right)\\) be the median of \\(\\hat{\\mathcal{E}}_{F}\\left[T_{n}-\\right.\\) \\(\\left.T\\left(F_{0}\\right)\\right]\\) and let \\(Q_{t}\\left(F, T_{n}\\right)\\) be a normalized \\(t\\)-quantile range of \\(\\hat{\\mathcal{E}}_{F}\\left(\\sqrt{n} T_{n}\\right)\\), where, for any distribution \\(G\\), the normalized \\(t\\)-quantile range is defined as</p> \\[ Q_{t}=\\frac{G^{-1}(1-t)-G^{-1}(t)}{\\Phi^{-1}(1-t)-\\Phi^{-1}(t)} \\] <p>\\(\\Phi\\) being the standard normal cumulative. The value of \\(t\\) is arbitrary, but fixed, say \\(t=0.25\\) (interquartile range) or \\(t=0.025\\) ( \\(95 \\%\\) range, which is convenient in view of the traditional \\(95 \\%\\) confidence intervals). For a normal distribution, \\(Q_{t}\\) coincides with the standard deviation; therefore \\(Q_{t}^{2}\\) is sometimes called pseudo-variance.</p> <p>Then define the maximum asymptotic bias and variance, respectively, as</p> \\[ \\begin{aligned} &amp; b(\\varepsilon)=\\lim _{n} \\sup _{F \\in \\mathscr{F}_{e}}\\left|M\\left(F, T_{n}\\right)\\right| \\\\ &amp; v(\\varepsilon)=\\lim _{n} \\sup _{F \\in \\mathscr{F}_{e}} Q_{t}\\left(F, T_{n}\\right)^{2} \\end{aligned} \\] <p>THEOREM 4.1 If \\(b_{1}\\) and \\(v_{1}\\) are well-defined, we have \\(b(\\varepsilon) \\geqslant b_{1}(\\varepsilon)\\) and \\(v(\\varepsilon) \\geqslant v_{1}(\\varepsilon)\\). Proof Let \\(T\\left(F_{0}\\right)=0\\) for simplicity and assume that \\(T_{n}\\) is consistent: \\(T\\left(F_{n}\\right) \\rightarrow T(F)\\). Then \\(\\lim _{n} M\\left(F, T_{n}\\right)=T(F)\\), and we have the following obvious inequality, valid for any \\(F \\in \\mathscr{F}_{e}\\) :</p> \\[ b(\\varepsilon)=\\lim _{n} \\sup _{F \\in \\mathscr{F}_{e}}\\left|M\\left(F, T_{n}\\right)\\right| \\geqslant \\lim _{n}\\left|M\\left(F, T_{n}\\right)\\right|=|T(F)| \\] <p>hence</p> \\[ b(\\varepsilon) \\geqslant \\sup _{F \\in \\mathscr{F}_{e}}|T(F)|=b_{1}(\\varepsilon) \\] <p>Similarly, if \\(\\sqrt{n}\\left[T_{n}-T(F)\\right]\\) has a limiting normal distribution, we have \\(\\lim _{n} Q_{t}\\left(F, T_{n}\\right)^{2}=A(F, T)\\), and \\(v(\\varepsilon) \\geqslant v_{1}(\\varepsilon)\\) follows in the same fashion as above.</p> <p>The quantities \\(b\\) and \\(v\\) are awkward to handle, so we usually work with \\(b_{1}\\) and \\(v_{1}\\) instead. We are then, however, obliged to check whether, for the</p> <p>particular \\(\\mathscr{P}_{\\varepsilon}\\) and \\(T\\) under consideration, we have \\(b_{1}=b\\) and \\(v_{1}=v\\). Fortunately, this is usually true.</p> <p>THEOREM 4.2 If \\(\\mathscr{P}_{\\varepsilon}\\) is the L\u00e9vy neighborhood, then \\(b(\\varepsilon) \\leqslant b_{1}(\\varepsilon+0)=\\) \\(\\lim _{\\eta \\downarrow \\varepsilon} b_{1}(\\eta)\\). Proof According to the Glivenko-Cantelli theorem, we have \\(\\sup \\left|F_{n}(x)-\\right.\\) \\(F(x) \\mid \\rightarrow 0\\) in probability, uniformly in \\(F\\). Thus for any \\(\\delta&gt;0\\), the probability of \\(F_{n} \\in \\mathscr{P}_{\\delta}(F)\\), and hence of \\(F_{n} \\in \\mathscr{P}_{\\varepsilon+\\delta}\\left(F_{0}\\right)\\), will tend to 1 , uniformly in \\(F\\) for \\(F \\in \\mathscr{P}_{\\varepsilon}\\left(F_{0}\\right)\\). Hence \\(b(\\varepsilon) \\leqslant b_{1}(\\varepsilon+\\delta)\\) for all \\(\\delta&gt;0\\).</p> <p>Note that, for the above types of neighborhoods, \\(\\mathscr{P}_{1}=\\mathscr{R}\\) is the set of all probability measures on the sample space, so \\(b(1)\\) is the worst possible value of \\(b\\) (usually \\(\\infty\\) ). We define the asymptotic breakdown point of \\(T\\) at \\(F_{0}\\) as</p> \\[ \\varepsilon^{*}=\\varepsilon^{*}\\left(F_{0}, T\\right)=\\sup \\{\\varepsilon \\mid b(\\varepsilon)&lt;b(1)\\} \\] <p>Roughly speaking, the breakdown point gives the limiting fraction of bad outliers the estimator can cope with. In many cases \\(\\varepsilon^{*}\\) does not depend on \\(F_{0}\\), and it is often the same for all the usual choices for \\(\\mathscr{P}_{\\varepsilon}\\). Example 4.1 The breakdown point of the \\(\\alpha\\)-trimmed mean is \\(\\varepsilon^{*}=\\alpha\\). (This is intuitively obvious; for a formal derivation see Section 3.3.)</p> <p>Similarly we may also define an asymptotic variance breakdown point</p> \\[ \\varepsilon^{* *}=\\varepsilon^{* *}\\left(F_{0}, T\\right)=\\sup \\{\\varepsilon \\mid v(\\varepsilon)&lt;v(1)\\} \\] <p>but this is a much less useful notion.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#15-infinitesimal-aspects","title":"1.5 INFINITESIMAL ASPECTS","text":"<p>What happens if we add one more observation with value \\(x\\) to a very large sample? Its suitably normed limiting influence on the value of an estimate or test statistic \\(T\\left(F_{n}\\right)\\) can be expressed as</p> \\[ I C(x, F, T)=\\lim _{s \\rightarrow 0} \\frac{T\\left((1-s) F+s \\delta_{x}\\right)-T(F)}{s} \\] <p>where \\(\\delta_{x}\\) denotes the pointmass 1 at \\(x\\). The above quantity, considered as a function of \\(x\\), has been introduced by Hampel \\((1968,1974)\\) under the name influence curve (IC) or influence function, and is perhaps the most</p> <p>useful heuristic tool of robust statistics. It is treated in more detail in Section 2.5 .</p> <p>If \\(T\\) is sufficiently regular, it can be linearized near \\(F\\) in terms of \\(I C(x, F, T)\\); if \\(G\\) is near \\(F\\), then the leading terms of a Taylor expansion are</p> \\[ T(G)=T(F)+\\int I C(x, F, T)[G(d x)-F(d x)]+\\cdots \\] <p>We have</p> \\[ \\int I C(x, F, T) F(d x)=0 \\] <p>and, if we substitute the empirical distribution \\(F_{n}\\) for \\(G\\) in the above expansion, we obtain</p> \\[ \\begin{aligned} \\sqrt{n}\\left(T\\left(F_{n}\\right)-T(F)\\right) &amp; =\\sqrt{n} \\int I C(x, F, T) F_{n}(d x)+\\cdots \\\\ &amp; =\\frac{1}{\\sqrt{n}} \\sum I C\\left(x_{i}, F, T\\right)+\\cdots \\end{aligned} \\] <p>By the central limit theorem, the leading term on the right-hand side is asymptotically normal with mean 0 , if the \\(x_{i}\\) are independent with common distribution \\(F\\). Since it is often true (but not easy to prove) that the remaining terms are asymptotically negligible, \\(\\sqrt{n}\\left[T\\left(F_{n}\\right)-T(F)\\right]\\) is then asymptotically normal with mean 0 and variance</p> \\[ A(F, T)=\\int I C(x, F, T)^{2} F(d x) \\] <p>Thus the influence function has two main uses. First, it allows us to assess the relative influence of individual observations toward the value of an estimate or test statistic. If it is unbounded, an outlier might cause trouble. Its maximum absolute value,</p> \\[ \\gamma^{*}=\\sup _{x}|I C(x, F, T)| \\] <p>has been called gross error sensitivity by Hampel. It is related to the maximum bias (4.5): take the gross error model (4.4), then, approximately,</p> \\[ T(F)-T\\left(F_{0}\\right) \\approx \\varepsilon \\int I C\\left(x, F_{0}, T\\right) H(d x) \\] <p>Hence</p> \\[ b_{1}(\\varepsilon)=\\sup \\left|T(F)-T\\left(F_{0}\\right)\\right| \\cong \\varepsilon \\gamma^{*} \\] <p>However some risky and possibly illegitimate interchanges of suprema and passages to the limit are involved here. We give two examples later (Section 3.5) where (1) \\(\\gamma^{*}&lt;\\infty\\) but \\(b_{1}(\\varepsilon)=\\infty\\) for all \\(\\varepsilon&gt;0\\). (2) \\(\\gamma^{*}=\\infty\\) but \\(\\lim b(\\varepsilon)=0\\) for \\(\\varepsilon \\rightarrow 0\\).</p> <p>Second, the influence curve allows an immediate and simple, heuristic assessment of the asymptotic properties of an estimate, since it allows us to guess an explicit formula (5.5) for the asymptotic variance (which then has to be proved rigorously by other means).</p> <p>There are several finite sample and/or difference quotient versions of (5.1); the most important ones are the sensitivity curve (Tukey 1970) and the jackknife (Quenouille 1956, Tukey 1958, Miller 1964, 1974). We obtain the sensitivity curve if we replace \\(F\\) by \\(F_{n-1}\\) and \\(s\\) by \\(1 / n\\) in (5.1):</p> \\[ \\begin{aligned} S C_{n-1}(x) &amp; =\\frac{T\\left(\\frac{n-1}{n} F_{n-1}+\\frac{1}{n} \\delta_{x}\\right)-T\\left(F_{n-1}\\right)}{\\frac{1}{n}} \\\\ &amp; =n\\left[T_{n}\\left(x_{1}, \\ldots, x_{n-1}, x\\right)-T_{n-1}\\left(x_{1}, \\ldots, x_{n-1}\\right)\\right] \\end{aligned} \\] <p>The jackknife is defined as follows. Consider an estimate \\(T_{n}\\left(x_{1}, \\ldots, x_{n}\\right)\\), which is essentially the \"same\" across different sample sizes (for instance, assume that it is a functional of the empirical distribution). Then the ith jackknifed pseudo-value is, by definition,</p> \\[ T_{n i}^{*}=n T_{n}-(n-1) T_{n-1}\\left(x_{1}, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_{n}\\right) \\] <p>For example, if \\(T_{n}\\) is the sample mean, then \\(T_{n i}^{*}=x_{i}\\). This is an approximation to \\(I C\\left(x_{i}\\right)\\); more precisely, if we substitute \\(F_{n}\\) for \\(F\\) and \\(-1 /(n-1)\\) for \\(s\\) in (5.1), we obtain</p> \\[ \\begin{aligned} &amp; \\frac{T\\left(\\frac{n}{n-1} F_{n}-\\frac{1}{n-1} \\delta_{x_{i}}\\right)-T\\left(F_{n}\\right)}{-\\frac{1}{n-1}} \\\\ &amp; =(n-1)\\left[T_{n}-T_{n-1}\\left(x_{1}, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_{n}\\right)\\right] \\\\ &amp; =T_{n i}^{*}-T_{n} \\end{aligned} \\] <p>If \\(T_{n}\\) is a consistent estimate of \\(\\theta\\), whose bias has the asymptotic expansion</p> \\[ E\\left(T_{n}-\\theta\\right)=\\frac{a_{1}}{n}+\\frac{a_{2}}{n^{2}}+O\\left(\\frac{1}{n^{3}}\\right) \\] <p>then</p> \\[ T_{n}^{*}=\\frac{1}{n} \\sum_{i} T_{n i}^{*} \\] <p>has a smaller bias:</p> \\[ E\\left(T_{n}^{*}-\\theta\\right)=-\\frac{a_{2}}{n^{2}}+O\\left(\\frac{1}{n^{3}}\\right) \\] <p>Example 5.1 If \\(T_{n}=1 / n \\sum\\left(x_{i}-\\bar{x}\\right)^{2}\\), then</p> \\[ T_{n i}^{*}=\\frac{n}{n-1}\\left(x_{i}-\\bar{x}\\right)^{2} \\] <p>and</p> \\[ T_{n}^{*}=\\frac{1}{n-1} \\sum\\left(x_{i}-\\bar{x}\\right)^{2} \\] <p>Tukey (1958) pointed out that</p> \\[ \\frac{1}{n(n-1)} \\sum\\left(T_{n i}^{*}-T_{n}^{*}\\right)^{2} \\] <p>(a finite sample version of 5.5 ) usually is a good estimator of the variance of \\(T_{n}\\). (It can also be used as an estimate of the variance of \\(T_{n}^{*}\\), but it is better matched to \\(T_{n}\\).)</p> <p>WARNING In some cases, namely when the influence function \\(I C(x ; F, T)\\) does not depend smoothly on \\(F\\), the jackknife is in trouble and may yield a variance that is worse than useless. This happens, in particular, for estimates that are based on a small number of order statistics, like the median.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#16-optimal-robustness","title":"1.6 OPTIMAL ROBUSTNESS","text":"<p>In Section 1.4 we introduced some quantitative measures of robustness. They certainly are not the only ones. But, as we defined robustness to</p> <p>mean insensitivity with regard to small deviations from the assumptions, any quantitative measure of robustness must somehow be concerned with the maximum degradation of performance possible for an \\(\\varepsilon\\)-deviation from the assumptions. An optimally robust procedure then minimizes this maximum degradation and hence will be a minimax procedure of some kind. As we have considerable freedom in how we quantize performance and \\(\\varepsilon\\)-deviations, we also have a host of notions of optimal robustness, of various usefulness, and of various mathematical manageability.</p> <p>Exact, finite sample minimax results are available for two simple, but important special cases: the first corresponds to a robustification of the Neyman-Pearson lemma, and the second yields interval estimates of location. They are treated in Chapter 10. While the resulting tests and estimates are quite simple, the approach does not generalize well. In particular, it does not seem possible to obtain explicit, finite-sample results when there are nuisance parameters (e.g., when scale is unknown).</p> <p>If we use asymptotic performance criteria (like asymptotic variances), we obtain asymptotic minimax estimates, treated in Chapters 4 to 6 . These asymptotic theories work well only if there is a high degree of symmetry (left-right symmetry, translation invariance, etc.), but they are able to cope with nuisance parameters. By a fortunate accident some of the asymptotic minimax estimates, although derived under quite different assumptions, coincide with certain finite-sample minimax estimates; this gives a strong heuristic support for using asymptotic optimality criteria.</p> <p>Multiparameter regression, and the estimation of covariance matrices possess enough symmetries that the above asymptotic optimality results are transferable (Chapters 7 and 8). However the value of this transfer is somewhat questionable because of the fact that in practice the number of observations per parameter tends to be uncomfortably low. Other, designrelated dangers, like leverage points, may become more important than distributional robustness itself.</p> <p>In problems lacking invariance, for instance in the general one-parameter estimation problem, Hampel (1968) has proposed optimizing robustness by minimizing the asymptotic variance at the model, subject to a bound on the gross-error sensitivity \\(\\gamma^{*}\\) defined by (5.6). This approach is technically the simplest one, but it has some conceptual drawbacks; reassuringly, it again yields the same estimates as those obtained by the exact, finite-sample minimax approach when the latter is applicable. For details, see Section 11.1.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#17-computation-of-robust-estimates","title":"1.7 COMPUTATION OF ROBUST ESTIMATES","text":"<p>In many practical applications of (say) the method of least squares, the actual setting up and solving of the least squares equations occupies only a</p> <p>small fraction of the total length of the computer program. We should therefore strive for robust algorithms that can easily be patched into existing programs, rather than for comprehensive robust packages.</p> <p>This is in fact possible. Technicalities are discussed in Chapter 7; the salient ideas are as follows.</p> <p>Assume we are doing a least squares fit on observations \\(y_{i}\\), yielding fitted values \\(\\hat{y}_{i}\\), and residuals \\(r_{i}=y_{i}-\\hat{y}_{i}\\). Let \\(s_{i}\\) be some estimate of the standard error of \\(y_{i}\\) (or, even better, of the standard error of \\(r_{i}\\) ).</p> <p>We metrically Winsorize the observations \\(y_{i}\\) and replace them by pseudoobservations \\(y_{i}^{*}\\) :</p> \\[ \\begin{aligned} y_{i}^{*} &amp; =y_{i}, &amp; &amp; \\text { if }\\left|r_{i}\\right| \\leqslant c s_{i} \\\\ &amp; =\\hat{y}_{i}-c s_{i}, &amp; &amp; \\text { if } r_{i}&lt;-c s_{i} \\\\ &amp; =\\hat{y}_{i}+c s_{i}, &amp; &amp; \\text { if } r_{i}&gt;c s_{i} \\end{aligned} \\] <p>The constant \\(c\\) regulates the amount of robustness; good choices are in the range between 1 and 2 , say \\(c=1.5\\).</p> <p>Then use the pseudo-observations \\(y_{i}^{*}\\) to calculate new fitted values \\(\\hat{y}_{i}\\) (and new \\(s_{i}\\) ), and iterate to convergence.</p> <p>If all observations are equally accurate, the classical estimate of the variance of a single observation would be</p> \\[ s^{2}=\\frac{1}{n-p} \\sum r_{i}^{2} \\] <p>and we can then estimate the standard error of the residual \\(r_{i}\\) by \\(s_{i}=\\sqrt{1-h_{i}} s\\), where \\(h_{i}\\) is the \\(i\\) th diagonal element of \\(H=X\\left(X^{T} X\\right)^{-1} X^{T}\\).</p> <p>If we use modified residuals \\(r_{i}^{*}=y_{i}^{*}-\\hat{y}_{i}\\) instead of the \\(r_{i}\\), we clearly would underestimate scale; we can correct this bias (to a zero order approximation) by putting</p> \\[ s^{2}=\\frac{\\frac{1}{n-p} \\sum r_{i}^{* 2}}{\\left(\\frac{m}{n}\\right)^{2}} \\] <p>where \\(n-p\\) is the number of observations minus the number of parameters, and \\(m\\) is the number of unmodified observations \\(\\left(y_{i}^{*}=y_{i}\\right)\\).</p> <p>It is evident that this procedure deflates the influence of outliers. Moreover there are versions of this procedure that are demonstrably convergent; they converge to a reasonably well-understood \\(M\\)-estimate.</p> <p>These ideas yield a completely general recipe to robustize almost any procedure: we first \"clean\" the data by pulling outliers towards their fitted values in the manner of (7.1), refitting iteratively until convergence is obtained. Then we apply the procedure in question to the pseudoobservations \\(y_{i}^{*}\\). Compare Huber (1979) and Kleiner et al. (1979) for specific examples.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-2","title":"CHAPTER 2","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#the-weak-topology-and-its-metrization","title":"The Weak Topology and its Metrization","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#21-general-remarks","title":"2.1 GENERAL REMARKS","text":"<p>This chapter attempts to give a more or less self-contained account of the formal mathematics underlying qualitative and quantitative robustness. It can be skipped by a reader who is willing to accept a number of results on faith: the more important ones are quoted and explained in an informal, heuristic fashion at the appropriate places elsewhere in this book.</p> <p>The principal background references for this chapter are Prohorov (1956) and Billingsley (1968); some details on Polish spaces are most elegantly treated in Neveu (1964).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#22-the-weak-topology","title":"2.2 THE WEAK TOPOLOGY","text":"<p>Ordinarily, our sample space \\(\\Omega\\) is a finite dimensional Euclidean space. Somewhat more generally, we assume throughout this chapter that \\(\\Omega\\) is a Polish space, that is, a topological space whose topology is metrizable by some metric \\(d\\), such that \\(\\Omega\\) is complete and separable (i.e., contains a countable dense subset). Let \\(\\mathscr{M}\\) be the space of all probability measures on \\((\\Omega, \\mathscr{B})\\), where \\(\\mathscr{B}\\) is the Borel- \\(\\sigma\\)-algebra (i.e., the smallest \\(\\sigma\\)-algebra containing the open subsets of \\(\\Omega\\) ). By \\(\\mathscr{M}^{\\prime}\\) we denote the set of finite signed measures on \\((\\Omega, \\mathscr{B})\\), that is the linear space generated by \\(\\mathscr{M}\\). We use capital latin italic letters for the elements of \\(\\Omega\\); if \\(\\Omega=\\mathbb{R}\\) is the real line, we use the same letter \\(F\\) for both the measure and the associated distribution function, with the convention that \\(F(\\cdot)\\) denotes the distribution function, \\(F\\{\\cdot\\}\\) the set function: \\(F(x)=F[(-\\infty, x)\\}\\).</p> <p>It is well known that every measure \\(F \\in \\mathscr{M}\\) is regular in the sense that any Borel set \\(B \\in \\mathscr{B}\\) ca. 1 be approximated in \\(F\\)-measure by compact sets \\(C\\)</p> <p>from below and by open sets \\(G\\) from above:</p> \\[ \\sup _{C \\subset B} F\\{C\\}=F\\{B\\}=\\inf _{G \\supset B} F\\{G\\} \\] <p>Compare, for example, Neveu (1964). The weak(-star) topology in \\(\\mathscr{M}\\) is the weakest topology such that, for every bounded continuous function \\(\\psi\\), the map</p> \\[ F \\rightarrow \\int \\psi d F \\] <p>from \\(\\mathscr{M}\\) into \\(\\mathbb{R}\\) is continuous. Let \\(L\\) be a linear functional on \\(\\mathscr{M}\\) (or, more precisely, the restriction to \\(\\mathscr{M}\\) of a linear functional on \\(\\left.\\mathscr{M}^{\\prime}\\right)\\).</p> <p>LEMMA 2.1 A linear functional \\(L\\) is weakly continuous on \\(\\mathscr{M}\\) iff it can be represented in the form</p> \\[ L(F)=\\int \\psi d F \\] <p>for some bounded continuous function \\(\\psi\\). Proof Evidently, every functional representable in this way is linear and weakly continuous on \\(\\mathscr{M}\\). Conversely, assume that \\(L\\) is weakly continuous and linear. Put</p> \\[ \\psi(x)=L\\left(\\delta_{x}\\right) \\] <p>where \\(\\delta_{x}\\) denotes the measure putting a pointmass 1 at \\(x\\). Then, because of linearity, (2.3) holds for all \\(F\\) with finite support. Clearly, whenever \\(x_{n}\\) is a sequence of points converging to \\(x\\), then \\(\\delta_{x_{n}} \\rightarrow \\delta_{x}\\) weakly; hence</p> \\[ \\psi\\left(x_{n}\\right)=L\\left(\\delta_{x_{n}}\\right) \\rightarrow L\\left(\\delta_{x}\\right)=\\psi(x) \\] <p>and \\(\\psi\\) must be continuous. If \\(\\psi\\) should be unbounded, say \\(\\sup \\psi(x)=\\infty\\), then choose a sequence of points such that \\(\\psi\\left(x_{n}\\right) \\geqslant n^{2}\\), and let (with an arbitrary \\(x_{0}\\) )</p> \\[ F_{n}=\\left(1-\\frac{1}{n}\\right) \\delta_{x_{0}}+\\frac{1}{n} \\delta_{x_{n}} \\] <p>Clearly, \\(F_{n} \\rightarrow \\delta_{x_{0}}\\) weakly, but \\(L\\left(F_{n}\\right)=\\psi\\left(x_{0}\\right)+(1 / n)\\left[\\psi\\left(x_{n}\\right)-\\psi\\left(x_{0}\\right)\\right]\\) diverges.</p> <p>This contradicts the assumed continuity of \\(L\\); hence \\(\\psi\\) must be bounded. Furthermore the measures with finite support are dense in \\(\\mathscr{M}\\) (for every \\(F \\in \\mathscr{M}\\) and every finite set \\(\\left\\{\\psi_{1}, \\ldots, \\psi_{n}\\right\\}\\) of bounded continuous functions, we can easily find a measure \\(F^{*}\\) with finite support such that \\(\\int \\psi_{i} d F^{*}-\\) \\(\\int \\psi_{i} d F\\) is arbitrarily small simultaneously for all \\(i\\) ); hence the representation (2.3) holds for all \\(F \\in \\mathscr{M}\\).</p> <p>LEMMA 2.2 The following statements are equivalent: (1) \\(F_{n} \\rightarrow F\\) weakly. (2) \\(\\liminf F_{n}\\{G\\} \\geqslant F\\{G\\}\\) for all open sets \\(G\\). (3) \\(\\limsup F_{n}\\{A\\} \\leqslant F\\{A\\}\\) for all closed sets \\(A\\). (4) \\(\\lim F_{n}\\{B\\}=F\\{B\\}\\) for all Borel sets with \\(F\\)-null boundary (i.e., \\(F\\{\\dot{B}\\}=F\\{B\\}=F\\{\\widetilde{B}\\}\\), where \\(\\dot{B}\\) denotes the interior and \\(\\widetilde{B}\\) the closure of \\(B\\).</p> <p>Proof We show \\((1) \\Rightarrow(2) \\Leftrightarrow(3) \\Rightarrow(4) \\Rightarrow(1)\\). Equivalence of (2) and (3) is obvious, and we now show that they imply (4).</p> <p>If \\(B\\) has \\(F\\)-null boundary, then it follows from (2) and (3) that</p> \\[ \\liminf F_{n}\\{\\dot{B}\\} \\geqslant F\\{\\dot{B}\\}=F\\{B\\}=F\\{\\widetilde{B}\\} \\geqslant \\lim \\sup F_{n}\\{\\widetilde{B}\\} \\] <p>As</p> \\[ F_{n}\\{\\dot{B}\\} \\leqslant F_{n}\\{B\\} \\leqslant F_{n}\\{\\widetilde{B}\\} \\] <p>(4) follows.</p> <p>We now show (1) \\(\\Rightarrow\\) (2). Let \\(\\varepsilon&gt;0\\), let \\(G\\) be open, and let \\(A \\subset G\\) be a closed set such that \\(F\\{A\\} \\geqslant F\\{G\\}-\\varepsilon\\) (remember that \\(F\\) is regular). By Urysohn's lemma [cf. Kelley (1955)] there is a continuous function \\(\\psi\\) satisfying \\(1_{A} \\leqslant \\psi \\leqslant 1_{G}\\). Hence (1) implies</p> \\[ \\liminf F_{n}\\{G\\} \\geqslant \\lim \\int \\psi d F_{n}=\\int \\psi d F \\geqslant F\\{A\\} \\geqslant F\\{G\\}-\\varepsilon \\] <p>Since \\(\\varepsilon\\) was arbitrary, (2) follows. It remains to show (4) \\(\\Rightarrow(1)\\). It suffices to verify \\(\\int \\psi d F_{n} \\rightarrow \\int \\psi d F\\) for positive \\(\\psi\\), say \\(0 \\leqslant \\psi \\leqslant M\\); thus we can write</p> \\[ \\int \\psi d F_{n}=\\int_{0}^{M} F_{n}\\{\\psi&gt;t\\} d t \\] <p>For almost all \\(t,\\{\\psi&gt;t\\}\\) is an open set with \\(F\\)-null boundary. Hence the</p> <p>integrand in (2.4) converges to \\(F\\{\\psi&gt;t\\}\\) for almost all \\(t\\), and (1) now follows from the dominated convergence theorem.</p> <p>COROLLARY 2.3 On the real line, weak convergence \\(F_{n} \\rightarrow F\\) holds iff the sequence of distribution functions converges at every continuity point of \\(F\\).</p> <p>Proof If \\(F_{n}\\) converges weakly, then (4) implies at once convergence at the continuity points of \\(F\\). Conversely, if \\(F_{n}\\) converges at the continuity points of \\(F\\), then a straightforward monotonicity argument shows that</p> \\[ F(x)=F(x-0) \\leqslant \\liminf F_{n}(x) \\leqslant \\limsup F_{n}(x+0) \\leqslant F(x+0) \\] <p>where \\(F(x+0)\\) and \\(F(x-0)\\) denote the left and right limits of \\(F\\) at \\(x\\), respectively. We now verify (2). Every open set \\(G\\) is a disjoint union of open intervals \\(\\left(a_{i}, b_{i}\\right)\\); thus</p> \\[ F_{n}(G)=\\sum\\left[F_{n}\\left(b_{i}\\right)-F_{n}\\left(a_{i}+0\\right)\\right] \\] <p>Fatou's lemma now yields, in view of (2.5),</p> \\[ \\begin{aligned} \\liminf F_{n}(G) &amp; \\geqslant \\sum \\liminf \\left[F_{n}\\left(b_{i}\\right)-F_{n}\\left(a_{i}+0\\right)\\right] \\\\ &amp; \\geqslant \\sum\\left[F\\left(b_{i}\\right)-F\\left(a_{i}+0\\right)\\right]=F(G) \\end{aligned} \\] <p>DEFINITION 2.4 A subset \\(\\mathbb{S} \\subset \\mathscr{M}\\) is called tight if, for every \\(\\varepsilon&gt;0\\), there is a compact set \\(K \\subset \\Omega\\) such that, for all \\(F \\in \\mathbb{S}, F\\{K\\} \\geqslant 1-\\varepsilon\\).</p> <p>In particular, every finite subset is tight [this follows from regularity (2.1)].</p> <p>LEMMA 2.5 A subset \\(\\mathbb{S} \\subset \\mathscr{M}\\) is tight iff, for every \\(\\varepsilon&gt;0, \\delta&gt;0\\), there is a finite union</p> \\[ B=\\bigcup_{i} B_{i} \\] <p>of \\(\\delta\\)-balls, \\(B_{i}=\\{y \\mid d\\left(x_{i}, y\\right) \\leqslant \\delta\\}\\), such that, for all \\(F \\in \\mathbb{S}, F(B) \\geqslant 1-\\varepsilon\\). Proof If \\(\\mathbb{S}\\) is tight, then the existence of such a finite union of \\(\\delta\\)-balls follows easily from the fact that every compact set \\(K \\subset \\Omega\\) can be covered by a finite union of open \\(\\delta\\)-balls.</p> <p>Conversely, given \\(\\varepsilon&gt;0\\), choose, for every natural number \\(k\\), a finite union \\(B_{k}=\\cup_{i=1}^{n_{k}} B_{k i}\\) of \\(1 / k\\)-balls \\(B_{k i}\\), such that, for all \\(F \\in \\mathbb{S}, F\\left(B_{k}\\right) \\geqslant 1-\\) \\(\\varepsilon 2^{-k}\\).</p> <p>Let \\(K=\\cap B_{k}\\), then evidently \\(F(K) \\geqslant 1-\\Sigma \\varepsilon 2^{-k}=1-\\varepsilon\\). We claim that \\(K\\) is compact. As \\(K\\) is closed it suffices to show that every sequence ( \\(x_{n}\\) ) with \\(x_{n} \\in K\\) has an accumulation point (for Polish spaces, sequential compactness implies compactness). For each \\(k, B_{k 1}, \\ldots, B_{k n_{k}}\\) form a finite cover of \\(K\\); hence it is possible to inductively choose sets \\(B_{k i_{k}}\\) such that, for all \\(m, A_{m}=\\cap_{k&lt;m} B_{k i_{k}}\\) contains infinitely many members of the sequence \\(\\left(x_{n}\\right)\\). Thus if we pick a subsequence \\(x_{n_{m}} \\in A_{m}\\), it will be a Cauchy sequence, \\(d\\left(x_{n_{m}}, x_{n_{l}}\\right) \\leqslant 2 / \\min (m, l)\\), and, since \\(\\Omega\\) is complete, it converges.</p> <p>THEOREM 2.6 (Prohorov) A set \\(\\mathfrak{S} \\subset \\mathscr{M}\\) is tight iff its weak closure is weakly compact.</p> <p>Proof In view of Lemma 2.2(3) a set is tight iff its weak closure is, so it suffices to prove the theorem for weakly closed sets \\(\\mathscr{S} \\subset \\mathscr{M}\\).</p> <p>Let \\(\\mathcal{C}\\) be the space of bounded continuous functions on \\(\\Omega\\). We rely on Daniell's theorem [see Neveu (1964), Proposition II.7.1], according to which a positive, linear functional \\(L\\) on \\(\\mathcal{C}\\), satisfying \\(L(1)=1\\), is induced by a probability measure \\(F: L(\\psi)=\\int \\psi d F\\) for some \\(F \\in \\mathscr{M}\\) iff \\(\\psi_{n} \\downarrow 0\\) (pointwise) implies \\(L\\left(\\psi_{n}\\right) \\downarrow 0\\).</p> <p>Let \\(\\mathcal{E}\\) be the space of positive linear functionals on \\(\\mathcal{C}\\), satisfying \\(L(1) \\leqslant 1\\), topologized by the topology of pointwise convergence on \\(\\mathcal{C}\\). Then \\(\\mathcal{E}\\) is compact, and \\(\\mathfrak{S}\\) can be identified with a subspace \\(\\mathfrak{S} \\subset \\mathcal{E}\\) in a natural way. Evidently, \\(\\mathfrak{S}\\) is compact iff it is closed as a subspace of \\(\\mathcal{E}\\).</p> <p>Now assume that \\(\\mathfrak{S}\\) is tight. Let \\(L \\in \\mathcal{E}\\) be in the closure of \\(\\mathfrak{S}\\); we want to show that \\(L\\left(\\psi_{n}\\right) \\downarrow 0\\) for every monotone decreasing sequence \\(\\psi_{n} \\downarrow 0\\) of bounded continuous functions. Without loss of generality we can assume \\(0 \\leqslant \\psi_{n} \\leqslant 1\\). Let \\(\\varepsilon&gt;0\\) and let \\(K\\) be such that, for all \\(F \\in \\mathcal{S}, F(K) \\geqslant 1-\\varepsilon\\). The restriction of \\(\\psi_{n}\\) to the compact set \\(K\\) converges not only pointwise but uniformly, say \\(\\psi_{n} \\leqslant \\varepsilon\\) on \\(K\\) for \\(n \\geqslant n_{0}\\). Thus for all \\(F \\in \\mathcal{S}\\) and all \\(n \\geqslant n_{0}\\),</p> \\[ \\begin{aligned} 0 \\leqslant \\int \\psi_{n} d F &amp; =\\int_{K} \\psi_{n} d F+\\int_{K^{\\tau}} \\psi_{n} d F \\\\ &amp; \\leqslant \\int_{K} \\varepsilon d F+\\int_{K^{\\tau}} 1 d F \\leqslant 2 \\varepsilon \\end{aligned} \\] <p>It follows that \\(0 \\leqslant L\\left(\\psi_{n}\\right) \\leqslant 2 \\varepsilon\\); hence \\(\\lim L\\left(\\psi_{n}\\right)=0\\), since \\(\\varepsilon\\) was arbitrary. Thus \\(L\\) is induced by a probability measure; hence it lies in \\(\\mathfrak{S}\\) (which by assumption is a weakly closed subset of \\(\\mathscr{M}\\) ), and thus \\(\\mathfrak{S}\\) is compact ( \\(\\mathfrak{S}\\) being closed in \\(\\mathcal{E}\\) ).</p> <p>Conversely, assume that \\(\\mathfrak{S}\\) is compact, and let \\(\\psi_{n} \\in \\mathcal{C}\\) and \\(\\psi_{n} \\downarrow 0\\). Then \\(\\int \\psi_{n} d F \\downarrow 0\\) pointwise on the compact set \\(\\mathfrak{S}\\); thus, also uniformly, \\(\\sup _{F \\in \\mathcal{S}} \\int \\psi_{n} d F \\downarrow 0\\). We now choose \\(\\psi_{n}\\) as follows. Let \\(\\delta&gt;0\\) be given. Let \\(\\left(x_{n}\\right)\\) be a dense sequence in \\(\\Omega\\), and by Urysohn's lemma, let \\(\\varphi_{i}\\) be a continuous</p> <p>function with values between 0 and 1 , such that \\(\\varphi_{i}(x)=0\\) for \\(d\\left(x_{i}, x\\right) \\leqslant \\delta / 2\\) and \\(\\varphi_{i}(x)=1\\) for \\(d\\left(x_{i}, x\\right) \\geqslant \\delta\\). Put \\(\\psi_{n}(x)=\\inf \\left\\{\\varphi_{i}(x) \\mid i \\leqslant n\\right\\}\\). Then \\(\\psi_{n} \\downarrow 0\\) and \\(\\psi_{n} \\geqslant 1_{A_{n}^{c}}\\), where \\(A_{n}\\) is the union of the \\(\\delta\\)-balls around \\(x_{i}, i=1, \\ldots, n\\). Hence \\(\\sup _{F \\in \\mathbb{R}} F\\left(A_{n}^{c}\\right) \\leqslant \\sup _{F \\in \\mathbb{R}} \\int \\psi_{n} d F \\downarrow 0\\), and the conclusion follows from Lemma 2.5.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#23-levy-and-prohorov-metrics","title":"2.3 L\u00c9VY AND PROHOROV METRICS","text":"<p>We now show that the space \\(\\mathscr{M}\\) of probability measures on a Polish space \\(\\Omega\\), topologized by the weak topology, is itself a Polish space, that is complete separable metrizable.</p> <p>For the real line \\(\\Omega=\\mathbb{R}\\), the most manageable metric metrizing \\(\\mathscr{M}\\) is the so-called L\u00e9vy distance.</p> <p>DEFINITION 3.1 The L\u00e9vy distance between two distribution functions \\(F\\) and \\(G\\) is</p> \\[ d_{L}(F, G)=\\inf \\{\\varepsilon \\mid \\forall x F(x-\\varepsilon)-\\varepsilon \\leqslant G(x) \\leqslant F(x+\\varepsilon)+\\varepsilon\\} \\] <p>LEMMA 3.2 \\(d_{L}\\) is a metric. Proof We have to verify (1) \\(d_{L}(F, G) \\geqslant 0, d_{L}(F, G)=0\\) iff \\(F=G\\); (2) \\(d_{L}(F, G)=d_{L}(G, F)\\); (3) \\(d_{L}(F, H) \\leqslant d_{L}(F, G)+d_{L}(G, H)\\). All of this is immediate.</p> <p>NOTE \\(\\sqrt{2} d_{L}(F, G)\\) is the maximum distance between the graphs of \\(F\\) and \\(G\\), measured along a \\(45^{\\circ}\\)-direction (see Exhibit 2.3.1).</p> <p>THEOREM 3.3 The L\u00e9vy distance metrizes the weak topology. </p> <p>Exhlbl 2.3.1</p> <p>Proof In view of Lemma 2.3 it suffices to show that convergence of \\(F_{n} \\rightarrow F\\) at the continuity points of \\(F\\) and \\(d_{L}\\left(F, F_{n}\\right) \\rightarrow 0\\) are equivalent. (1) Assume \\(d_{L}\\left(F, F_{n}\\right) \\rightarrow 0\\). If \\(x\\) is a continuity point of \\(F\\), then \\(F(x \\pm \\varepsilon) \\pm \\varepsilon \\rightarrow F(x)\\) as \\(\\varepsilon \\rightarrow 0\\); hence \\(F_{n}\\) converges at \\(x\\). (2) Assume that \\(F_{n} \\rightarrow F\\) at the continuity points of \\(F\\). Let \\(x_{0}&lt;x_{1}&lt;\\cdots&lt;\\cdots&lt;x_{N}\\) be continuity points of \\(F\\) such that \\(F\\left(x_{0}\\right)&lt;\\varepsilon / 2, F\\left(x_{N}\\right)&gt;1-\\varepsilon / 2\\), and that \\(x_{i+1}-x_{i}&lt;\\varepsilon\\). Let \\(n_{0}\\) be so large that, for all \\(i\\) and all \\(n \\geqslant n_{0},\\left|F_{n}\\left(x_{i}\\right)-F\\left(x_{i}\\right)\\right|&lt;\\varepsilon / 2\\). Then for \\(x_{i-1} \\leqslant x \\leqslant x_{i}\\),</p> \\[ F_{n}(x) \\leqslant F_{n}\\left(x_{i}\\right)&lt;F\\left(x_{i}\\right)+\\frac{\\varepsilon}{2} \\leqslant F(x+\\varepsilon)+\\varepsilon \\] <p>This bound obviously also holds for \\(x&lt;x_{0}\\) and for \\(x&gt;x_{N}\\). In the same way we establish \\(F_{n}(x) \\geqslant F(x-\\varepsilon)-\\varepsilon\\).</p> <p>For general Polish sample spaces \\(\\Omega\\), the weak topology in \\(\\mathscr{M}\\) can be metrized by the so-called Prohorov distance. Conceptually, this is the most attractive metric; however, it is not very manageable for actual calculations. We need a few preparatory definitions.</p> <p>For any subset \\(A \\subset \\Omega\\), we define the closed \\(\\delta\\)-neighborhood of \\(A\\) as</p> \\[ A^{\\delta}=\\{x \\in \\Omega \\mid \\inf _{y \\in A} d(x, y) \\leqslant \\delta\\} \\] <p>LEMMA 3.4 For any arbitrary set \\(A\\), we have</p> \\[ A^{\\delta}=\\overline{A^{\\delta}}=\\overline{A^{\\delta}}=\\overline{\\overline{A^{\\delta}}} \\] <p>(where an overbar denotes closure). In particular, \\(A^{\\delta}\\) is closed. Proof It suffices to show</p> \\[ \\overline{\\overline{A^{\\delta}}} \\subset A^{\\delta} \\] <p>Let</p> \\[ \\eta&gt;0 \\quad \\text { and } \\quad x \\in \\overline{\\overline{A^{\\delta}}} \\] <p>Then we can successively find \\(y \\in \\overline{A^{\\delta}}, z \\in \\bar{A}\\), and \\(t \\in A\\), such that \\(d(x, y)&lt;\\eta\\), \\(d(y, z)&lt;\\delta+\\eta\\), and \\(d(z, t)&lt;\\eta\\). Thus \\(d(x, t)&lt;\\delta+3 \\eta\\), and, since \\(\\eta\\) was arbitrary, \\(x \\in A^{\\delta}\\).</p> <p>Let \\(G \\in \\mathscr{M}\\) be a fixed probability measure, and let \\(\\varepsilon, \\delta&gt;0\\). Then the set</p> \\[ \\mathscr{P}_{\\varepsilon, \\delta}=\\left\\{F \\in \\mathscr{M} \\mid F\\{A\\} \\leqslant G\\left\\{A^{\\delta}\\right\\}+\\varepsilon \\text { for all } A \\in \\mathscr{B}\\right\\} \\] <p>is called a Prohorov neighborhood of \\(G\\). Often we assume \\(\\varepsilon=\\delta\\).</p> <p>DEFINITION 3.5 The Prohorov distance between two members \\(F, G \\in \\mathscr{D}\\) is</p> \\[ d_{p}(F, G)=\\inf \\{\\varepsilon&gt;0 \\mid F\\{A\\} \\leqslant G\\left\\{A^{\\varepsilon}\\right\\}+\\varepsilon \\text { for all } A \\in \\mathscr{S}\\} \\] <p>We have to show that this is a metric. First, we show that it is symmetric in \\(F\\) and \\(G\\); this follows immediately from the following lemma.</p> <p>LEMMA 3.6 If \\(F\\{A\\} \\leqslant G\\left\\{A^{\\delta}\\right\\}+\\varepsilon\\) for all \\(A \\in \\mathscr{S}\\), then \\(G\\{A\\} \\leqslant F\\left\\{A^{\\delta}\\right\\}+\\varepsilon\\) for all \\(A \\in \\mathscr{S}\\).</p> <p>Proof Let \\(\\delta^{\\prime}&gt;\\delta\\) and insert \\(A=B^{\\delta^{\\prime} c}\\) into the premiss (here superscript \\(c\\) denotes complementation). This yields \\(G\\left(B^{\\delta^{\\prime} c \\delta c}\\right) \\leqslant F\\left(B^{\\delta^{\\prime}}\\right)+\\varepsilon\\). We now show that \\(B \\subset B^{\\delta^{\\prime} c \\delta c}\\), or, which is the same, \\(B^{\\delta^{\\prime} c \\delta} \\subset B^{c}\\). Assume \\(x \\in B^{\\delta^{\\prime} c \\delta}\\), then \\(\\exists y \\notin B^{\\delta^{\\prime}}\\) with \\(d(x, y) \\leqslant \\delta^{\\prime}\\); thus \\(x \\notin B\\), because otherwise \\(d(x, y)&gt;\\delta^{\\prime}\\). It follows that \\(G(B) \\leqslant F\\left(B^{\\delta^{\\prime}}\\right)+\\varepsilon\\). Since \\(B^{\\delta}=\\cap_{\\delta^{\\prime}&gt;\\delta} B^{\\delta^{\\prime}}\\), the assertion of the lemma follows.</p> <p>We now show that \\(d_{p}(F, G)=0\\) implies \\(F=G\\). Since \\(\\cap_{\\varepsilon&gt;0} A^{\\varepsilon}=\\bar{A}\\), it follows from \\(d_{p}(F, G)=0\\) that \\(F\\{A\\} \\leqslant G\\{A\\}\\) and \\(G\\{A\\} \\leqslant F\\{A\\}\\) for all closed sets \\(A\\); this implies \\(F=G\\) (remember that all our measures are regular). To prove the triangle inequality, assume \\(d_{p}(F, G) \\leqslant \\varepsilon\\) and \\(d_{p}(G, H) \\leqslant \\delta\\), then \\(F\\{A\\} \\leqslant G\\left\\{A^{\\varepsilon}\\right\\}+\\varepsilon \\leqslant H\\left\\{\\left(A^{\\varepsilon}\\right)^{\\delta}\\right\\}+\\varepsilon+\\delta\\). Thus it suffices to verify \\(\\left(A^{\\varepsilon}\\right)^{\\delta} \\subset A^{\\varepsilon+\\delta}\\), which is a simple consequence of the triangle inequality for \\(d\\).</p> <p>THEOREM 3.7 (Strassen) The following two statements are equivalent: (1) \\(F\\{A\\} \\leqslant G\\left\\{A^{\\delta}\\right\\}+\\varepsilon\\) for all \\(A \\in \\mathscr{S}\\). (2) There are (dependent) random variables \\(X\\) and \\(Y\\) with values in \\(\\Omega\\), such that \\(\\mathcal{L}(X)=F, \\mathcal{L}(Y)=G\\), and \\(P\\{d(X, Y) \\leqslant \\delta\\} \\geqslant 1-\\varepsilon\\).</p> <p>Proof As \\(\\{X \\in A\\} \\subset\\left\\{Y \\in A^{\\delta}\\right\\} \\cup\\{d(X, Y)&gt;\\delta\\}\\), (1) is an immediate consequence of (2). The proof of the converse is contained in a famous paper of Strassen [(1965), p. 436 ff\\(]\\).</p> <p>NOTE 1 In the above theorem we may put \\(\\delta=0\\). Then, since \\(F\\) and \\(G\\) are regular, (1) is equivalent to the assumption that the difference in total variation between \\(F\\) and \\(G\\) satisfies \\(d_{T V}(F, G)=\\sup _{A \\in \\mathscr{B}}|F(A)-G(A)|&lt;\\varepsilon\\). In this case Strassen's theorem implies that there are two random variables \\(X\\) and \\(Y\\) with marginal distributions \\(F\\) and \\(G\\), respectively, such that \\(P(X \\neq Y)\\) \\(\\leqslant \\varepsilon\\). However, the total variation distance does not metrize the weak topology.</p> <p>NOTE 2 If \\(G\\) is the idealized model and \\(F\\) is the true underlying distribution, such that \\(d_{p}(F, G)&lt;\\varepsilon\\), then Strassen's theorem shows that we can always assume that there is an ideal (but unobservable) random variable \\(Y\\) with \\(\\mathcal{E}(Y)=G\\), and an observable \\(X\\) with \\(\\mathcal{E}(X)=F\\), such that \\(P\\{d(X, Y)&lt;\\varepsilon\\}\\) \\(&gt;1-\\varepsilon\\), that is, the Prohorov distance provides both for small errors occurring with large probability, and for large errors occurring with low probability, in a very explicit, quantitative fashion.</p> <p>THEOREM 3.8 The Prohorov metric metrizes the weak topology in \\(\\mathfrak{R}\\). Proof Let \\(P \\in \\mathfrak{R}\\) be fixed. Then a basis for the neighborhood system of \\(P\\) in the weak topology is furnished by the sets of the form</p> \\[ \\left\\{Q \\in \\mathfrak{R} \\mid\\left|\\int \\varphi_{i} d Q-\\int \\varphi_{i} d P\\right|&lt;\\varepsilon, i=1, \\ldots, k\\right\\} \\] <p>where the \\(\\varphi_{i}\\) are bounded continuous functions. In view of Lemma 2.2 there are three other bases for this neighborhood system, namely: those furnished by the sets</p> \\[ \\left\\{Q \\in \\mathfrak{R} \\mid Q\\left(G_{i}\\right)&gt;P\\left(G_{i}\\right)-\\varepsilon, i=1, \\ldots, k\\right\\} \\] <p>where the \\(G_{i}\\) are open; those furnished by the sets</p> \\[ \\left\\{Q \\in \\mathfrak{R} \\mid Q\\left(A_{i}\\right)&lt;P\\left(A_{i}\\right)+\\varepsilon, i=1, \\ldots, k\\right\\} \\] <p>where the \\(A_{i}\\) are closed; and those furnished by the sets</p> \\[ \\left\\{Q \\in \\mathfrak{R} \\mid\\left|Q\\left(B_{i}\\right)-P\\left(B_{i}\\right)\\right|&lt;\\varepsilon, i=1, \\ldots, k\\right\\} \\] <p>where the \\(B_{i}\\) have \\(P\\)-null boundary. We first show that each neighborhood of the form (3.5) contains a Prohorov neighborhood. Assume that \\(P, \\varepsilon\\), and a closed set \\(A\\) are given. Clearly, we can find a \\(\\delta, 0&lt;\\delta&lt;\\varepsilon\\), such that \\(P\\left(A^{\\delta}\\right)&lt;P(A)+\\frac{1}{2} \\varepsilon\\). If \\(d_{p}(P, Q)\\) \\(&lt;\\frac{1}{2} \\delta\\), then</p> \\[ Q(A)&lt;P\\left(A^{\\delta}\\right)+\\frac{1}{2} \\delta&lt;P(A)+\\varepsilon \\] <p>It follows that (3.5) contains a Prohorov neighborhood. In order to show the converse, let \\(\\varepsilon&gt;0\\) be given. Choose \\(\\delta&lt;\\frac{1}{2} \\varepsilon\\). In view of Lemma 2.5 there is a finite union of sets \\(A_{i}\\) with diameter \\(&lt;\\delta\\) such that</p> \\[ P\\left(\\bigcup_{i=1}^{k} A_{i}\\right)&gt;1-\\delta \\] <p>We can choose the \\(A_{i}\\) to be disjoint and to have \\(P\\)-null boundaries. If \\(\\mathscr{Q}\\) is the (finite) class of unions of \\(A_{i}\\), then every element of \\(\\mathscr{Q}\\) has a \\(P\\)-null boundary. By (3.6) there is a weak neighborhood \\(\\mathscr{R}\\) of \\(P\\) such that</p> \\[ \\mathscr{R}=\\{Q \\| Q(B)-P(B) \\mid&lt;\\delta \\text { for } B \\in \\mathscr{Q}\\} \\] <p>We now show that \\(d_{P}(P, Q)&lt;\\varepsilon\\) if \\(Q \\in \\mathscr{R}\\). Let \\(B \\in \\mathscr{B}\\) be an arbitrary set, and let \\(A\\) be the union of the sets \\(A_{i}\\) that ints rsect \\(B\\). Then</p> \\[ B \\subset A \\cup\\left[\\bigcup_{1}^{k} A_{i}\\right]^{c} \\quad \\text { and } \\quad A \\subset B^{\\delta} \\] <p>and hence</p> \\[ P(B)&lt;P(A)+\\delta&lt;Q(A)+2 \\delta&lt;Q\\left(B^{\\delta}\\right)+2 \\delta \\] <p>The assertion follows. THEOREM 3.9 \\(\\mathscr{R}\\) is a Polish space. Proof It remains to show that \\(\\mathscr{R}\\) is separable and complete. We already noted (proof of Lemma 2.1) that the measures with finite support are dense in \\(\\mathscr{R}\\). Now let \\(\\Omega_{0} \\subset \\Omega\\) be a countable dense subset; then it is easy to see that already the countable set \\(\\mathscr{R}_{0}\\) is dense in \\(\\mathscr{R}\\), where \\(\\mathscr{R}_{0}\\) consists of the measures whose finite support is contained in \\(\\Omega_{0}\\) and that have rational masses. This establishes separability. Now let \\(\\left\\{P_{n}\\right\\}\\) be a Cauchy sequence in \\(\\mathscr{R}\\). Let \\(\\varepsilon&gt;0\\) be given, and chose \\(n_{0}\\) such that \\(d_{P}\\left(P_{n}, P_{m}\\right)&lt;\\varepsilon / 2\\) for \\(m, n \\geqslant n_{0}\\), that is \\(P_{m}(A) \\leqslant P_{n}\\left(A^{\\varepsilon / 2}\\right)+\\varepsilon / 2\\). The finite sequence \\(\\left\\{P_{m}\\right\\}_{m&lt;n_{0}}\\) is tight, so by Lemma 2.5 there is a finite union \\(B\\) of \\(\\varepsilon / 2\\)-balls such that \\(P_{m}(B) \\geqslant 1-\\varepsilon / 2\\) for \\(m \\leqslant n_{0}\\). But then \\(P_{n}\\left(B^{\\varepsilon / 2}\\right) \\geqslant P_{n}(B)-\\varepsilon / 2 \\geqslant 1-\\varepsilon\\), and, since \\(B^{\\varepsilon / 2}\\) is contained in a finite union of \\(\\varepsilon\\)-balls (with the same centers as the balls forming \\(B\\) ), we conclude from Lemma 2.5 that the sequence \\(\\left\\{P_{n}\\right\\}\\) is tight. Hence it has an accumulation point in \\(\\mathscr{R}\\) (which by necessity is unique).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#24-the-bounded-lipschitz-metric","title":"2.4 THE BOUNDED LIPSCHITZ METRIC","text":"<p>The weak topology can also be metrized by other metrics. An interesting one is the so-called bounded Lipschitz metric \\(d_{B L}\\). Assume that the distance function \\(d\\) in \\(\\Omega\\) is bounded by 1 {if necessary, replace it by \\(d(x, y) /[1+\\) \\(d(x, y)]\\}\\). Then define</p> \\[ d_{B L}(F, G)=\\sup \\left|\\int \\psi d F-\\int \\psi d G\\right| \\] <p>where the supremum is taken over all functions \\(\\psi\\) satisfying the Lipschitz condition</p> \\[ |\\psi(x)-\\psi(y)| \\leqslant d(x, y) \\] <p>LEMMA 4.1 \\(d_{B L}\\) is a metric. Proof The only nontrivial part is to show that \\(d_{B L}(F, G)=0\\) implies \\(F=G\\). Clearly, it implies \\(\\int \\psi d F=\\int \\psi d G\\) for all functions satisfying the Lipschitz condition \\(|\\psi(x)-\\psi(y)| \\leqslant c d(x, y)\\) for some \\(c\\). In particular, let \\(\\psi(x)=(1-c d(x, A))^{+}\\), with \\(d(x, A)=\\inf \\{d(x, y) \\mid y \\in A\\}\\); then \\(|\\psi(x)-\\) \\(\\psi(y)|&lt;c d(x, y)\\) and \\(1_{A} \\leqslant \\psi \\leqslant 1_{A^{1 / 2}}\\). Let \\(c \\rightarrow \\infty\\), then it follows that \\(F(A)=\\) \\(G(A)\\) for all closed sets \\(A\\); hence \\(F=G\\).</p> <p>Also for this metric an analogue of Strassen's theorem holds [first proved by Kantorovi\u010d and Rubinstein (1958) in a special case].</p> <p>THEOREM 4.2 The following two statements are equivalent: (1) \\(d_{B L}(F, G) \\leqslant \\varepsilon\\). (2) There are random variables \\(X\\) and \\(Y\\) with \\(\\bar{E}(X)=F\\), and \\(\\bar{E}(Y)=G\\), such that</p> \\[ E d(X, Y) \\leqslant \\varepsilon \\] <p>Proof \\((2) \\Rightarrow(1)\\) is trivial:</p> \\[ \\left|\\int \\psi d F-\\int \\psi d G\\right|=|E \\psi(X)-E \\psi(Y)| \\leqslant E|\\psi(X)-\\psi(Y)| \\leqslant E d(X, Y) \\] <p>To prove the reverse implication, we first assume that \\(\\Omega\\) is a finite set. Then the assertion is, essentially, a particular case of the Kuhn-Tucker (1951) theorem, but a proof from scratch may be more instructive. Assume that the elements of \\(\\Omega\\) are numbered from 1 to \\(n\\); then the probability measures \\(F\\) and \\(G\\) are represented by \\(n\\)-tuples \\(\\left(f_{1}, \\ldots, f_{n}\\right)\\) and \\(\\left(g_{1}, \\ldots, g_{n}\\right)\\) of real numbers, and we are looking for a probability on \\(\\Omega \\times \\Omega\\), represented by a matrix \\(u_{i j}\\). Thus we attempt to minimize</p> \\[ E d(X, Y)=\\sum d_{i j} u_{i j} \\] <p>under the side conditions</p> \\[ \\begin{gathered} u_{i j} \\geqslant 0 \\\\ \\sum_{i} u_{i j}=g_{j} \\\\ \\sum_{j} u_{i j}=f_{i} \\end{gathered} \\] <p>where \\(d_{i j}\\) satisfies</p> \\[ \\begin{aligned} &amp; d_{i j} \\geqslant 0, \\quad d_{i i}=0 \\\\ &amp; d_{i j}=d_{j i} \\\\ &amp; d_{i k} \\leqslant d_{i j}+d_{j k} \\end{aligned} \\] <p>There exist matrices \\(u_{i j}\\) satisfying the side conditions, for example \\(u_{i j}=f_{i} g_{j}\\), and it follows from a simple compactness argument that there is a solution to our minimum problem. With the aid of Lagrange multipliers \\(\\lambda_{i}\\) and \\(\\mu_{j}\\), it can be turned into an unconstrained minimum problem: minimize</p> \\[ \\sum\\left(d_{i j}-\\lambda_{i}-\\mu_{j}\\right) u_{i j} \\] <p>on the orthant</p> \\[ u_{i j} \\geqslant 0 \\] <p>At the minimum (which we know to exist) we must have the following implications:</p> \\[ \\begin{array}{lll} u_{i j}&gt;0 &amp; \\Rightarrow &amp; d_{i j}=\\lambda_{i}+\\mu_{j} \\\\ u_{i j}=0 &amp; \\Rightarrow &amp; d_{i j} \\geqslant \\lambda_{i}+\\mu_{j} \\end{array} \\] <p>because otherwise (4.6) could be decreased through a suitable small change in some of the \\(u_{i j}\\). We note that (4.4), (4.7), and (4.8) imply that the minimum value \\(\\eta\\) of (4.3) satisfies</p> \\[ \\eta=\\sum d_{i j} u_{i j}=\\sum\\left(\\lambda_{i}+\\mu_{j}\\right) u_{i j}=\\sum \\lambda_{i} f_{i}+\\sum \\mu_{i} g_{i} \\] <p>Assume for the moment that \\(\\mu_{i}=-\\lambda_{i}\\) for all \\(i\\) [this would follow from (4.7) if \\(u_{i i}&gt;0\\) for all \\(i\\) ]. Then (4.7) and (4.8) show that \\(\\lambda\\) satisfies the Lipschitz condition \\(\\left|\\lambda_{i}-\\lambda_{j}\\right| \\leqslant d_{i j}\\), and (4.9) now gives \\(\\eta \\leqslant \\varepsilon\\); thus assertion (2) of the theorem holds.</p> <p>In order to establish \\(\\mu_{i}=-\\lambda_{i}\\), for a fixed \\(i\\), assume first that both \\(f_{i}&gt;0\\) and \\(g_{i}&gt;0\\). Then both the \\(i\\) th row and the \\(i\\) th column of the matrix \\(\\left\\{u_{i j}\\right\\}\\) must contain a strictly positive element. If \\(u_{i i}&gt;0\\), then (4.7) implies \\(\\lambda_{i}+\\mu_{i}=\\) \\(d_{i i}=0\\), and we are finished. If \\(u_{i i}=0\\), then there must be a \\(u_{i i}&gt;0\\) and a \\(u_{k i}&gt;\\) 0 . Therefore</p> \\[ \\begin{aligned} &amp; \\lambda_{i}+\\mu_{j}=d_{i j} \\\\ &amp; \\lambda_{k}+\\mu_{i}=d_{k i} \\end{aligned} \\] <p>and the triangle inequality gives</p> \\[ \\lambda_{k}+\\mu_{j} \\leqslant d_{k j} \\leqslant d_{k i}+d_{i j}=\\lambda_{k}+\\mu_{i}+\\lambda_{i}+\\mu_{j} \\] <p>hence</p> \\[ 0 \\leqslant \\mu_{i}+\\lambda_{i} \\leqslant d_{i i}=0 \\] <p>and thus \\(\\lambda_{i}+\\mu_{i}=0\\). In the case \\(f_{i}=g_{i}=0\\) there is nothing to prove (we may drop the \\(i\\) th point from consideration).</p> <p>The most troublesome case is when just one of \\(f_{i}\\) and \\(g_{i}\\) is 0 , say \\(f_{i}&gt;0\\) and \\(g_{i}=0\\). Then \\(u_{k i}=0\\) for all \\(k\\), and \\(\\lambda_{k}+\\mu_{i} \\leqslant d_{k i}\\), but \\(\\mu_{i}\\) is not uniquely determined in general; in particular, note that its coefficient in (4.9) then is 0 . So we increase \\(\\mu_{i}\\) until, for the first time, \\(\\lambda_{k}+\\mu_{i}=d_{k i}\\) for some \\(k\\). If \\(k=i\\), we are finished. If not, there must be some \\(j\\) for which \\(u_{i j}&gt;0\\) since \\(f_{i}&gt;0\\); thus \\(\\lambda_{i}+\\mu_{j}=d_{i j}\\), and we can repeat the argument with the triangle inequality from before.</p> <p>This proves the theorem for finite sets \\(\\Omega\\). We now show that it holds whenever the support of \\(F\\) and \\(G\\) is finite, say \\(\\left\\{x_{1}, \\ldots, x_{n}\\right\\}\\). In order to do this, it suffices to show that any function \\(\\psi\\) defined on the set \\(\\left\\{x_{1}, \\ldots, x_{n}\\right\\}\\) and satisfying the Lipschitz condition \\(\\left|\\psi\\left(x_{i}\\right)-\\psi\\left(x_{j}\\right)\\right| \\leqslant d\\left(x_{i}, x_{j}\\right)\\) can be extended to a function satisfying the Lipschitz condition everywhere in \\(\\Omega\\).</p> <p>Let \\(x_{1}, x_{2}, \\ldots\\) be a dense sequence in \\(\\Omega\\), and assume inductively that \\(\\psi\\) is defined and satisfies the Lipschitz condition on \\(\\left\\{x_{1}, \\ldots, x_{n}\\right\\}\\). Then \\(\\psi\\) will satisfy it on \\(\\left\\{x_{1}, \\ldots, x_{n+1}\\right\\}\\) iff \\(\\psi\\left(x_{n+1}\\right)\\) can be defined such that</p> \\[ \\psi\\left(x_{n+1}\\right) \\in\\left\\{\\max _{1 \\leqslant i \\leqslant n}\\left[\\psi\\left(x_{i}\\right)-d\\left(x_{i}, x_{n+1}\\right)\\right], \\min _{1 \\leqslant i \\leqslant n}\\left[\\psi\\left(x_{i}\\right)+d\\left(x_{i}, x_{n+1}\\right)\\right]\\right\\} \\] <p>It suffices to show that the interval in question is not empty, that is, for all \\(i, j \\leqslant n\\),</p> \\[ \\psi\\left(x_{i}\\right)-d\\left(x_{i}, x_{n+1}\\right) \\leqslant \\psi\\left(x_{j}\\right)+d\\left(x_{j}, x_{n+1}\\right) \\] <p>or equivalently,</p> \\[ \\psi\\left(x_{i}\\right)-\\psi\\left(x_{j}\\right) \\leqslant d\\left(x_{i}, x_{n+1}\\right)+d\\left(x_{j}, x_{n+1}\\right) \\] <p>and this is obviously true in view of the triangle inequality.</p> <p>Thus it is possible to extend the definition of \\(\\psi\\) to a dense set, and from there, by uniform continuity, to the whole of \\(\\Omega\\).</p> <p>For general measures \\(F\\) and \\(G\\), the theorem now follows from a straightforward passage to the limit, as follows.</p> <p>First, we show that, for every \\(\\delta&gt;0\\) and every \\(F\\), there is a measure \\(F^{*}\\) with finite support such that \\(d_{B L}\\left(F, F^{*}\\right)&lt;\\delta\\). In order to see this, find first a compact \\(K \\subset \\Omega\\) such that \\(F(K)&gt;1-\\delta / 2\\), cover \\(K\\) by a finite number of disjoint sets \\(U_{1}, \\ldots, U_{n}\\) with diameter \\(&lt;\\delta / 2\\), put \\(U_{0}=K^{c}\\), and select points \\(x_{i} \\in U_{i}, i=0, \\ldots, n\\). Define \\(F^{*}\\) with support \\(\\left\\{x_{0}, \\ldots, x_{n}\\right\\}\\) by \\(F^{*}\\left\\{x_{i}\\right\\}=F\\left\\{U_{i}\\right\\}\\). Then, for any \\(\\psi\\) satisfying the Lipschitz condition, we have</p> \\[ \\begin{aligned} \\left|\\int \\psi d F^{*}-\\int \\psi d F\\right| &amp; \\leqslant \\sum\\left[\\max _{U_{i}} \\psi(x)-\\min _{U_{i}} \\psi(x)\\right] F^{*}\\left\\{x_{i}\\right\\} \\\\ &amp; \\leqslant F^{*}\\left\\{x_{0}\\right\\}+\\sum_{i&gt;0} \\frac{\\delta}{2} F^{*}\\left\\{x_{i}\\right\\}&lt;\\delta \\end{aligned} \\] <p>Thus we can approximate \\(F\\) and \\(G\\) by \\(F^{*}\\) and \\(G^{*}\\), respectively, such that the starred measures have finite support, and</p> \\[ d_{B L}\\left(F^{*}, G^{*}\\right)&lt;\\varepsilon+2 \\delta \\] <p>Then find a measure \\(P^{*}\\) on \\(\\Omega \\times \\Omega\\) with marginals \\(F^{*}\\) and \\(G^{*}\\) such that</p> \\[ \\int d(X, Y) d P^{*}&lt;\\varepsilon+2 \\delta \\] <p>If we take a sequence of \\(\\delta\\) values converging to 0 , then the corresponding sequence \\(P^{*}\\) clearly is tight in the space of probability measures on \\(\\Omega \\times \\Omega\\), and the marginals converge weakly to \\(F\\) and \\(G\\), respectively. Hence there is a weakly convergent subsequence of the \\(P^{*}\\), whose limit \\(P\\) then satisfies (2). This terminates the proof of the theorem.</p> <p>COROLLARY 4.3 For all \\(F, G \\in \\mathscr{R}\\), we have</p> \\[ d_{P}(F, G)^{2} \\leqslant d_{B L}(F, G) \\leqslant 2 d_{P}(F, G) \\] <p>In particular, \\(d_{P}\\) and \\(d_{B L}\\) define the same topology. Proof For any probability measure \\(P\\) on \\(\\Omega \\times \\Omega\\), we have</p> \\[ \\begin{aligned} \\int d(X, Y) d P &amp; \\leqslant \\varepsilon P\\{d(X, Y) \\leqslant \\varepsilon\\}+P\\{d(X, Y)&gt;\\varepsilon\\} \\\\ &amp; =\\varepsilon+(1-\\varepsilon) P\\{d(X, Y)&gt;\\varepsilon\\} \\end{aligned} \\] <p>If \\(d_{P}(F, G)&lt;\\varepsilon\\), we can (by Theorem 3.8) choose \\(P\\) so that this is bounded by \\(\\leqslant \\varepsilon+(1-\\varepsilon) \\varepsilon \\leqslant 2 \\varepsilon\\), which establishes \\(d_{B L} \\leqslant 2 d_{P}\\). On the other hand Markov's inequality gives</p> \\[ P\\{(d(X, Y)&gt;\\varepsilon\\} \\leqslant \\frac{\\int d(X, Y) d P}{\\varepsilon}&lt;\\varepsilon \\] <p>if \\(d_{B L}(F, G) \\leqslant \\varepsilon^{2}\\); thus \\(d_{P}^{2}&lt;d_{B L}\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#some-further-inequalities","title":"Some Further Inequalities","text":"<p>The total variation distance</p> \\[ d_{T V}(F, G)=\\sup _{A \\in \\mathscr{B}}|F\\{A\\}-G\\{A\\}| \\] <p>and, on the real line, the Kolmogorov distance</p> \\[ d_{K}(F, G)=\\sup |F(x)-G(x)| \\] <p>do not generate the weak topology, but they possess other convenient properties. In particular, we have the inequalities</p> \\[ \\begin{aligned} &amp; d_{L} \\leqslant d_{P} \\leqslant d_{T V} \\\\ &amp; d_{L} \\leqslant d_{K} \\leqslant d_{T V} \\end{aligned} \\] <p>Proof \\((4.13,4.14)\\) The defining equation for the Prohorov distance</p> \\[ d_{P}(F, G)=\\inf \\{\\varepsilon \\mid \\forall A \\in \\mathscr{B}, F\\{A\\} \\leqslant G\\left\\{A^{\\varepsilon}\\right\\}+\\varepsilon\\} \\] <p>is turned into a definition of the L\u00e9vy distance if we decrease the range of conditions to sets \\(A\\) of the form \\((-\\infty, x]\\) and \\([x, \\infty)\\). It is turned into a definition of the total variation distance if we replace \\(A^{\\varepsilon}\\) by \\(A\\) and thus make the condition harder to fulfill. This again can be converted into a definition of Kolmogorov distance if we restrict the range of \\(A\\) to sets \\((-\\infty, x]\\) and \\([x, \\infty)\\). Finally, if we increase \\(A\\) on the right-hand side of the inequality in (4.15) and replace it by \\(A^{\\varepsilon}\\), we decrease the infimum and obtain the Levy distance.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#25-frechet-and-gateaux-derivatives","title":"2.5 FR\u00c9CHET AND G\u00c2TEAUX DERIVATIVES","text":"<p>Assume that \\(d_{*}\\) is a metric [or pseudo-metric-we shall not actually need \\(\\left.d_{*}(F, G)=0 \\Rightarrow F=G\\right]\\), in the space \\(\\mathscr{M}\\) of probability measures, that: (1) Is compatible with the weak topology in the sense that \\(\\left\\{F \\mid d_{*}(G, F)&lt;\\varepsilon\\right\\}\\) is open for all \\(\\varepsilon&gt;0\\).</p> <p>(2) Is compatible with the affine structure of \\(\\mathscr{R}\\) : let \\(F_{t}=(1-t) F_{0}+t F_{1}\\), then \\(d_{*}\\left(F_{t}, F_{s}\\right)=O(|t-s|)\\). The \"usual\" distance functions metrizing the weak topology of course satisfy the first condition; they also satisfy the second, but this has to be checked in each case.</p> <p>In the case of the L\u00e9vy metric we note that</p> \\[ \\left|F_{t}(x)-F_{s}(x)\\right|=|t-s|\\left|F_{1}(x)-F_{0}(x)\\right| \\leqslant|t-s| \\] <p>hence \\(d_{K}\\left(F_{t}, F_{s}\\right) \\leqslant|t-s|\\) and, a fortiori,</p> \\[ d_{L}\\left(F_{t}, F_{s}\\right) \\leqslant|t-s| \\] <p>In the case of the Prohorov metric, we have, similarly,</p> \\[ \\left|F_{t}(A)-F_{s}(A)\\right|=|t-s| \\cdot\\left|F_{1}(A)-F_{0}(A)\\right| \\leqslant|t-s| \\] <p>hence</p> \\[ d_{P}\\left(F_{t}, F_{s}\\right) \\leqslant|t-s| \\] <p>In the case of the bounded Lipschitz metric, we have, for any \\(\\psi\\) satisfying the Lipschitz condition,</p> \\[ \\left|\\int \\psi d F_{t}-\\int \\psi d F_{s}\\right|=|t-s| \\cdot\\left|\\int \\psi d F_{1}-\\int \\psi d F_{0}\\right| \\] <p>Let \\(\\bar{\\psi}=\\sup \\psi(x)\\), and \\(\\psi=\\inf \\psi(x)\\), then \\(\\bar{\\psi}-\\psi \\leqslant \\sup d(x, y) \\leqslant 1\\); thus \\(\\int \\psi d F_{1}\\) \\(-\\int \\psi d F_{0} \\leqslant \\int \\bar{\\psi} d F_{1}-\\int \\bar{\\psi} d F_{0} \\leqslant 1\\). It follows that \\(d_{B L}\\left(F_{t}, F_{s}\\right) \\leqslant|t-s|\\). We say that a statistical functional \\(T\\) is Fr\u00e9chet differentiable at \\(F\\) if it can be approximated by a linear functional \\(L\\) (defined on the space of finite signed measures) such that, for all \\(G\\),</p> \\[ |T(G)-T(F)-L(G-F)|=o\\left[d_{*}(F, G)\\right] \\] <p>Of course \\(L=L_{F}\\) depends on the base point \\(F\\). It is easy to see that \\(L\\) is (essentially) unique: if \\(L_{1}\\) and \\(L_{2}\\) are two such linear functionals, then their difference satisfies</p> \\[ \\left|\\left(L_{1}-L_{2}\\right)(G-F)\\right|=o\\left[d_{*}(F, G)\\right] \\] <p>and, in particular, with \\(F_{t}=(1-t) F+t G\\), we obtain</p> \\[ \\begin{aligned} \\left|\\left(L_{1}-L_{2}\\right)\\left(F_{t}-F\\right)\\right| &amp; =t\\left|\\left(L_{1}-L_{2}\\right)(G-F)\\right| \\\\ &amp; =o\\left(d_{*}\\left(F, F_{t}\\right)\\right)=o(t) \\end{aligned} \\] <p>hence \\(L_{1}(G-F)=L_{2}(G-F)\\) for all \\(G\\). It follows that \\(L\\) is uniquely determined on the space of finite signed measures of total mass 0 , and we may arbitrarily standardize it by putting \\(L(F)=0\\).</p> <p>If \\(T\\) were defined not just on some convex set, but in a full, open neighborhood of \\(F\\) in some linear space, weak continuity of \\(T\\) at \\(F\\) together with (5.1) would imply that \\(L\\) is continuous in \\(G\\) at \\(G=F\\), and, since \\(L\\) is linear, it then would follow that \\(L\\) is continuous everywhere.</p> <p>Unfortunately, this is not so, and thus a somewhat more roundabout approach appears necessary.</p> <p>We note first that, if we define</p> \\[ \\psi(x)=L\\left(\\delta_{x}-F\\right) \\] <p>then, by linearity of \\(L\\),</p> \\[ L(G-F)=\\int \\psi d G \\] <p>for all \\(G\\) with finite support. In particular, with \\(F_{t}=(1-t) F+t G\\), we then obtain</p> \\[ \\begin{aligned} \\left|T\\left(F_{t}\\right)-T(F)-L\\left(F_{t}-F\\right)\\right| &amp; =\\left|T\\left(F_{t}\\right)-T(F)-t L(G-F)\\right| \\\\ &amp; =\\left|T\\left(F_{t}\\right)-T(F)-t \\int \\psi d G\\right| \\\\ &amp; =o\\left(d_{*}\\left(F, F_{t}\\right)\\right)=o(t) \\end{aligned} \\] <p>Assume that \\(T\\) is continuous at \\(F\\); then \\(d_{*}\\left(F, F_{t}\\right)=O(t)\\) implies that \\(\\left|T\\left(F_{t}\\right)-T(F)\\right|=o(1)\\) uniformly in \\(G\\). A comparison with the preceding formula (5.4) yields that \\(\\psi\\) must be bounded.</p> <p>We may rewrite (5.4) as</p> \\[ \\left|\\int \\psi d G-\\frac{T\\left(F_{t}\\right)-T(F)}{t}\\right|=o(1) \\] <p>holding uniformly in \\(G\\). Now, if \\(T\\) is continuous in a neighborhood of \\(F\\), and if \\(G_{n} \\rightarrow G\\) weakly, then</p> \\[ F_{n, t}=(1-t) F+t G_{n} \\rightarrow F_{t} \\quad \\text { weakly } \\] <p>Since \\(t\\) can be chosen arbitrarily small, we must have \\(\\int \\psi d G_{n} \\rightarrow \\int \\psi d G\\). In particular, by letting \\(G_{n}=\\delta_{x_{n}}\\), with \\(x_{n} \\rightarrow x\\), we obtain that \\(\\psi\\) must be continuous. If \\(G\\) is an arbitrary probability measure, and the \\(G_{n}\\) are approximations to \\(G\\) with finite support, then the same argument shows that \\(\\int \\psi d G_{n}\\) converges simultaneously to \\(\\int \\psi d G\\) (since \\(\\psi\\) is bounded and continuous) and to \\(L(G-F)\\); hence \\(L(G-F)=\\int \\psi d G_{n}\\) holds for all \\(G \\in \\mathscr{M}\\). Thus we have proved the following proposition.</p> <p>PROPOSITION 5.1 If \\(T\\) is weakly continuous in a neighborhood of \\(F\\) and Fr\u00e9chet differentiable at \\(F\\), then its Fr\u00e9chet derivative at \\(F\\) is a weakly continuous linear functional, and it is representable as</p> \\[ L(G-F)=\\int \\psi_{F} d G \\] <p>with \\(\\psi_{F}\\) bounded and continuous, and \\(\\int \\psi_{F} d F=0\\). Unfortunately the concept of Fr\u00e9chet differentiability appears to be too strong: in too many cases, the Fr\u00e9chet derivative does not exist, and even if it does, the fact is difficult to establish.</p> <p>About the weakest concept of differentiability is the G\u00e2teaux derivative [in the statistical literature it has usually been called the Volterra derivative, but this happens to be a misnomer, cf. Reeds (1976)]. We say that a functional \\(T\\) is G\u00e2teaux differentiable at \\(F\\) if there is a linear functional \\(L=L_{F}\\) such that, for all \\(G \\in \\mathscr{M}\\),</p> \\[ \\lim _{t \\rightarrow 0} \\frac{T\\left(F_{t}\\right)-T(F)}{t}=L_{F}(G-F) \\] <p>with</p> \\[ F_{t}=(1-t) F+t G \\] <p>Clearly, if \\(T\\) is Fr\u00e9chet differentiable, it is also G\u00e2teaux differentiable, and the two derivatives \\(L_{F}\\) agree. We usually assume in addition that the G\u00e2teaux derivative \\(L_{F}\\) is representable by a measurable function \\(\\psi_{F}\\), conveniently standardized such that \\(\\int \\psi_{F} d F=0\\) :</p> \\[ L_{F}(G-F)=\\int \\psi_{F} d G \\] <p>[Note that there are discontinuous linear functionals that cannot be represented as integrals with respect to measurable function \\(\\psi\\), e.g., \\(L(F)\\) \\(=\\) sum of the jumps \\(F(x+0)-F(x-0)\\) of the distribution function \\(F\\).]</p> <p>If we put \\(G=\\delta_{x}\\), (5.6) gives the value of \\(\\psi_{F}(x)\\); following Hampel (1968, 1974b) we write</p> \\[ I C(x ; F, T)=\\lim _{t \\rightarrow 0} \\frac{T\\left(F_{t}\\right)-T(F)}{t} \\] <p>where \\(F_{t}=(1-t) F+t \\delta_{x}\\) and \\(I C\\) stands for influence curve. The G\u00e2teaux derivative is, after all, nothing but the ordinary derivative of the real valued function \\(T\\left(F_{t}\\right)\\) with respect to the real parameter \\(t\\). If we integrate the derivative of an absolutely continuous function, we get back the function; in this particular case we obtain the useful identity</p> \\[ T\\left(F_{1}\\right)-T\\left(F_{0}\\right)=\\int_{0}^{1} \\int I C\\left(x ; F_{t}, T\\right) d\\left(F_{1}-F_{0}\\right) d t \\] <p>Proof We have</p> \\[ T\\left(F_{1}\\right)-T\\left(F_{0}\\right)=\\int_{0}^{1} \\frac{d}{d t} T\\left(F_{t}\\right) d t \\] <p>Now</p> \\[ \\frac{d}{d t} T\\left(F_{t}\\right)=\\lim _{h \\rightarrow 0} \\frac{T\\left(F_{t+h}\\right)-T\\left(F_{t}\\right)}{h} \\] <p>and, since</p> \\[ F_{t+h}=\\left(1-\\frac{h}{1-t}\\right) F_{t}+\\frac{h}{1-t} F_{1} \\] <p>we obtain, provided the G\u00e2teaux derivative exists at \\(F_{t}\\),</p> \\[ \\begin{aligned} \\frac{d}{d t} T\\left(F_{t}\\right) &amp; =\\frac{1}{1-t} \\int I C\\left(x ; F_{t}, T\\right) d\\left(F_{1}-F_{t}\\right) \\\\ &amp; =\\int I C\\left(x ; F_{t}, T\\right) d\\left(F_{1}-F_{0}\\right) \\end{aligned} \\] <p>If the empirical distribution \\(F_{n}\\) converges to the true one at the rate \\(n^{-1 / 2}\\),</p> \\[ d_{*}\\left(F, F_{n}\\right)=O_{p}\\left(n^{-1 / 2}\\right) \\] <p>and if \\(T\\) has a Fr\u00e9chet derivative at \\(F\\), (5.1) and (5.5) allow a one-line asymptotic normality proof</p> \\[ \\begin{aligned} \\sqrt{n}\\left[T\\left(F_{n}\\right)-T(F)\\right] &amp; =\\sqrt{n} \\int \\psi_{F} d F_{n}+\\sqrt{n} o\\left[d_{*}\\left(F, F_{n}\\right)\\right] \\\\ &amp; =\\frac{1}{\\sqrt{n}} \\sum \\psi_{F}\\left(x_{i}\\right)+o_{F}(1) \\end{aligned} \\] <p>hence the left-hand side is asymptotically normal with mean 0 and variance \\(\\int \\psi_{F}^{2} d F\\).</p> <p>For the L\u00e9vy distance, (5.9) is true; this follows at once from (4.14) and the well-known properties of the Kolmogorov-Smirnov test statistic. Unfortunately (5.9) is false for both the Prohorov and the bounded Lipschitz distance, as soon as \\(F\\) has sufficiently long tails [rational tails \\(F(|X|&gt;t) \\sim\\) \\(t^{-k}\\) for some \\(k\\), suffice for (5.9) to fail].</p> <p>Proof The idea behind the proof is that, for long-tailed distributions \\(F\\), the extreme order statistics are widely scattered, so, if we surrounded them by \\(\\varepsilon\\)-neighborhoods, we catch very little of the mass of \\(F\\). To be specific assume that \\(F(x)=|x|^{-k}\\) for large negative \\(x\\), let \\(\\delta\\) be a small positive number to be fixed later, let \\(m=n^{1 / 2+\\delta}\\), and let \\(A=\\left\\{x_{(1)}, \\ldots, x_{(m)}\\right\\}\\) be the set of the \\(m\\) leftmost order statistics. Put \\(\\varepsilon=\\frac{1}{2} n^{-1 / 2+\\delta}\\). We intend to show that, for large \\(n\\),</p> \\[ F_{n}(A)-\\varepsilon \\geqslant F\\left\\{A^{\\varepsilon}\\right\\} \\] <p>hence \\(d_{F}\\left(F, F_{n}\\right) \\geqslant \\varepsilon\\). We have \\(F_{n}(A)=m / n=2 \\varepsilon\\), so it suffices to show \\(F\\left(A^{\\varepsilon}\\right) \\leqslant \\varepsilon\\). We only sketch the calculations.</p> <p>We have, approximately,</p> \\[ F\\left(A^{\\varepsilon}\\right) \\lesssim \\sum_{1}^{m} 2 \\varepsilon f\\left(x_{(i)}\\right) \\] <p>where \\(f\\) is the density of \\(F\\). Now \\(x_{(i)}\\) can be represented as \\(F^{-1}\\left(u_{(i)}\\right)\\), where \\(u_{(i)}\\) is the \\(i\\) th order statistic from the uniform distribution on \\((0,1)\\), and \\(f\\left(F^{-1}(t)\\right)=k t^{(k+1) / k}\\). Thus</p> \\[ F\\left(A^{\\varepsilon}\\right) \\lesssim 2 \\varepsilon k \\sum_{1}^{m}\\left(u_{(i)}\\right)^{(k+1) / k} \\] <p>Since \\(u_{(i)} \\approx i /(n+1)\\), we can approximate the right-hand side</p> \\[ \\begin{aligned} &amp; \\approx 2 \\varepsilon k \\sum_{1}^{m}\\left(\\frac{i}{n+1}\\right)^{(k+1) / k} \\\\ &amp; \\approx 2 \\varepsilon k n \\int_{0}^{m / n} t^{(k+1) / k} d t \\\\ &amp; =\\frac{2 \\varepsilon k n}{2+1 / k}\\left(\\frac{m}{n}\\right)^{2+1 / k} \\\\ &amp; =\\frac{2 k}{2+1 / k} \\frac{1}{2} n^{-1 / 2+\\delta} n\\left(n^{-1 / 2+\\delta}\\right)^{2+1 / k} \\\\ &amp; =\\frac{k}{2+1 / k} n^{-(1 / 2)-(1 / 2 k)+[3+(1 / k)] \\delta} \\end{aligned} \\] <p>If we choose \\(\\delta\\) sufficiently small, this is of a smaller order of magnitude than \\(\\varepsilon\\).</p> <p>Compare also Kersting (1978). On the other hand (5.9) is true for \\(d_{P}\\) and \\(d_{B L}\\) if \\(F\\) is the uniform distribution on a finite interval, but it fails again for the uniform distribution on the unit cube in three or more dimensions [see Dudley (1969) for details].</p> <p>It seems we are in trouble here because of a phenomenon that has been colorfully called the \"curse of dimensionality\"; the higher the dimension, the more empty space there is, and it becomes progressively more difficult to relate the coarse and sparse empirical measure to the true one.</p> <p>Mere G\u00e2teaux differentiability does not suffice to establish asymptotic normality [unless we also have higher order derivatives, cf. von Mises (1937, 1947), who introduced the concept of differentiable statistical functionals, and Filippova (1962)]. The most promising intermediate approach seems to be the one by Reeds (1976), which is based on the notion of compact differentiability (Averbukh and Smolyanov 1967, 1968).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#26-hampels-theorem","title":"2.6 HAMPEL'S THEOREM","text":"<p>We recall Hampel's definition of qualitative asymptotic robustness (cf. Section 1.3).</p> <p>Let the observations \\(x_{i}\\) be independent, with common distribution \\(F\\), and let \\(T_{n}=T_{n}\\left(x_{1}, \\ldots, x_{n}\\right)\\) be a sequence of estimates or test statistics with</p> <p>values in \\(\\mathbb{R}^{k}\\). This sequence is called robust at \\(F=F_{0}\\) if the sequence of maps of distributions</p> \\[ F \\rightarrow \\hat{\\mathcal{C}}_{F}\\left(T_{n}\\right) \\] <p>is equicontinuous at \\(F_{0}\\), that is, if, for every \\(\\varepsilon&gt;0\\), there is a \\(\\delta&gt;0\\) and an \\(n_{0}\\) such that, for all \\(F\\) and all \\(n \\geqslant n_{0}\\),</p> \\[ d_{*}\\left(F_{0}, F\\right)&lt;\\delta \\Rightarrow d_{*}\\left(\\hat{\\mathcal{C}}_{F_{0}}\\left(T_{n}\\right), \\hat{\\mathcal{C}}_{F}\\left(T_{n}\\right)\\right)&lt;\\varepsilon \\] <p>Here, \\(d_{*}\\) is any metric generating the weak topology. It is by no means clear whether different metrics give rise to equivalent robustness notions; to be specific we work the L\u00e9vy metric for \\(F\\) and the Prohorov metric for \\(\\mathcal{E}\\left(T_{n}\\right)\\).</p> <p>Assume that \\(T_{n}=T\\left(F_{n}\\right)\\) derives from a functional \\(T\\), which is defined on some weakly open subset of \\(\\mathfrak{M}\\).</p> <p>PROPOSITION 6.1 If \\(T\\) is weakly continuous at \\(F\\), then \\(\\left\\{T_{n}\\right\\}\\) is consistent at \\(F\\), in the sense that \\(T_{n} \\rightarrow T(F)\\) in probability and almost surely. Proof It follows from the Glivenko-Cantelli theorem and (4.14) that, in probability and almost surely,</p> \\[ d_{L}\\left(F, F_{n}\\right) \\leqslant d_{K}\\left(F, F_{n}\\right) \\rightarrow 0 \\] <p>hence \\(F_{n} \\rightarrow F\\) weakly, and thus \\(T\\left(F_{n}\\right) \\rightarrow T(F)\\). The following is a variant of somewhat more general results first proved by Hampel (1971).</p> <p>THEOREM 6.2 Assume that \\(\\left\\{T_{n}\\right\\}\\) is consistent in a neighborhood of \\(F_{0}\\). Then \\(T\\) is continuous at \\(F_{0}\\) iff \\(\\left\\{T_{n}\\right\\}\\) is robust at \\(F_{0}\\).</p> <p>Proof Assume first that \\(T\\) is continuous at \\(F_{0}\\). We can write</p> \\[ d_{P}\\left(\\hat{\\mathcal{C}}_{F_{0}}\\left(T_{n}\\right), \\hat{\\mathcal{C}}_{F}\\left(T_{n}\\right)\\right) \\leqslant d_{P}\\left(\\delta_{T\\left(F_{0}\\right)}, \\hat{\\mathcal{C}}_{F_{0}}\\left(T_{n}\\right)\\right)+d_{P}\\left(\\delta_{T\\left(F_{0}\\right)}, \\hat{\\mathcal{C}}_{F}\\left(T_{n}\\right)\\right) \\] <p>where \\(\\delta_{T\\left(F_{0}\\right)}\\) denotes the degenerate law concentrated at \\(T\\left(F_{0}\\right)\\). Thus robustness at \\(F_{0}\\) is proved if we can show that, for each \\(\\varepsilon&gt;0\\), there is a \\(\\delta&gt;0\\) and an \\(n_{0}\\), such that \\(d_{L}\\left(F_{0}, F\\right) \\leqslant \\delta\\) implies</p> \\[ d_{P}\\left(\\delta_{T\\left(F_{0}\\right)}, \\hat{\\mathcal{C}}_{F}\\left(T\\left(F_{n}\\right)\\right)\\right) \\leqslant \\frac{1}{2} \\varepsilon, \\quad \\text { for } n \\geqslant n_{0} \\] <p>It follows from the easy part of Strassen's theorem (Theorem 3.7) that this</p> <p>last inequality holds if we can show</p> \\[ P_{F}\\left\\{d\\left(T\\left(F_{0}\\right), T\\left(F_{n}\\right)\\right)&lt;\\frac{1}{2} \\varepsilon\\right\\} \\geqslant 1-\\frac{1}{2} \\varepsilon \\] <p>But, since \\(T\\) is continuous at \\(F_{0}\\), there is a \\(\\delta&gt;0\\) such that \\(d_{L}\\left(F_{0}, F\\right) \\leqslant 2 \\delta\\) implies \\(d\\left(T\\left(F_{0}\\right), T(F)\\right) \\leqslant \\frac{1}{2} \\varepsilon\\), so it suffices to show</p> \\[ P_{F}\\left\\{d_{L}\\left(F_{0}, F_{n}\\right) \\leqslant 2 \\delta\\right\\} \\geqslant 1-\\frac{1}{2} \\varepsilon \\] <p>We note that Glivenko-Cantelli convergence is uniform in \\(F\\) : for each \\(\\delta&gt;0\\) and \\(\\varepsilon&gt;0\\), there is an \\(n_{0}\\) such that, for all \\(F\\) and all \\(n \\geqslant n_{0}\\),</p> \\[ P_{F}\\left\\{d_{L}\\left(F, F_{n}\\right) \\leqslant \\delta\\right\\} \\geqslant 1-\\frac{1}{2} \\varepsilon \\] <p>But, since \\(d_{*}\\left(F_{0}, F_{n}\\right) \\leqslant d_{*}\\left(F_{0}, F\\right)+d_{*}\\left(F, F_{n}\\right)\\), we have established robustness at \\(F_{0}\\).</p> <p>Conversely, assume that \\(\\left\\{T_{n}\\right\\}\\) is robust at \\(F_{0}\\). We note that, for degenerate laws \\(\\delta_{x}\\), which put all mass on a single point \\(x\\), Prohorov distance degenerates to the ordinary distance: \\(d_{P}\\left(\\delta_{x}, \\delta_{y}\\right)=d(x, y)\\).</p> <p>Since \\(T_{n}\\) is consistent for each \\(F\\) in some neighborhood of \\(F_{0}\\), we have \\(d_{P}\\left(\\delta_{T(F)}, \\hat{\\mathrm{E}}_{F}\\left(T_{n}\\right)\\right) \\rightarrow 0\\). Hence (6.1) implies, in particular,</p> \\[ d_{L}\\left(F_{0}, F\\right) \\leqslant \\delta \\Rightarrow d_{P}\\left(\\delta_{T\\left(F_{0}\\right)}, \\delta_{T(F)}\\right)=d\\left(T\\left(F_{0}\\right), T(F)\\right) \\leqslant \\varepsilon \\] <p>It follows that \\(T\\) is continuous at \\(F_{0}\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-3","title":"CHAPTER 3","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#the-basic-types-of-estimates","title":"The Basic Types of Estimates","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#31-general-remarks","title":"3.1 GENERAL REMARKS","text":"<p>This chapter introduces three basic types of estimates ( \\(M, L\\), and \\(R\\) ) and discusses their qualitative and quantitative robustness properties. They correspond, respectively, to maximum likelihood type estimates, linear combinations of order statistics, and estimates derived from rank tests.</p> <p>For reasons discussed in more detail near the end of Section 3.5, the emphasis is on the first type, the \\(M\\)-estimates: they are the most flexible ones, and they generalize straightforwardly to multiparameter problems, even though (or, perhaps, because) they are not automatically scale invariant and have to be supplemented for practical applications by an auxiliary estimate of scale (see Chapters 6 ff ).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#32-maximum-likelihood-type-estimates-m-estimates","title":"3.2 MAXIMUM LIKELIHOOD TYPE ESTIMATES (M-ESTIMATES)","text":"<p>Any estimate \\(T_{n}\\), defined by a minimum problem of the form</p> \\[ \\sum \\rho\\left(x_{i} ; T_{n}\\right)=\\min ! \\] <p>or by an implicit equation</p> \\[ \\sum \\psi\\left(x_{i} ; T_{n}\\right)=0 \\] <p>where \\(\\rho\\) is an arbitrary function, \\(\\psi(x ; \\theta)=(\\partial / \\partial \\theta) \\rho(x ; \\theta)\\), is called an \\(M\\)-estimate [or maximum likelihood type estimate; note that the choice \\(\\rho(x ; \\theta)=-\\log f(x ; \\theta)\\) gives the ordinary ML estimate].</p> <p>We are particularly interested in location estimates</p> \\[ \\sum \\rho\\left(x_{i}-T_{n}\\right)=\\min ! \\] <p>or</p> \\[ \\sum \\psi\\left(x_{i}-T_{n}\\right)=0 \\] <p>This last equation can be written equivalently as</p> \\[ \\sum w_{i} \\cdot\\left(x_{i}-T_{n}\\right)=0 \\] <p>with</p> \\[ w_{i}=\\frac{\\psi\\left(x_{i}-T_{n}\\right)}{x_{i}-T_{n}} \\] <p>this gives a formal representation of \\(T_{n}\\) as a weighted mean</p> \\[ T_{n}=\\frac{\\sum w_{i} x_{i}}{\\sum w_{i}} \\] <p>with weights depending on the sample. Remark The functional version of (2.1) may cause trouble: we cannot in general define \\(T(F)\\) to be a value of \\(t\\) that minimizes</p> \\[ \\int \\rho(x ; t) F(d x) \\] <p>Note, for instance, that the median corresponds to \\(\\rho(x ; t)=|x-t|\\), but that</p> \\[ \\int|x-t| F(d x) \\equiv \\infty \\] <p>identically in \\(t\\) unless \\(F\\) has a finite first absolute moment. There is a simple remedy: replace \\(\\rho(x ; t)\\) by \\(\\rho(x ; t)-\\rho\\left(x ; t_{0}\\right)\\) for some fixed \\(t_{0}\\), that is, in the case of the median, minimize</p> \\[ \\int(|x-t|-|x|) F(d x) \\] <p>instead of (2.9). The functional derived from (2.2), defining \\(T(F)\\) by</p> \\[ \\int \\psi(x ; T(F)) F(d x)=0 \\] <p>does not suffer from this difficulty, but it may have more solutions [corresponding to local minima of (2.8)].</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#influence-function-of-boldsymbolm-estimates","title":"Influence Function of \\(\\boldsymbol{M}\\)-Estimates","text":"<p>To calculate the influence function of an \\(M\\)-estimate, we insert \\(F_{t}=(1-t) F\\) \\(+t G\\) for \\(F\\) into (2.11) and take the derivative with respect to \\(t\\) at \\(t=0\\). In detail, if we put for short</p> \\[ \\dot{T}=\\lim _{t \\rightarrow 0} \\frac{T\\left(F_{t}\\right)-T(F)}{t} \\] <p>we obtain, by differentiation of the defining equation (2.11),</p> \\[ \\dot{T} \\int \\frac{\\partial}{\\partial \\theta} \\psi(x ; T(F)) F(d x)+\\int \\psi(x ; T(F))(d G-d F)=0 \\] <p>For the moment we do not worry about regularity conditions. We recall from (2.5.7) that, for \\(G=\\delta_{x}, \\dot{T}\\) gives the value of the influence function at \\(x\\), so, by solving (2.12) for \\(\\dot{T}\\), we obtain</p> \\[ I C(x ; F, T)=\\frac{\\psi(x ; T(F))}{-\\int(\\partial / \\partial \\theta) \\psi(x ; T(F)) F(d x)} \\] <p>In other words the influence function of an \\(M\\)-estimate is proportional to \\(\\psi\\).</p> <p>In the special case of a location problem, \\(\\psi(x ; \\theta)=\\psi(x-\\theta)\\), we obtain</p> \\[ I C(x ; F, T)=\\frac{\\psi[x-T(F)]}{\\int \\psi^{\\prime}[x-T(F)] F(d x)} \\] <p>We conclude from this in a heuristic way that \\(\\sqrt{n}\\left[T_{n}-T(F)\\right]\\) is asymptotically normal with mean 0 and variance</p> \\[ A(F, T)=\\int I C(x ; F, T)^{2} F(d x) \\] <p>However, this must be checked by a rigorous proof.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#asymptotic-properties-of-boldsymbolm-estimates","title":"Asymptotic Properties of \\(\\boldsymbol{M}\\)-Estimates","text":"<p>A fairly simple and straightforward theory is possible if \\(\\psi(x ; \\theta)\\) is monotone in \\(\\theta\\); more general cases are treated in Chapter 6.</p> <p></p> <p>Exhibit 3.2.1</p> <p>Assume that \\(\\psi(x ; \\theta)\\) is measurable in \\(x\\) and decreasing (i.e., nonincreasing) in \\(\\theta\\), from strictly positive to strictly negative values. Put</p> \\[ \\begin{aligned} T_{n}^{*} &amp; =\\sup \\left\\{t \\mid \\sum_{1}^{n} \\psi\\left(x_{i} ; t\\right)&gt;0\\right\\} \\\\ T_{n}^{* *} &amp; =\\inf \\left\\{t \\mid \\sum_{1}^{n} \\psi\\left(x_{i} ; t\\right)&lt;0\\right\\} \\end{aligned} \\] <p>Clearly, \\(-\\infty&lt;T_{n}^{*} \\leqslant T_{n}^{* *}&lt;\\infty\\), and any value \\(T_{n}\\) satisfying \\(T_{n}^{*} \\leqslant T_{n} \\leqslant T_{n}^{* *}\\) can serve as our estimate. Exhibit 3.2.1 may help with the interpretation of \\(T_{n}^{*}\\) and \\(T_{n}^{* *}\\).</p> <p>Note that</p> \\[ \\begin{aligned} &amp; \\left\\{T_{n}^{*}&lt;t\\right\\} \\subset\\left\\{\\sum \\psi\\left(x_{i} ; t\\right) \\leqslant 0\\right\\} \\subset\\left\\{T_{n}^{*} \\leqslant t\\right\\} \\\\ &amp; \\left\\{T_{n}^{* *}&lt;t\\right\\} \\subset\\left\\{\\sum \\psi\\left(x_{i} ; t\\right)&lt;0\\right\\} \\subset\\left\\{T_{n}^{* *} \\leqslant t\\right\\} \\end{aligned} \\] <p>Hence</p> \\[ \\begin{aligned} P\\left\\{T_{n}^{*}&lt;t\\right\\} &amp; =P\\left\\{\\sum \\psi\\left(x_{i} ; t\\right) \\leqslant 0\\right\\} \\\\ P\\left\\{T_{n}^{* *}&lt;t\\right\\} &amp; =P\\left\\{\\sum \\psi\\left(x_{i} ; t\\right)&lt;0\\right\\} \\end{aligned} \\] <p>at the continuity points \\(t\\) of the left-hand side.</p> <p>The distribution of the customary midpoint estimate \\(\\frac{1}{2}\\left(T_{n}^{*}+T_{n}^{* *}\\right)\\) is somewhat difficult to work out, but the randomized estimate \\(T_{n}\\), which selects one of \\(T_{n}^{*}\\) or \\(T_{n}^{* *}\\) at random with equal probability, has an explicitly expressible distribution function</p> \\[ P\\left\\{T_{n}&lt;t\\right\\}=\\frac{1}{2} P\\left\\{\\sum \\psi\\left(x_{i} ; t\\right) \\leqslant 0\\right\\}+\\frac{1}{2} P\\left\\{\\sum \\psi\\left(x_{i} ; t\\right)&lt;0\\right\\} \\] <p>It follows that the exact distributions of \\(T_{n}^{*}, T_{n}^{* *}\\), and \\(T_{n}\\) can be calculated from the convolution powers of \\(\\mathcal{E}(\\psi(x ; t))\\).</p> <p>Asymptotic approximations can be found by expanding \\(G_{n}=\\mathcal{E}\\left(\\sum_{1}^{n} \\psi\\left(x_{i} ; t\\right)\\right)\\) into an asymptotic series.</p> <p>We may take the traditional Edgeworth expansion</p> \\[ G_{n}(x) \\sim \\Phi(x)+\\varphi(x)\\left[\\frac{1}{\\sqrt{n}} R_{3}(x)+\\frac{1}{n} R_{4}(x)+\\cdots\\right] \\] <p>However, this gives a somewhat poor approximation in the tails, that is, precisely in the region where we are most interested. Therefore it is preferable to use so-called saddlepoint techniques and recenter the distributions at the point of interest. Thus if we have independent random variables \\(Y_{t}\\) with density \\(f(x)\\) and would like to determine the distribution \\(G_{n}\\) of \\(Y_{1}+\\cdots+Y_{n}\\) at the point \\(t\\), we replace the original density \\(f\\) by a conjugate density \\(f_{t}\\) :</p> \\[ f_{t}(z)=c_{t} e^{a_{t} z} f(t+z) \\] <p>where \\(c_{t}\\) and \\(a_{t}\\) are chosen such that this is a probability density with expectation 0 . See Daniels (1954).</p> <p>Later Hampel (1973b) noticed that the principal error term of the saddlepoint method seems to reside in the normalizing constant (standardizing the total mass of \\(G_{n}\\) to 1 ), so it would be advantageous not to expand \\(G_{n}\\) or its density \\(g_{n}\\), but \\(g_{n}^{\\prime} / g_{n^{\\prime}}\\) and then to determine the normalizing constant by numerical integration. Thus his procedure can be summarized as follows. Determine the second and the normalized third moment of \\(f_{t}\\) :</p> \\[ \\begin{aligned} \\sigma_{t}^{2} &amp; =\\int z^{2} f_{t}(z) d z \\\\ \\lambda_{3, t} &amp; =\\frac{\\int z^{3} f_{t}(z) d z}{\\sigma_{t}^{3}} \\end{aligned} \\] <p>Then we get from the first two terms of the Edgeworth expansion, calculated at \\(x=0\\), that</p> \\[ \\frac{g_{n}^{\\prime}}{g_{n}} \\sim-n a_{t}-\\frac{\\lambda_{3, t}}{2 \\sigma_{t}} \\] <p>From this we obtain \\(g_{n}\\) and \\(G_{n}\\) by two numerical integrations and one exponentiation; the integration constant must be determined such that \\(G_{n}\\) has total mass 1. It turns out that the first integration can be done explicitly:</p> \\[ \\log g_{n}(t) \\sim-n c_{t}-\\log \\sigma_{t}+\\text { const. } \\] <p>This method appears to give fantastically accurate approximations down to very small sample sizes ( \\(n=3\\) or 4). See Field and Hampel (1980) for details.</p> <p>We now turn to the limiting distribution of \\(T_{n}\\). Put</p> \\[ \\lambda(t)=\\lambda(t, F)=E_{F} \\psi(X, t) \\] <p>If \\(\\lambda\\) exists and is finite for at least one value of \\(t\\), then it exists and is monotone (although not necessarily finite) for all \\(t\\). This follows at once from the remark that \\(\\psi(X ; t)-\\psi(X ; s)\\) is positive for \\(t \\leqslant s\\) and hence has a well defined expectation (possibly \\(+\\infty\\) ).</p> <p>PROPOSITION 2.1 Assume that there is a \\(t_{0}\\) such that \\(\\lambda(t)&gt;0\\) for \\(t&lt;t_{0}\\) and \\(\\lambda(t)&lt;0\\) for \\(t&gt;t_{0}\\). Then both \\(T_{n}^{*}\\) and \\(T_{n}^{* *}\\) converge in probability and almost surely to \\(t_{0}\\).</p> <p>Proof This follows easily from (2.18) and the weak (strong) law of large numbers applied to \\((1 / n) \\Sigma \\psi\\left(x_{t} ; t_{0} \\pm \\varepsilon\\right)\\).</p> <p>COROLLARY 2.2 If \\(T(F)\\) is uniquely defined, then \\(T_{n}\\) is consistent at \\(F\\) : \\(T_{n} \\rightarrow T(F)\\) in probability and almost surely.</p> <p>Note that \\(\\lambda(s ; F)=\\lambda(t ; F)\\) implies \\(\\psi(x ; s)=\\psi(x ; t)\\) a.e. \\([F]\\), so for many purposes \\(\\lambda(t)\\) furnishes a more convenient parameterization than \\(t\\) itself. If \\(\\lambda\\) is continuous, then Proposition 2.1 can be restated by saying that \\(\\lambda\\left(T_{n}\\right)\\) is a consistent estimate of 0 ; this holds also if \\(\\lambda\\) vanishes on a nondegenerate interval. Also other aspects of the asymptotic behavior of \\(T_{n}\\) are best studied through that of \\(\\lambda\\left(T_{n}\\right)\\). Since \\(\\lambda\\) is monotone decreasing, we have, in particular,</p> \\[ \\left\\{-\\lambda\\left(T_{n}\\right)&lt;-\\lambda(t)\\right\\} \\subset\\left\\{T_{n}&lt;t\\right\\} \\subset\\left\\{T_{n}&lt;t\\right\\} \\subset\\left\\{-\\lambda\\left(T_{n}\\right) \\leqslant-\\lambda(t)\\right\\} \\] <p>We now plan to show that \\(\\sqrt{n} \\lambda\\left(T_{n}\\right)\\) is asymptotically normal under the following.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#assumptions","title":"ASSUMPTIONS","text":"<p>(A-1) \\(\\psi(x, t)\\) is measurable in \\(x\\) and monotone decreasing in \\(t\\). (A-2) There is at least one \\(t_{0}\\) for which \\(\\lambda\\left(t_{0}\\right)=0\\). Let \\(\\Gamma_{0}\\) be the set of \\(t\\)-values for which \\(\\lambda(t)=0\\). (A-3) \\(\\lambda\\) is continuous in a neighborhood of \\(\\Gamma_{0}\\). (A-4) \\(\\sigma(t)^{2}=E_{F}\\left[\\psi(X ; t)^{2}\\right]-\\lambda(t, F)^{2}\\) is finite, nonzero, and continuous in a neighborhood of \\(\\Gamma_{0}\\). Put \\(\\sigma_{0}=\\sigma\\left(t_{0}\\right)\\).</p> <p>Asymptotically, all \\(T_{n}, T_{n}^{*} \\leqslant T_{n} \\leqslant T_{n}^{* *}\\), show the same behavior; formally, we work with \\(T_{n}^{*}\\).</p> <p>Let \\(y\\) be an arbitrary real number. With the aid of (A-3) define a sequence \\(t_{n}\\), for sufficiently large \\(n\\), such that \\(y=-\\sqrt{n} \\lambda\\left(t_{n}\\right)\\). Put</p> \\[ Y_{n i}=\\frac{\\psi\\left(x_{i} ; t_{n}\\right)-\\lambda\\left(t_{n}\\right)}{\\sigma\\left(t_{n}\\right)} \\] <p>The \\(Y_{n i}, 1 \\leqslant i \\leqslant n\\), are independent, identically distributed random variables with expectation 0 and variance 1 . We have, in view of (2.18) and (2.26),</p> \\[ \\begin{aligned} P\\left\\{-\\sqrt{n} \\lambda\\left(T_{n}^{*}\\right)&lt;y\\right\\} &amp; =P\\left\\{T_{n}^{*}&lt;t_{n}\\right\\} \\\\ &amp; =P\\left\\{\\frac{1}{\\sqrt{n}} \\sum Y_{n i} \\leqslant \\frac{y}{\\sigma\\left(t_{n}\\right)}\\right\\} \\end{aligned} \\] <p>if \\(y / \\sqrt{n}\\) is a continuity point of the distribution of \\(\\lambda\\left(T_{n}^{*}\\right)\\), that is, for almost all \\(y\\).</p> <p>LEMMA 2.3 When \\(n \\rightarrow \\infty\\), then</p> \\[ P\\left\\{\\frac{1}{\\sqrt{n}} \\sum Y_{n i}&lt;z\\right\\} \\rightarrow \\Phi(z) \\] <p>uniformly in \\(z\\). Proof We have to verify Lindeberg's condition, which in our case reads: for every \\(\\varepsilon&gt;0\\),</p> \\[ E\\left\\{Y_{n i}^{2} ;\\left|Y_{n i}\\right|&gt;\\sqrt{n} \\varepsilon\\right\\} \\rightarrow 0 \\] <p>as \\(n \\rightarrow \\infty\\). Since \\(\\lambda\\) and \\(\\sigma\\) are continuous, this is equivalent to: for every \\(\\varepsilon&gt;0\\), as \\(n \\rightarrow \\infty\\),</p> \\[ E\\left\\{\\psi\\left(x ; t_{n}\\right)^{2} ;\\left|\\psi\\left(x ; t_{n}\\right)\\right|&gt;\\sqrt{n} \\varepsilon\\right\\} \\rightarrow 0 \\] <p>Thus it suffices to show that the family of random variables \\(\\left(\\psi\\left(x ; t_{n}\\right)\\right)_{n \\geqslant n_{0}}\\) is uniformly integrable [cf. Neveu (1964), p. 48]. But, since \\(\\psi\\) is monotone,</p> \\[ \\psi(X ; s)^{2} \\leqslant \\psi\\left(X ; s_{0}\\right)^{2}+\\psi\\left(X ; s_{1}\\right)^{2} \\] <p>for \\(s_{0} \\leqslant s \\leqslant s_{1}\\); hence the family in view of (A-4) is majorized by an integrable random variable, and thus is uniformly integrable.</p> <p>In view of (2.28) we thus have the following theorem. THEOREM 2.4 Under assumptions (A-1) to (A-4)</p> \\[ P\\left\\{-\\sqrt{n} \\lambda\\left(T_{n}\\right)&lt;y\\right\\}-\\Phi\\left(\\frac{y}{\\sigma_{0}}\\right) \\rightarrow 0 \\] <p>uniformly in \\(y\\). In other words \\(\\sqrt{n} \\lambda\\left(T_{n}\\right)\\) is asymptotically normal \\(N\\left(0, \\sigma_{0}^{2}\\right)\\). Proof It only remains to show that the convergence is uniform. This is clearly true for any bounded \\(y\\)-interval \\(\\left[-y_{0}, y_{0}\\right]\\), so, if \\(\\varepsilon&gt;0\\) is given and if we choose \\(y_{0}\\) so large that \\(\\Phi\\left(-y_{0} / \\sigma_{0}\\right)&lt;\\varepsilon / 2\\) and \\(n_{0}\\) so large that (2.29) is \\(&lt;\\varepsilon / 2\\) for all \\(n \\geqslant n_{0}\\) and all \\(y \\in\\left[-y_{0}, y_{0}\\right]\\), it follows that (2.29) must be \\(&lt;\\varepsilon\\) for all \\(y\\).</p> <p>COROLLARY 2.5 If \\(\\lambda\\) has a derivative \\(\\lambda^{\\prime}\\left(t_{0}\\right)&lt;0\\), then \\(\\sqrt{n}\\left(T_{n}-t_{0}\\right)\\) is asymptotically normal with mean 0 and variance \\(\\sigma_{0}^{2} /\\left(\\lambda^{\\prime}\\left(t_{0}\\right)\\right)^{2}\\).</p> <p>Proof In this case</p> \\[ t_{n}=t_{0}-\\frac{y}{\\sqrt{n} \\lambda^{\\prime}\\left(t_{0}\\right)}+o\\left(\\frac{1}{\\sqrt{n}}\\right) \\] <p>so the corollary follows from a comparison between (2.28) and (2.29). If we compare this with the heuristically derived expression (2.15), we notice that the latter is correct only if we can interchange the order of integration and differentiation in the denominator of (2.13), that is, if</p> \\[ \\lambda^{\\prime}(t ; F)=\\frac{\\partial}{\\partial t} \\int \\psi(x ; t) F(d x)=\\int \\frac{\\partial}{\\partial t} \\psi(x ; t) F(d x) \\] <p>at \\(t=T(F)\\).</p> <p>To illustrate some of the issues, we take the location case, \\(\\psi(x ; t)=\\) \\(\\psi(x-t)\\). If \\(F\\) has a smooth density, we can write</p> \\[ \\lambda(t ; F)=\\int_{x} \\psi(x-t) f(x) d x=\\int \\psi(x) f(x+t) d x \\] <p>thus</p> \\[ \\lambda^{\\prime}(t ; F)=\\int \\psi(x) f^{\\prime}(x+t) d x \\] <p>may be well behaved even if \\(\\psi\\) is not differentiable. If \\(F=(1-\\varepsilon) G+\\varepsilon \\delta_{x_{0}}\\) is a mixture of a smooth distribution and a pointmass, we have</p> \\[ \\lambda(t ; F)=(1-\\varepsilon) \\int \\psi(x-t) g(x) d x+\\varepsilon \\psi\\left(x_{0}-t\\right) \\] <p>and</p> \\[ \\lambda^{\\prime}(t, F)=(1-\\varepsilon) \\int \\psi(x) g^{\\prime}(x+t) d x-\\varepsilon \\psi^{\\prime}\\left(x_{0}-t\\right) \\] <p>Hence if \\(\\psi^{\\prime}\\) is discontinuous and happens to have a jump at the point \\(x_{0}-T(F)\\), then the left-hand side and the right-hand side derivatives of \\(\\lambda\\) at \\(t=T(F)\\) exist but are different. As a consequence \\(\\sqrt{n}\\left[T_{n}-T(F)\\right]\\) has a nonnormal limiting distribution: it is pieced together from the left half and the right half of two normal distributions with different standard deviations.</p> <p>Hitherto we have been concerned with a fixed underlying distribution \\(F\\). From the point of view of robustness, such a result is of limited use; we would really like to have the convergence in Theorem 2.4 be uniform with respect to \\(F\\) in some neighborhood of the model distribution \\(F_{0}\\). For this we need more stringent regularity conditions.</p> <p>For instance, let us assume that \\(\\psi(x ; t)\\) is bounded and continuous as a function of \\(x\\) and that the map \\(t \\rightarrow \\psi(\\cdot ; t)\\) is continuous for the topology of uniform convergence. Then \\(\\lambda(t ; F)\\) and \\(\\sigma(t ; F)\\) depend continuously on both \\(t\\) and \\(F\\). With the aid of the Berry-Esseen theorem, it is then possible to put a bound on (2.29) that is uniform in \\(F\\) [cf. Feller (1966), p. 515 ff ].</p> <p>Of course, this does not yet suffice to make the asymptotic variance of \\(\\sqrt{n}\\left[T_{n}-T(F)\\right]\\)</p> \\[ A(F, T)=\\frac{\\sigma(T(F) ; F)^{2}}{\\left(\\lambda^{\\prime}(T(F) ; F)\\right)^{2}} \\] <p>continuous as a function of \\(F\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#quantitative-and-qualitative-robustness-of-boldsymbolm-estimates","title":"Quantitative and Qualitative Robustness of \\(\\boldsymbol{M}\\)-Estimates","text":"<p>We now calculate the maximum bias \\(b_{1}\\) (see Section 1.4) for \\(M\\)-estimates. Specifically, we consider the location case, \\(\\psi(x ; t)=\\psi(x-t)\\), with a monotone increasing \\(\\psi\\), and for \\(\\mathscr{P}_{\\varepsilon}\\) we take a L\u00e9vy neighborhood (the results for Prohorov neighborhoods happen to be the same). For simplicity we assume that the target value is \\(T\\left(F_{0}\\right)=0\\).</p> <p>Put</p> \\[ b_{+}(\\varepsilon)=\\sup \\left\\{T(F) \\mid d_{L}\\left(F_{0}, F\\right)&lt;\\varepsilon\\right\\} \\] <p>and</p> \\[ b_{-}(\\varepsilon)=\\inf \\left\\{T(F) \\mid d_{L}\\left(F_{0}, F\\right)&lt;\\varepsilon\\right\\} \\] <p>then</p> \\[ b_{1}(\\varepsilon)=\\max \\left\\{b_{+}(\\varepsilon),-b_{-}(\\varepsilon)\\right\\} \\] <p>In view of Theorems 1.4.1 and 1.4.2, we have \\(b_{1}(\\varepsilon)=b(\\varepsilon)\\) at the continuity points of \\(b_{1}\\).</p> <p>As before we let</p> \\[ \\lambda(t ; F)=\\int \\psi(x-t) F(d x) \\] <p>We note that \\(\\lambda\\) is decreasing in \\(t\\), and that it increases if \\(F\\) is made stochastically larger [see, e.g., Lehmann (1959), p. 74, Lemma 2(i)]. The solution \\(t=T(F)\\) of \\(\\lambda(t ; F)=0\\) is not necessarily unique; we have \\(T^{*}(F) \\leqslant\\) \\(T(F) \\leqslant T^{* *}(F)\\) with</p> \\[ \\begin{aligned} T^{*}(F) &amp; =\\sup \\{t \\mid \\lambda(t ; F)&gt;0\\} \\\\ T^{* *}(F) &amp; =\\inf \\{t \\mid \\lambda(t ; F)&lt;0\\} \\end{aligned} \\] <p>and we are concerned with the worst possible choice of \\(T(F)\\) when we determine \\(b_{+}\\)and \\(b_{-}\\).</p> <p>The stochastically largest member of the set \\(d_{L}\\left(F_{0}, F\\right)&lt;\\varepsilon\\) is the (improper) distribution \\(F_{1}\\) (it puts mass \\(\\varepsilon\\) at \\(+\\infty\\) ):</p> \\[ F_{1}(x)=\\left(F_{0}(x-\\varepsilon)-\\varepsilon\\right)^{+} ; \\] <p>that is,</p> \\[ \\begin{aligned} F_{1}(x) &amp; =0, &amp; &amp; \\text { for } x \\leqslant x_{0}+\\varepsilon \\\\ &amp; =F_{0}(x-\\varepsilon)-\\varepsilon, &amp; &amp; \\text { for } x&gt;x_{0}+\\varepsilon \\end{aligned} \\] <p>with \\(x_{0}\\) satisfying</p> \\[ F_{0}\\left(x_{0}\\right)=\\varepsilon \\] <p>We gloss over some (inessential) complications that arise in the discontinuous case, when \\(\\varepsilon\\) does not belong to the set of values of \\(F_{0}\\).</p> <p>Thus</p> \\[ \\lambda(t ; F) \\leqslant \\lambda\\left(t ; F_{1}\\right)=\\int_{x_{0}}^{\\infty} \\psi(x-t+\\varepsilon) F_{0}(d x)+\\varepsilon \\psi(\\infty) \\] <p>and</p> \\[ b_{+}(\\varepsilon)=\\inf \\left\\{t \\mid \\lambda\\left(t ; F_{1}\\right)&lt;0\\right\\} \\] <p>The other quantity \\(b_{-}(\\varepsilon)\\) is calculated analogously; in the important special case where \\(F_{0}\\) is symmetric and \\(\\psi\\) is an odd function, we have, of course,</p> \\[ b_{1}(\\varepsilon)=b_{+}(\\varepsilon)=-b_{-}(\\varepsilon) \\] <p>We conclude that \\(b_{+}(\\varepsilon)&lt;b_{+}(1)=\\infty\\), provided \\(\\psi(+\\infty)&lt;\\infty\\) and</p> \\[ \\lim _{t \\rightarrow \\infty} \\lambda\\left(t ; F_{1}\\right)=(1-\\varepsilon) \\psi(-\\infty)+\\varepsilon \\psi(+\\infty)&lt;0 \\] <p>Thus in order to avoid breakdown on the right-hand side, we should have \\(\\varepsilon /(1-\\varepsilon)&lt;-\\psi(-\\infty) / \\psi(+\\infty)\\). If we also take the left-hand side into account, we obtain that the breakdown point is</p> \\[ \\varepsilon^{*}=\\frac{\\eta}{1+\\eta} \\] <p>with</p> \\[ \\eta=\\min \\left\\{-\\frac{\\psi(-\\infty)}{\\psi(+\\infty)},-\\frac{\\psi(+\\infty)}{\\psi(-\\infty)}\\right\\} \\] <p>and that it reaches its best possible value \\(\\varepsilon^{*}=\\frac{1}{2}\\) if \\(\\psi(-\\infty)=-\\psi(+\\infty)\\). If \\(\\psi\\) is unbounded, we have \\(\\varepsilon^{*}=0\\).</p> <p>The continuity properties of \\(T\\) are also easy to establish. Put</p> \\[ \\|\\psi\\|=\\psi(+\\infty)-\\psi(-\\infty) \\] <p>then (2.36) implies</p> \\[ \\lambda\\left(t+\\varepsilon ; F_{0}\\right)-\\|\\psi\\| \\varepsilon \\leqslant \\lambda(t ; F) \\leqslant \\lambda\\left(t-\\varepsilon ; F_{0}\\right)+\\|\\psi\\| \\varepsilon \\] <p>Hence if \\(\\psi\\) is bounded and \\(\\lambda\\left(t ; F_{0}\\right)\\) has a unique zero at \\(t=T\\left(F_{0}\\right)\\), then \\(T(F) \\rightarrow T\\left(F_{0}\\right)\\) as \\(\\varepsilon \\rightarrow 0\\), and \\(T\\) thus is continuous at \\(F_{0}\\). On the other hand if \\(\\psi\\) is unbounded, or if the zero of \\(\\lambda\\left(t ; F_{0}\\right)\\) is not unique, then \\(T\\) cannot be continuous at \\(F_{0}\\), as we can easily verify.</p> <p>We summarize these results in a theorem. THEOREM 2.6 Let \\(\\psi\\) be a monotone increasing, but not necessarily continuous, function that takes values of both signs. Then the \\(M\\)-estimator \\(T\\) of location, defined by \\(\\int \\psi(x-T(F)) F(d x)=0\\), is weakly continuous at \\(F_{0}\\) iff \\(\\psi\\) is bounded and \\(T\\left(F_{0}\\right)\\) is unique. The breakdown point \\(\\varepsilon^{*}\\) is given by (2.39) and (2.40) and reaches its maximal value \\(\\varepsilon^{*}=\\frac{1}{2}\\) whenever \\(\\psi(-\\infty)\\) \\(=-\\psi(+\\infty)\\). Example 2.1 The median, corresponding to \\(\\psi(x)=\\operatorname{sign}(x)\\), is a continuous functional at every \\(F_{0}\\) whose median is uniquely defined. Example 2.2 If \\(\\psi\\) is bounded and strictly monotone, then the corresponding \\(M\\)-estimate is everywhere continuous.</p> <p>If \\(\\psi\\) is not monotone, the situation is much more complicated. To be specific take</p> \\[ \\begin{aligned} \\psi(x) &amp; =\\sin (x), &amp; &amp; \\text { for }-\\pi \\leqslant x \\leqslant \\pi \\\\ &amp; =0, &amp; &amp; \\text { elsewhere } \\end{aligned} \\] <p>(an estimate proposed by D. F. Andrews). Then \\(\\Sigma \\psi\\left(x_{i}-T_{n}\\right)\\) has many distinct zeros in general, and even vanishes identically for large absolute values of \\(T_{n}\\). Two possibilities for narrowing down the choice of solutions are: (1) Take the absolute minimum of \\(\\Sigma \\rho\\left(x_{i}-T_{n}\\right)\\), with</p> \\[ \\begin{aligned} \\rho(x) &amp; =1-\\cos (x), &amp; &amp; \\text { for }-\\pi \\leqslant x \\leqslant \\pi \\\\ &amp; =2, &amp; &amp; \\text { elsewhere. } \\end{aligned} \\] <p>(2) Take the solution nearest to the sample median.</p> <p>For computational reasons we prefer (2) or a variant thereof; start an iterative root-finding procedure at the sample median and accept whatever root it converges to.</p> <p>In case (2), the procedure inherits the high breakdown point \\(\\varepsilon^{*}=\\frac{1}{2}\\) from the median.</p> <p>Consistency and asymptotic normality of \\(M\\)-estimates are treated again in Sections 6.2 and 6.3.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#33-linear-combinations-of-order-statistics-l-estimates","title":"3.3 LINEAR COMBINATIONS OF ORDER STATISTICS ( \\(L\\)-ESTIMATES)","text":"<p>Consider a statistic that is a linear combination of order statistics, or more generally, of some function \\(h\\) of them:</p> \\[ T_{n}=\\sum_{i=1}^{n} a_{n i} h\\left(x_{(i)}\\right) \\] <p>We assume that the weights are generated by a (signed) measure \\(M\\) on \\((0,1)\\) :</p> \\[ a_{n i}=\\frac{1}{2} M\\left\\{\\left(\\frac{i-1}{n}, \\frac{i}{n}\\right)\\right\\}+\\frac{1}{2} M\\left\\{\\left[\\frac{i-1}{n}, \\frac{i}{n}\\right]\\right\\} \\] <p>(This choice preserves the total mass, \\(\\Sigma_{i} a_{n i}=M\\{(0,1)\\}\\), and symmetry of the coefficients, if \\(M\\) is symmetric about \\(t=\\frac{1}{2}\\).)</p> <p>Then \\(T_{n}=T\\left(F_{n}\\right)\\) derives from the functional</p> \\[ T(F)=\\int h\\left(F^{-1}(s)\\right) M(d s) \\] <p>We have exact equality \\(T_{n}=T\\left(F_{n}\\right)\\) if we regularize the integrand at its discontinuity points and replace it by</p> \\[ \\frac{1}{2} h\\left(F_{n}^{-1}(s-0)\\right)+\\frac{1}{2} h\\left(F_{n}^{-1}(s+0)\\right) \\] <p>but only asymptotic equivalence if we do not care. Here, the inverse of any distribution function \\(F\\) is defined in the usual way as</p> \\[ F^{-1}(s)=\\inf \\{x \\mid F(x) \\geqslant s\\} \\quad 0&lt;s&lt;1 \\]"},{"location":"research/note/lecture-note-automation/result/ocr_result/#influence-function-of-l-estimates","title":"Influence Function of \\(L\\)-Estimates","text":"<p>It is now a matter of plain calculus to find the influence function \\(I C(x ; F, T)\\) of \\(T\\) : insert \\(F_{t}=(1-t) F+t G\\) into (3.3), and take the derivative with respect to \\(t\\) at \\(t=0\\), for \\(G=\\delta_{x}\\).</p> <p>We begin with the derivative of \\(T_{s}=F_{t}^{-1}(s)\\), that is, of the \\(s\\)-quantile. If we differentiate the identity</p> \\[ F_{t}\\left(F_{t}^{-1}(s)\\right)=s \\] <p>with respect to \\(t\\) at \\(t=0\\), we obtain</p> \\[ G\\left(F^{-1}(s)\\right)-F\\left(F^{-1}(s)\\right)+f\\left(F^{-1}(s)\\right) \\dot{T}_{s}=0 \\] <p>or</p> \\[ \\dot{T}_{s}=\\frac{s-G\\left(F^{-1}(s)\\right)}{f\\left(F^{-1}(s)\\right)} \\] <p>If \\(G=\\delta_{x}\\) is the pointmass 1 at \\(x\\), this gives the value of the influence function of \\(T_{s}\\) :</p> \\[ \\begin{aligned} I C\\left(x ; F, T_{s}\\right) &amp; =\\frac{s-1}{f\\left(F^{-1}(s)\\right)}, &amp; &amp; \\text { for } \\quad x&lt;F^{-1}(s) \\\\ &amp; =\\frac{s}{f\\left(F^{-1}(s)\\right)}, &amp; &amp; \\text { for } \\quad x&gt;F^{-1}(s) \\end{aligned} \\] <p>Quite clearly, these calculations make sense only if \\(F\\) has a nonzero finite derivative \\(f\\) at \\(F^{-1}(s)\\), but then they are legitimate.</p> <p>By the chain rule for differentiation, the influence function of \\(h\\left(T_{s}\\right)\\) is</p> \\[ I C\\left(x ; F, h\\left(T_{s}\\right)\\right)=I C\\left(x ; F, T_{s}\\right) h^{\\prime}\\left(T_{s}\\right) \\] <p>and that of \\(T\\) itself then is</p> \\[ \\begin{aligned} I C(x ; F, T) &amp; =\\int I C\\left(x ; F, h\\left(T_{s}\\right)\\right) M(d s) \\\\ &amp; =\\int \\frac{s h^{\\prime}\\left(F^{-1}(s)\\right)}{f\\left(F^{-1}(s)\\right)} M(d s)-\\int_{F(x)}^{1} \\frac{h^{\\prime}\\left(F^{-1}(s)\\right)}{f\\left(F^{-1}(s)\\right)} M(d s) \\end{aligned} \\] <p>Of course, the legitimacy of taking the derivative under the integral sign in (3.3) must be checked in each particular case.</p> <p>If \\(M\\) has a density \\(m\\), it may be more convenient to write (3.11) as</p> \\[ I C(x ; F, T)=\\int_{-\\infty}^{x} h^{\\prime}(y) m(F(y)) d y-\\int_{-\\infty}^{\\infty}(1-F(y)) h^{\\prime}(y) m(F(y)) d y \\] <p>This can be easily remembered through its derivative:</p> \\[ \\frac{d}{d x} I C(x ; F, T)=h^{\\prime}(x) m(F(x)) \\] <p>The last two formulas also hold if \\(F\\) does not have a density. This can easily be seen by starting from an alternative version of (3.3):</p> \\[ \\begin{aligned} T(F) &amp; =\\int h\\left(F^{-1}(s)\\right) m(s) d s=\\int h(y) m(F(y)) F(d y) \\\\ &amp; =-\\int h^{\\prime}(y) M(F(y)) d y \\end{aligned} \\] <p>If we now insert \\(F_{s}\\) and differentiate, we obtain (3.12). Of course, here also the legitimacy of the integration by parts and of the differentiation under the integral sign must be checked but, for the \"usual\" \\(h\\) and \\(m\\), this does not present a problem.</p> <p>Example 3.1 For the median \\(\\left(s=\\frac{1}{2}\\right)\\) we have</p> \\[ \\begin{aligned} I C\\left(x ; F, T_{1 / 2}\\right) &amp; =\\frac{-1}{2 f\\left(F^{-1}\\left(\\frac{1}{2}\\right)\\right)} &amp; &amp; \\text { for } x&lt;F^{-1}\\left(\\frac{1}{2}\\right) \\\\ &amp; =\\frac{1}{2 f\\left(F^{-1}\\left(\\frac{1}{2}\\right)\\right)} &amp; &amp; \\text { for } x&gt;F^{-1}\\left(\\frac{1}{2}\\right) \\end{aligned} \\] <p>Example 3.2 If \\(T(F)=\\sum \\beta_{i} F^{-1}\\left(s_{i}\\right)\\), then \\(I C(x ; F, T)\\) has jumps of size \\(\\beta_{i} / f\\left(F^{-1}\\left(s_{i}\\right)\\right)\\) at the points \\(x=F^{-1}\\left(s_{i}\\right)\\). Example 3.3 The \\(\\alpha\\)-trimmed mean corresponds to \\(h(x)=x\\) and</p> \\[ \\begin{aligned} m(s) &amp; =\\frac{1}{1-2 \\alpha}, &amp; &amp; \\text { for } \\alpha&lt;s&lt;1-\\alpha \\\\ &amp; =0, &amp; &amp; \\text { otherwise } \\end{aligned} \\] <p>thus</p> \\[ T(F)=\\frac{1}{1-2 \\alpha} \\int_{\\alpha}^{1-\\alpha} F^{-1}(s) d s \\] <p>Note that the \\(\\alpha\\)-trimmed mean \\(T\\left(F_{n}\\right)\\), as defined by (3.17), has the following property: if \\(\\alpha n\\) is an integer, then \\(\\alpha n\\) observations are removed from each end of the sample and the mean of the rest is taken. If it is not an integer, say \\(\\alpha n=|\\alpha n|+p\\), then \\(|\\alpha n|\\) observations are removed from each end, and the next observations \\(x_{(s[\\alpha n]+1)}\\) and \\(x_{(n-[\\alpha n])}\\) are given the reduced weight \\(1-p\\).</p> <p>The influence function of the \\(\\alpha\\)-trimmed mean is, according to (3.12),</p> \\[ \\begin{aligned} I C(x) &amp; =\\frac{1}{1-2 \\alpha}\\left[F^{-1}(\\alpha)-W(F)\\right], &amp; &amp; \\text { for } x&lt;F^{-1}(\\alpha) \\\\ &amp; =\\frac{1}{1-2 \\alpha}[x-W(F)], &amp; &amp; \\text { for } F^{-1}(\\alpha) \\leqslant x \\leqslant F^{-1}(1-\\alpha) \\\\ &amp; =\\frac{1}{1-2 \\alpha}\\left[F^{-1}(1-\\alpha)-W(F)\\right], &amp; &amp; \\text { for } x&gt;F^{-1}(1-\\alpha) \\end{aligned} \\] <p>Here \\(W\\) is the functional corresponding to the so-called \\(\\alpha\\)-Winsorized mean:</p> \\[ \\begin{aligned} W(F) &amp; =\\int_{\\alpha}^{1-\\alpha} F^{-1}(s) d s+\\alpha F^{-1}(\\alpha)+\\alpha F^{-1}(1-\\alpha) \\\\ &amp; =(1-2 \\alpha) T(F)+\\alpha F^{-1}(\\alpha)+\\alpha F^{-1}(1-\\alpha) \\end{aligned} \\] <p>Clearly, there will be trouble if the corner points \\(F^{-1}(\\alpha)\\) and \\(F^{-1}(1-\\alpha)\\) are not uniquely determined (i.e. if \\(F^{-1}\\) has jumps there).</p> <p>Example 3.4 The \\(\\alpha\\)-Winsorized mean (3.19) has the influence curve</p> \\[ \\begin{array}{rlrl} I C(x ; F, W) &amp; &amp; &amp; \\\\ &amp; =F^{-1}(\\alpha)-\\frac{\\alpha}{f\\left(F^{-1}(\\alpha)\\right)}-C(F), &amp; &amp; \\text { for } x&lt;F^{-1}(\\alpha) \\\\ &amp; =x-C(F), &amp; &amp; \\text { for } F^{-1}(\\alpha)&lt;x&lt;F^{-1}(1-\\alpha) \\\\ &amp; =F^{-1}(1-\\alpha)+\\frac{\\alpha}{f\\left(F^{-1}(1-\\alpha)\\right)}-C(F), &amp; &amp; \\text { for } x&gt;F^{-1}(1-\\alpha) \\end{array} \\] <p>with</p> \\[ C(F)=W(F)-\\frac{\\alpha^{2}}{f\\left(F^{-1}\\left(f\\left(F^{-1}(\\alpha)\\right)\\right.\\right.}-\\frac{\\alpha^{2}}{f\\left(F^{-1}(1-\\alpha)\\right)} \\] <p>Thus the influence curve has jumps at \\(F^{-1}(\\alpha)\\) and \\(F^{-1}(1-\\alpha)\\). The \\(\\alpha\\)-Winsorized mean corresponds to: replace the values of the \\(\\alpha n\\) leftmost observations by that of \\(x_{(\\alpha n+1)}\\), the values of the \\(\\alpha n\\) rightmost observations by that of \\(x_{(n-\\alpha n)}\\), and take the mean of this modified sample. The heuristic idea behind this proposal is that we did not want to \"throw away\" the \\(\\alpha n\\) leftmost and rightmost observations as in the trimmed mean, but wanted only to reduce their influences to those of a more moderate order statistic. This exemplifies how unreliable our intuition can be; we know now from looking at the influence functions that the trimmed mean does not throw away all of the information sitting in the discarded observations, but that it does exactly what the Winsorized mean was supposed to do!</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#quantitative-and-qualitative-robustness-of-l-estimates","title":"Quantitative and Qualitative Robustness of \\(L\\)-Estimates","text":"<p>We now calculate the maximum bias \\(b_{1}\\) (see Section 1.4) for \\(L\\)-estimates. To fix the idea assume that \\(h(x)=x\\) and that \\(M\\) is a positive measure with total mass 1. Clearly, the resulting functional then corresponds to a location estimate; if \\(F_{a X+b}\\) denotes the distribution of the random variable \\(a X+b\\), we have</p> \\[ T\\left(F_{a X+b}\\right)=a T\\left(F_{X}\\right)+b, \\quad \\text { for } \\quad a \\geqslant 0 \\] <p>It is rather evident that \\(T\\) cannot be continuous if the support of \\(M\\) (i.e., the smallest closed set with total mass 1) contains 0 or 1 . Let \\(\\alpha\\) be the largest real number such that \\([\\alpha, 1-\\alpha]\\) contains the support of \\(M\\); then, also evidently, the breakdown point satisfies \\(\\varepsilon^{*} \\leqslant \\alpha\\). We now show that \\(\\varepsilon^{*}=\\alpha\\).</p> <p>Assume that the target value is \\(T\\left(F_{0}\\right)=0\\), let \\(0&lt;\\varepsilon&lt;\\alpha\\), and define \\(b_{+}, b_{-}\\) as in (2.31) and (2.32). Then with \\(F_{1}\\) as in (2.35), we have</p> \\[ b_{+}(\\varepsilon)=\\int F_{1}^{-1}(s) M(d s)=\\varepsilon+\\int_{\\alpha}^{1-\\alpha} F_{0}^{-1}(s+\\varepsilon) M(d s) \\] <p>and symmetrically,</p> \\[ b_{-}(\\varepsilon)=-\\varepsilon+\\int_{\\alpha}^{1-\\alpha} F_{0}^{-1}(s-\\varepsilon) M(d s) \\] <p>and \\(b_{1}(\\varepsilon)\\) is again given by (2.33).</p> <p>As \\(F_{0}{ }^{-1}(s+\\varepsilon)-F_{0}{ }^{-1}(s-\\varepsilon) \\downarrow 0\\) for \\(\\varepsilon \\downarrow 0\\), except at the discontinuity points of \\(F_{0}{ }^{-1}\\), we conclude that \\(b_{1}(\\varepsilon) \\leqslant b_{+}(\\varepsilon)-b_{-}(\\varepsilon) \\downarrow 0\\) iff the distribution function of \\(M\\) and \\(F_{0}{ }^{-1}\\) do not have common discontinuity points, and then \\(T\\) is continuous at \\(F_{0}\\). Since \\(b_{1}(\\varepsilon)\\) is finite for \\(\\varepsilon&lt;\\alpha\\), we must have \\(\\varepsilon^{*} \\geqslant \\alpha\\).</p> <p>In particular, the \\(\\alpha\\)-trimmed mean with \\(0&lt;\\alpha&lt;\\frac{1}{2}\\) is everywhere continuous. The \\(\\alpha\\)-Winsorized mean is continuous at \\(F_{0}\\) if \\(F_{0}^{-1}(\\alpha)\\) and \\(F_{0}^{-1}(1-\\alpha)\\) are uniquely determined (i.e. if \\(F_{0}{ }^{-1}\\) does not have jumps there).</p> <p>The generalization to signed measures is immediate, as far as sufficiency is concerned: if \\(M=M^{+}-M^{-}\\), then continuity of \\(T^{+}(F)=\\) \\(\\int F^{-1}(s) M^{+}(d s)\\) and \\(T^{-}(F)=\\int F^{-1}(s) M^{-}(d s)\\) implies continuity of \\(T(F)=\\int F^{-1}(s) M(d s)\\); if both \\(T^{+}\\)and \\(T^{-}\\)have breakdown points \\(\\geqslant \\alpha\\), then \\(T\\) also has.</p> <p>The necessity part is trickier, but the arguments given above carry through if there are neighborhoods of the endpoints \\(\\alpha\\) and \\(1-\\alpha\\) of the support, respectively, where the measure \\(M\\) is of one sign only. We conjecture that \\(\\varepsilon^{*}=\\alpha\\) holds generally, but it has not even been proved that \\(\\alpha=0\\) implies discontinuity of \\(T\\) in the signed case.</p> <p>We summarize the results in a theorem.</p> <p>THEOREM 3.1 Let \\(M=M^{+}-M^{-}\\)be a finite signed measure on \\((0,1)\\) and let \\(T(F)=\\int F^{-1}(s) M(d s)\\). Let \\(\\alpha\\) be the largest real number such that \\([\\alpha, 1-\\alpha]\\) contains the support of \\(M^{+}\\)and \\(M^{-}\\). If \\(\\alpha&gt;0\\), then \\(T\\) is weakly continuous at \\(F_{0}\\), provided \\(M\\) does not put any pointmass on a discontinuity point of \\(F_{0}{ }^{-1}\\). The breakdown point satisfies \\(\\varepsilon^{*} \\geqslant \\alpha\\). If \\(M\\) is positive, we have \\(\\varepsilon^{*}=\\alpha\\), and \\(\\alpha=0\\) implies that \\(T\\) is discontinuous.</p> <p>Since weak continuity of \\(T\\) at \\(F\\) implies consistency, \\(T\\left(F_{\\alpha}\\right) \\rightarrow T(F)\\), the above theorem also gives a simple sufficient condition for consistency. Of course, it does not cover the case \\(\\alpha=0\\).</p> <p>The asymptotic properties of \\(L\\)-estimates are in fact rather tricky to establish. In the case \\(\\alpha=0\\) (which is only of limited interest to us, because of its lack of robustness) some awkward smoothness conditions on the tails of \\(F\\) and \\(M\\) seem to be needed [cf. Chernoff et al. (1967)]. Even if \\(\\alpha&gt;0\\), there is no blanket theorem covering all the more interesting cases simultaneously. But if \\(\\sqrt{n}\\left(T\\left(F_{\\alpha}\\right)-T(F)\\right)\\) is asymptotically normal, then \\(\\int I C(x ; F, T)^{2} F(d x)\\) always seems to give the correct asymptotic variance. For our purposes the most useful version is the following.</p> <p>THEOREM 3.2 Let \\(M\\) be an absolutely continuous signed measure with density \\(m\\), whose support is contained in \\([\\alpha, 1-\\alpha], \\alpha&gt;0\\). Let \\(T(F)=\\) \\(\\int F^{-1}(s) m(s) d s\\). Then \\(\\sqrt{n}\\left(T\\left(F_{\\alpha}\\right)-T(F)\\right)\\) is asymptotically normal with</p> <p>mean 0 and variance \\(\\left\\{I C(x ; F, T)^{2} F(d x)\\right.\\), provided both (1) and (2) hold: (1) \\(m\\) is of bounded total variation (so all its discontinuities are jumps). (2) No discontinuity of \\(m\\) coincides with a discontinuity of \\(F^{-1}\\).</p> <p>Proof See, for instance, Huber (1969). Condition (2) is necessary; without it not even the influence function would be well defined [see the remark at the end of Example 3.3, and Stigler (1969)].</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#34-estimates-derived-from-rank-tests-r-estimates","title":"3.4 ESTIMATES DERIVED FROM RANK TESTS ( \\(R\\)-ESTIMATES)","text":"<p>Consider a two-sample rank test for shift: let \\(x_{1}, \\ldots, x_{m}\\) and \\(y_{1}, \\ldots, y_{n}\\) be two independent samples from the distributions \\(F(x)\\) and \\(G(x)=F(x-\\Delta)\\), respectively. Merge the two samples into one of size \\(m+n\\) and let \\(R_{i}\\) be the rank of \\(x_{i}\\) in the combined sample. Let \\(a_{i}=a(i), 1 \\leqslant i \\leqslant m+n\\), be some given scores; then base a test of \\(\\Delta=0\\) against \\(\\Delta&gt;0\\) on the test statistic</p> \\[ S_{m, n}=\\frac{1}{m} \\sum_{i=1}^{m} a\\left(R_{i}\\right) \\] <p>Usually, we assume that the scores \\(a_{i}\\) are generated by some function \\(J\\) as follows:</p> \\[ a_{i}=J\\left(\\frac{i}{m+n+1}\\right) \\] <p>There are several other possibilities for deriving scores \\(a_{i}\\) from \\(J\\), for example,</p> \\[ a_{i}=J\\left(\\frac{i-\\frac{1}{2}}{m+n}\\right) \\] <p>or</p> \\[ a_{i}=(m+n) \\int_{(i-1) /(m+n)}^{i /(m+n)} J(s) d s \\] <p>and in fact we prefer to work with this last version. Of course, for \"nice\" \\(J\\) and \\(F\\), all these scores lead to asymptotically equivalent tests. In the case of the Wilcoxon test, \\(J(t)=t-\\frac{1}{2}\\), the above three variants even create exactly the same tests.</p> <p>To simplify the presentation, from now on we assume that \\(m=n\\). In terms of functionals (4.1) can then be written as</p> \\[ S(F, G)=\\int J\\left[\\frac{1}{2} F(x)+\\frac{1}{2} G(x)\\right] F(d x) \\] <p>or, if we substitute \\(F(x)=s\\),</p> \\[ S(F, G)=\\int J\\left[\\frac{1}{2} s+\\frac{1}{2} G\\left(F^{-1}(s)\\right)\\right] d s \\] <p>If \\(F\\) is continuous and strictly monotone, the two formulas (4.5) and (4.6) are equivalent. For discontinuous distributions, for instance if we insert the empirical distributions \\(F_{n}\\) and \\(G_{n}\\) corresponding to the \\(x\\) - and \\(y\\)-samples, the exact equivalence is destroyed. Moreover, (4.5) is no longer well defined (its value depends on the arbitrary convention about the value of \\(H=\\frac{1}{2} F+\\frac{1}{2} G\\) at its jump points).</p> <p>If we standardize \\(H(x)=\\frac{1}{2} H(x-0)+\\frac{1}{2} H(x+0)\\), then (4.5) combined with the scores (4.3) gives (4.1). In any case (4.6) with (4.4) gives (4.1); we assume that there are no ties between \\(x\\) - and \\(y\\)-values. To fix the ideas from now on we work with (4.6) and (4.4). We also assume once and for all that</p> \\[ \\int J(s) d s=0 \\] <p>corresponding to</p> \\[ \\sum a_{i}=0 \\] <p>Then the expected value of (4.1) under the null hypothesis is 0 . We can derive estimates of shift \\(\\Delta_{n}\\) and of location \\(T_{n}\\) from such rank tests: (1) In the two sample case, adjust \\(\\Delta_{n}\\) such that \\(S_{n, n} \\approx 0\\) when computed from \\(\\left(x_{1}, \\ldots, x_{n}\\right)\\) and \\(\\left(y_{1}-\\Delta_{n}, \\ldots, y_{n}-\\Delta_{n}\\right)\\). (2) In the one-sample case, adjust \\(T_{n}\\) such that \\(S_{n, n} \\approx 0\\) when computed from \\(\\left(x_{1}, \\ldots, x_{n}\\right)\\) and \\(\\left(2 T_{n}-x_{1}, \\ldots, 2 T_{n}-x_{n}\\right)\\). In this case a mirror image of the first sample serves as a stand-in for the missing second sample.</p> <p>In other words we shift the second sample until the test is least able to detect a difference in location. Note that it may not be possible to achieve an exact zero, \\(S_{n, n}\\) being a discontinuous function.</p> <p>Example 4.1 The Wilcoxon test, \\(J(t)=t-\\frac{1}{2}\\), leads to the Hodges-Lehmann estimates \\(\\Delta_{n}=\\operatorname{med}\\left\\{y_{i}-x_{j}\\right\\}\\) and \\(T_{n}=\\operatorname{med}\\left\\{\\frac{1}{2}\\left(x_{i}+x_{j}\\right)\\right\\}\\). Note that our recipe in the second case leads to the median of the set of all \\(n^{2}\\) pairs; the more customary versions use only the pairs \\(i&lt;j\\) or \\(i&lt;j\\), but asymptotically all three versions are equivalent.</p> <p>Thus our location estimate \\(T_{n}\\) derives from a functional \\(T(F)\\), defined by the implicit equation</p> \\[ \\int J\\left\\{\\frac{1}{2}\\left[s+1-F\\left(2 T(F)-F^{-1}(s)\\right)\\right]\\right\\} d s=0 \\]"},{"location":"research/note/lecture-note-automation/result/ocr_result/#influence-function-of-boldsymbolr-estimates","title":"Influence Function of \\(\\boldsymbol{R}\\)-Estimates","text":"<p>We now derive the influence function of \\(T(F)\\). To shorten the notation we introduce the distribution function of the pooled population:</p> \\[ K(x)=\\frac{1}{2}[F(x)+1-F(2 T(F)-x)] \\] <p>Assume that \\(F\\) has a strictly positive density \\(f\\). We insert \\(F_{t}=(1-t) F+t G\\) for \\(F\\) in (4.9) and take the derivative \\(\\partial / \\partial t\\) (denoted by a dot \\(\\left.{ }^{-}\\right)\\)at \\(t=0\\). This gives</p> \\[ \\begin{aligned} &amp; \\int J^{\\prime}\\left(K\\left(F^{-1}(s)\\right)\\right)\\left[\\dot{F}\\left(2 T-F^{-1}(s)\\right)+\\frac{f\\left(2 T-F^{-1}(s)\\right)}{f\\left(F^{-1}(s)\\right)} \\dot{F}\\left(F^{-1}(s)\\right)\\right. \\\\ &amp; \\left.\\quad+2 f\\left(2 T-F^{-1}(s)\\right) \\dot{T}\\right] d s=0 \\end{aligned} \\] <p>We separate this expression in a sum of three integrals and substitute \\(x=2 T-F^{-1}(s)\\) in the first [thus \\(s=F(2 T-x)\\) ], but \\(x=F^{-1}(s)\\) in the second and third integrals. This gives</p> \\[ \\begin{aligned} &amp; \\dot{T} \\int J^{\\prime}(K(x)) f(2 T-x) f(x) d x \\\\ &amp; \\quad+\\int \\frac{1}{2}\\left[J^{\\prime}(K(x))+J^{\\prime}(1-K(x))\\right] f(2 T-x) \\dot{F}(x) d x=0 \\end{aligned} \\] <p>Let us now assume that the scores-generating function is symmetric in the</p> <p>sense that</p> \\[ J(1-t)=-J(t), \\quad 0&lt;t&lt;1 \\] <p>(asymmetric functions do not make much sense in the one-sample problem); then we can simplify (4.12) by introducing the function \\(U(x)\\), being an indefinite integral of</p> \\[ U^{\\prime}(x)=J^{\\prime}\\left\\{\\frac{1}{2}[F(x)+1-F(2 T(F)-x)]\\right\\} f(2 T(F)-x) \\] <p>Then (4.12) turns into</p> \\[ \\dot{T} \\int U^{\\prime}(x) f(x) d x+\\int U^{\\prime}(x) \\dot{F}(x) d x=0 \\] <p>Integration by parts of the second integral yields</p> \\[ \\int U^{\\prime}(x) \\dot{F}(x) d x=-\\int U(x) \\dot{F}(d x) \\] <p>As \\(\\dot{F}=G-F\\) any additive constant in \\(U\\) cancels out on the right-hand side. With \\(G=\\delta_{x}\\) we now obtain the influence function from (4.15) by solving for \\(\\dot{T}\\) :</p> \\[ I C(x ; F, T)=\\frac{U(x)-\\int U(x) f(x) d x}{\\int U^{\\prime}(x) f(x) d x} \\] <p>For symmetric \\(F\\) this can be simplified considerably, since then \\(U(x)=\\) \\(J(F(x)):\\)</p> \\[ I C(x ; F, T)=\\frac{J(F(x))}{\\int J^{\\prime}(F(x)) f(x)^{2} d x} \\] <p>Example 4.2 The influence function of the Hodges-Lehmann estimate \\(\\left(J(t)=t-\\frac{1}{2}\\right)\\) is</p> \\[ I C(x ; F, T)=\\frac{\\frac{1}{2}-F(2 T(F)-x)}{\\int f(2 T(F)-x) f(x) d x} \\] <p>with \\(T(F)\\) defined by</p> \\[ \\int F(2 T(F)-x) F(d x)=\\frac{1}{2} \\] <p>For symmetric \\(F\\) this simplifies to</p> \\[ I C(x ; F, T)=\\frac{F(x)-\\frac{1}{2}}{\\int f(x)^{2} d x} \\] <p>and the asymptotic variance of \\(\\sqrt{n}\\left[T\\left(F_{n}\\right)-T(F)\\right]\\) is indeed known to be</p> \\[ A(F, T)=\\int I C^{2} d F=\\frac{1}{12\\left[\\int f(x)^{2} d x\\right]^{2}} \\] <p>[Formula (4.18) suggests that the Hodges-Lehmann estimate will be quite poor for certain asymmetric densities, since the denominator of the influence function might become very small.] Example 4.3 The normal scores estimate is defined by \\(J(t)=\\Phi^{-1}(t)\\). For symmetric \\(F\\) its influence function is</p> \\[ I C(x ; F, T)=\\frac{\\Phi^{-1}(F(x))}{\\int \\frac{f(x)^{2}}{\\varphi\\left\\{\\Phi^{-1}[F(x)]\\right\\}} d x} \\] <p>where \\(\\varphi=\\Phi^{\\prime}\\) is the standard normal density. In particular, for \\(F=\\Phi\\), we obtain</p> \\[ I C(x ; \\Phi, T)=x \\]"},{"location":"research/note/lecture-note-automation/result/ocr_result/#quantitative-and-qualitative-robustness-of-boldsymbolr-estimates","title":"Quantitative and Qualitative Robustness of \\(\\boldsymbol{R}\\)-Estimates","text":"<p>We now calculate the maximum bias (see Section 1.4) for \\(R\\)-estimates. We assume that the scores function \\(J\\) is monotone increasing and symmetric, \\(J(1-t)=-J(t)\\). In order that (4.6) be well defined, we must require</p> \\[ \\int|J(s)| d s&lt;\\infty \\] <p>The function</p> \\[ \\lambda(t ; F)=\\int J\\left\\{\\frac{1}{2}\\left[s+1-F\\left(2 t-F^{-1}(s)\\right)\\right]\\right\\} d s \\] <p>then is monotone decreasing in \\(t\\), and it increases if \\(F\\) is made stochastically larger. Thus among all \\(F\\) satisfying \\(d_{L}\\left(F_{0}, F\\right)&lt;\\varepsilon\\) [or also \\(d_{P}\\left(F_{0}, F\\right)&lt;\\varepsilon\\) ], \\(\\lambda(t, F)\\) is largest at the (improper) distribution \\(F_{1}\\) of (2.35). Thus we have to calculate \\(\\lambda\\left(t ; F_{1}\\right)\\).</p> <p>We note first that</p> \\[ \\begin{aligned} F_{1}^{-1}(s) &amp; =F_{0}^{-1}(s+\\varepsilon)+\\varepsilon, &amp; &amp; \\text { for } 0 \\leqslant s \\leqslant 1-\\varepsilon \\\\ &amp; =\\infty, &amp; &amp; \\text { for } s&gt;1-\\varepsilon . \\end{aligned} \\] <p>Thus provided the two side conditions</p> \\[ 0 \\leqslant s \\leqslant 1-\\varepsilon \\] <p>and</p> \\[ 2 t-F_{1}^{-1}(s) \\geqslant x_{0}+\\varepsilon \\] <p>where</p> \\[ F_{0}\\left(x_{0}\\right)=\\varepsilon \\] <p>are satisfied, we have</p> \\[ F_{1}\\left[2 t-F_{1}^{-1}(s)\\right]=F_{0}\\left[2 t-2 \\varepsilon-F_{0}^{-1}(s+\\varepsilon)\\right]-\\varepsilon \\] <p>The second side condition can be written as</p> \\[ s \\leqslant F_{0}\\left(2 t-2 \\varepsilon-x_{0}\\right)-\\varepsilon \\] <p>Putting things together we obtain</p> \\[ \\begin{aligned} \\lambda\\left(t ; F_{1}\\right)= &amp; \\int_{0}^{s_{0}} J\\left(\\frac{1}{2}\\left[s+\\varepsilon+1-F_{0}\\left(2(t-\\varepsilon)-F_{0}^{-1}(s+\\varepsilon)\\right)\\right]\\right) d s \\\\ &amp; +\\int_{s_{0}}^{1} J\\left[\\frac{1}{2}(s+1)\\right] d s \\end{aligned} \\] <p>with</p> \\[ s_{0}=\\left[F_{0}\\left(2(t-\\varepsilon)-x_{0}\\right)-\\varepsilon\\right]^{+} \\] <p>We then have</p> \\[ b_{+}(\\varepsilon)=\\inf \\left\\{t \\mid \\lambda\\left(t ; F_{1}\\right)&lt;0\\right\\} \\] <p>and symmetrically, we also calculate \\(b_{-}(\\varepsilon)\\); if \\(F_{0}\\) is symmetric, we have of course</p> \\[ b_{1}(\\varepsilon)=b_{+}(\\varepsilon)=-b_{-}(\\varepsilon) \\] <p>With regard to breakdown we note that \\(b_{+}(\\varepsilon)&lt;\\infty\\) iff</p> \\[ \\lim _{t \\rightarrow \\infty} \\lambda\\left(t ; F_{1}\\right)&lt;0 \\] <p>Since</p> \\[ \\begin{aligned} \\lim _{t \\rightarrow \\infty} \\lambda\\left(t ; F_{1}\\right) &amp; =\\int_{0}^{1-\\varepsilon} J\\left[\\frac{1}{2}(s+\\varepsilon)\\right] d s+\\int_{1-\\varepsilon}^{1} J\\left[\\frac{1}{2}(s+1)\\right] d s \\\\ &amp; =2\\left[\\int_{\\varepsilon / 2}^{1 / 2} J(s) d s+\\int_{1-\\varepsilon / 2}^{1} J(s) d s\\right] \\end{aligned} \\] <p>(using symmetry of \\(J\\) ):</p> \\[ =2\\left[\\int_{1-\\varepsilon / 2}^{1} J(s) d s-\\int_{1 / 2}^{1-\\varepsilon / 2} J(s) d s\\right] \\] <p>the breakdown point \\(\\varepsilon^{*}\\) is that value \\(\\varepsilon\\) for which</p> \\[ \\int_{1 / 2}^{1-\\varepsilon / 2} J(s) d s=\\int_{1-\\varepsilon / 2}^{1} J(s) d s \\] <p>Example 4.4 For the Hodges-Lehmann estimates, \\(J(t)=t-\\frac{1}{2}\\), we obtain as breakdown point</p> \\[ \\varepsilon^{*}=1-\\frac{1}{\\sqrt{2}} \\approx 0.293 \\] <p>Example 4.5 For the normal scores estimate, \\(J(t)=\\Phi^{-1}(t)\\), we obtain as breakdown point</p> \\[ \\varepsilon^{*}=2 \\Phi(-\\sqrt{\\ln 4}) \\approx 0.239 \\] <p>When \\(\\varepsilon \\downarrow 0\\) the integrand in (4.26) decreases and converges to the integrand corresponding to \\(F_{0}\\) for almost all \\(s\\) and \\(t\\). It follows from the monotone convergence theorem that \\(\\lambda\\left(t ; F_{1}\\right) \\downarrow \\lambda\\left(t ; F_{0}\\right)\\) at the continuity points of \\(\\lambda\\left(\\cdot ; F_{0}\\right)\\). Hence if \\(\\lambda\\left(t ; F_{0}\\right)\\) has a unique zero, that is, if \\(T\\left(F_{0}\\right)\\) is uniquely defined, \\(T\\) is continuous at \\(F_{0}\\). If \\(T\\left(F_{0}\\right)\\) is not unique, then \\(T\\) of course cannot be continuous at \\(F_{0}\\). A sufficient condition for uniqueness is, for instance, that the derivative of \\(\\lambda\\left(t ; F_{0}\\right)\\) with regard to \\(t\\) exists and is not equal to 0 at \\(T=T\\left(F_{0}\\right)\\); this derivative occurred already (with the opposite sign) as the denominator of (4.16) and (4.17).</p> <p>We summarize the results in a theorem. THEOREM 4.1 Assume that the scores generating function \\(J\\) is monotone increasing, integrable, and symmetric: \\(J(1-t)=-J(t)\\). If the \\(R\\) estimate \\(T\\left(F_{0}\\right)\\) is uniquely defined by (4.9), then \\(T\\) is weakly continuous at \\(F_{0}\\). The breakdown point of \\(T\\) is given by (4.27).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#35-asymptotically-efficient-m-l-and-r-estimates","title":"3.5 ASYMPTOTICALLY EFFICIENT \\(M\\)-, \\(L\\)-, AND \\(R\\)-ESTIMATES","text":"<p>The main purpose of this section is to develop some heuristic guidelines for the selection of the functions \\(\\psi, m\\), and \\(J\\) characterizing \\(M\\)-, \\(L\\)-, and \\(R\\)-estimates, respectively. The arguments, as they stand, are rigorous for Fr\u00e9chet differentiable functionals only.</p> <p>Let \\(\\left(F_{\\theta}\\right)_{\\theta \\in \\Theta}\\) be a parametric family of distributions, and let the functional \\(T\\) be a Fisher consistent estimate of \\(\\theta\\), that is,</p> \\[ T\\left(F_{\\theta}\\right)=\\theta, \\quad \\text { for all } \\theta \\] <p>Assume that \\(T\\) is Fr\u00e9chet differentiable at \\(F\\). We intend to show that the corresponding estimate is asymptotically efficient at \\(F_{\\theta}\\) iff its influence function satisfies</p> \\[ I C\\left(x ; F_{\\theta}, T\\right)=\\frac{1}{I\\left(F_{\\theta}\\right)} \\frac{\\partial}{\\partial \\theta}\\left(\\log f_{\\theta}\\right) \\] <p>Here, \\(f_{\\theta}\\) is the density of \\(F_{\\theta}\\), and</p> \\[ I\\left(F_{\\theta}\\right)=\\int\\left(\\frac{\\partial}{\\partial \\theta} \\log f_{\\theta}\\right)^{2} d F_{\\theta} \\] <p>is the Fisher information.</p> <p>Assume that \\(d_{L}\\left(F_{\\theta}, F_{\\theta+\\delta}\\right)=O(\\delta)\\), that</p> \\[ \\frac{f_{\\theta+\\delta}-f_{\\theta}}{\\delta f_{\\theta}} \\rightarrow \\frac{\\partial}{\\partial \\theta} \\log f_{\\theta} \\] <p>converges in the \\(L_{2}\\left(F_{\\theta}\\right)\\)-sense, and that</p> \\[ 0&lt;I\\left(F_{\\theta}\\right)&lt;\\infty \\] <p>Then by definition of the Fr\u00e9chet derivative,</p> \\[ T\\left(F_{\\theta+\\delta}\\right)-T\\left(F_{\\theta}\\right)-\\int I C\\left(x ; F_{\\theta}, T\\right)\\left(f_{\\theta+\\delta}-f_{\\theta}\\right) d x=o\\left(d_{L}\\left(F_{\\theta}, F_{\\theta+\\delta}\\right)\\right)=o(\\delta) \\] <p>We divide this by \\(\\delta\\) and let \\(\\delta \\rightarrow 0\\). In view of (5.1) and (5.4) we obtain</p> \\[ \\int I C\\left(x ; F_{\\theta}, T\\right) \\frac{\\partial}{\\partial \\theta}\\left(\\log f_{\\theta}\\right) f_{\\theta} d x=1 \\] <p>The Schwarz inequality applied to (5.7) gives: first, that the asymptotic variance \\(A\\left(F_{\\theta}, T\\right)\\) of \\(\\sqrt{n}\\left[T\\left(F_{n}\\right)-T\\left(F_{\\theta}\\right)\\right]\\) satisfies</p> \\[ A\\left(F_{\\theta}, T\\right)=\\int I C\\left(x ; F_{\\theta}, T\\right)^{2} d F_{\\theta} \\geqslant \\frac{1}{I\\left(F_{\\theta}\\right)} \\] <p>and second, that we can have equality in (5.8) (i.e., asymptotic efficiency) only if \\(I C\\left(x ; F_{\\theta}, T\\right)\\) is proportional to \\((\\partial / \\partial \\theta) \\log f_{\\theta}\\). The factor of proportionality is easy to determine, and this gives the result announced in (5.2).</p> <p>Remark It is possible to establish a variant of (5.2), not even assuming G\u00e2teaux differentiability of \\(T\\). Assume (5.4), and that the sequence \\(T_{n}\\) is efficient at \\(F_{\\theta}\\), or, more precisely, that the limit of an expression similar to (1.4.9) satisfies</p> \\[ \\lim _{\\varepsilon \\rightarrow 0} \\lim _{n} \\sup _{|\\delta|&lt;\\varepsilon} Q_{t}\\left(F_{\\theta+\\delta}, T_{n}\\right)^{2} \\leqslant \\frac{1}{I\\left(F_{\\theta}\\right)} \\] <p>Then it follows that \\(\\sqrt{n}\\left(T_{n}-\\theta\\right)\\) is asymptotically normal with mean 0 and variance \\(1 / I\\left(F_{\\theta}\\right)\\), and that, in fact, we must have asymptotic equivalence</p> \\[ \\sqrt{n}\\left(T_{n}-\\theta\\right) \\sim \\frac{1}{I\\left(F_{\\theta}\\right)} \\sum \\frac{\\partial}{\\partial \\theta} \\log f_{\\theta}\\left(x_{i}\\right) \\] <p>This is, for all practical purposes, the same as (5.2). For details see Hajek (1972), and earlier work by LeCam (1953) and Huber (1966).</p> <p>Let us now check whether it is possible to achieve (5.2) with \\(M\\)-, \\(L\\)-, and \\(R\\)-estimates, at least in the case of a location parameter, \\(f_{\\theta}(x)=f_{0}(x-\\theta)\\). (1) For \\(M\\)-estimates it suffices to choose</p> \\[ \\psi(x)=-c \\frac{f_{0}^{\\prime}(x)}{f_{0}(x)}, \\quad c \\neq 0 \\] <p>compare (2.14). (2) For \\(L\\)-estimates we must take \\(h(x)=x\\) (otherwise we do not have translation invariance and thus lose consistency). Then the proper choice, suggested by (3.13), is</p> \\[ m\\left(F_{0}(x)\\right)=\\frac{-1}{l\\left(F_{0}\\right)}\\left(\\log f_{0}(x)\\right)^{\\prime \\prime} \\] <p>and it is easy to check that \\(\\int m(s) d s=1\\) (translation invariance). If \\(f_{0}\\) is not twice differentiable, we have to replace (5.12) by a somewhat more complicated integrated version for \\(M\\) itself. (3) For \\(R\\)-estimates we assume that \\(F_{0}\\) is symmetric. Then (4.17) suggests the choice</p> \\[ J\\left(F_{0}(x)\\right)=-c \\frac{f_{0}^{\\prime}(x)}{f_{0}(x)}, \\quad c \\neq 0 \\] <p>and this indeed gives (5.2). For asymmetric \\(F_{0}\\) we cannot achieve full efficiency with \\(R\\)-estimates.</p> <p>Of course, we must check in each individual case whether these estimates are indeed efficient (the stringent regularity conditions-Fr\u00e9chet differentiability-that we used to derive efficiency will rarely be satisfied).</p> <p>Example 5.1 Normal Distribution \\(f_{0}(x)=(1 / \\sqrt{2 \\pi}) e^{-x^{2} / 2}\\)</p> \\(M:\\) \\(\\psi(x)=x\\) sample mean, nonrobust, \\(L:\\) \\(m(t)=1\\) sample mean, nonrobust, \\(R:\\) \\(J(t)=\\Phi^{-1}(t)\\) normal scores estimate, robust. <p>Example 5.2 Logistic Distribution \\(F_{0}(x)=1 /\\left(1+e^{-x}\\right)\\) \\(M: \\quad \\psi(x)=\\tanh (x / 2) \\quad\\) robust, \\(L: \\quad m(t)=6 t(1-t) \\quad\\) nonrobust, \\(R: \\quad J(t)=t-\\frac{1}{2} \\quad\\) Hodges-Lehmann, robust.</p> <p>Example 5.3 Cauchy Distribution \\(f_{0}(x)=1 /\\left[\\pi\\left(1+x^{2}\\right)\\right]\\)</p> \\(M: \\quad \\psi(x)=2 x /\\left(1+x^{2}\\right)\\) robust, \\(L: \\quad m(t)=2 \\cos (2 \\pi t)[\\cos (2 \\pi t)-1]\\) nonrobust, \\(R: \\quad J(t)=-\\sin (2 \\pi t)\\) robust(?). <p>Example 5.4 \"Least Informative\" Distribution (see Example 4.5.2)</p> \\[ \\begin{aligned} f_{0}(x) &amp; =C e^{-x^{2} / 2}, &amp; &amp; |x|&lt;c \\\\ &amp; =C e^{-c|x|+c^{2} / 2}, &amp; &amp; |x|&gt;c \\end{aligned} \\] <p>\\(M: \\quad \\psi(x)=\\max [-c, \\min (c, x)], \\quad\\) Huber-estimate, robust. \\(L: \\quad m(t)=\\frac{1}{1-2 \\alpha}, \\quad\\) for \\(\\alpha&lt;t&lt;1-\\alpha, \\alpha=F_{0}(-c)\\), \\(=0, \\quad\\) otherwise, \\(\\alpha\\)-trimmed mean, robust, \\(R\\) : the corresponding estimate has occasionally been mentioned in the literature, but does not have a simple description; robust. Some of these estimates deserve a closer look: (1) The efficient \\(R\\)-estimate for the normal distribution, the normal scores estimate, has an unbounded influence curve and hence infinite gross error sensitivity \\(\\gamma^{*}=\\infty\\) (Section 1.5). Nevertheless it is robust! I would hesitate, though, to recommend it for practical use; its quantitative robustness indicators \\(b(\\varepsilon)\\) and \\(v(\\varepsilon)\\) increase steeply when we depart from the normal model, and the estimate very soon falls behind, for example, the Hodges-Lehmann estimate, (see Exhibit 6.6.2). (2) The efficient \\(L\\)-estimate for the logistic is not robust, and \\(b_{1}(\\varepsilon)=\\infty\\) for all \\(\\varepsilon&gt;0\\), even though its \"gross error sensitivity\" \\(\\gamma^{*}\\) at \\(F_{0}\\)</p> <p>(Section 1.5) is finite. But note that its influence function for general (not necessarily logistic) \\(F\\) satisfies</p> \\[ \\frac{d}{d x} I C(x ; F, T)=6 F(x)[1-F(x)] \\] <p>Thus if \\(F\\) has Cauchy-like tails, the influence function becomes unbounded.</p> <p>The lesson to be learned from these two examples is that it is not enough to look at the influence function at the model distribution only; we must also take into account its behavior in a neighborhood of the model. In the case of the normal scores estimate, a longer tailed \\(F\\) deflates the tails of the influence curve; in the case of the logistic \\(L\\)-estimate, the opposite happens. \\(M\\)-estimates are more straightforward to handle, since for them the shape of the influence function is fixed by \\(\\psi\\).</p> <p>It is somewhat tricky to construct \\(L\\) - and \\(R\\)-estimates with prescribed robustness properties. For \\(M\\)-estimates the task is more straightforward. If we want to make a robust estimate that has good efficiency at the model \\(F_{0}\\), we should choose a \\(\\psi\\) that is bounded, but otherwise closely proportional to \\(-\\left(\\log f_{0}\\right)^{\\prime}\\). If we feel that very far-out outliers should be totally discarded, we should choose a \\(\\psi\\) that goes to zero (or is zero) for large absolute 0 . This finds its theoretical justification also in the remark that, for heavier-than-exponential tails, the influence curve of the efficient estimate decreases to zero (compare Examples 5.3 and 5.4). For \\(L\\)-estimates such an effect would be impossible to achieve over an entire range of distributions. With \\(R\\)-estimates we could do it, but not particularly well, because a change of the influence function in the extreme \\(x\\)-range selectively affects long-tailed distributions, while changes in the extreme \\(t\\)-range \\([t=F(x)]\\) affect all distributions equally.</p> <p>In one-parameter location problems, \\(L\\)-estimates, in particular trimmed means, are very attractive because they are simple to calculate. However, unless we use relatively inefficient high trimming rates (i.e., essentially the sample median), the \\(\\alpha\\)-trimmed mean has very poor breakdown properties. The situation is particularly bad for small sample sizes. For instance, for sample sizes below 20 the \\(10 \\%\\) trimmed mean cannot cope with more than one outlier!</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-4","title":"CHAPTER 4","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#asymptotic-minimax-theory-for-estimating-a-location-parameter","title":"Asymptotic Minimax Theory for Estimating a Location Parameter","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#41-general-remarks","title":"4.1 GENERAL REMARKS","text":"<p>Qualitative robustness is of little help in the actual selection of a robust procedure suited for a particular application. In order to make a rational choice, we must introduce quantitative aspects as well.</p> <p>Anscombe's (1960) comparison of the situation with an insurance problem is very helpful. Typically a so-called classical procedure is the optimal procedure for some ideal (usually normal) model. If it happens to be nonrobust and we want to insure against accidents caused by deviations from the model, we clearly will have to pay for it by sacrificing some efficiency at the model. The questions are, of course, how much efficiency we are willing to sacrifice, and against how bad a deviation we would like to insure.</p> <p>One possible approach is to fix a certain neighborhood of the model and to safeguard within that neighborhood (Huber 1964). In the simple location case, this leads to quite manageable minimax problems (even though the space of pure strategies for Nature is not dominated), both for asymptotic performance criteria (asymptotic bias or variance, treated in this chapter) and for finite sample ones (Chapter 10). If we take asymptotic variance as our performance criterion, then the least favorable situation \\(F_{0}\\) (the minimax strategy for Nature) can be characterized intrinsically; it minimizes Fisher information in the chosen neighborhood, and the minimax strategy for the Statistician is efficient for \\(F_{0}\\). Typically, if the neighborhood of the model is chosen not too large, the least favorable \\(F_{0}\\) is a quite realistic distribution (which is closer to the error distributions observed in actual samples than the normal distribution), and so we even escape the perennial criticism directed against minimax methods, namely, that they safeguard against unlikely contingencies.</p> <p>Unfortunately, this approach does not carry beyond problems possessing a high degree of symmetry (e.g., translation or scale invariance). Still it</p> <p>suffices to deal successfully with a very large part of traditional statistics; in particular, the results carry over straightforwardly to regression.</p> <p>Another approach [proposed by Hampel (1968)] remains even closer to Anscombe's idea; it minimizes the asymptotic variance at the model (i.e., it minimizes the efficiency loss), subject to a bound on the gross error sensitivity (also at the model). This approach has the conceptual flaw that it allows only infinitesimal deviations from the model, but, precisely because of this, it works for arbitrary one-parameter families of distributions; it is discussed in Chapter 11.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#42-minimax-bias","title":"4.2 MINIMAX BIAS","text":"<p>Assume that the true underlying shape \\(F\\) lies in some neighborhood \\(\\mathscr{P}_{\\varepsilon}\\) of the assumed model distribution \\(F_{0}\\), that the observations are independent with common distribution \\(F(x-\\theta)\\), and that the location parameter \\(\\theta\\) is to be estimated. In this section we plan to optimize the robustness properties of such a location estimate by minimizing its maximum asymptotic bias \\(b(\\varepsilon)\\) for distributions \\(F \\in \\mathscr{P}_{\\varepsilon}\\). For the reasons mentioned in Section 1.4, we begin with minimizing the maximum bias \\(b_{1}(\\varepsilon)\\) of the functional \\(T\\) underlying the estimate; it is then a trivial matter to verify that \\(b(\\varepsilon)=b_{1}(\\varepsilon)\\); compare Theorems 1.4.1 and 1.4.2.</p> <p>To fix the idea, consider the case of \\(\\varepsilon\\)-contaminated normal distributions</p> \\[ \\mathscr{P}_{\\varepsilon}=\\{F \\mid F=(1-\\varepsilon) \\Phi+\\varepsilon H, H \\in \\mathscr{N} \\] <p>We show that the median minimizes \\(b_{1}(\\varepsilon)\\). Clearly the maximum absolute bias \\(b_{1}(\\varepsilon)\\) of the median is attained whenever the total contaminating mass sits on one side, say on the right, and then its value is given by the solution \\(x_{0}\\) of</p> \\[ (1-\\varepsilon) \\Phi\\left(x_{0}\\right)=\\frac{1}{2} \\] <p>or</p> \\[ b_{1}(\\varepsilon)=x_{0}=\\Phi^{-1}\\left(\\frac{1}{2(1-\\varepsilon)}\\right) \\] <p>We now construct two \\(\\varepsilon\\)-contaminated normal distributions \\(F_{+}\\)and \\(F_{-}\\), which are symmetric about \\(x_{0}\\) and \\(-x_{0}\\), respectively, and which are translates of each other. \\(F_{+}\\)is given by its density (cf. Exhibit 4.2.1)</p> \\[ \\begin{aligned} f_{+}(x) &amp; =(1-\\varepsilon) \\varphi(x), \\quad \\text { for } x&lt;x_{0} \\\\ &amp; =\\left(1-\\varepsilon\\right) \\varphi\\left(x-2 x_{0}\\right), \\quad \\text { for } x&gt;x_{0} \\end{aligned} \\] <p></p> <p>Exhlbit 4.2.1 where \\(\\varphi=\\Phi^{\\prime}\\) is the standard normal density, and</p> \\[ F_{-}(x)=F_{+}\\left(x+2 x_{0}\\right) \\] <p>Thus</p> \\[ T\\left(F_{+}\\right)-T\\left(F_{-}\\right)=2 x_{0} \\] <p>for any translation invariant functional, and it is evident that none can have an absolute bias smaller than \\(x_{0}\\) at \\(F_{+}\\)and \\(F_{-}\\)simultaneously.</p> <p>This shows that the median achieves the smallest maximum bias among all translation invariant functionals. It is trivial to verify that, for the median, \\(b(\\varepsilon)=b_{1}(\\varepsilon)\\), so we have proved that the sample median solves the minimax problem of minimizing the maximum asymptotic bias.</p> <p>Evidently, we have not used any particular property of the normal distribution, except symmetry and unimodality, and the same kind of argument also carries through for other neighborhoods. For example, with a L\u00e9vy neighborhood</p> \\[ \\mathscr{P}_{\\varepsilon, \\delta}=\\{F \\mid \\forall x \\Phi(x-\\varepsilon)-\\delta \\leqslant F(x) \\leqslant \\Phi(x+\\varepsilon)+\\delta\\} \\] <p>the expression (2.2) for \\(b_{1}\\) is replaced by</p> \\[ b_{1}(\\varepsilon, \\delta)=\\Phi^{-1}\\left(\\frac{1}{2}+\\delta\\right)+\\varepsilon \\] <p>but everything else goes through without change. Thus minimizing the maximum bias leads to a rather uneventful theory; for symmetric unimodal distributions, the solution invariably is the sample median.</p> <p>The sample median thus is the estimate of choice for extremely large samples, where the standard deviation of the estimate (which is of the order \\(1 / \\sqrt{n}\\) ) is comparable to or smaller than the bias \\(b(\\varepsilon)\\). Exhibit 4.2.2 evaluates (2.2) and gives the values of \\(n\\) for which \\(b(\\varepsilon)=1 / \\sqrt{n}\\). It appears</p> \\(\\varepsilon\\) \\(b(\\varepsilon)\\) \\(n=b(\\varepsilon)^{-2}\\) 0.25 0.4307 5 0.1 0.1396 50 0.05 0.0660 230 0.01 0.0126 6300 <p>Exhibit 4.2.2 from this table that, for the customary sample sizes and not too large \\(\\varepsilon\\) (i.e., \\(\\varepsilon \\leqslant 0.1\\) ), the statistical variability of the estimate will be more important than its bias.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#43-minimax-variance-preliminaries","title":"4.3 MINIMAX VARIANCE: PRELIMINARIES","text":"<p>Minimizing the maximal variance \\(v(\\varepsilon)\\) leads to a deeper theory. We begin by minimizing the more tractable</p> \\[ v_{1}(\\varepsilon)=\\sup _{F \\in \\mathscr{F}_{e}} A(F, T) \\] <p>(cf. Section 1.4), and, since \\(\\varepsilon\\) will be kept fixed, we suppress it in the notation.</p> <p>We assume that the observations are independent with common distribution function \\(F(x-\\theta)\\). The location parameter \\(\\theta\\) is to be estimated, while the shape \\(F\\) may lie anywhere in some given set \\(\\mathscr{F}=\\mathscr{F}_{e}\\) of distribution functions. There are some difficulties of a topological nature; for certain existence proofs we would like \\(\\mathscr{F}\\) to be compact, but the more interesting neighborhoods \\(\\mathscr{F}_{e}\\) are not tight, and thus their closure is not compact in the weak topology. As a way out we propose to take an even weaker topology, the vague topology (see below); then we can enforce compactness, but at the cost of including substochastic measures in \\(\\mathscr{F}\\) (or, equivalently, probability measures that put nonzero mass at \\(\\pm \\infty\\) ). These measures may be thought to formalize the possibility of infinitely bad outliers. From now on we assume that \\(\\mathscr{F}\\) is vaguely closed and hence compact.</p> <p>The vague topology in the space \\(\\mathscr{M}_{+}+\\)of substochastic measures on \\(\\Omega\\) is the weakest topology making the maps</p> \\[ F \\rightarrow \\int \\psi d F \\] <p>continuous for all continuous \\(\\psi\\) having a compact support. Note that we</p> <p>are now working on the real line; thus \\(\\Omega=\\mathbb{R}\\) is not only Polish, but also locally compact. Then \\(\\mathscr{M}_{+}\\)is compact [see, e.g., Bourbaki (1952), Ch. III].</p> <p>Let \\(F_{0}\\) be the distribution having the smallest Fisher information</p> \\[ I(f)=\\int\\left(\\frac{f^{\\prime}}{f}\\right)^{2} f d x \\] <p>among the members of \\(\\mathscr{P}\\). Under quite general conditions there is one and only one such \\(F_{0}\\), as we see below.</p> <p>For any sequence \\(\\left(T_{n}\\right)\\) of estimates, the asymptotic variance of \\(\\sqrt{n} T_{n}\\) at \\(F_{0}\\) is at best \\(1 / I\\left(F_{0}\\right)\\); see Section 3.5. If we can find a sequence \\(\\left(T_{n}\\right)\\) such that its asymptotic variance does not exceed \\(1 / I\\left(F_{0}\\right)\\) for any \\(F \\in \\mathscr{P}\\), we have clearly solved the minimax problem.</p> <p>In particular, this sequence \\(\\left(T_{n}\\right)\\) must be asymptotically efficient for \\(F_{0}\\), which gives a hint where to look for asymptotic minimax estimates.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#44-distributions-minimizing-fisher-information","title":"4.4 DISTRIBUTIONS MINIMIZING FISHER INFORMATION","text":"<p>First of all, we extend the definition of Fisher information so that it is infinite whenever the classical expression (3.2) does not make sense. More precisely, we define it as follows.</p> <p>DEFINITION 4.1 The Fisher information for location of a distribution \\(F\\) on the real line is</p> \\[ I(F)=\\sup _{\\psi} \\frac{\\left(\\int \\psi^{\\prime} d F\\right)^{2}}{\\int \\psi^{2} d F} \\] <p>where the supremum is taken over the set \\(\\mathcal{C}_{K}^{1}\\) of all continuously differentiable functions with compact support, satisfying \\(\\int \\psi^{2} d F&gt;0\\).</p> <p>THEOREM 4.2 The following two assertions are equivalent: (1) \\(I(F)&lt;\\infty\\). (2) \\(F\\) has an absolutely continuous density \\(f\\), and \\(\\int\\left(f^{\\prime} / f\\right)^{2} f d x&lt;\\infty\\).</p> <p>In either case, we have \\(I(F)=f\\left(f^{\\prime} / f\\right)^{2} f d x\\).</p> <p>Proof If \\(\\int\\left(f^{\\prime} / f\\right)^{2} f d x&lt;\\infty\\), then integration by parts and the Schwarz inequality give</p> \\[ \\left(\\int \\psi^{\\prime} f d x\\right)^{2}=\\left(\\int \\psi \\frac{f^{\\prime}}{f} f d x\\right)^{2}&lt;\\int \\psi^{2} f d x \\int\\left(\\frac{f^{\\prime}}{f}\\right)^{2} f d x \\] <p>hence</p> \\[ I(F)&lt;\\int\\left(\\frac{f^{\\prime}}{f}\\right)^{2} f d x&lt;\\infty \\] <p>Conversely, assume that \\(I(F)&lt;\\infty\\), or, which is the same, the linear functional \\(A\\), defined by</p> \\[ A \\psi=-\\int \\psi^{\\prime} d F \\] <p>on the dense subset \\(\\mathcal{C}_{K}^{1}\\) of the Hilbert space \\(L_{2}(F)\\) of square \\(F\\)-integrable functions, is bounded:</p> \\[ \\|A\\|^{2}=\\sup \\frac{|A \\psi|^{2}}{\\|\\psi\\|^{2}}=I(F)&lt;\\infty \\] <p>Hence \\(A\\) can be extended by continuity to the whole Hilbert space \\(L_{2}(F)\\), and moreover, by Riesz' theorem, there is a \\(g \\in L_{2}(F)\\) such that</p> \\[ A \\psi=\\int \\psi g d F \\] <p>for all \\(\\psi \\in L_{2}(F)\\). Note that</p> \\[ A 1=\\int g d F=0 \\] <p>[this follows easily from the continuity of \\(A\\) and (4.2), if we approximate 1 by smooth functions with compact support].</p> <p>We do not know, at this stage of the proof, whether \\(F\\) has an absolutely continuous density \\(f\\), but if it has, then integration by parts of (4.2) gives</p> \\[ A \\psi=-\\int \\psi^{\\prime} f d x=\\int \\psi \\frac{f^{\\prime}}{f} f d x \\] <p>hence \\(g=f^{\\prime} / f\\). So we define a function \\(f\\) by</p> \\[ f(x)=\\int_{y&lt;x} g(y) F(d y) \\] <p>and we have to check that this is indeed a version of the density of \\(F\\). The Schwarz inequality applied to (4.6) yields that \\(f\\) is bounded,</p> \\[ |f(x)|^{2} \\leqslant F(x) \\int g^{2} d F \\] <p>and tends to 0 for \\(x \\rightarrow-\\infty\\) [and symmetrically also for \\(x \\rightarrow+\\infty\\); here we use (4.5)]. If \\(\\psi \\in \\dot{C}_{K}^{1}\\), then Fubini's theorem gives</p> \\[ -\\int \\psi^{\\prime}(x) f(x) d x=-\\int_{y&lt;x} \\int \\psi^{\\prime}(x) g(y) F(d y) d x=\\int \\psi(y) g(y) F(d y)=A \\psi \\] <p>A comparison with the definition (4.2) of \\(A\\) now shows that \\(f(x) d x\\) and \\(F(d x)\\) define the same linear functional on the set \\(\\left\\{\\psi^{\\prime} \\mid \\psi \\in \\dot{C}_{K}^{1}\\right\\}\\), which is dense in \\(L_{2}(F)\\). It follows that they define the same measure, and so \\(f\\) is a version of the density of \\(F\\).</p> <p>Evidently, we then have</p> \\[ I(F)=\\|A\\|^{2}=\\int g^{2} d F=\\int\\left(\\frac{f^{\\prime}}{f}\\right)^{2} f d x \\] <p>[This theorem was first proved by Huber (1964); the elegant proof given above is based on an oral suggestion by T. Liggett.]</p> <p>If the set \\(\\mathscr{P}\\) is endowed with the vague topology, then Fisher information (4.1) is lower-semicontinuous as a function of \\(F\\) (it is the pointwise supremum of a set of vaguely continuous functions).</p> <p>It follows that \\(I(F)\\) attains its infimum on any vaguely compact set \\(\\mathscr{P}\\), so we have proved the following proposition.</p> <p>PROPOSITION 4.3 (EXISTENCE) If \\(\\mathscr{P}\\) is vaguely compact, then there is an \\(F_{0} \\in \\mathscr{P}\\) minimizing \\(I(F)\\).</p> <p>We note furthermore that \\(I(F)\\) is a convex function of \\(F\\). This follows at once from the remark that \\(\\int \\psi^{\\prime} d F\\) and \\(\\int \\psi^{2} d F\\) are linear functions of \\(F\\), and from the following lemma.</p> <p>LEMMA 4.4 Let \\(u(t), v(t)\\) be linear functions of \\(t\\) such that \\(v(t)&gt;0\\) for \\(0&lt;t&lt;1\\). Then \\(w(t)=u(t)^{2} / v(t)\\) is convex for \\(0&lt;t&lt;1\\).</p> <p>Proof The second derivative of \\(w\\) is</p> \\[ w^{\\prime \\prime}(t)=\\frac{2\\left[u^{\\prime} v(t)-u(t) v^{\\prime}\\right]^{2}}{v(t)^{3}} \\geqslant 0 \\] <p>for \\(0&lt;t&lt;1\\). We are now ready to prove also the uniqueness of \\(F_{0}\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#proposition-45-uniqueness-assume-that","title":"PROPOSITION 4.5 (UNIQUENESS) Assume that","text":"<p>(1) \\(\\mathscr{P}\\) is convex. (2) \\(F_{0} \\in \\mathscr{P}\\) minimizes \\(I(F)\\) in \\(\\mathscr{P}\\), and \\(0&lt;I\\left(F_{0}\\right)&lt;\\infty\\). (3) The set where the density \\(f_{0}\\) of \\(F_{0}\\) is strictly positive is convex and contains the support of every distribution in \\(\\mathscr{P}\\). Then \\(F_{0}\\) is the unique member of \\(\\mathscr{P}\\) minimizing \\(I(F)\\).</p> <p>Proof Assume that \\(F_{1}\\) also minimizes \\(I(F)\\). Then by convexity \\(I\\left(F_{t}\\right)\\) must be constant on the segment \\(0 \\leqslant t \\leqslant 1\\), where \\(F_{t}=(1-t) F_{0}+t F_{1}\\). Without loss of generality we may assume that \\(F_{0}\\) is absolutely continuous with respect to \\(F_{1}\\) (if not, replace \\(F_{1}\\) by \\(F_{t_{0}}\\) for some fixed \\(0&lt;t_{0}&lt;1\\) ).</p> <p>Evidently, the integrand in</p> \\[ I\\left(F_{t}\\right)=\\int \\frac{\\left(f_{t}^{\\prime}\\right)^{2}}{f_{t}} d x \\] <p>is a convex function of \\(t\\). If we may differentiate twice under the integral sign, we obtain</p> \\[ 0=\\frac{d^{2}}{d t^{2}} I\\left(F_{t}\\right)=\\int 2\\left(\\frac{f_{1}^{\\prime}}{f_{1}}-\\frac{f_{0}^{\\prime}}{f_{0}}\\right)^{2} \\frac{f_{0}^{2} f_{1}^{2}}{f_{t}^{3}} d x \\] <p>This is indeed permissible; if</p> \\[ Q(t)=\\int q_{t}(x) d x \\] <p>where \\(q_{t}(x)\\) is any function convex in \\(t\\), then the integrand in</p> \\[ \\frac{Q(t+h)-Q(t)}{h}=\\int \\frac{q_{t+h}-q_{t}}{h} d x \\] <p>is monotone in \\(h\\). Hence</p> \\[ Q^{\\prime}(t)=\\int q_{t}^{\\prime} d x \\] <p>by the monotone convergence theorem. Moreover, the integrand in</p> \\[ \\frac{Q^{\\prime}(t+h)-Q^{\\prime}(t)}{h}=\\int \\frac{q_{t+h}^{\\prime}-q_{t}^{\\prime}}{h} d x \\] <p>is positive; hence, by Fatou's lemma,</p> \\[ Q^{\\prime \\prime}(t) \\geqslant \\int q_{t}^{\\prime \\prime} d x \\geqslant 0 \\] <p>and (4.8) follows. Thus we must have</p> \\[ \\frac{f_{1}^{\\prime}}{f_{1}}=\\frac{f_{0}^{\\prime}}{f_{0}} \\quad \\text { a.e. } \\] <p>If we integrate this relation, we obtain</p> \\[ f_{1}=c f_{0} \\] <p>for some constant \\(c\\) (here we have used assumption (3) of Proposition 4.5: the set where \\(f_{0}\\) and \\(f_{1}\\) are different from 0 is convex and hence, in particular, connected). Since</p> \\[ I\\left(F_{1}\\right)=\\int\\left(\\frac{f_{1}^{\\prime}}{f_{1}}\\right)^{2} f_{1} d x=\\int\\left(\\frac{f_{0}^{\\prime}}{f_{0}}\\right)^{2} c f_{0} d x=c I\\left(F_{0}\\right) \\] <p>it follows that \\(c=1\\). NOTE 1 We have not assumed that our measures have total mass 1 [note in particular the argument showing that \\(c=1\\) in (4.10)]. In principle the minimizing \\(F_{0}\\) could be substochastic. However, we do not know of any realistic set \\(\\mathscr{F}\\) where this occurs, that is, where the least informative \\(F_{0}\\) would put pointmasses at \\(\\pm \\infty\\), and there is a good intuitive reason for this. For a \"realistic\" \\(\\mathscr{F}\\), any masses at \\(\\pm \\infty\\) are not genuinely at infinity, but must have arisen as a limit of contamination that has escaped to infinity, and it is intuitively clear that, by shifting these masses again to finite values, the task of the statistician can be made harder, since they would no longer be immediately recognizable as outliers.</p> <p>NOTE 2 Proposition 4.5 is wrong without some form of assumption (3); this was overlooked in Huber (1964). For example, let \\(F_{0}\\) and \\(F_{1}\\) be defined by their densities</p> \\[ \\begin{aligned} f_{0}(x) &amp; =C x^{2}(1+x)^{2}, &amp; &amp; \\text { for }-1 \\leqslant x \\leqslant 0 \\\\ &amp; =0, &amp; &amp; \\text { otherwise, } \\\\ f_{1}(x) &amp; =C x^{2}(1-x)^{2}, &amp; &amp; \\text { for } 0 \\leqslant x \\leqslant 1 \\\\ &amp; =0, &amp; &amp; \\text { otherwise, } \\end{aligned} \\] <p>and let \\(\\mathscr{F}=\\left\\{F_{t} \\mid t \\in[0,1]\\right\\}\\). Then \\(I(F)\\) is finite and constant on \\(\\mathscr{F}\\).</p> <p>There are several other equivalent expressions for Fisher information if \\(f(x ; \\theta)\\) is sufficiently smooth. For the sake of reference, we list a few (we denote differentiation with respect to \\(\\theta\\) by a prime):</p> \\[ \\begin{aligned} I(F ; \\theta) &amp; =\\int\\left[(\\log f)^{\\prime}\\right]^{2} f d x \\\\ &amp; =-\\int(\\log f)^{\\prime \\prime} f d x \\\\ &amp; =-4 \\int \\frac{(\\sqrt{f})^{\\prime \\prime}}{\\sqrt{f}} f d x \\end{aligned} \\]"},{"location":"research/note/lecture-note-automation/result/ocr_result/#45-determination-of-f_0-by-variational-methods","title":"4.5 DETERMINATION OF \\(F_{0}\\) BY VARIATIONAL METHODS","text":"<p>Assume that \\(\\mathscr{P}\\) is convex. Because of convexity of \\(I(\\cdot), F_{0} \\in \\mathscr{P}\\) minimizes Fisher information iff \\((d / d t) I\\left(F_{t}\\right) \\geqslant 0\\) at \\(t=0\\) for every \\(F_{1} \\in \\mathscr{P}_{1}\\), where \\(\\mathscr{P}_{1}\\) is the set of all \\(F \\in \\mathscr{P}\\) with \\(I(F)&lt;\\infty\\). A straightforward differentiation of (4.7) under the integral sign, justified by the monotone convergence theorem, gives</p> \\[ \\left[\\frac{d}{d t} I\\left(F_{t}\\right)\\right]_{t=0}=\\int\\left[2 \\frac{f_{0}^{\\prime}}{f_{0}}\\left(f_{1}^{\\prime}-f_{0}^{\\prime}\\right)-\\left(\\frac{f_{0}^{\\prime}}{f_{0}}\\right)^{2}\\left(f_{1}-f_{0}\\right)\\right] d x \\geqslant 0 \\] <p>If we introduce \\(\\psi(x)=-f_{0}^{\\prime}(x) / f_{0}(x)\\), and if \\(\\psi\\) has a derivative \\(\\psi^{\\prime}\\) so that integration by parts is possible, (5.1) can be rewritten in the more convenient form</p> \\[ \\int\\left(2 \\psi^{\\prime}-\\psi^{2}\\right)\\left(f_{1}-f_{0}\\right) d x \\geqslant 0 \\] <p>or also as</p> \\[ -4 \\int \\frac{\\left(\\sqrt{f_{0}}\\right)^{\\prime \\prime}}{\\sqrt{f_{0}}}\\left(f_{1}-f_{0}\\right) d x \\geqslant 0 \\] <p>for all \\(F_{1} \\in \\mathscr{P}_{1}\\).</p> <p>Among the following examples the first one highlights an amusing connection between least informative distributions and the ground-state solution in quantum mechanics; the second one is of central importance to robust estimation.</p> <p>Example 5.1 Let \\(\\mathscr{P}\\) be the set of all probability distributions \\(F\\) such that</p> \\[ \\int V(x) F(d x) \\leqslant 0 \\] <p>where \\(V\\) is some given function. For the \\(F_{0}\\) minimizing Fisher information in \\(\\mathscr{P}\\), we have equality in (5.3) and (5.4). If we combine (5.3), (5.4), and</p> \\[ \\int F(d x)=1 \\] <p>with the aid of Lagrange multipliers \\(\\alpha\\) and \\(\\beta\\), we obtain the differential equation</p> \\[ 4 \\frac{\\sqrt{f_{0}}^{\\prime \\prime}}{\\sqrt{f_{0}}}-\\alpha V+\\beta=0 \\] <p>or, with \\(u=\\sqrt{f_{0}}\\),</p> \\[ 4 u^{\\prime \\prime}-(\\alpha V-\\beta) u=0 \\] <p>This is, essentially, the Schr\u00f6dinger equation for an electron moving in the potential \\(V\\).</p> <p>If \\(f_{0}\\) is a solution of (5.6) satisfying the side conditions (5.4) and (5.5), then (5.3) holds provided \\(\\alpha&gt;0\\). If we multiply (5.6) by \\(f_{0}\\) and integrate over \\(x\\), we obtain \\(I\\left(F_{0}\\right)=\\beta\\); hence (using the quantum mechanical jargon) we are interested in the ground-state solution corresponding to the lowest eigenvalue \\(\\beta\\).</p> <p>In the particular case \\(V(x)=x^{2}-1\\), the well-known solution for the ground-state of the harmonic oscillator yields the result, which is also well-known, that, among all distributions with variance \\(\\leqslant 1\\), the standard normal has the smallest Fisher information for location.</p> <p>From the point of view of robust estimation, a \"box\" potential is more interesting:</p> \\[ \\begin{aligned} V(x) &amp; =-a&lt;0, &amp; &amp; \\text { for }|x|&lt;1 \\\\ &amp; =b&gt;0, &amp; &amp; \\text { for }|x|&gt;1 \\end{aligned} \\] <p>It is easy to see that the solution of (5.6) then is of the general form</p> \\[ \\begin{aligned} f_{0}(x) &amp; =\\frac{C}{\\cos ^{2}(\\omega / 2)} \\cos ^{2}\\left(\\frac{\\omega x}{2}\\right), \\quad \\text { for }|x| \\leqslant 1 \\\\ &amp; =C e^{\\lambda} e^{-\\lambda|x|}, \\quad \\text { for }|x|&gt;1 \\end{aligned} \\] <p>for some constants \\(\\omega\\) and \\(\\lambda\\). In order that \\(f_{0}\\) be strictly positive, we should have \\(0&lt;\\omega&lt;\\pi\\). We have already arranged the integration constants so that \\(f_{0}\\) is continuous; if \\(\\psi=-\\left(\\log f_{0}\\right)^{\\prime}\\) is also to be continuous, we must have</p> \\[ \\lambda=\\omega \\tan \\frac{\\omega}{2} \\] <p>and \\(C\\) must be determined such that \\(\\int f_{0} d x=1\\), that is,</p> \\[ C=\\frac{\\cos ^{2}(\\omega / 2)}{1+2 /[\\omega \\tan (\\omega / 2)]} \\] <p>Note that then</p> \\[ \\begin{aligned} -4 \\frac{\\sqrt{f_{0}} \"}{\\sqrt{f_{0}}} &amp; =\\omega^{2}, \\quad \\text { for }|x|&lt;1 \\\\ &amp; =-\\lambda^{2}, \\quad \\text { for }|x|&gt;1 \\end{aligned} \\] <p>hence</p> \\[ I\\left(F_{0}\\right)=-4 \\int \\frac{\\sqrt{f_{0}} \"}{\\sqrt{f_{0}}} f_{0} d x=\\frac{\\omega^{2}}{1+2 /[\\omega \\tan (\\omega / 2)]} \\] <p>It is now straightforward to check that (5.3) is satisfied, that is, that this \\(F_{0}\\) minimizes Fisher information among all probability distributions \\(F\\) satisfying</p> \\[ F\\{(-1,1)\\} \\geqslant F_{0}\\{(-1,1)\\}=1-\\frac{2 C}{\\lambda} \\] <p>Example 5.2 Let \\(G\\) be a fixed probability distribution having a twice differentiable density \\(g\\), such that \\(-\\log g(x)\\) is convex on the convex support of \\(G\\). Let \\(\\varepsilon&gt;0\\) be given, and let \\(\\varphi\\) be the set of all probability</p> <p>distributions arising from \\(G\\) through \\(\\varepsilon\\)-contamination:</p> \\[ \\mathscr{P}=\\{F \\mid F=(1-\\varepsilon) G+\\varepsilon H, H \\in \\mathscr{M}\\} \\] <p>Here \\(\\mathscr{M}\\) is, as usual, the set of all probability measures on the real line, but we can also take \\(\\mathscr{M}\\) to be the set of all substochastic measures, in order to make \\(\\mathscr{P}\\) vaguely compact.</p> <p>In view of (5.3) it is plausible that the density \\(f_{0}\\) of the least informative distribution behaves as follows. There is a central part where \\(f_{0}\\) touches the boundary, \\(f_{0}(x)=(1-\\varepsilon) g(x)\\); in the tails \\(\\left(\\sqrt{f_{0}}\\right)^{\\prime \\prime} / \\sqrt{f_{0}}\\) is constant, that is, \\(f_{0}\\) is exponential, \\(f_{0}(x)=C e^{-\\lambda|x|}\\). This is indeed so, and we now give the solution \\(f_{0}\\) explicitly.</p> <p>Let \\(x_{0}&lt;x_{1}\\) be the endpoints of the interval where \\(\\left|g^{\\prime} / g\\right| \\leqslant k\\), and where \\(k\\) is related to \\(\\varepsilon\\) through</p> \\[ \\int_{x_{0}}^{x_{1}} g(x) d x+\\frac{g\\left(x_{0}\\right)+g\\left(x_{1}\\right)}{k}=\\frac{1}{1-\\varepsilon} \\] <p>Either \\(x_{0}\\) or \\(x_{1}\\) may be infinite. Then put</p> \\[ \\begin{array}{rlr} f_{0}(x) &amp; =(1-\\varepsilon) g\\left(x_{0}\\right) e^{k\\left(x-x_{0}\\right)}, &amp; \\text { for } x \\leqslant x_{0} \\\\ &amp; =(1-\\varepsilon) g(x), &amp; \\text { for } x_{0}&lt;x&lt;x_{1} \\\\ &amp; =(1-\\varepsilon) g\\left(x_{1}\\right) e^{-k\\left(x-x_{1}\\right)}, &amp; \\text { for } x \\geqslant x_{1} \\end{array} \\] <p>Condition (5.16) ensures that \\(f_{0}\\) integrates to 1 ; hence the contamination distribution \\(H_{0}=\\left[F_{0}-(1-\\varepsilon) G\\right] / \\varepsilon\\) also has total mass 1 , and it remains to be checked that its density \\(h_{0}\\) is nonnegative. But this follows at once from the remark that the convex function \\(-\\log g(x)\\) lies above its tangents at the points \\(x_{0}\\) and \\(x_{1}\\), that is</p> \\[ g(x) \\leqslant g\\left(x_{0}\\right) e^{k\\left(x-x_{0}\\right)} \\quad \\text { and } \\quad g(x) \\leqslant g\\left(x_{1}\\right) e^{-k\\left(x-x_{1}\\right)} \\] <p>Clearly, both \\(f_{0}\\) and its derivative are continuous; we have</p> \\[ \\begin{array}{rlrl} \\psi(x) &amp; =-\\left[\\log f_{0}(x)\\right]^{\\prime}=-k, &amp; &amp; \\text { for } x \\leqslant x_{0} \\\\ &amp; =\\frac{-g^{\\prime}(x)}{g(x)}, &amp; &amp; \\text { for } x_{0}&lt;x&lt;x_{1} \\\\ &amp; =k, &amp; &amp; \\text { for } x \\geqslant x_{1} \\end{array} \\] <p>We now check that (5.2) holds. As \\(\\psi^{\\prime}(x) \\geqslant 0\\) and as</p> \\[ \\begin{aligned} k^{2}+2 \\psi^{\\prime}-\\psi^{2} &amp; \\geqslant 0, &amp; &amp; \\text { for } x_{0} \\leqslant x \\leqslant x_{1} \\\\ &amp; =0, &amp; &amp; \\text { otherwise } \\end{aligned} \\] <p>it follows that</p> \\[ \\begin{aligned} \\int\\left(2 \\psi^{\\prime}-\\psi^{2}\\right)\\left(f_{1}-f_{0}\\right) d x= &amp; \\int_{x_{0}}^{x_{1}}\\left(k^{2}+2 \\psi^{\\prime}-\\psi^{2}\\right)\\left(f_{1}-f_{0}\\right) d x-k^{2} \\int\\left(f_{1}-f_{0}\\right) d x \\\\ &amp; \\geqslant 0 \\end{aligned} \\] <p>since \\(f_{1} \\geqslant f_{0}\\) in the interval \\(x_{0}&lt;x&lt;x_{1}\\), and since \\(\\int\\left(f_{1}-f_{0}\\right) d x&lt;0\\) (we may allow \\(F_{1}\\) to be substochastic!).</p> <p>Because of their importance we state the results for the case where \\(G=\\Phi\\) is the standard normal cumulative separately. In this case Fisher information is minimized by</p> \\[ \\begin{aligned} f_{0}(x) &amp; =\\frac{1-\\varepsilon}{\\sqrt{2 \\pi}} e^{-x^{2} / 2}, &amp; &amp; \\text { for }|x| \\leqslant k \\\\ &amp; =\\frac{1-\\varepsilon}{\\sqrt{2 \\pi}} e^{k^{2} / 2-k|x|}, &amp; &amp; \\text { for }|x|&gt;k \\end{aligned} \\] <p>with \\(k\\) and \\(\\varepsilon\\) connected through</p> \\[ \\frac{2 \\varphi(k)}{k}-2 \\Phi(-k)=\\frac{\\varepsilon}{1-\\varepsilon} \\] <p>( \\(\\varphi=\\Phi^{\\prime}\\) being the standard normal density). In this case</p> \\[ \\psi(x)=-\\left[\\log f_{0}(x)\\right]^{\\prime}=\\max [-k, \\min (k, x)] \\] <p>Compare Exhibit 4.5.1 for some numerical results. Example 5.3 Let \\(\\mathscr{P}\\) be the set of all distributions differing at most \\(\\varepsilon\\) in Kolmogorov distance from the standard normal cumulative</p> \\[ \\sup |F(x)-\\Phi(x)|&lt;\\varepsilon \\] <p>It is easy to guess that the solution \\(F_{0}\\) is symmetric and that there will be two (possibly coinciding) constants \\(0&lt;x_{0}&lt;x_{1}\\) such that \\(F_{0}(x)=\\Phi(x)-\\varepsilon\\)</p> \\(\\varepsilon\\) \\(k\\) \\(F_{0}(-k)\\) \\(1 / I\\left(F_{0}\\right.\\) 0 \\(\\infty\\) 0 1.000 0.001 2.630 0.005 1.010 0.002 2.435 0.008 1.017 0.005 2.160 0.018 1.037 0.01 1.945 0.031 1.065 0.02 1.717 0.052 1.116 0.05 1.399 0.102 1.256 0.10 1.140 0.164 1.490 0.15 0.980 0.214 1.748 0.20 0.862 0.256 2.046 0.25 0.766 0.291 2.397 0.3 0.685 0.323 2.822 0.4 0.550 0.375 3.996 0.5 0.436 0.416 5.928 0.65 0.291 0.460 12.48 0.80 0.162 0.487 39.0 1 0 0.5 \\(\\infty\\) <p>Exhblt 4.5.1 The \\(\\varepsilon\\)-contaminated normal distributions least informative for location. for \\(x_{0} \\leqslant x \\leqslant x_{1}\\), with strict inequality \\(\\left|F_{0}(x)-\\Phi(x)\\right|&lt;\\varepsilon\\) for all other positive \\(x\\). In view of (5.3) we expect that \\(\\sqrt{f_{0}}{ }^{\\prime \\prime} / \\sqrt{f_{0}}\\) is constant in the intervals \\(\\left(0, x_{0}\\right)\\) and \\(\\left(x_{1}, \\infty\\right)\\); hence we try a solution of the form</p> \\[ \\begin{aligned} f_{0}(x)=f_{0}(-x) &amp; =\\frac{\\varphi\\left(x_{0}\\right)}{\\cos ^{2}\\left(\\omega x_{0} / 2\\right)} \\cos ^{2}\\left(\\frac{\\omega x}{2}\\right), &amp; &amp; \\text { for } 0 \\leqslant x \\leqslant x_{0} \\\\ \\varphi(x), &amp; &amp; \\text { for } x_{0} \\leqslant x \\leqslant x_{1} \\\\ &amp; =\\varphi\\left(x_{1}\\right) e^{-\\lambda\\left(x-x_{1}\\right)}, &amp; &amp; \\text { for } x \\geqslant x_{1} \\end{aligned} \\] <p>We now distinguish two cases. Case A Small Values of \\(\\varepsilon, x_{0}&lt;x_{1}\\) In order that</p> \\[ \\begin{aligned} \\psi(x)=-\\left[\\log f_{0}(x)\\right]^{\\prime} &amp; =\\omega \\tan \\left(\\frac{\\omega x}{2}\\right), &amp; &amp; \\text { for } 0 \\leqslant x \\leqslant x_{0} \\\\ &amp; =x, &amp; &amp; \\text { for } x_{0} \\leqslant x \\leqslant x_{1} \\\\ &amp; =\\lambda, &amp; &amp; \\text { for } x \\geqslant x_{1} \\end{aligned} \\] <p>be continuous, we must require</p> \\[ \\begin{aligned} \\omega \\tan \\left(\\frac{\\omega x_{0}}{2}\\right) &amp; =x_{0} \\\\ \\lambda &amp; =x_{1} \\end{aligned} \\] <p>In order that \\(F_{0}(x)=\\Phi(x)-\\varepsilon\\) for \\(x_{0} \\leqslant x \\leqslant x_{1}\\), and that its total mass be 1 , we must have</p> \\[ \\int_{0}^{x_{0}} f_{0}(x) d x=\\int_{0}^{x_{0}} \\varphi(x) d x-\\varepsilon \\] <p>and</p> \\[ \\int_{x_{1}}^{\\infty} f_{0}(x) d x=\\int_{x_{1}}^{\\infty} \\varphi(x) d x+\\varepsilon \\] <p>For a given \\(\\varepsilon,(5.26)\\) to (5.29) determine the four quantities \\(x_{0}, x_{1}, \\omega\\), and \\(\\lambda\\). For the actual calculation it is advantageous to use</p> \\[ u=\\omega x_{0} \\] <p>as the independent variable, \\(0&lt;u&lt;\\pi\\), and to express everything in terms of \\(u\\) instead of \\(\\varepsilon\\). Then from (5.26), (5.30), and (5.28), we obtain, respectively,</p> \\[ \\begin{aligned} x_{0} &amp; =\\left(u \\tan \\frac{u}{2}\\right)^{1 / 2} \\\\ \\omega &amp; =\\frac{u}{x_{0}} \\\\ \\varepsilon &amp; =\\Phi\\left(x_{0}\\right)-\\frac{1}{2}-x_{0} \\varphi\\left(x_{0}\\right) \\frac{1+\\sin u / u}{1+\\cos u} \\end{aligned} \\] <p>and finally, \\(x_{1}\\) has to be determined from (5.29), that is, from</p> \\[ \\varepsilon=\\frac{\\varphi\\left(x_{1}\\right)}{x_{1}}-\\Phi\\left(-x_{1}\\right) \\] <p>It turns out that \\(x_{0}&lt;x_{1}\\) so long as \\(\\varepsilon&lt;\\varepsilon_{0} \\cong 0.0303\\). It remains to check (5.23) and (5.3). The first one follows easily from \\(f_{0}\\left(x_{0}\\right)=\\varphi\\left(x_{0}\\right), f_{0}\\left(x_{1}\\right)=\\) \\(\\varphi\\left(x_{1}\\right)\\), and from the remark that</p> \\[ -\\left[\\log f_{0}(x)\\right]^{\\prime}=\\psi(x) \\leqslant-\\left[\\log \\varphi(x)\\right]^{\\prime}, \\quad \\text { for } x \\geqslant 0 \\] <p>If we integrate this relation, we obtain that \\(f_{0}(x) \\leqslant \\varphi(x)\\) for \\(0 \\leqslant x \\leqslant x_{0}\\) and \\(f_{0}(x) \\geqslant \\varphi(x)\\) for \\(x \\geqslant x_{1}\\). In conjunction with \\(F_{0}(x)=\\Phi(x)-\\varepsilon\\) for \\(x_{0} \\leqslant x \\leqslant x_{1}\\), this establishes (5.23). In order to check (5.3), we first note that it suffices to consider symmetric distributions for \\(F_{1}\\) (since \\(I(F)\\) is convex, the symmetrized distribution \\(\\widehat{F}(x)=\\frac{1}{2}[F(x)+1-F(-x)]\\) has a smaller Fisher information than \\(F\\) ). We have</p> \\[ \\begin{aligned} -4 \\frac{\\sqrt{f_{0}^{\\prime \\prime}}}{\\sqrt{f_{0}^{\\prime}}} &amp; =\\omega^{2}, &amp; &amp; \\text { for } 0 \\leqslant x&lt;x_{0} \\\\ &amp; =2-x^{2}, &amp; &amp; \\text { for } x_{0}&lt;x&lt;x_{1} \\\\ &amp; =-x_{1}^{2}, &amp; &amp; \\text { for } x&gt;x_{1} \\end{aligned} \\] <p>Thus with \\(G=F_{1}-F_{0}\\) the left-hand side of (5.3) becomes twice</p> \\[ \\begin{aligned} \\int_{0}^{x_{0}} \\omega^{2} d G &amp; +\\int_{x_{0}}^{x_{1}}\\left(2-x^{2}\\right) d G-\\int_{x_{1}}^{\\infty} x_{1}^{2} d G \\\\ &amp; =\\left(\\omega^{2}+x_{0}^{2}-2\\right) G\\left(x_{0}\\right)+2 G\\left(x_{1}\\right)-x_{1}^{2} G(\\infty)+\\int_{x_{0}}^{x_{1}} x G(x) d x \\end{aligned} \\] <p>We note that</p> \\[ \\omega^{2}+x_{0}^{2}-2=\\frac{u}{\\tan \\left(\\frac{1}{2} u\\right)}+u \\tan \\left(\\frac{1}{2} u\\right)-2=2\\left(\\frac{u}{\\sin u}-1\\right) \\geqslant 0 \\] <p>that \\(G(x) \\geqslant 0\\) for \\(x_{0} \\leqslant x \\leqslant x_{1}\\), and that \\(G(\\infty)&lt;0\\). Hence all terms are positive and (5.3) is verified.</p> <p>Case B Large Values of \\(\\varepsilon, x_{0}=x_{1}\\) In this case (5.24) simplifies to</p> \\[ \\begin{aligned} f_{0}(x) &amp; =f_{0}(-x)=\\frac{\\varphi\\left(x_{0}\\right)}{\\cos ^{2}\\left(\\omega x_{0} / 2\\right)} \\cos ^{2}\\left(\\frac{\\omega x}{2}\\right), \\quad \\text { for } 0 \\leqslant x \\leqslant x_{0} \\\\ &amp; =\\varphi\\left(x_{0}\\right) e^{-\\lambda\\left(x-x_{0}\\right)}, \\quad \\text { for } x&gt;x_{0} \\end{aligned} \\] <p>Apart from a change of scale, this is the distribution already encountered in (5.9).</p> <p>In order that</p> \\[ \\begin{aligned} \\psi(x)=-\\left[\\log f_{0}(x)\\right]^{\\prime} &amp; =\\omega \\tan \\left(\\frac{\\omega x}{2}\\right), &amp; &amp; \\text { for } 0 \\leqslant x \\leqslant x_{0} \\\\ &amp; =\\lambda, &amp; &amp; \\text { for } x&gt;x_{0} \\end{aligned} \\] <p>be continuous, we must require that</p> \\[ \\lambda x_{0}=\\omega x_{0} \\tan \\frac{\\omega x_{0}}{2} \\] <p>and \\(f_{0}\\) integrates to 1 if</p> \\[ x_{0} \\varphi\\left(x_{0}\\right)=\\cos ^{2}\\left(\\frac{u}{2}\\right) \\frac{1}{1+2 /[u \\tan (u / 2)]} \\] <p>with \\(u=\\omega x_{0}\\); compare (5.11). It is again convenient to use \\(u=\\omega x_{0}\\) instead of \\(\\varepsilon\\) as the independent variable. We first determine \\(x_{0} \\geqslant 1\\) from (5.38) (there is also a solution \\(&lt;1\\) ), and then \\(\\lambda\\) from (5.37). From (5.29) we get</p> \\[ \\varepsilon=\\frac{\\varphi\\left(x_{0}\\right)}{\\lambda}-\\Phi\\left(-x_{0}\\right) \\] <p>This solution holds for \\(\\varepsilon \\geqslant \\varepsilon_{0} \\cong 0.0303\\). It is somewhat tricky to prove that \\(F_{0}\\) satisfies (5.23); see Sacks and Ylvisaker (1972). Exhibit 4.5.2 gives some numerical results.</p> \\(\\varepsilon\\) \\(x_{0}\\) \\(\\omega\\) \\(\\lambda\\left(=x_{1}\\right)\\) \\(1 / I\\left(F_{0}\\right)\\) 0 0 1.4142 \\(\\infty\\) 1. 0.001 0.6533 1.3658 2.4364 1.019 0.002 0.7534 1.3507 2.2317 1.034 0.005 0.9118 1.3234 1.9483 1.075 0.01 1.0564 1.2953 1.7241 1.136 0.02 1.2288 1.2587 1.4921 1.256 0.03033 1.3496 1.2316 1.3496 1.383 0.05 1.3216 1.1788 1.1637 1.656 0.10 1.3528 1.0240 0.8496 2.613 0.15 1.4335 0.8738 0.6322 4.200 0.20 1.5363 0.7363 0.4674 6.981 0.25 1.6568 0.6108 0.3384 12.24 0.3 1.7974 0.4950 0.2360 23.33 0.4 2.1842 0.2803 0.0886 144.2 <p>Exhibit 4.5.2 Least informative distributions for sup \\(|F(x)-\\Phi(x)|&lt;\\varepsilon\\) [cf. Example 4.5.3; (5.25), (5.35)].</p> <p>We have now determined a small collection of least informative situations and we should take some time out to reflect how realistic or unrealistic they are.</p> <p>First, it may surprise us that the least informative \\(F_{0}\\) do not have excessively long tails. On the contrary we might perhaps argue that they have unrealistically short tails, since they do not provide for the extreme outliers we sometimes encounter.</p> <p>Second, we should compare them with actual, supposedly normal, distributions. For that we need very large samples, and these seem to be quite rare; some impressive examples have been collected by Romanowski and Green (1965). Their largest sample ( \\(n=8688\\) ), when plotted on normal probability paper (Exhibit 4.5.3), is seen to behave very much like a least informative \\(2 \\%\\)-contaminated normal distribution [it lies between the slightly different curves for the least favorable \\(F_{0}\\) for location and the least favorable one for scale (5.6.15)]. For their smaller samples the conclusions are less clear-cut because of the higher random variability, but there also the sample distribution functions are close to some least informative \\(\\varepsilon\\)-contaminated \\(F_{0}\\) (with \\(\\varepsilon\\) in the range 0.01 to 0.1 ).</p> <p>Thus it makes very good sense to use minimax procedures safeguarding against \\(\\varepsilon\\)-contamination, for an \\(\\varepsilon\\) in the range just mentioned. </p> <p>Exhibit 4.5.3 1: Normal cumulative. 2: Least favorable for location ( \\(\\varepsilon=0.02\\) ). 3: Empirical cumulative. 4: Least favorable for scale ( \\(\\varepsilon=0.02\\) ). \\(n=8688\\). Data from Romanowski and Green (1965).</p> <p></p> <p></p> <p>Exhibit 4.5.4 plots, on normal probability paper, the symmetrized empirical distributions of several large samples taken from Romanowski and Green (1965). Also shown are the asymptotic variances of the \\(\\alpha\\)-trimmed mean and of the logarithm of the \\(\\alpha\\)-trimmed standard deviation (this corresponds to sampling with replacement from the symmetrized empirical distributions).</p> <p>These are all good data sets, so the classical estimates do not fare badly, but note that moderate trimming would never do much harm, but sometimes considerable good.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#46-asymptotically-minimax-m-estimates","title":"4.6 ASYMPTOTICALLY MINIMAX M-ESTIMATES","text":"<p>Assume that \\(F_{0}\\) has minimal Fisher information for location in the convex set \\(\\mathscr{P}\\) of distribution functions. We now show that the asymptotically efficient \\(M\\)-estimate of location for \\(F_{0}\\) in fact possesses certain minimax properties in \\(\\mathscr{P}\\).</p> <p>According to (3.5.11) we must choose</p> \\[ \\psi(x)=\\frac{-c f_{0}^{\\prime}(x)}{f_{0}^{\\prime}(x)} \\] <p>in order to achieve asymptotic efficiency at \\(F_{0}\\) (the value of the constant \\(c \\neq 0\\) is irrelevant). We do not worry about regularity conditions for the moment, but we note that, in all examples of Section 4.5, the function (6.1) is monotone, so the theory of Section 3.2 is applicable, and the \\(M\\)-estimate, defined by</p> \\[ \\int \\psi(x-T(F)) F(d x)=0 \\] <p>is asymptotically normal</p> \\[ \\mathcal{E}\\left\\{\\sqrt{n}\\left[T\\left(F_{n}\\right)-T(F)\\right]\\right\\} \\rightarrow \\mathfrak{R}(0, A(F, T)) \\] <p>with asymptotic variance</p> \\[ A(F, T)=\\frac{\\int \\psi(x-T(F))^{2} F(d x)}{\\left[\\lambda^{\\prime}(T(F))\\right]^{2}}=\\frac{\\int \\psi(x-T(F))^{2} F(d x)}{\\left[\\int \\psi^{\\prime}(x-T(F)) F(d x)\\right]^{2}} \\] <p>In particular,</p> \\[ A\\left(F_{0}, T\\right)=\\frac{1}{I\\left(F_{0}\\right)} \\] <p>Without loss of generality we may assume \\(T\\left(F_{0}\\right)=0\\). But we now run into an awkward technical difficulty, caused by the variable term \\(T(F)\\) in the expression (6.4) for the asymptotic variance. If \\(\\mathscr{P}\\) consists of symmetric distributions only, then</p> \\[ T(F)=0, \\quad \\text { for all } F \\in \\mathscr{P} \\] <p>and the difficulty disappears. Traditionally and conveniently, most of the robustness literature therefore adopts the assumption of symmetry. However, it should be pointed out that a restriction to exactly symmetric distributions: (1) Violates the very spirit of robustness. (2) Is out of the question if the model distribution itself already is asymmetric.</p> <p>We therefore adopt a slightly different approach. We replace \\(\\mathscr{P}\\) by the convex subset</p> \\[ \\mathscr{P}_{0}=\\{F \\in \\mathscr{P} \\mid T(F)=0\\} \\] <p>This enforces (6.6) and eliminates the explicit dependence of (6.4) on \\(T(F)\\). Moreover, it leads to a \"cleaner\" problem; we do not have to worry about the asymptotic bias of \\(T\\left(F_{n}\\right)\\) while investigating its asymptotic variance on \\(\\mathscr{P}_{0}\\). Clearly, the behavior of \\(T(F)\\) and \\(A(F, T)\\) on \\(\\mathscr{P} \\backslash \\mathscr{P}_{0}\\) still must be checked separately (see Section 4.9).</p> <p>According to Lemma 4.4, \\(1 / A(F, T)\\) is a convex function of \\(F \\in \\mathscr{P}_{0}\\). Let \\(F_{t}=(1-t) F_{0}+t F_{1}\\) with \\(F_{1} \\in \\mathscr{P}_{0} \\cap \\mathscr{P}_{1}\\), where \\(\\mathscr{P}_{1}\\) is the subset of \\(\\mathscr{P}\\) consisting of distributions with finite Fisher information (cf. Section 4.5). Then an explicit calculation and a comparison with (5.1) and (5.2) gives</p> \\[ \\begin{aligned} {\\left[\\frac{d}{d t} \\frac{1}{A(F, T)}\\right]_{t=0} } &amp; =\\int\\left[2 \\psi^{\\prime}(x)-\\psi(x)^{2}\\right]\\left[F_{1}(d x)-F_{0}(d x)\\right] \\\\ &amp; =\\left[\\frac{d}{d t} I\\left(F_{t}\\right)\\right]_{t=0} \\geqslant 0 \\end{aligned} \\] <p>It follows from the convexity of \\(1 / A(F, T)\\) that</p> \\[ A(F, T) \\leqslant A\\left(F_{0}, T\\right), \\quad \\text { for all } F \\in \\mathscr{P}_{0} \\cap \\mathscr{P}_{1} \\] <p>In other words the maximum likelihood estimate for location based on the least informative \\(F_{0}\\) minimizes the maximum asymptotic variance for alternatives in \\(\\mathscr{P}_{0} \\cap \\mathscr{P}_{1}\\). If \\(\\mathscr{P}_{1}\\) is dense in \\(\\mathscr{P}\\), the estimate usually is minimax for the whole of \\(\\mathscr{P}_{0}\\), but each case seems to need a separate investigation.</p> <p>For instance, take the case of Example 5.2, assuming that \\((-\\log g)^{\\prime \\prime}\\) is continuous. We rely heavily on the asymptotic normality proof given in Section 3.2.</p> <p>First, it is evident that</p> \\[ \\int \\psi(x)^{2} F(d x) \\leqslant \\int \\psi(x)^{2} F_{0}(d x), \\quad \\text { for all } F \\in \\mathscr{P}_{0} \\] <p>since \\(F_{0}\\) puts all contamination on the maximum of \\(\\psi^{2}\\). Some difficulties arise with</p> \\[ \\lambda(t, F)=\\int \\psi(x-t) F(d x) \\] <p>since it may fail to have a derivative. To see what is going on, put \\(u_{i}=(-\\log g)^{\\prime \\prime}\\left(x_{i}\\right), i=0,1\\), with \\(x_{i}\\) as in Example 5.2. If \\(F\\) puts pointmasses \\(\\varepsilon_{i}\\) at \\(x_{i}\\), then a straightforward calculation shows that \\(\\lambda(\\cdot, F)\\) still has (possibly different) one-sided derivatives at \\(t=0\\); in fact</p> \\[ \\lambda^{\\prime}(+0 ; F)-\\lambda^{\\prime}(-0 ; F)=\\varepsilon_{0} u_{0}-\\varepsilon_{1} u_{1} \\] <p>In any case we have</p> \\[ -\\lambda^{\\prime}( \\pm 0 ; F) \\geqslant-\\lambda^{\\prime}\\left(0 ; F_{0}\\right)&gt;0 \\] <p>for all \\(F \\in \\mathscr{P}_{0}\\). Theorem 3.2.4 remains valid; a closer look at the limiting distribution of \\(\\sqrt{n} T\\left(F_{n}\\right)\\) shows that it is no longer normal, but pieced together from the right half of a normal distribution whose variance (6.4) is determined by the right derivative of \\(\\lambda\\), and from the left half of a normal distribution whose variance is determined by the left derivative of \\(\\lambda\\).</p> <p>But (6.10) and (6.12) together imply that, nevertheless,</p> \\[ A(F ; T) \\leqslant A\\left(F_{0} ; T\\right) \\] <p>even if \\(A(F ; T)\\) now may have different values on the left- and the right-hand sides of the median of the distribution of \\(\\sqrt{n} T\\left(F_{n}\\right)\\). Moreover,</p> <p>there is enough uniformity in the convergence of (3.2.29) to imply</p> \\[ v(\\varepsilon)=v_{1}(\\varepsilon)=A\\left(F_{0} ; T\\right) \\] <p>(see Section 1.4) when \\(F\\) varies over \\(\\mathscr{R}_{0}\\). Remark An interesting limiting case. Consider the general \\(\\varepsilon\\)-contaminated case of Example 5.2, and let \\(\\varepsilon \\rightarrow 1\\). Then \\(k \\rightarrow 0\\) and \\(f_{0} \\rightarrow 0\\), so there is no proper limiting distribution. But the asymptotically efficient \\(M\\)-estimate for \\(F_{0}\\) tends to a nontrivial limit, namely, apart from an additive constant, to the sample median. This may be seen as follows: \\(\\psi\\) can be multiplied by a constant, without changing the estimate, and in particular</p> \\[ \\begin{aligned} \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{k} \\psi(x) &amp; =-1, &amp; &amp; \\text { for } x&lt;x^{*} \\\\ &amp; =1, &amp; &amp; \\text { for } x&gt;x^{*} \\end{aligned} \\] <p>where \\(x^{*}\\) is defined by \\(g^{\\prime}\\left(x^{*}\\right) / g\\left(x^{*}\\right)=0\\). Hence the limiting estimate is determined as the solution of</p> \\[ \\sum_{i=1}^{n} \\operatorname{sign}\\left(x_{i}-x^{*}-T_{n}\\right)=0 \\] <p>and thus</p> \\[ T_{n}=\\operatorname{med}\\left\\{x_{i}\\right\\}-x^{*} \\] <p>Example 6.1 Because of its importance, we single out the minimax \\(M\\)-estimate of location for the \\(\\varepsilon\\)-contaminated normal distribution. There, the least informative distribution is given by (5.20) and (5.21), and the estimate \\(T_{n}\\) is defined by</p> \\[ \\sum \\psi\\left(x_{i}-T_{n}\\right)=0 \\] <p>with \\(\\psi\\) given by (5.22).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#47-on-the-minimax-property-for-l-and-r-estimates","title":"4.7 ON THE MINIMAX PROPERTY FOR \\(L\\) - AND \\(R\\)-ESTIMATES","text":"<p>For \\(L\\) - and \\(R\\)-estimates \\(1 / A(F ; T)\\) is no longer a convex function of \\(F\\). Although (6.8) still holds [this is shown either by explicit calculation, or it</p> <p>can also be inferred on general grounds from the remark that \\(I(F)=\\) \\(\\sup _{T} 1 / A(F, T)\\), with \\(T\\) ranging over either class of estimates] we can no longer conclude that the asymptotically efficient estimate for \\(F_{0}\\) is asymptotically minimax, even if we restrict \\(\\varphi\\) to symmetric and smooth distributions. In fact Sacks and Ylvisaker (1972) have constructed counterexamples. However, in the important Example 5.2 ( \\(\\varepsilon\\)-contamination), the conclusion is true (Jaeckel 1971a). We assume throughout that all distributions are symmetric.</p> <p>Consider first the case of \\(L\\)-estimates, where the efficient one (cf. Section 3.5) is characterized by the weight density</p> \\[ \\begin{aligned} \\mathrm{m}\\left(\\mathrm{~F}_{0}(\\mathrm{x})\\right) &amp; =-\\left(\\frac{g^{\\prime}(x)}{g(x)}\\right)^{\\prime} \\frac{1}{I\\left(F_{0}\\right)} \\geqslant 0, &amp; &amp; \\text { for }|\\mathrm{x}| \\leqslant \\mathrm{x}_{1}=-\\mathrm{x}_{0} \\\\ &amp; =0, &amp; &amp; \\text { otherwise } \\end{aligned} \\] <p>with \\(g\\) as in Example 5.2. The influence function is skew symmetric, and, for \\(x \\geqslant 0\\), it satisfies</p> \\[ I C(x ; F, T)=\\int_{0}^{x} m(F(y)) d y \\] <p>or, for \\(\\frac{1}{2} \\leqslant t&lt;1\\),</p> \\[ I C\\left(F^{-1}(t) ; F, T\\right)=\\int_{1 / 2}^{t} \\frac{m(s)}{f\\left(F^{-1}(s)\\right)} d s \\] <p>We have</p> \\[ F(x) \\geqslant F_{0}(x), \\quad \\text { for } 0 \\leqslant x \\leqslant x_{1} \\] <p>and</p> \\[ F^{-1}(t) \\leqslant F_{0}^{-1}(t), \\quad \\text { for } \\frac{1}{2} \\leqslant t \\leqslant F_{0}\\left(x_{1}\\right) \\] <p>Thus for \\(\\frac{1}{2} \\leqslant t \\leqslant F_{0}\\left(x_{1}\\right)\\),</p> \\[ \\begin{aligned} I C\\left(F^{-1}(t) ; F, T\\right) &amp; =\\int_{1 / 2}^{t} \\frac{m(s)}{f\\left(F^{-1}(s)\\right)} d s \\\\ &amp; \\leqslant \\int_{1 / 2}^{t} \\frac{m(s)}{f_{0}\\left(F^{-1}(s)\\right)} d s \\leqslant \\int_{1 / 2}^{t} \\frac{m(s)}{f_{0}\\left(F_{0}^{-1}(s)\\right)} d s \\\\ &amp; =I C\\left(F_{0}^{-1}(t) ; F, T\\right) \\end{aligned} \\] <p>Since \\(I C\\left(F^{-1}(t) ; F, T\\right)\\) is constant for \\(F_{0}\\left(x_{1}\\right) \\leqslant t \\leqslant 1\\), and as</p> \\[ A(F, T)=2 \\int_{1 / 2}^{1} I C\\left(F^{-1}(t) ; F, T\\right)^{2} d t \\] <p>it follows that \\(A(F, T) \\leqslant A\\left(F_{0}, T\\right)\\); hence the minimax property holds. Now consider the \\(R\\)-estimate. The optimal scores function \\(J(t)\\) is given by</p> \\[ J\\left(F_{0}(x)\\right)=-\\frac{f_{0}^{\\prime}(x)}{f_{0}(x)} \\] <p>The value of the influence function at \\(x=F^{-1}(t)\\) is</p> \\[ I C\\left(F^{-1}(t) ; F, t\\right)=\\frac{J(t)}{\\int J^{\\prime}(F(x)) f(x)^{2} d x}=\\frac{J(t)}{\\int J^{\\prime}(s) f\\left(F^{-1}(s)\\right) d s} \\] <p>Since \\(J^{\\prime}(t)=0\\) outside of the interval \\(\\left(F_{0}\\left(x_{0}\\right), F_{0}\\left(x_{1}\\right)\\right)\\), and since in this interval</p> \\[ f\\left(F^{-1}(t)\\right) \\geqslant f_{0}\\left(F^{-1}(t)\\right) \\geqslant f_{0}\\left(F_{0}^{-1}(t)\\right) \\] <p>we conclude that, for \\(t \\geqslant \\frac{1}{2}\\),</p> \\[ I C\\left(F^{-1}(t) ; F, T\\right) \\leqslant I C\\left(F_{0}^{-1}(t) ; F_{0}, T\\right) \\] <p>hence, as above,</p> \\[ A(F, T) \\leqslant A\\left(F_{0}, T\\right) \\] <p>and the minimax property holds. Example 7.1 In the \\(\\varepsilon\\)-contaminated normal case, the least informative distribution \\(F_{0}\\) is given by (5.20) and (5.21), and all of the following three estimates are asymptotically minimax: (1) The \\(M\\)-estimate with \\(\\psi\\) given by (5.22). (2) The \\(\\alpha\\)-trimmed mean with \\(\\alpha=F_{0}(-k)=(1-\\varepsilon) \\Phi(-k)+\\varepsilon / 2\\). (3) The \\(R\\)-estimate defined through the scores generating function \\(J(t)=\\psi\\left(F_{0}^{-1}(t)\\right)\\), that is,</p> \\[ \\begin{array}{rlrl} J(t) &amp; =-k, &amp; &amp; \\text { for } t \\leqslant \\alpha \\\\ &amp; =\\Phi^{-1}\\left(\\frac{t-\\varepsilon / 2}{1-\\varepsilon}\\right), &amp; &amp; \\text { for } \\alpha \\leqslant t \\leqslant 1-\\alpha \\\\ &amp; =k, &amp; &amp; \\text { for } t \\geqslant 1-\\alpha . \\end{array} \\]"},{"location":"research/note/lecture-note-automation/result/ocr_result/#48-descending-m-estimates","title":"4.8 DESCENDING M-ESTIMATES","text":"<p>We have already noted that the least informative distributions tend to have exponential tails, that is, they might be slimmer (!) than what we would expect in practice. So it might be worthwhile to increase the maximum risk slightly beyond its minimax value in order to gain a better performance at very long-tailed distributions.</p> <p>This can be done as follows. Consider \\(M\\)-estimates, and minimize the maximal asymptotic variance subject to the side condition</p> \\[ \\psi(x)=0, \\quad \\text { for }|x|&gt;c \\] <p>where \\(c\\) can be chosen arbitrarily. For \\(\\varepsilon\\)-contaminated normal distributions, the solution is of the following form:</p> \\[ \\begin{aligned} \\psi(x) &amp; =-\\psi(-x)=x, &amp; &amp; \\text { for } 0 \\leqslant x \\leqslant a \\\\ &amp; =b \\tanh \\left[\\frac{1}{2} b(c-x)\\right], &amp; &amp; \\text { for } a \\leqslant x \\leqslant c \\\\ &amp; =0, &amp; &amp; \\text { for } x \\geqslant c \\end{aligned} \\] <p>see Exhibit 4.8.1. The values of \\(a\\) and \\(b\\), of course, depend on \\(\\varepsilon\\). The above estimate is a maximum likelihood estimate based on a truncated sample, for an underlying density</p> \\[ \\begin{aligned} f_{0}(x) &amp; =f_{0}(-x)=(1-\\varepsilon) \\varphi(x), &amp; &amp; \\text { for } 0 \\leqslant x \\leqslant a \\\\ &amp; =\\frac{(1-\\varepsilon) \\varphi(a)}{\\cosh ^{2}\\left[\\frac{1}{2} b(c-a)\\right]} \\cosh ^{2}\\left[\\frac{1}{2} b(c-x)\\right], &amp; &amp; \\text { for } a \\leqslant x \\leqslant c \\\\ &amp; =(1-\\varepsilon) \\varphi(x), &amp; &amp; \\text { for } x \\geqslant c \\end{aligned} \\] <p>Note that this density is discontinuous at \\(\\pm c\\). In order that \\(f_{0}\\) integrate to 1 , we must have</p> \\[ 2 \\int_{a}^{c}\\left[f_{0}(x)-(1-\\varepsilon) \\varphi(x)\\right] d x=\\varepsilon \\] <p>this gives one relation between \\(\\varepsilon\\) and \\(a, b\\); the other one is continuity of \\(\\psi\\) at \\(a\\) :</p> \\[ a=b \\tanh \\left[\\frac{1}{2} b(c-a)\\right] \\] <p></p> <p>Exhibit 4.8.1</p> <p>This solution can be found by essentially the same variational methods as used in Section 4.5; for a given \\(F\\) the best choice of \\(\\psi\\) is</p> \\[ \\begin{aligned} \\psi_{F}(x) &amp; =-\\frac{f^{\\prime}(x)}{f(x)}, &amp; &amp; \\text { for }|x|&lt;c \\\\ &amp; =0, &amp; &amp; \\text { otherwise } \\end{aligned} \\] <p>and the corresponding asymptotic variance is \\(1 / I_{c}(F)\\), with</p> \\[ I_{c}(F)=\\int \\psi_{F}^{2} d F \\] <p>compare Section 6.3. Now minimize \\(I_{c}(F)\\); the variational conditions imply that \\(-4 \\sqrt{f_{0}}{ }^{\\prime \\prime} / \\sqrt{f_{0}}=\\) const. on the set where \\(f_{0}(x)&gt;(1-\\varepsilon) \\varphi(x)\\), and that \\(\\psi_{F_{0}}( \\pm c)=0\\). This yields (8.2) to (8.5), and it only remains to check that this indeed is a solution. For details see Collins (1976).</p> \\(c\\) \\(b\\) \\(a\\) \\(1 / I_{c}\\left(F_{0}\\right)\\) \\(\\varepsilon=0.01\\) 2 2.747 1.539 1.727 3 2.451 2.032 1.166 4 2.123 2.055 1.082 5 1.991 1.982 1.068 \\(\\infty\\) 1.945 1.945 1.065 \\(\\varepsilon=0.05\\) 2 1.714 1.105 2.640 3 1.693 1.460 1.503 4 1.550 1.488 1.314 5 1.461 1.445 1.271 \\(\\infty\\) 1.399 1.399 1.256 \\(\\varepsilon=0.10\\) 2 1.307 0.838 4.129 3 1.376 1.171 1.963 4 1.289 1.220 1.621 5 1.217 1.194 1.532 \\(\\infty\\) 1.140 1.140 1.490 \\(\\varepsilon=0.25\\) 2 0.692 0.356 21.741 3 0.912 0.711 4.575 4 0.905 0.810 3.089 5 0.865 0.820 2.683 \\(\\infty\\) 0.766 0.766 2.397 <p>Exhibit 4.8.2 The minimax descending \\(M\\)-estimate [cf. (8.2) and (8.3)].</p> <p>Exhibit 4.8.2 shows some of the quantitative aspects. The last column gives the maximal risk \\(1 / I_{c}\\left(F_{0}\\right)\\). Clearly, a choice \\(c \\geqslant 5\\) will increase it only by a negligible amount beyond its minimax value \\((c=\\infty)\\), but a choice \\(c&lt;3\\) may have quite poor consequences. In other words it appears that descending \\(\\psi\\)-functions are much more sensitive to wrong scaling than monotone ones.</p> <p>The actual performance of such an estimate does not seem to depend very much on the exact shape of \\(\\psi\\). Other proposals for redescending \\(M\\)-estimates have been Hampel's piecewise linear function:</p> \\[ \\begin{aligned} \\psi(x)=-\\psi(-x) &amp; =x, &amp; &amp; \\text { for } 0 \\leqslant x \\leqslant a \\\\ &amp; =a, &amp; &amp; \\text { for } a \\leqslant x&lt;b \\\\ &amp; =\\frac{c-x}{c-b} a, &amp; &amp; \\text { for } b \\leqslant x&lt;c \\\\ &amp; =0, &amp; &amp; \\text { for } x \\geqslant c \\end{aligned} \\] <p>Andrews' sine wave:</p> \\[ \\begin{aligned} \\psi(x) &amp; =\\sin (x), &amp; &amp; \\text { for }-\\pi \\leqslant x \\leqslant \\pi \\\\ &amp; =0, &amp; &amp; \\text { otherwise } \\end{aligned} \\] <p>and Tukey's biweight:</p> \\[ \\begin{aligned} \\psi(x) &amp; =x\\left(1-x^{2}\\right)^{2}, &amp; &amp; \\text { for }|x| \\leqslant 1 \\\\ &amp; =0, &amp; &amp; \\text { otherwise. } \\end{aligned} \\] <p>Compare Andrews et al. (1972) and Exhibit 4.8.1. When choosing a redescending \\(\\psi\\), we must take care that it does not descend too steeply; if it does contamination sitting on the slopes may play havoc with the denominator in the expression for the asymptotic variance</p> \\[ A(F, T)=\\frac{\\int \\psi^{2} d F}{\\left(\\int \\psi^{\\prime} d F\\right)^{2}} \\] <p>This effect is particularly harmful when a large negative value \\(\\psi^{\\prime}(x)\\) combines with a large positive value \\(\\psi(x)^{2}\\), and there is a cluster of outliers near \\(x\\). (Some people have used quite dangerous Hampel estimates in their computer programs, with slopes between \\(b\\) and \\(c\\) that are much too steep.)</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#a-word-of-caution","title":"A Word of Caution","text":"<p>It seems to me that, in some discussions, the importance of using redescending \\(\\psi\\)-functions has been exaggerated beyond all proportion. They are certainly beneficial if there are extreme outliers, but the improvement is relatively minor (a few percent of the asymptotic variance) and is counterbalanced by an increase of the minimax risk. If we are really interested in these few percentage points of potential improvement, then a removal of the \"impossible\" data points through a careful data screening based on physical expertise might be more effective and less risky than the routine use of a poorly tuned redescending \\(\\psi\\). Note, in particular, the increased sensitivity to a wrong scale. Unless we are careful we may even get trapped in a local minimum of \\(\\sum \\rho\\left(x_{i}-T_{n}\\right)\\). The situation gets particularly acute in multiparameter regression.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#49-questions-of-asymmetric-contamination","title":"4.9 QUESTIONS OF ASYMMETRIC CONTAMINATION","text":"<p>In the preceding sections we have determined estimates minimizing the maximal asymptotic variance over some subset of \\(\\mathscr{P}=\\mathscr{P}_{\\varepsilon}\\); only symmetric \\(F\\), or, slightly more generally, only those \\(F \\in \\mathscr{P}\\) were admitted whose bias for the selected estimate was zero, \\(T(F)=0\\). We now check the behavior of these estimates over the rest of \\(\\mathscr{P}_{\\varepsilon}\\).</p> <p>We have to answer two questions: (1) How large is the maximal asymptotic bias \\(b(\\varepsilon)\\) and how does it compare to the bias of the median (which is minimax, Section 4.2)? (2) How large is the maximal asymptotic variance \\(v_{\\alpha}(\\varepsilon)\\) when \\(F\\) ranges over all of \\(\\mathscr{P}_{\\varepsilon}\\), and how does it compare to the restricted maximal asymptotic variance \\(v_{\\varepsilon}(\\varepsilon)\\), where \\(F\\) ranges only over the symmetric \\(F \\in \\mathscr{P}_{\\varepsilon}\\) ?</p> <p>The discussion of breakdown properties (Sections 3.2 to 3.4) suggests that \\(L\\)-estimates are more sensitive to asymmetries than either \\(M\\) - or \\(R\\)-estimates. We therefore restrict ourselves to \\(\\alpha\\)-trimmed means and \\(\\varepsilon\\)-contaminated normal distributions.</p> <p>For small \\(\\varepsilon\\) we have [see (1.5.8)]</p> \\[ b(\\varepsilon) \\cong \\varepsilon \\sup _{x}|I C(x ; \\Phi, T)| \\] <p>We thus tabulate \\(b(\\varepsilon) / \\varepsilon\\) in order to obtain more nearly constant numbers; see Exhibit 4.9.1. The bottom row \\((\\alpha=0.5)\\) corresponds to the median.</p> \\(\\varepsilon\\) \\(\\alpha\\) 0 0.01 0.02 0.05 0.1 0.15 0.2 0.25 0.3 0.4 0.5 0.01 2.37 2.71 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.02 2.14 2.26 2.51 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.05 1.83 1.88 1.94 2.27 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.1 1.60 1.63 1.66 1.78 2.13 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.15 1.48 1.50 1.53 1.60 1.77 2.10 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.2 1.40 1.42 1.44 1.50 1.63 1.80 2.12 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.25 1.35 1.37 1.38 1.44 1.54 1.67 1.85 2.18 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.3 1.31 1.33 1.34 1.39 1.48 1.59 1.73 1.93 2.29 \\(\\infty\\) \\(\\infty\\) 0.4 1.26 1.28 1.29 1.33 1.41 1.51 1.62 1.76 1.95 2.73 \\(\\infty\\) 0.5 1.25 1.26 1.28 1.32 1.39 1.48 1.59 1.72 1.89 2.42 \\(\\infty\\) <p>Exhibit 4.9.1 Maximal bias of \\(\\alpha\\)-trimmed means for \\(\\varepsilon\\)-contaminated normal distributions (tabulated: \\(b(\\varepsilon) / \\varepsilon\\) ).</p> \\(\\varepsilon\\) \\(\\alpha\\) 0 0.01 0.02 0.05 0.1 0.15 0.2 0.25 0.3 0.4 0.5 0.01 \\(\\begin{gathered} 1.004 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.08 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) 0.02 \\(\\begin{gathered} 1.009 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.07 \\\\ 1.0065 \\end{gathered}\\) \\(\\begin{gathered} 1.14 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) 0.05 \\(\\begin{gathered} 1.027 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.07 \\\\ 1.0017 \\end{gathered}\\) \\(\\begin{gathered} 1.12 \\\\ 1.0084 \\end{gathered}\\) \\(\\begin{gathered} 1.30 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) 0.1 \\(\\begin{gathered} 1.061 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.09 \\\\ 1.0007 \\end{gathered}\\) \\(\\begin{gathered} 1.13 \\\\ 1.0031 \\end{gathered}\\) \\(\\begin{gathered} 1.26 \\\\ 1.027 \\end{gathered}\\) \\(\\begin{gathered} 1.54 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 2.03 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) 0.15 \\(\\begin{gathered} 1.100 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.13 \\\\ 1.0004 \\end{gathered}\\) \\(\\begin{gathered} 1.16 \\\\ 1.0019 \\end{gathered}\\) \\(\\begin{gathered} 1.27 \\\\ 1.014 \\end{gathered}\\) \\(\\begin{gathered} 1.49 \\\\ 1.08 \\end{gathered}\\) \\(\\begin{gathered} 1.80 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 2.25 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 3.07 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) 0.2 \\(\\begin{gathered} 1.144 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.17 \\\\ 1.0003 \\end{gathered}\\) \\(\\begin{gathered} 1.20 \\\\ 1.0013 \\end{gathered}\\) \\(\\begin{gathered} 1.30 \\\\ 1.010 \\end{gathered}\\) \\(\\begin{gathered} 1.50 \\\\ 1.05 \\end{gathered}\\) \\(\\begin{gathered} 1.75 \\\\ 1.18 \\end{gathered}\\) \\(\\begin{gathered} 2.08 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 2.56 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 3.28 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) 0.25 \\(\\begin{gathered} 1.195 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.22 \\\\ 1.0003 \\end{gathered}\\) \\(\\begin{gathered} 1.25 \\\\ 1.0010 \\end{gathered}\\) \\(\\begin{gathered} 1.35 \\\\ 1.007 \\end{gathered}\\) \\(\\begin{gathered} 1.53 \\\\ 1.04 \\end{gathered}\\) \\(\\begin{gathered} 1.76 \\\\ 1.11 \\end{gathered}\\) \\(\\begin{gathered} 2.05 \\\\ 1.29 \\end{gathered}\\) \\(\\begin{gathered} 2.42 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 2.93 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 4.81 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} \\infty \\\\ \\infty \\end{gathered}\\) 0.3 \\(\\begin{gathered} 1.252 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.28 \\\\ 1.0002 \\end{gathered}\\) \\(\\begin{gathered} 1.31 \\\\ 1.0009 \\end{gathered}\\) \\(\\begin{gathered} 1.40 \\\\ 1.006 \\end{gathered}\\) \\(\\begin{gathered} 1.58 \\\\ 1.03 \\end{gathered}\\) \\(\\begin{gathered} 1.79 \\\\ 1.08 \\end{gathered}\\) \\(\\begin{gathered} 2.06 \\\\ 1.18 \\end{gathered}\\) \\(\\begin{gathered} 2.40 \\\\ 1.45 \\end{gathered}\\) \\(\\begin{gathered} 2.83 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 4.20 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 7.26 \\\\ \\infty \\end{gathered}\\) 0.4 \\(\\begin{gathered} 1.393 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.42 \\\\ 1.0002 \\end{gathered}\\) \\(\\begin{gathered} 1.45 \\\\ 1.0007 \\end{gathered}\\) \\(\\begin{gathered} 1.55 \\\\ 1.005 \\end{gathered}\\) \\(\\begin{gathered} 1.73 \\\\ 1.02 \\end{gathered}\\) \\(\\begin{gathered} 1.94 \\\\ 1.06 \\end{gathered}\\) \\(\\begin{gathered} 2.20 \\\\ 1.12 \\end{gathered}\\) \\(\\begin{gathered} 2.51 \\\\ 1.24 \\end{gathered}\\) \\(\\begin{gathered} 2.90 \\\\ 1.47 \\end{gathered}\\) \\(\\begin{gathered} 4.01 \\\\ \\infty \\end{gathered}\\) \\(\\begin{gathered} 5.94 \\\\ \\infty \\end{gathered}\\) 0.5 \\(\\begin{gathered} 1.571 \\\\ 1 \\end{gathered}\\) \\(\\begin{gathered} 1.60 \\\\ 1.0002 \\end{gathered}\\) \\(\\begin{gathered} 1.64 \\\\ 1.0007 \\end{gathered}\\) \\(\\begin{gathered} 1.74 \\\\ 1.004 \\end{gathered}\\) \\(\\begin{gathered} 1.94 \\\\ 1.02 \\end{gathered}\\) \\(\\begin{gathered} 2.17 \\\\ 1.05 \\end{gathered}\\) \\(\\begin{gathered} 2.45 \\\\ 1.11 \\end{gathered}\\) \\(\\begin{gathered} 2.79 \\\\ 1.20 \\end{gathered}\\) \\(\\begin{gathered} 3.21 \\\\ 1.38 \\end{gathered}\\) \\(\\begin{gathered} 4.36 \\\\ 2.55 \\end{gathered}\\) \\(\\begin{gathered} 6.28 \\\\ \\infty \\end{gathered}\\) Minimax  bound for \\(v_{s}\\) \\(\\begin{gathered} 1.000 \\\\ 1.000 \\end{gathered}\\) \\(\\begin{gathered} 1.065 \\\\ 1.065 \\end{gathered}\\) \\(\\begin{gathered} 1.116 \\\\ 1.116 \\end{gathered}\\) \\(\\begin{gathered} 1.256 \\\\ 1.256 \\end{gathered}\\) \\(\\begin{gathered} 1.490 \\\\ 1.490 \\end{gathered}\\) \\(\\begin{gathered} 1.748 \\\\ 1.748 \\end{gathered}\\) \\(\\begin{gathered} 2.046 \\\\ 2.046 \\end{gathered}\\) \\(\\begin{gathered} 2.397 \\\\ 2.397 \\end{gathered}\\) \\(\\begin{gathered} 2.822 \\\\ 2.822 \\end{gathered}\\) \\(\\begin{gathered} 3.996 \\\\ 3.996 \\end{gathered}\\) \\(\\begin{gathered} 5.928 \\\\ \\text { 5.928 } \\end{gathered}\\) <p>Exhibit 4.9.2 Maximal symmetric and asymmetric variance of \\(\\alpha\\)-trimmed means for \\(\\varepsilon\\)-contaminated normal distributions [tabulated: \\(v_{s}(\\varepsilon), v_{\\alpha}(\\varepsilon) / v_{s}(\\varepsilon)\\) ].</p> <p>Exhibit 4.9.2 is concerned with asymptotic variances; it tabulates \\(v_{s}(\\varepsilon)\\) and \\(v_{\\alpha}(\\varepsilon) / v_{s}(\\varepsilon)\\) (cf. the second question above).</p> <p>For the \\(\\alpha\\)-trimmed mean, asymptotic bias and variance apparently are maximized if the entire contaminating mass \\(\\varepsilon\\) is put at \\(+\\infty\\). This is trivially true for the bias, and highly plausible (but not yet proved) for the variance. Calculating Exhibits 4.9.1 and 4.9.2 is, by the way, an instructive exercise in the use of several formulas derived in Section 3.3.</p> <p>The following features deserve some comments. Note that \\(b(\\varepsilon) / \\varepsilon\\) increases only very slowly with \\(\\varepsilon\\) and in fact stays bounded right up to the breakdown point.</p> <p>For small \\(\\varepsilon\\) the excess of \\(v_{a}\\) beyond \\(v_{s}\\) is negligible (it is of the order \\(\\varepsilon^{2}\\) ). This gives an a posteriori justification for restricting attention to symmetric distributions when minimizing asymptotic variances. For larger \\(\\varepsilon\\), however, the discrepancies can become sizeable. Take \\(\\varepsilon=0.2\\), and the \\(25 \\%\\)-trimmed mean, which is very nearly minimax there for symmetric contamination; then \\(v_{a} / v_{s} \\cong 1.29\\).</p> <p>Exhibits 4.9.1 and 4.9.2 also illustrate the two breakdown points \\(\\varepsilon^{*}\\) and \\(\\varepsilon^{* *}\\), defined in Section 1.4: \\(b(\\varepsilon)=\\infty\\) for \\(\\varepsilon&gt;\\varepsilon^{*}=\\alpha\\), and \\(v_{s}(\\varepsilon)=\\infty\\) for \\(\\varepsilon \\geqslant \\varepsilon^{* *}=2 \\alpha\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-5","title":"CHAPTER 5","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#scale-estimates","title":"Scale Estimates","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#51-general-remarks","title":"5.1 GENERAL REMARKS","text":"<p>By scale estimate we denote any positive statistic \\(S_{n}\\) that is equivariant under scale transformations:</p> \\[ S_{n}\\left(a x_{1}, \\ldots, a x_{n}\\right)=a S_{n}\\left(x_{1}, \\ldots, x_{n}\\right), \\quad \\text { for } \\quad a&gt;0 \\] <p>Many scale estimates are also invariant under changes of sign and shifts:</p> \\[ \\begin{aligned} S_{n}\\left(-x_{1}, \\ldots,-x_{n}\\right) &amp; =S_{n}\\left(x_{1}, \\ldots, x_{n}\\right) \\\\ S_{n}\\left(x_{1}+b, \\ldots, x_{n}+b\\right) &amp; =S_{n}\\left(x_{1}, \\ldots, x_{n}\\right) \\end{aligned} \\] <p>Pure scale problems are rare; in practice scale usually occurs as a nuisance parameter in location, and, more generally, in regression problems. Therefore we should tune the properties of the scale estimate to that of the location estimate to which it is subordinated. For instance, we would not want to spoil the good breakdown properties of a location estimate by an early breakdown of the scale estimate. For related reasons it appears to be more important to keep the bias of the scale estimate small than to strive for a small (asymptotic) variance.</p> <p>As a result the so-called median absolute deviation (MAD) has emerged as the single most useful ancillary estimate of scale. It is defined as the median of the absolute deviations from the median:</p> \\[ \\mathrm{MAD}_{n}=\\operatorname{med}\\left\\{\\left|x_{i}-M_{n}\\right|\\right\\} \\] <p>where</p> \\[ M_{n}=\\operatorname{med}\\left\\{x_{i}\\right\\} \\] <p>For symmetric distributions this is asymptotically equivalent to one-half of the interquartile distance, but it has better breakdown properties under \\(\\varepsilon\\)-contamination ( \\(\\varepsilon^{*}=0.5\\), as against \\(\\varepsilon^{*}=0.25\\) for the interquartile distance).</p> <p>Note that this clashes with the widespread opinion that, because most of the information for scale sits in the tails, we should give more consideration to the tails, and thus use a lower rejection or trimming rate in scale problems. This may be true for the pure scale problem, but is not so when scale is just a nuisance parameter.</p> <p>The pure scale problem is a kind of stepping stone toward more complex estimation problems. It has the advantage that it can be converted into a location problem by taking logarithms, so the machinery of the preceding chapters is applicable. But the distributions resulting from this transformation are highly asymmetric, and there is no natural scale (corresponding to the center of symmetry). In most cases it is convenient to standardize the estimates such that they are consistent at the ideal model distribution (cf. the remarks at the end of Section 1.2). For instance, in order to make MAD consistent at the normal distribution, we must divide it by \\(\\Phi^{-1}\\left(\\frac{3}{4}\\right) \\cong\\) 0.6745 .</p> <p>This chapter closely follows and parallels many sections of the preceding two chapters; we again concentrate on estimates that are functionals of the empirical distribution function, \\(S_{n}=S\\left(F_{n}\\right)\\), and we again exploit the heuristic approach through influence functions.</p> <p>As the asymptotic variance \\(A(F, S)\\) of \\(\\sqrt{n}\\left[S\\left(F_{n}\\right)-S(F)\\right]\\) depends on the arbitrary standardization of \\(S\\), it is a poor measure of asymptotic performance. We use the relative asymptotic variance of \\(S\\) instead, that is, the asymptotic variance</p> \\[ A(F, \\log S)=\\frac{A(F, S)}{S(F)^{2}} \\] <p>of</p> \\[ \\sqrt{n} \\log \\frac{S\\left(F_{n}\\right)}{S(F)} \\] <p>Another important scale-type problem concerns the estimation of the variability of a given estimate; we have briefly touched upon this topic in Section 1.5. In the classical normal theory, the two cases are often confounded-after all, the classical estimates for the standard error of a single observation and of the sample mean differ only by a factor \\(\\sqrt{n}\\)-but we must keep them conceptually separate. We defer the discussion of this kind of problem till the end of Chapter 6.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#52-m-estimates-of-scale","title":"5.2 M-ESTIMATES OF SCALE","text":"<p>An \\(M\\)-estimate \\(S\\) of scale is defined by an implicit relation of the form</p> \\[ \\int \\chi\\left(\\frac{x}{S(F)}\\right) F(d x)=0 \\] <p>Typically (but not necessarily), \\(\\chi\\) is an even function \\(\\chi(-x)=\\chi(x)\\). From (3.2.13) we obtain the influence function</p> \\[ I C(x ; F, S)=\\frac{\\chi(x / S(F)) S(F)}{\\int \\chi^{\\prime}(x / S(F))(x / S(F)) F(d x)} \\] <p>Example 2.1 The maximum likelihood estimate of \\(\\sigma\\) for the scale family of densities \\(\\sigma^{-1} f(x / \\sigma)\\) is an \\(M\\)-estimate with</p> \\[ \\chi(x)=-x \\frac{f^{\\prime}(x)}{f(x)}-1 \\] <p>Example 2.2 Huber (1964) proposed the choice</p> \\[ \\begin{aligned} \\chi(x) &amp; =x^{2}-\\beta, &amp; &amp; \\text { for }|x| \\leqslant k \\\\ &amp; =k^{2}-\\beta, &amp; &amp; \\text { for }|x|&gt;k \\end{aligned} \\] <p>for some constant \\(k\\), with \\(\\beta\\) determined such that \\(S(\\Phi)=1\\), that is, \\(\\int \\chi(x) \\Phi(d x)=0\\).</p> <p>Example 2.3 The choice</p> \\[ \\chi(x)=\\operatorname{sign}(|x|-1) \\] <p>yields the median absolute deviation \\(S=\\operatorname{med}(|X|)\\), that is, that number \\(S\\) for which \\(F(S)-F(-S)=\\frac{1}{2}\\). (More precisely, this is the median absolute deviation from 0 , to be distinguished from the median absolute deviation from the median.)</p> <p>Continuity and breakdown properties can be worked out just as in the location case in Section 3.2, except that everything is slightly more complicated. Therefore we only show how the breakdown point under \\(\\varepsilon\\) contamination can be worked out.</p> <p>Assume that \\(\\chi\\) is even and monotone increasing for positive arguments. Let \\(\\|\\chi\\|=\\chi(\\infty)-\\chi(0)\\). We write (2.1) as</p> \\[ \\int\\left[\\chi\\left(\\frac{x}{S(F)}\\right)-\\chi(0)\\right] F(d x)+\\chi(0)=0 \\] <p>Assuming the gross error model, it is easy to see that a contamination \\(\\varepsilon&gt;-\\chi(0) /\\|\\chi\\|\\) at \\(|x|=\\infty\\) forces the left-hand side of (2.6) to be greater than 0 for all values of \\(S(F)\\). Similarly, a contamination \\(\\varepsilon&gt;1+\\chi(0) /\\|\\chi\\|\\) at 0 forces it to be less than 0 for all values of \\(S(F)\\). [As \\(0&lt;-\\chi(0) /\\|\\chi\\| \\leqslant \\frac{1}{2}\\) in the more interesting cases, we can usually disregard the second contingency.] On the other hand if \\(\\varepsilon\\) satisfies the opposite strict inequalities, then the solution \\(S(F)\\) of (2.6) is bounded away from 0 and \\(\\infty\\).</p> <p>We conclude that, for \\(\\varepsilon\\)-contamination (and also for Prohorov distance), the breakdown point is given by</p> \\[ \\varepsilon^{*}=\\frac{-\\chi(0)}{\\|\\chi\\|} \\leqslant \\frac{1}{2} \\] <p>For indeterminacy in terms of Kolmogorov or L\u00e9vy distance, this number must be halved:</p> \\[ \\varepsilon^{*}=-\\frac{1}{2} \\frac{\\chi(0)}{\\|\\chi\\|} \\leqslant 0.25 \\] <p>The reason for this different behavior is as follows. By taking away a mass \\(\\varepsilon\\) from the central part of a distribution \\(F\\) and moving one-half of it to the extreme left, the other half to the extreme right, we get a distribution that is within Prohorov distance \\(\\varepsilon\\), but within L\u00e9vy distance \\(\\varepsilon / 2\\), of the original \\(F\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#53-l-estimates-of-scale","title":"5.3 L-ESTIMATES OF SCALE","text":"<p>The general results of Section 3.3 apply without much change. In view of scale invariance (1.1), only the following types of functionals appear feasible:</p> \\[ \\begin{array}{ll} S(F)=\\left[\\int F^{-1}(t)^{q} M(d t)\\right]^{1 / q}, &amp; \\text { with integral } q \\neq 0 \\\\ S(F)=\\left[\\int\\left|F^{-1}(t)\\right|^{q} M(d t)\\right]^{1 / q}, &amp; \\text { with real } q \\neq 0 \\\\ S(F)=\\exp \\left[\\int \\log \\left|F^{-1}(t)\\right| M(d t)\\right], &amp; \\text { with } M\\{(0,1)\\}=1 \\end{array} \\] <p>We encounter estimates of both the first type (interquantile range, trimmed variance) and the second type (median deviation), but in what follows now we consider only (3.1).</p> <p>From (3.3.11) and the chain rule, we obtain the influence function</p> \\[ I C(x ; F, S)=\\frac{1}{S(F)^{q-1}}\\left[\\int s \\frac{F^{-1}(s)^{q-1}}{f\\left(F^{-1}(s)\\right)} M(d s)-\\int_{F(x)}^{1} \\frac{F^{-1}(s)^{q-1}}{f\\left(F^{-1}(s)\\right)} M(d s)\\right] \\] <p>Or if \\(M\\) has a density \\(m\\), then</p> \\[ \\frac{d}{d x} I C(x ; F, S)=\\frac{x^{q-1}}{S(F)^{q-1}} m(F(x)) \\] <p>Example 3.1 The \\(t\\)-quantile range</p> \\[ S(F)=F^{-1}(1-t)-F^{-1}(t), \\quad 0&lt;t&lt;\\frac{1}{2} \\] <p>has the influence function</p> \\[ \\begin{aligned} I C(x ; F, S) &amp; =\\frac{1}{f\\left(F^{-1}(t)\\right)}-c(F), &amp; &amp; \\text { for } x&lt;F^{-1}(t) \\\\ &amp; =-c(F), &amp; &amp; \\text { for } F^{-1}(t)&lt;x&lt;F^{-1}(1-t) \\\\ &amp; =\\frac{1}{f\\left(F^{-1}(1-t)\\right)}-c(F), &amp; &amp; \\text { for } x&gt;F^{-1}(1-t) \\end{aligned} \\] <p>where</p> \\[ c(F)=t\\left(\\frac{1}{f\\left(F^{-1}(t)\\right)}+\\frac{1}{f\\left(F^{-1}(1-t)\\right)}\\right) \\] <p>If \\(F\\) is symmetric these formulas simplify to</p> \\[ \\begin{aligned} I C(x ; F, S) &amp; =\\frac{1-2 t}{f\\left(F^{-1}(t)\\right)}, &amp; &amp; \\text { for } x&lt;F^{-1}(t) \\text { or } x&gt;F^{-1}(1-t) \\\\ &amp; =\\frac{-2 t}{f\\left(F^{-1}(t)\\right)}, &amp; &amp; \\text { for } F^{-1}(t)&lt;x&lt;F^{-1}(1-t) \\end{aligned} \\] <p>Then the asymptotic variance of \\(\\sqrt{n}\\left[S\\left(F_{n}\\right)-S(F)\\right]\\) is given by</p> \\[ A(F, S)=\\frac{2 t(1-2 t)}{f\\left(F^{-1}(t)\\right)^{2}} \\] <p>and that of \\(\\sqrt{n} \\log \\left[S\\left(F_{n}\\right) / S(F)\\right]\\) is</p> \\[ A(F, \\log S)=\\frac{2 t(1-2 t)}{\\left[2 F^{-1}(t) f\\left(F^{-1}(t)\\right)\\right]^{2}} \\] <p>Some numerical results are given in Exhibit 5.7.3. For example, the interquartile range \\((t=0.25)\\) has an asymptotic variance \\(A(\\Phi, \\log S)=1.361\\) and an asymptotic relative efficiency (relative to the standard deviation) of \\(0.5 / 1.361=0.3674\\). The same is true for the MAD.</p> <p>Example 3.2 The \\(\\alpha\\)-trimmed variance is defined as the suitably scaled variance of the \\(\\alpha\\)-trimmed sample:</p> \\[ S(F)^{2}=\\gamma(\\alpha) \\int_{\\alpha}^{1-\\alpha} F^{-1}(s)^{2} d s \\] <p>The normalizing factor is fixed such that \\(S(\\Phi)=1\\), that is,</p> \\[ \\frac{1}{\\gamma(\\alpha)}=\\int_{-\\xi}^{\\xi} x^{2} \\varphi(x) d x=1-2 \\alpha-2 \\xi \\varphi(\\xi) \\] <p>with \\(\\xi=\\Phi^{-1}(1-\\alpha)\\). According to (3.5) the influence function of the \\(\\alpha\\) trimmed variance then satisfies</p> \\[ \\begin{aligned} \\frac{d}{d x} I C(x ; F, S) &amp; =\\gamma(\\alpha) \\frac{x}{S(F)}, &amp; &amp; \\text { for } \\alpha&lt;F(x)&lt;1-\\alpha \\\\ &amp; =0, &amp; &amp; \\text { otherwise } \\end{aligned} \\] <p>hence</p> \\[ \\begin{array}{rlr} I C(x ; F, S) &amp; =\\frac{\\gamma(\\alpha)}{2 S(F)}\\left[F^{-1}(\\alpha)^{2}-c(F)\\right], &amp; &amp; \\text { for } x&lt;F^{-1}(\\alpha) \\\\ &amp; =\\frac{\\gamma(\\alpha)}{2 S(F)}\\left[x^{2}-c(F)\\right], &amp; &amp; \\text { for } F^{-1}(\\alpha)&lt;x&lt;F^{-1}(1-\\alpha) \\\\ &amp; =\\frac{\\gamma(\\alpha)}{2 S(F)}\\left[F^{-1}(1-\\alpha)^{2}-c(F)\\right], &amp; &amp; \\text { for } x&gt;F^{-1}(1-\\alpha) \\end{array} \\] <p>where</p> \\[ c(F)=\\int_{F^{-1}(\\alpha)}^{F^{-1}(1-\\alpha)} x^{2} d F+\\alpha\\left[F^{-1}(\\alpha)^{2}+F^{-1}(1-\\alpha)^{2}\\right] \\] <p>is the \\(\\alpha\\)-Winsorized variance. Example 3.3 Define</p> \\[ S(F)=\\gamma(\\alpha) \\int_{\\alpha}^{1-\\alpha} F^{-1}(t) \\Phi^{-1}(t) d t \\] <p>with \\(\\gamma(\\alpha)\\) as in (3.13). Then the influence function can be found by integrating</p> \\[ \\begin{aligned} \\frac{d}{d x} I C(x ; F, S) &amp; =\\gamma(\\alpha) \\Phi^{-1}(F(x)), &amp; &amp; \\text { for } \\alpha&lt;F(x)&lt;1-\\alpha \\\\ &amp; =0, &amp; &amp; \\text { otherwise. } \\end{aligned} \\] <p>All of the above functionals \\(S\\) also have a symmetrized version \\(\\tilde{S}\\), which is obtained as follows. Put</p> \\[ \\tilde{F}(x)=1-F(-x+0) \\] <p>and</p> \\[ \\tilde{F}(x)=\\frac{1}{2}[F(x)+\\tilde{F}(x)] \\] <p>We say that \\(\\tilde{F}\\) is obtained from \\(F\\) by symmetrizing at the origin (alternatively, we could also symmetrize at the median, etc.). Then define</p> \\[ \\tilde{S}(F)=S(\\tilde{F}) \\] <p>It is immediate that</p> \\[ I C(x ; F, \\tilde{S})=\\frac{1}{2}[I C(x ; \\tilde{F}, S)+I C(-x ; \\tilde{F}, S)] \\] <p>Thus if \\(S\\) is symmetric [i.e., \\(S(F)=S(\\tilde{F})\\) for all \\(F\\) ], and if the true underlying \\(F\\) is symmetric \\((F=\\tilde{F})\\), then \\(\\tilde{S}(F)=S(F)\\), and \\(S\\) and \\(\\tilde{S}\\) have the same influence function at \\(F\\). Hence for symmetric \\(F\\) their asymptotic properties agree.</p> <p>For asymmetric \\(F\\) (and also for small samples from symmetric underlying distributions) the symmetrized and nonsymmetrized estimates behave</p> <p>quite differently. This is particularly evident in their breakdown behavior. For example, take an estimate of the form (3.1), where \\(M\\) is either a positive measure ( \\(q\\) even), or positive on \\(\\left[\\frac{1}{2}, 1\\right]\\), negative on \\(\\left[0, \\frac{1}{2}\\right](q\\) odd \\()\\), and let \\(\\alpha\\) be the largest real number such that \\([\\alpha, 1-\\alpha]\\) contains the support of \\(M\\). Then according to Theorem 3.3.1 and the remarks preceding it, for a nonsymmetrized estimate of scale, breakdown occurs at \\(\\varepsilon^{*}=\\alpha\\) (for \\(\\varepsilon\\)-contamination, total variation, Prohorov, L\u00e9vy, or Kolmogorov distance).</p> <p>For the symmetrized version breakdown still happens at \\(\\varepsilon^{*}=\\alpha\\) with L\u00e9vy and Kolmogorov distance, but is boosted to \\(\\varepsilon^{*}=2 \\alpha\\) in the other three cases. It appears therefore that symmetrized scale estimates are generally preferable.</p> <p>Example 3.4 Let \\(S\\) be one-half of the interquartile distance:</p> \\[ S(F)=\\frac{1}{2}\\left[F^{-1}\\left(\\frac{3}{4}\\right)-F^{-1}\\left(\\frac{1}{4}\\right)\\right] \\] <p>Then \\(\\hat{S}\\) is the median absolute deviation (Example 2.3).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#54-r-estimates-of-scale","title":"5.4 R-ESTIMATES OF SCALE","text":"<p>Rank tests for scale compare relative scale between two or more samples; there is no substitute for the left-right symmetry that makes one-sample rank-tests and estimates of location possible (though we can obtain surrogate one-sample rank tests and estimates for scale if we use a synthetic second sample, e.g., expected order statistics from a normal distribution). A brief sketch of a possible approach should suffice.</p> <p>Let \\(\\left(x_{1}, \\ldots, x_{m}\\right)\\) and \\(\\left(y_{1}, \\ldots, y_{n}\\right)\\) be the two samples, and let \\(R_{i}\\) be the rank of \\(x_{i}\\) in the pooled sample of size \\(N=m+n\\). Then form the test statistic</p> \\[ \\sum_{i=1}^{m} a\\left(R_{i}\\right) \\] <p>with \\(a_{i}=a(i)\\) defined by</p> \\[ a_{i}=N \\int_{(i-1) / N}^{i / N} J(s) d s \\] <p>for some scores generating function \\(J\\), just as in Section 3.4. Typically, \\(J\\) is</p> <p>a function of \\(\\left|t-\\frac{1}{2}\\right|\\), for example,</p> \\[ \\begin{array}{ll} J(t)=\\left|t-\\frac{1}{2}\\right|-\\frac{1}{4} &amp; \\text { (Ansari-Bradley-Siegel-Tukey) } \\\\ J(t)=\\left(t-\\frac{1}{2}\\right)^{2}-\\frac{1}{12} &amp; (\\text { Mood }) \\\\ J(t)=\\Phi^{-1}(t)^{2}-1 &amp; (\\text { Klotz }) \\end{array} \\] <p>We convert such tests into estimates of relative scale. Let \\(0&lt;\\lambda&lt;1\\) be fixed (we shall later choose \\(\\lambda=m / N\\) ), and define a functional \\(S=S(F, G)\\) such that</p> \\[ \\int J\\left[\\lambda F(x)+(1-\\lambda) G\\left(\\frac{x}{S}\\right)\\right] F(d x)=0 \\] <p>or, preferably [after substituting \\(F(x)=t\\) ],</p> \\[ \\int J\\left(\\lambda t+(1-\\lambda) G\\left(\\frac{F^{-1}(t)}{S}\\right)\\right) d t=0 \\] <p>If we assume that \\(\\int J(t) d t=0\\), then \\(S(F, G)\\), if well defined by (4.7), is a measure of relative scale satisfying</p> \\[ S\\left(F_{a X}, F_{X}\\right)=a \\] <p>where \\(F_{a X}\\) denotes the distribution of the random variable \\(a X\\). We now insert \\(F_{u}=(1-u) F+u F_{1}\\) and \\(G_{u}=(1-u) G+u G_{1}\\) into (4.7) and differentiate with respect to \\(u\\) at \\(u=0\\). If \\(F=G\\), the resulting expressions remain quite manageable; we obtain</p> \\[ \\dot{S}=\\left[\\frac{d}{d u} S\\left(F_{u}, G_{u}\\right)\\right]_{u=0}=\\frac{\\int J(F(x)) F_{1}(d x)-\\int J(F(x)) G_{1}(d x)}{\\int J^{\\prime}(F(x)) x f(x)^{2} d x} \\] <p>The G\u00e2teaux derivatives of \\(S(F, G)\\) with respect to \\(F\\) and \\(G\\), at \\(F=G\\), can now be read off from (4.9).</p> <p>If both samples come from the same \\(F\\), and if we insert the respective empirical distributions \\(F_{m}\\) and \\(G_{n}\\) for \\(F_{1}\\) and \\(G_{1}\\), we obtain the Taylor expansion (with \\(u=1\\) )</p> \\[ S\\left(F_{m}, G_{n}\\right)=1+\\dot{S}+\\cdots \\] <p>or, approximately (with \\(\\lambda=m / N\\) ),</p> \\[ \\sqrt{N}\\left(S\\left(F_{m}, G_{n}\\right)-1\\right) \\cong \\frac{\\sqrt{\\frac{1}{\\lambda}} \\frac{1}{\\sqrt{m}} \\sum J\\left(F\\left(x_{i}\\right)\\right)-\\sqrt{\\frac{1}{1-\\lambda}} \\frac{1}{\\sqrt{n}} \\sum J\\left(F\\left(y_{j}\\right)\\right)}{\\int J^{\\prime}(F(x)) x f(x)^{2} d x} \\] <p>We thus can expect that (4.11) is asymptotically normal with mean 0 and variance</p> \\[ A(F, S)=\\frac{1}{\\lambda(1-\\lambda)} \\frac{\\int J(t)^{2} d t}{\\int J^{\\prime}(F(x)) x f(x)^{2} d x]^{2}} \\] <p>This should hold if \\(m\\) and \\(n\\) go to \\(\\infty\\) comparably fast; if \\(m / n \\rightarrow 0\\), then \\(\\sqrt{m}\\left[S\\left(F_{m}, G_{n}\\right)-1\\right]\\) will be asymptotically normal with the same variance (4.12), except that the factor \\(1 /[\\lambda(1-\\lambda)]\\) is replaced by 1 .</p> <p>The above derivations of course are only heuristic; for a rigorous theory we would have to refer to the extensive literature on the behavior of rank tests under alternatives, in particular H\u00e1jek (1968) and H\u00e1jek and Dupa\u010d (1969). These results on tests can be translated in a relatively straightforward way into results about the behavior of estimates; compare Section 10.6 .</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#55-asymptotically-efficient-scale-estimates","title":"5.5 ASYMPTOTICALLY EFFICIENT SCALE ESTIMATES","text":"<p>The parametric pure scale problem corresponds to estimating \\(\\sigma\\) for the family of densities</p> \\[ p(x ; \\sigma)=\\frac{1}{\\sigma} f\\left(\\frac{x}{\\sigma}\\right), \\quad \\sigma&gt;0 \\] <p>As</p> \\[ \\frac{\\partial}{\\partial \\sigma} \\log p(x ; \\sigma)=-\\frac{f^{\\prime}(x / \\sigma)}{f(x / \\sigma)} \\frac{x}{\\sigma^{2}}-\\frac{1}{\\sigma} \\] <p>Fisher information for scale is</p> \\[ I(F ; \\sigma)=\\frac{1}{\\sigma^{2}} \\int\\left[-\\frac{f^{\\prime}(x)}{f(x)} x-1\\right]^{2} f(x) d x \\] <p>Without loss of generality we assume now that the true scale is \\(\\sigma=1\\). Evidently, in order to obtain full asymptotic efficiency at \\(F\\), we should arrange that</p> \\[ I C(x ; F, S)=\\frac{1}{I(F ; 1)}\\left[-\\frac{f^{\\prime}(x)}{f(x)} x-1\\right] \\] <p>see Section 3.5 . Thus for \\(M\\)-estimates (2.1) it suffices to choose, up to a multiplicative constant,</p> \\[ \\chi(x)=-\\frac{f^{\\prime}(x)}{f(x)} x-1 \\] <p>For an \\(L\\)-estimate (3.1) the proper choice is a measure \\(M\\) with density \\(m\\) given by</p> \\[ m(F(x))=-\\frac{\\left[f^{\\prime}(x) / f(x)\\right]^{\\prime} x^{-q+1}}{I(F ; 1)} \\] <p>For \\(R\\)-estimates of relative scale, one should choose, up to an arbitrary multiplicative constant,</p> \\[ J(F(x))=-\\frac{f^{\\prime}(x)}{f(x)} x-1 \\] <p>Example 5.1 Let \\(f(x)=\\varphi(x)\\) be the standard normal density. Then the asymptotically efficient \\(M\\)-estimate is, of course,</p> \\[ S\\left(F_{n}\\right)^{2}=\\frac{1}{n} \\sum x_{i}^{2} \\] <p>The efficient \\(L\\)-estimate for \\(q=2\\) is exactly the same. With \\(q=1\\), we obtain \\(m(t)=\\Phi^{-1}(t)\\), that is,</p> \\[ S(F)=\\int F^{-1}(t) \\Phi^{-1}(t) d t \\] <p>thus</p> \\[ S\\left(F_{n}\\right)=\\sum a_{i} x_{(i)} \\] <p>with</p> \\[ a_{i}=\\int_{(i-1) / n}^{i / n} \\Phi^{-1}(t) d t \\] <p>The efficient \\(R\\)-estimate corresponds to the Klotz test (4.5).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#56-distributions-minimizing-fisher-information-for-scale","title":"5.6 DISTRIBUTIONS MINIMIZING FISHER INFORMATION FOR SCALE","text":"<p>Let \\(\\mathscr{P}\\) be a convex set of distribution functions that is such that, with each \\(F \\in \\mathscr{P}\\), it also contains its mirror image \\(\\bar{F}\\) and thus its symmetrization \\(\\bar{F}\\) [cf. (3.19) and (3.20)]. Assume that the observations \\(X_{t}\\) are distributed according to \\(F(x / \\sigma)\\), where \\(\\sigma\\) is to be estimated.</p> <p>We note that, for any parametric family of densities \\(f(x ; \\theta)\\), Fisher information is convex: on any segment \\(f_{t}(x, \\theta)=(1-t) f_{0}(x, \\theta)+t f_{1}(x, \\theta)\\), \\(0 \\leqslant t \\leqslant 1\\),</p> \\[ I\\left(f_{t} ; \\theta\\right)=\\int \\frac{\\left[\\partial / \\partial \\theta f_{t}(x ; \\theta)\\right]^{2}}{f_{t}(x ; \\theta)} d x \\] <p>is a convex function of \\(t\\) according to Lemma 4.4.4. Clearly, \\(F\\) and \\(\\bar{F}\\) have the same Fisher information for scale, and it follows that</p> \\[ I(\\bar{F} ; \\sigma) \\leqslant I(F ; \\sigma)=I(\\bar{F} ; \\sigma) \\] <p>Hence it suffices to consider symmetric distributions \\(F\\) when minimizing Fisher information for scale.</p> <p>Then \\(Y_{t}=\\log \\left|X_{t}\\right|\\) is a sufficient statistic for \\(X_{t}\\), and it has the distribution function \\(F^{*}(y-\\tau)\\), where</p> \\[ F^{*}(y)=F\\left(e^{y}\\right)-F\\left(-e^{y}\\right) \\] <p>and</p> \\[ \\tau=\\log \\sigma \\] <p>The corresponding density is</p> \\[ f^{*}(y)=2 e^{y} f\\left(e^{y}\\right) \\] <p>Note that Fisher information for scale \\(\\sigma\\)</p> \\[ I(F ; \\sigma)=\\frac{1}{\\sigma^{2}} \\int\\left[-x \\frac{f^{\\prime}(x)}{f(x)}-1\\right]^{2} f(x) d x \\] <p>agrees, apart from the factor \\(1 / \\sigma^{2}\\), with Fisher information for location \\(\\tau\\) :</p> \\[ \\begin{aligned} I\\left(F^{*} ; \\tau\\right) &amp; =\\int\\left[\\frac{d}{d y} \\log f^{*}(y)\\right]^{2} f^{*}(y) d y \\\\ &amp; =\\int\\left[-x \\frac{f^{\\prime}(x)}{f(x)}-1\\right]^{2} f(x) d x \\end{aligned} \\] <p>Thus minimizing \\(I\\left(F^{*} ; \\tau\\right)\\) is equivalent to minimizing \\(\\sigma^{2} I(F ; \\sigma)\\); for reasons of scale invariance we should prefer this latter expression to \\(I(F ; \\sigma)\\) anyway.</p> <p>Moreover, if a set \\(\\mathscr{P}\\) of distributions is convex, then the transformed set \\(\\mathscr{P}^{*}=\\left\\{F^{*} \\mid F \\in \\mathscr{P}\\right\\}\\) is also convex, and the methods and results of Sections 4.4 and 4.5 apply to \\(\\mathscr{P}^{*}\\). In particular, as \\(\\varepsilon\\)-contamination in \\(F\\) is transformed into \\(\\varepsilon\\)-contamination in \\(F^{*}\\), the treatment of the gross error model goes through without change. For the other neighborhoods some more care is needed.</p> <p>We consider only the \\(\\varepsilon\\)-contaminated normal case. Let \\(\\varphi\\) be the standard normal density; then</p> \\[ \\varphi^{*}(y)=\\sqrt{\\frac{2}{\\pi}} \\exp \\left(y-\\frac{1}{2} e^{2 y}\\right) \\] <p>thus</p> \\[ -\\log \\varphi^{*}(y)=\\frac{1}{2} e^{2 y}-y+\\frac{1}{2} \\log \\left(\\frac{1}{2} \\pi\\right) \\] <p>is convex, and</p> \\[ \\left[-\\log \\varphi^{*}(y)\\right]^{\\prime}=e^{2 y}-1 \\] <p>is monotone.</p> <p>Example 4.5.2 now shows how to find a distribution minimizing Fisher information. We have to distinguish two cases.</p> <p>Case A. Large \\(\\varepsilon\\). Define two numbers \\(y_{0} \\leqslant y_{1}\\) by</p> \\[ \\begin{aligned} &amp; e^{2 y_{0}}-1=-k \\\\ &amp; e^{2 y_{1}}-1=k \\end{aligned} \\] <p>where \\(k&lt;1\\) is related to \\(\\varepsilon\\) by</p> \\[ \\int_{y_{0}}^{y_{1}} \\varphi^{*}(y) d y+\\frac{\\varphi^{*}\\left(y_{0}\\right)+\\varphi^{*}\\left(y_{1}\\right)}{k}=\\frac{1}{1-\\varepsilon} \\] <p>The least informative element of \\(\\mathscr{P}^{*}\\) then has the density</p> \\[ \\begin{array}{rlrl} f_{0}^{*}(y) &amp; =(1-\\varepsilon) \\varphi^{*}\\left(y_{0}\\right) e^{k\\left(y-y_{0}\\right)}, &amp; &amp; \\text { for } y&lt;y_{0} \\\\ &amp; =(1-\\varepsilon) \\varphi^{*}(y), &amp; &amp; \\text { for } y_{0} \\leqslant y \\leqslant y_{1} \\\\ &amp; =(1-\\varepsilon) \\varphi^{*}\\left(y_{1}\\right) e^{-k\\left(y-y_{1}\\right)}, &amp; &amp; \\text { for } y&gt;y_{1} \\end{array} \\] <p>If we transform these equations back into \\(x\\)-space, we obtain, instead of (6.10) to (6.12),</p> \\[ \\begin{aligned} x_{0}^{2} &amp; =(1-k)^{+}, \\\\ x_{1}^{2} &amp; =1+k, \\\\ 2 \\int_{x_{0}}^{x_{1}} \\varphi(x) d x+\\frac{2 x_{0} \\varphi\\left(x_{0}\\right)+2 x_{1} \\varphi\\left(x_{1}\\right)}{x_{1}^{2}-1} &amp; =\\frac{1}{1-\\varepsilon} \\\\ f_{0}(x) &amp; =(1-\\varepsilon) \\varphi\\left(x_{0}\\right)\\left(\\frac{x_{0}}{|x|}\\right)^{\\left(x_{0}^{2}\\right)}, &amp; &amp; \\text { for }|x|&lt;x_{0} \\\\ &amp; =(1-\\varepsilon) \\varphi(x), &amp; &amp; \\text { for } x_{0} \\leqslant|x| \\leqslant x_{1} \\\\ &amp; =(1-\\varepsilon) \\varphi\\left(x_{1}\\right)\\left(\\frac{x_{1}}{|x|}\\right)^{\\left(x_{1}^{2}\\right)}, &amp; &amp; \\text { for }|x|&gt;x_{1} \\end{aligned} \\] <p>Case B. Small \\(\\varepsilon\\) In this case, the left boundary point is \\(y_{0}=-\\infty\\), and correspondingly \\(x_{0}=0\\). Nothing else is changed, and formulas (6.13) to (6.15) remain valid as they stand (with \\(k \\geqslant 1\\) ).</p> <p>Note that Case A yields a highly pathological least informative distribution \\(F_{0}\\); its density is \\(\\infty\\) at \\(x=0\\). In Case B, \\(F_{0}\\) corresponds to a distribution that is normal in the middle and behaves like a \\(t\\)-distribution with \\(k=x_{1}^{2}-1 \\geqslant 1\\) degrees of freedom in the tails. The boundary case between Cases A and B corresponds to \\(x_{0}=0, x_{1}=\\sqrt{2}\\), and \\(\\varepsilon=0.205\\). Exhibit 5.6 .1 shows some numerical results.</p> <p>We now determine the asymptotically efficient \\(M\\) - and \\(L\\)-estimates of scale for these least informative distributions (cf. Section 5.5). The efficient \\(M\\)-estimate (2.1) of scale is defined by</p> \\[ \\begin{aligned} \\chi(x)=-x \\frac{f_{0}^{\\prime}(x)}{f_{0}(x)}-1 &amp; =x_{0}^{2}-1, &amp; &amp; \\text { for }|x|&lt;x_{0} \\\\ &amp; =x^{2}-1, &amp; &amp; \\text { for } x_{0} \\leqslant|x| \\leqslant x_{1} \\\\ &amp; =x_{1}^{2}-1, &amp; &amp; \\text { for }|x|&gt;x_{1} \\end{aligned} \\] \\(\\varepsilon\\) \\(x_{0}\\) \\(x_{1}\\) \\(F_{0}\\left(-x_{0}\\right)\\) \\(F_{0}\\left(-x_{1}\\right)\\) \\(1 / I\\left(F_{0}^{*}\\right)\\) 0 0 \\(\\infty\\) 0.5 0 0.50 0.001 0 2.88 0.5 0.002 0.52 0.002 0 2.70 0.5 0.004 0.53 0.005 0 2.46 0.5 0.009 0.56 0.01 0 2.27 0.5 0.016 0.60 0.02 0 2.07 0.5 0.029 0.66 0.05 0 1.81 0.5 0.059 0.81 0.1 0 1.62 0.5 0.098 1.02 0.15 0 1.50 0.5 0.132 1.23 0.20 0 1.42 0.5 0.162 1.45 0.205 0 1.414 0.5 0.165 1.472 0.25 0.35 1.37 0.388 0.182 1.72 0.30 0.45 1.34 0.357 0.192 1.98 0.40 0.60 1.28 0.313 0.210 2.82 0.50 0.70 1.23 0.299 0.223 4.16 0.65 0.81 1.16 0.267 0.237 8.72 0.80 0.90 1.09 0.255 0.246 28.6 1 1 1 0.25 0.25 \\(\\infty\\) <p>Exhibit 5.6.1 The \\(\\varepsilon\\)-contaminated normal distributions that are least favorable for scale.</p> <p>The efficient \\(L\\)-estimate [(3.1) with \\(q=2\\) ] is a kind of trimmed variance, in Case A trimmed also on the inside; its weight density is given by</p> \\[ \\begin{aligned} m(t) &amp; =\\frac{2}{I\\left(F_{0}^{*}, \\tau\\right)}, &amp; &amp; \\text { for } F_{0}\\left(x_{0}\\right)&lt;t&lt;F_{0}\\left(x_{1}\\right) \\\\ &amp; =0, &amp; &amp; \\text { and for } F_{0}\\left(-x_{1}\\right)&lt;t&lt;F_{0}\\left(-x_{0}\\right) \\\\ &amp; \\text { otherwise. } \\end{aligned} \\] <p>The limiting case \\(\\varepsilon \\rightarrow 1\\) leads to an interesting nontrivial estimate, just as in the location case: the limiting \\(M\\) - and \\(L\\)-estimates of \\(\\tau\\) then coincide with the median of \\(\\left\\{\\log \\left|x_{i}\\right|\\right\\}\\), so the corresponding estimate of \\(\\sigma\\) is the median of \\(\\left\\{\\left|x_{i}\\right|\\right\\}\\). Hence the median absolute deviation [cf. (1.4), Example 2.3, and Example 3.4] is the candidate for being the \"most robust estimate of scale.\"</p> <p>Note that the above estimates (6.16) and (6.17) are biased when applied to normal data; in order to make them asymptotically unbiased at \\(\\Phi\\), we have to divide them by a suitable constant, namely \\(S(\\Phi)\\) (see Exhibits 5.7.1 to 5.7.3 for these constants). Equivalently, we could replace the subtractive constant 1 in (6.16) by a different number \\(\\beta\\) such that \\(E_{\\Phi} \\chi=0\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#57-minimax-properties","title":"5.7 MINIMAX PROPERTIES","text":"<p>The general results of Section 4.6 show that the \\(M\\)-estimate determined in the preceding section is minimax with regard to asymptotic variance for the collection of \\(\\varepsilon\\)-contaminated normal distributions satisfying \\(S(F)=1\\), that is,</p> \\[ \\int \\chi(x) F(d x)=0 \\] <p>This is a rather restrictive condition, particularly in Case B, where \\(x_{0}=0\\) and \\(x_{1}&gt;\\sqrt{2}\\); it means that those and only those distributions \\(F=(1-\\varepsilon) \\Phi\\) \\(+\\varepsilon H\\) that put all their contaminating mass \\(\\varepsilon H\\) outside of \\(\\left[-x_{1}, x_{1}\\right]\\) are admitted for competition. For any such distribution the asymptotic behavior of \\(S\\) is the same:</p> \\[ \\begin{aligned} S(F) &amp; =S\\left(F_{0}\\right)=1 \\\\ A(F, S) &amp; =A\\left(F_{0}, S\\right)=\\frac{1}{I\\left(F_{0} ; 1\\right)} \\end{aligned} \\] <p>Is it possible to remove this inconvenient condition (7.1)? We have the partial answer that this is indeed the case, provided \\(\\varepsilon\\) is sufficiently small ( \\(\\varepsilon&lt;0.04\\) and thus \\(x_{1} \\geqslant 1.88\\) suffices); it would be interesting to know the precise range of \\(\\varepsilon\\)-values for which it is true.</p> <p>The point of the whole problem is, of course, that we defined our asymptotic loss as \\(A(F, S) / S(F)^{2}\\). Thus if we move some contamination from the outside to the inside of \\(\\left[-x_{1}, x_{1}\\right]\\), we decrease both the numerator and the denominator, and it is not evident whether the quotient decreases or increases.</p> <p>From the influence function (2.2) we obtain that</p> \\[ \\frac{S(F)^{2}}{A(F, S)}=\\frac{\\left[\\int \\chi^{\\prime}(x / S)(x / S) F(d x)\\right]^{2}}{\\int \\chi(x / S)^{2} F(d x)} \\] <p>with the side condition (determining \\(S\\) )</p> \\[ \\int \\chi\\left(\\frac{x}{S}\\right) f(d x)=0 \\] <p>where</p> \\[ \\begin{aligned} \\chi(x) &amp; =x^{2}-1, &amp; &amp; \\text { for }|x| \\leqslant x_{1} \\\\ &amp; =x_{1}^{2}-1, &amp; &amp; \\text { for }|x|&gt;x_{1} \\geqslant \\sqrt{2} \\end{aligned} \\] <p>We have to show that \\(F_{0}\\) minimizes (7.2) among all</p> \\[ F \\in \\mathscr{P}_{\\varepsilon}=\\{F \\mid F=(1-\\varepsilon) \\Phi+\\varepsilon H, H \\in \\mathscr{P} \\mathbb{R}\\} \\] <p>not only among those \\(F\\) satisfying (7.1). We first note that the subsets of \\(\\mathscr{P}_{\\varepsilon}\\) for which \\(S(F)\\) has a given fixed value are convex, and that on each of them (7.2) is a convex function of \\(F\\) (Lemma 4.4.4).</p> <p>Moreover, if we keep \\(S(F)\\) fixed, then (7.2) is minimized by a contamination \\(\\varepsilon H\\) sitting on \\(\\{0\\} \\cup\\left[-x_{1}, x_{1}\\right]^{c}\\). The intuitive reason for this is that such a contamination evidently minimizes the numerator under the side condition \\(S(F)=\\) const., and it also maximizes the variance of \\(\\chi(x / S)\\), that is, the denominator, as it sits on the extreme values of \\(\\chi\\) [note that \\(S(F) \\leqslant 1]\\). It is not difficult to make this intuitive reasoning precise by a variational argument (in view of convexity, local properties suffice); the details are left to the reader.</p> <p>It is convenient to substitute</p> \\[ \\chi(x)=\\psi(x)^{2}-1 \\] <p>then (7.2) and (7.3) can be rewritten as</p> \\[ \\begin{gathered} \\frac{S(F)^{2}}{A(F, S)}=\\frac{\\left[\\int_{-S x_{1}}^{S x_{1}}(x / S)^{2} F(d x)\\right]^{2}}{\\int \\psi(x / S)^{4} F(d x)-1} \\\\ \\int \\psi\\left(\\frac{x}{S}\\right)^{2} F(d x)=1 \\end{gathered} \\] <p>As it suffices to minimize (7.5) over contaminations sitting on \\(\\{0\\} \\cup\\) \\(\\left[-x_{1}, x_{1}\\right]^{\\varepsilon}\\), we now assume that \\(\\varepsilon H\\) puts mass \\(\\varepsilon-\\varepsilon_{1}\\) on \\(\\{0\\}\\), and mass \\(\\varepsilon_{1}\\) on \\(\\left[-x_{1}, x_{1}\\right]^{\\varepsilon}\\). then (7.5) and (7.6) are further transformed into</p> \\[ \\begin{gathered} \\frac{S(F)^{2}}{A(F, S)}=\\frac{\\left[(1-\\varepsilon) \\int_{-S x_{1}}^{S x_{1}}(x / S)^{2} \\Phi(d x)\\right]^{2}}{(1-\\varepsilon) \\int \\psi(x / S)^{4} \\Phi(d x)+\\varepsilon_{1} x_{1}^{4}-1} \\\\ (1-\\varepsilon) \\int \\psi\\left(\\frac{x}{S}\\right)^{2} \\Phi(d x)+\\varepsilon_{1} x_{1}^{2}=1 \\end{gathered} \\] <p>We now have to find out for which values of \\(\\varepsilon\\) the choice \\(\\varepsilon_{1}=\\varepsilon\\) minimizes (7.7) subject to the side condition (7.8).</p> <p>From (7.8) we can determine the derivative of \\(S\\) with respect to \\(\\varepsilon_{1}\\). If \\(\\varepsilon&lt;0.04\\), we find (with the aid of some numerical calculations) that the numerator and denominator of (7.7) have a negative and a positive derivative with respect to \\(\\varepsilon_{1}\\), respectively, and this is true over the entire range possible for \\(S\\). Hence for \\(\\varepsilon&lt;0.04\\) the minimum of (7.7) is reached at \\(\\varepsilon_{1}=\\varepsilon\\). For larger \\(\\varepsilon\\) the situation becomes more complicated, and we do not know whether the result remains true.</p> <p>Exhibits 5.7.1 to 5.7.3 compare the asymptotic performances of several estimates of scale for a normal distribution, symmetrically \\(\\varepsilon\\)-contaminated near \\(\\pm \\infty\\). To facilitate comparisons, the values of \\(x_{1}\\) in Exhibit 5.7.1 were adjusted so that the performance at the normal distribution agrees with that of an \\(\\alpha\\)-trimmed standard deviation. In Exhibits 5.7.1 and 5.7.2 the value \\(\\varepsilon_{\\min }\\) indicates for which least informative distribution the estimate is asymptotically efficient (cf. Exhibit 5.6.1).</p> \\(S(F) / S(\\Phi)\\) and \\(A(F, \\log S)\\) for \\(\\varepsilon\\) \\(x_{1}\\) \\((\\alpha)\\) \\(\\varepsilon_{\\text {min }}\\) \\(S(\\Phi)\\) 0 0.005 0.01 0.02 0.05 0.10 0.15 0.20 2.370 0.0069 0.982 1.000 1.013 1.027 1.056 1.163 1.458 2.361 \\(\\infty\\) (0.01) 0.530 0.566 0.605 0.697 1.138 3.677 38.14 \\(\\infty\\) 2.130 0.016 0.964 1.000 0.011 1.022 1.046 1.128 1.323 1.690 3.045 (0.02) 0.557 0.581 0.607 0.665 0.909 1.854 6.059 91.43 1.804 0.051 0.912 1.000 1.008 1.017 1.035 1.094 1.215 1.384 1.650 (0.05) 0.640 0.654 0.668 0.698 0.810 1.110 1.741 3.525 1.555 0.123 0.824 1.000 1.007 1.014 1.028 1.075 1.165 1.276 1.419 (0.10) 0.796 0.805 0.813 0.831 0.892 1.031 1.244 1.608 1.414 0.205 0.736 1.000 1.006 1.013 1.025 1.066 1.144 1.235 1.346 (0.149) 0.989 0.995 1.001 1.014 1.056 1.146 1.269 1.449 1.311 - 0.642 1.000 1.006 1.012 1.024 1.061 1.131 1.212 1.308 (0.20) 1.257 1.262 1.266 1.276 1.308 1.372 1.455 1.566 1.234 - 0.547 1.000 1.006 1.011 1.022 1.058 1.124 1.199 1.285 (0.25) 1.630 1.633 1.637 1.645 1.669 1.718 1.778 1.855 <p>Exhlbl 5.7.1 Huber's scale; \\(\\int \\chi[x / S(F)] F(d x)=0\\) with \\(\\chi\\) as in (6.16), \\(x_{0}=0\\); asymptotic values and asymptotic variances for far-out symmetric \\(\\varepsilon\\)-contamination.</p> \\(S(F) / S(\\Phi)\\) and \\(A(F, \\log S)\\) for \\(\\varepsilon\\) \\(\\alpha\\) \\(\\varepsilon_{\\text {min }}\\) \\(S(\\Phi)\\) 0 0.005 0.01 0.02 0.05 0.1 0.15 0.2 0.25 0.01 0.005 0.925 1.000 1.014 1.029 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.530 0.565 0.617 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.02 0.013 0.873 1.000 1.011 1.023 1.048 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.557 0.579 0.605 0.678 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.05 0.041 0.749 1.000 1.008 1.017 1.035 1.097 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.640 0.652 0.664 0.691 0.816 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.10 0.103 0.592 1.000 1.007 1.014 1.029 1.076 1.169 1.293 \\(\\infty\\) \\(\\infty\\) 0.796 0.803 0.810 0.825 0.879 1.022 1.351 \\(\\infty\\) \\(\\infty\\) 0.15 0.180 0.466 1.000 1.006 1.013 1.025 1.067 1.145 1.238 1.356 1.513 0.994 0.998 1.003 1.014 1.049 1.128 1.249 1.462 1.963 0.20 - 0.359 1.000 1.006 1.012 1.024 1.061 1.132 1.213 1.310 1.428 1.257 1.261 1.264 1.272 1.298 1.352 1.425 1.530 1.693 0.25 - 0.267 1.000 1.005 1.011 1.022 1.058 1.124 1.199 1.286 1.388 1.630 1.633 1.636 1.642 1.662 1.702 1.753 1.820 1.912 <p>Exhlbl 5.7.2 Trimmed standard deviations \\(S(F)=\\left[\\int_{\\alpha}^{1-\\alpha} F^{-1}(t)^{2} d t\\right]^{1 / 2}\\); asymptotic values and asymptotic variances for far-out symmetric \\(\\varepsilon\\)-contamination.</p> \\(S(F) / S(\\Phi)\\) and \\(A(F, \\log S)\\) for \\(\\varepsilon\\) \\(\\alpha\\) \\(S(\\Phi)\\) 0 0.005 0.01 0.02 0.05 0.1 0.15 0.2 0.01 2.327 1.000 1.045 1.106 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 1.277 1.940 3.556 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.02 2.054 1.000 1.026 1.055 1.129 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.972 1.162 1.433 2.531 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.05 1.645 1.000 1.014 1.028 1.059 1.178 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.782 0.828 0.880 1.008 1.786 \\(\\infty\\) \\(\\infty\\) \\(\\infty\\) 0.10 1.282 1.000 1.009 1.018 1.037 1.102 1.243 1.475 \\(\\infty\\) 0.791 0.808 0.827 0.867 1.026 1.548 3.465 \\(\\infty\\) 0.15 1.036 1.000 1.007 1.015 1.030 1.080 1.178 1.304 1.480 0.899 0.909 0.920 0.942 1.021 1.213 1.554 2.306 0.20 0.841 1.000 1.006 1.013 1.026 1.069 1.150 1.247 1.367 1.081 1.088 1.095 1.110 1.160 1.268 1.425 1.672 0.25 0.674 1.000 1.006 1.012 1.024 1.062 1.134 1.217 1.316 1.361 1.366 1.371 1.382 1.417 1.488 1.583 1.713 <p>Exhibit 5.7.3 Interquantile distances; \\(S(F)=\\frac{1}{2}\\left[F^{-1}(1-\\alpha)-F^{-1}(\\alpha)\\right]\\); asymptotic values and asymptotic variances for far-out symmetric \\(\\varepsilon\\)-contamination.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-6","title":"CHAPTER 6","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#multiparameter-problems-in-particular-joint-estimation-of-location-and-scale","title":"Multiparameter Problems, in Particular Joint Estimation of Location and Scale","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#61-general-remarks","title":"6.1 GENERAL REMARKS","text":"<p>We have already mentioned (Section 5.1) that \\(M\\)-estimates of location in practice will have to be supplemented by a simultaneous estimate of scale, since they are not scale invariant [except the median, \\(\\psi(x)=\\operatorname{sign}(x)\\) ]. Thus we are faced with a two-parameter problem.</p> <p>The step going from one to two (or more) parameters is a troublesome one-we lose the technical advantages offered by the natural ordering of the real line, and proofs get more complicated.</p> <p>L- and \\(R\\)-estimates of location are scale invariant, so this difficulty does not exist. On the other hand they rely so heavily on ordering that they do not generalize well beyond one-parameter location or scale problems. In fact they lose their advantages, for example the simplicity of \\(L\\)-estimates like the trimmed mean, or the existence of nonparametric confidence intervals for \\(R\\)-estimates, and their computation is quite complicated.</p> <p>We therefore deal exclusively with \\(M\\)-estimates in this chapter. Sections 6.2 and 6.3 give some very general results (without proofs) on consistency and asymptotic normality of multiparameter \\(M\\)-estimates; the remaining sections discuss simultaneous estimates of location and scale (the latter being considered as a nuisance parameter).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#62-consistency-of-m-estimates","title":"6.2 CONSISTENCY OF M-ESTIMATES","text":"<p>In this section we state two theorems on consistency of \\(M\\)-estimates. The first one is concerned with estimates defined through a minimum property, the second one with estimates defined through a system of implicit equations. Proofs can be found in Huber (1967).</p> <p>Case A. Estimates Defined Through a Minimum Property Assume that the parameter set \\(\\Theta\\) is a locally compact space with a countable base (e.g., an open subset of a Euclidean space), \\((\\mathscr{K}, \\mathscr{Q}, P)\\) is a probability space, and \\(\\rho(x, \\theta)\\) is some real-valued function on \\(\\mathscr{K} \\times \\Theta\\).</p> <p>Assume that \\(x_{1}, x_{2}, \\ldots\\) are independent random variables with values in \\(\\mathscr{K}\\), having the common probability distribution \\(P\\). Let \\(T_{n}\\left(x_{1}, \\ldots, x_{n}\\right)\\) be any sequence of functions \\(T_{n}: \\mathscr{K}^{n} \\rightarrow \\Theta\\), measurable or not, such that</p> \\[ \\frac{1}{n} \\sum_{i=1}^{n} \\rho\\left(x_{i}, T_{n}\\right)-\\inf _{\\theta} \\frac{1}{n} \\sum_{i=1}^{n} \\rho\\left(x_{i}, \\theta\\right) \\rightarrow 0 \\] <p>almost surely (or in probability). In most cases the left-hand side of (2.1)-let us denote it by \\(Z_{n}\\)-will be identically zero, but it is simpler to work with (2.1) than to add extraneous conditions only to guarantee the existence of a \\(T_{n}\\) minimizing \\(1 / n \\sum \\rho\\left(x_{i}, \\theta\\right)\\). Since \\(Z_{n}\\) need not be measurable, we should more precisely speak of convergence in outer probability \\(\\left[P^{*}\\left(\\left|Z_{n}\\right|&gt;\\varepsilon\\right) \\rightarrow 0\\right.\\) for all \\(\\left.\\varepsilon\\right]\\) instead of convergence in probability.</p> <p>We now give sufficient conditions that each sequence \\(T_{n}\\) satisfying (2.1) will converge almost surely (or in probability, respectively) to some constant \\(\\theta_{0}\\), which is characterized below.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#assumptions_1","title":"ASSUMPTIONS","text":"<p>(A-1) For each fixed \\(\\theta \\in \\Theta, \\rho(x, \\theta)\\) is \\(\\mathscr{Q}\\)-measurable, and \\(\\rho\\) is separable in the sense of Doob; that is, there is a \\(P\\)-null set \\(N\\) and a countable subset \\(\\Theta^{\\prime} \\subset \\Theta\\) such that, for every open set \\(U \\subset \\Theta\\) and every closed interval \\(A\\), the sets</p> \\[ \\{x \\mid \\rho(x, \\theta) \\in A, \\forall \\theta \\in U\\}, \\quad\\{x \\mid \\rho(x, \\theta) \\in A, \\forall \\theta \\in U \\cap \\Theta^{\\prime}\\} \\] <p>differ by at most a subset of \\(N\\). This assumption ensures measurability of the infima and limits occurring in (A-2) and (A-5). For a fixed \\(P, \\rho\\) might always be replaced by a separable version [see Doob (1953), p. 56 ff]. (A-2) The function \\(\\rho\\) is a.s. lower semicontinuous in \\(\\theta\\), that is,</p> \\[ \\inf _{\\theta^{\\prime} \\in U} \\rho\\left(x, \\theta^{\\prime}\\right) \\rightarrow \\rho(x, \\theta) \\quad \\text { a.s. } \\] <p>as the neighborhood \\(U\\) of \\(\\theta\\) shrinks to \\(\\{\\theta\\}\\).</p> <p>(A-3) There is a measurable function \\(a(x)\\) such that</p> \\[ \\begin{array}{ll} E(\\rho(x, \\theta)-a(x))^{-}&lt;\\infty, &amp; \\text { for all } \\theta \\in \\Theta \\\\ E(\\rho(x, \\theta)-a(x))^{+}&lt;\\infty, &amp; \\text { for some } \\theta \\in \\Theta \\end{array} \\] <p>Thus \\(\\gamma(\\theta)=E(\\rho(x, \\theta)-a(x))\\) is well defined for all \\(\\theta\\). (A-4) There is a \\(\\theta_{0} \\in \\Theta\\) such that \\(\\gamma(\\theta)&gt;\\gamma\\left(\\theta_{0}\\right)\\) for all \\(\\theta \\neq \\theta_{0}\\). If \\(\\Theta\\) is not compact, let \\(\\infty\\) denote the point at infinity in its one-point compactification. (A-5) There is a continuous function \\(b(\\theta)&gt;0\\) such that: (i)</p> \\[ \\inf _{\\theta \\in \\Theta} \\frac{\\rho(x, \\theta)-a(x)}{b(\\theta)} \\geqslant h(x) \\] <p>for some integrable function \\(h\\). (ii)</p> \\[ \\liminf _{\\theta \\rightarrow \\infty} b(\\theta)&gt;\\gamma\\left(\\theta_{0}\\right) \\] \\[ E\\left\\{\\liminf _{\\theta \\rightarrow \\infty} \\frac{\\rho(x, \\theta)-a(x)}{b(\\theta)}\\right\\} \\geqslant 1 \\] <p>If \\(\\Theta\\) is compact, then (ii) and (iii) are redundant. Example 2.1 Let \\(\\Theta=\\mathscr{C}\\) be the real axis, and let \\(P\\) be any probability distribution having a unique median \\(\\theta_{0}\\). Then (A-1) to (A-5) are satisfied for \\(\\rho(x, \\theta)=|x-\\theta|, a(x)=|x|, b(\\theta)=|\\theta|+1, h(x)=-1\\). This will imply that the sample median is a consistent estimate of the median.</p> <p>Taken together (A-2), (A-3), and (A-5) (i) imply by monotone convergence the following strengthened version of (A-2). (A-2') As the neighborhood \\(U\\) of \\(\\theta\\) shrinks to \\(\\{\\theta\\}\\),</p> \\[ E \\inf _{\\theta^{\\prime} \\in U}\\left\\{\\rho\\left(x, \\theta^{\\prime}\\right)-a(x)\\right\\} \\rightarrow E\\{\\rho(x, \\theta)-a(x)\\} \\] <p>Note that the set \\(\\{\\theta \\in \\Theta|E[|\\rho(x, \\theta)-a(x)|]&lt;\\infty\\}\\) is independent of the particular choice of \\(a(x)\\); if there is an \\(a(x)\\) satisfying (A-3), then we might take \\(a(x)=\\rho\\left(x, \\theta_{0}\\right)\\).</p> <p>For the sake of simplicity we absorb \\(a(x)\\) into \\(\\rho(x, \\theta)\\) from now on.</p> <p>LEMMA 2.1 If (A-1), (A-3), and (A-5) hold, then there is a compact set \\(C \\subset \\Theta\\) such that every sequence \\(T_{n}\\) satisfying (2.1) ultimately almost surely stays in \\(C\\) (or, with probability tending to 1 , respectively).</p> <p>THEOREM 2.2 If (A-1), (A-2'), (A-3), and (A-4) hold, then every sequence \\(T_{n}\\) satisfying (2.1) and the conclusion of Lemma 2.1 converges to \\(\\theta_{0}\\) almost surely (or, in probability, respectively).</p> <p>Quite often (A-5) is not satisfied-in particular, if location and scale are estimated simultaneously-but the conclusion of Lemma 2.1 can be verified without too much trouble by ad hoc methods. I do not know of any fail-safe replacement for (A-5).</p> <p>In the location-scale case this problem poses itself as follows. To be specific take the maximum likelihood estimate of \\(\\theta=(\\xi, \\sigma), \\sigma&gt;0\\), based on a density \\(f_{0}\\) (the true underlying distribution \\(P\\) may be different). Then</p> \\[ \\rho(x, \\theta)=\\rho(x ; \\xi, \\sigma)=\\log \\sigma-\\log f_{0}\\left(\\frac{x-\\xi}{\\sigma}\\right) \\] <p>The trouble is that, if \\(\\theta\\) tends to \"infinity,\" that is, to the boundary \\(\\sigma=0\\) by letting \\(\\xi=x, \\sigma \\rightarrow 0\\), then \\(\\rho \\rightarrow-\\infty\\). If \\(P\\) is continuous, so that the probability of ties between the \\(x_{i}\\) 's is zero, the following trick helps: take pairs \\(y_{n}=\\left(x_{2 n-1}, x_{2 n}\\right)\\) of the original observations as our new observations. Then the corresponding \\(\\rho_{2}\\),</p> \\[ \\rho_{2}(y, \\theta)=\\rho\\left(x_{1} ; \\xi, \\sigma\\right)+\\rho\\left(x_{2} ; \\xi, \\sigma\\right) \\] <p>will avoid the above-mentioned difficulty. Somewhat more generally we are saved if we can show directly that the ML estimate \\(\\hat{\\theta}_{n}=\\left(\\hat{\\xi}_{n}, \\hat{\\sigma}_{n}\\right)\\) ultimately satisfies \\(\\hat{\\sigma}_{n} \\geqslant \\delta&gt;0\\) for some \\(\\delta\\). (This again is tricky if the true underlying distribution is discontinuous and \\(f_{0}\\) has very long tails.) Case B. Estimates Defined Through Implicit Equations Let \\(\\Theta\\) be locally compact with a countable base, let \\((\\mathscr{R}, \\mathscr{Q}, P)\\) be a probability space, and let \\(\\psi(x, \\theta)\\) be some function on \\(\\mathscr{R} \\times \\Theta\\) with values in \\(m\\)-dimensional Euclidean space \\(\\mathbb{R}^{m}\\).</p> <p>Assume that \\(x_{1}, x_{2}, \\ldots\\) are independent random variables with values in \\(\\mathscr{R}\\), having the common probability distribution \\(P\\). We intend to give sufficient conditions that any sequence of functions \\(T_{n}: \\mathbb{X}^{n} \\rightarrow \\Theta\\) such that</p> \\[ \\frac{1}{n} \\sum_{1}^{n} \\psi\\left(x_{i} ; T_{n}\\right) \\rightarrow 0 \\] <p>almost surely (or in probability) converges almost surely (or in probability) to some constant \\(\\theta_{0}\\).</p> <p>If \\(\\Theta\\) is an open subset of \\(\\mathbb{R}^{m}\\), and if \\(\\psi(x, \\theta)=(\\partial / \\partial \\theta) \\log f(x, \\theta)\\) for a differentiable parametric family of probability densities, then the ML estimate of course will satisfy (2.8). However, our \\(\\psi\\) need not be a total differential. (This is important; for instance, it allows us to piece together joint estimates of location and scale from two essentially unrelated \\(M\\) estimates of location and scale, respectively.)</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#assumptions_2","title":"ASSUMPTIONS","text":"<p>(B-1) For each fixed \\(\\theta \\in \\Theta, \\psi(x, \\theta)\\) is \\(\\mathcal{G}\\)-measurable in \\(x\\), and \\(\\psi\\) is separable (see A-1). (B-2) The function \\(\\psi\\) is a.s. continuous in \\(\\theta\\) :</p> \\[ \\lim _{\\theta^{\\prime} \\rightarrow \\theta}\\left|\\psi\\left(x, \\theta^{\\prime}\\right)-\\psi(x, \\theta)\\right|=0 \\quad \\text { a.s. } \\] <p>(B-3) The expected value \\(\\lambda(\\theta)=E \\psi(x, \\theta)\\) exists for all \\(\\theta \\in \\Theta\\), and has a unique zero at \\(\\theta=\\theta_{0}\\). (B-4) There exists a continuous function \\(b(\\theta)\\) that is bounded away from zero, \\(b(\\theta) \\geqslant b_{0}&gt;0\\), such that</p> \\[ \\sup _{\\theta} \\frac{|\\psi(x, \\theta)|}{b(\\theta)} \\text { is integrable } \\] \\[ \\liminf _{\\theta \\rightarrow \\infty} \\frac{|\\lambda(\\theta)|}{b(\\theta)} \\geqslant 1 \\] \\[ E\\left[\\limsup _{\\theta \\rightarrow \\infty} \\frac{|\\psi(x, \\theta)-\\lambda(\\theta)|}{b(\\theta)}&lt;1\\right] \\] <p>In view of (B-4) (i), (B-2) can be strengthened to (B-2') As the neighborhood \\(U\\) of \\(\\theta\\) shrinks to \\(\\{\\theta\\}\\),</p> \\[ E\\left[\\sup _{\\theta^{\\prime} \\in U}\\left|\\psi\\left(x, \\theta^{\\prime}\\right)-\\psi(x, \\theta)\\right|\\right] \\rightarrow 0 \\] <p>It follows from (B-2') that \\(\\lambda\\) is continuous. Moreover, if there is a function \\(b\\) satisfying (B-4), we can take</p> \\[ b(\\theta)=\\max \\left(|\\lambda(\\theta)|, b_{0}\\right) \\] <p>LEMMA 2.3 If (B-1) and (B-4) hold, then there is a compact set \\(C \\subset \\Theta\\) such that any sequence \\(T_{n}\\) satisfying (2.8) a.s. ultimately stays in \\(C\\). THEOREM 2.4 If (B-1), (B-2'), and (B-3) hold, then every sequence \\(T_{n}\\) satisfying (2.8) and the conclusion of Lemma 2.3 converges to \\(\\theta_{0}\\) almost surely. An analogous statement is true for convergence in probability.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#63-asymptotic-normality-of-m-estimates","title":"6.3 ASYMPTOTIC NORMALITY OF M-ESTIMATES","text":"<p>In the following \\(\\Theta\\) is an open subset of \\(m\\)-dimensional Euclidean space \\(\\mathbb{R}^{m},(\\mathscr{H}, \\mathscr{Q}, P)\\) is a probability space, and \\(\\psi: \\mathscr{H} \\times \\Theta \\rightarrow \\mathbb{R}^{m}\\) is some function.</p> <p>Assume that \\(x_{1}, x_{2}, \\ldots\\) are independent random variables with values in \\(\\mathscr{H}\\) and common distribution \\(P\\). We give sufficient conditions to ensure that every sequence \\(T_{n}=T_{n}\\left(x_{1}, \\ldots, x_{n}\\right)\\) satisfying</p> \\[ \\frac{1}{\\sqrt{n}} \\sum \\psi\\left(x_{i}, T_{n}\\right) \\rightarrow 0 \\] <p>in probability is asymptotically normal; we assume that consistency of \\(T_{n}\\) has already been proved by some other means.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#assumptions_3","title":"ASSUMPTIONS","text":"<p>(N-1) For each fixed \\(\\theta \\in \\Theta, \\psi(x, \\theta)\\) is \\(\\mathscr{Q}\\)-measurable, and \\(\\psi\\) is separable [see the preceding section, (A-1)].</p> <p>Put</p> \\[ \\begin{aligned} \\lambda(\\theta) &amp; =E \\psi(x, \\theta) \\\\ u(x, \\theta, d) &amp; =\\sup _{|\\tau-\\theta|&lt;d}|\\psi(x, \\tau)-\\psi(x, \\theta)| . \\end{aligned} \\] <p>Expectations are always taken with respect to the true underlying \\(P\\). (N-2) There is a \\(\\theta_{0}\\) such that \\(\\lambda\\left(\\theta_{0}\\right)=0\\). (N-3) There are strictly positive numbers \\(a, b, c, d_{0}\\) such that (i)</p> \\[ |\\lambda(\\theta)| \\geqslant a\\left|\\theta-\\theta_{0}\\right|, \\quad \\text { for } \\quad\\left|\\theta-\\theta_{0}\\right| \\leqslant d_{0} \\] <p>(ii)</p> \\[ E u(x, \\theta, d) \\leqslant b d, \\quad \\text { for } \\quad\\left|\\theta-\\theta_{0}\\right|+d \\leqslant d_{0} \\] <p>(iii)</p> \\[ E\\left[U(x, \\theta, d)^{2}\\right] \\leqslant c d \\quad \\text { for } \\quad\\left|\\theta-\\theta_{0}\\right|+d \\leqslant d_{0} \\] <p>Here, \\(|\\theta|\\) denotes any norm equivalent to Euclidean norm. Condition (iii) is somewhat stronger than needed; the proof can still be pushed through with \\(E\\left[u(x, \\theta, d)^{2}\\right] \\leqslant o\\left(|\\log d|^{-1}\\right)\\). (N-4) The expectation \\(E\\left(\\left|\\psi\\left(x, \\theta_{0}\\right)\\right|^{2}\\right)\\) is nonzero and finite. THEOREM 3.1 Assume that ( \\(\\mathrm{N}-1\\) ) to ( \\(\\mathrm{N}-4\\) ) hold and that \\(T_{n}\\) satisfies (3.1). If \\(P\\left(\\left|T_{n}-\\theta_{0}\\right| \\leqslant d_{0}\\right) \\rightarrow 1\\), then</p> \\[ \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi\\left(x_{i}, \\theta_{0}\\right)+\\sqrt{n} \\lambda\\left(T_{n}\\right) \\rightarrow 0 \\] <p>in probability.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#proof-see-huber-1967","title":"Proof See Huber (1967).","text":"<p>COROLLARY 3.2 In addition to the assumptions of Theorem 3.1, assume that \\(\\lambda\\) has a nonsingular derivative matrix \\(\\Lambda\\) at \\(\\theta_{0}\\) [i.e., \\(\\left.\\left|\\lambda(\\theta)-\\lambda\\left(\\theta_{0}\\right)\\right.\\right.\\) \\(\\left.\\left.-\\Lambda \\cdot\\left(\\theta-\\theta_{0}\\right)\\right|=o\\left(\\left|\\theta-\\theta_{0}\\right|\\right)\\right]\\). Then \\(\\sqrt{n}\\left(T_{n}-\\theta_{0}\\right)\\) is asymptotically normal with mean 0 and covariance matrix \\(\\Lambda^{-1} C\\left(\\Lambda^{T}\\right)^{-1}\\), where \\(C\\) is the covariance matrix of \\(\\psi\\left(x, \\theta_{0}\\right)\\).</p> <p>Consider now the ordinary ML estimator, that is, assume that \\(d P=\\) \\(f\\left(x, \\theta_{0}\\right) d \\mu\\) and that \\(\\psi(x, \\theta)=(\\partial / \\partial \\theta) \\log f(x, \\theta)\\). Assume that \\(\\psi(x, \\theta)\\) is jointly measurable, that ( \\(\\mathrm{N}-1\\) ), ( \\(\\mathrm{N}-3\\) ), and ( \\(\\mathrm{N}-4\\) ) hold locally uniformly in \\(\\theta_{0}\\), and that the ML estimator is consistent. Assume furthermore that the Fisher information matrix</p> \\[ I(\\theta)=\\int \\psi(x, \\theta) \\psi(x, \\theta)^{T} f(x, \\theta) d \\mu \\] <p>is continuous at \\(\\theta_{0}\\). PROPOSITION 3.3 Under the assumptions just mentioned, we have \\(\\lambda\\left(\\theta_{0}\\right)=0, \\Lambda=-C=-I\\left(\\theta_{0}\\right)\\), and, in particular, \\(\\Lambda^{-1} C\\left(\\Lambda^{T}\\right)^{-1}=I\\left(\\theta_{0}\\right)^{-1}\\). That is, the ML estimator is efficient.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#proof-see-huber-1967_1","title":"Proof See Huber (1967).","text":"<p>Example 3.1 \\(L_{p}\\)-Estimates Define an \\(m\\)-dimensional estimate \\(T_{n}\\) of location by the property that it minimizes \\(\\Sigma\\left|x_{i}-T_{n}\\right|^{p}\\), where \\(1 \\leqslant p \\leqslant 2\\), and || denotes the usual Euclidean norm. Equivalently, we could define it through \\(\\Sigma \\psi\\left(x_{i} ; T_{n}\\right)=0\\) with</p> \\[ \\psi(x, \\theta)=-\\frac{1}{p} \\frac{\\partial}{\\partial \\theta}\\left(|x-\\theta|^{p}\\right)=|x-\\theta|^{p-2}(x-\\theta) \\] <p>Assume that \\(m \\geqslant 2\\).</p> <p>A straightforward calculation shows that \\(u\\) and \\(u^{2}\\) satisfy Lipschitz conditions of the form</p> \\[ \\begin{gathered} u(x, \\theta, d) \\leqslant c_{1} \\cdot d \\cdot|x-\\theta|^{p-2} \\\\ u^{2}(x, \\theta, d) \\leqslant c_{2} \\cdot d \\cdot|x-\\theta|^{p-2} \\end{gathered} \\] <p>for \\(0 \\leqslant d \\leqslant d_{0}&lt;\\infty\\). Thus assumptions ( \\(\\mathrm{N}-3\\) ) (ii) and (iii) are satisfied, provided</p> \\[ E\\left(|x-\\theta|^{p-2}\\right) \\leqslant K&lt;\\infty \\] <p>in some neighborhood of \\(\\theta_{0}\\). This certainly holds if the true underlying distribution has a density with respect to Lebesgue measure. Furthermore under the same condition (3.9), we have</p> \\[ \\frac{\\partial}{\\partial \\theta} \\lambda(\\theta)=E \\frac{\\partial \\psi(x, \\theta)}{\\partial \\theta} \\] <p>Thus</p> \\[ \\operatorname{tr} \\frac{\\partial \\lambda}{\\partial \\theta}=E \\operatorname{tr} \\frac{\\partial \\psi}{\\partial \\theta}=-(m+p-2) E\\left(|x-\\theta|^{p-2}\\right)&lt;0 \\] <p>hence (N-3) (i) is also satisfied. Assumption ( \\(\\mathrm{N}-1\\) ) is immediate, ( \\(\\mathrm{N}-2\\) ) and ( \\(\\mathrm{N}-4\\) ) hold if \\(E\\left(|x|^{2 p-2}\\right)&lt;\\infty\\), and consistency follows either from verifying (B-1) to (B-4) [with \\(b(\\theta)=\\) \\(\\left.\\max \\left(1,|\\theta|^{p-1}\\right)\\right]\\) or from an easy ad hoc proof using convexity of \\(\\rho(x, \\theta)=\\) \\(|x-\\theta|^{p}\\).</p> <p>Occasionally, the theorems of this and of the preceding section are also useful in the one-dimensional case. Example 3.2 Let \\(\\mathscr{R}=\\Theta=\\mathbb{R}\\), and let</p> \\[ \\begin{aligned} \\rho(x, \\theta) &amp; =\\frac{1}{2}(x-\\theta)^{2}, &amp; &amp; \\text { for } \\quad|x-\\theta| \\leqslant k \\\\ &amp; =\\frac{1}{2} k^{2} &amp; &amp; \\text { for } \\quad|x-\\theta|&gt;k . \\end{aligned} \\] <p>Assumption (A-4) of Section 6.2, namely unicity of \\(\\theta_{0}\\), imposes a restriction on the true underlying distribution; the other assumptions (A-1), (A-2), (A-3), and (A-5) are trivially satisfied [with \\(a(x) \\equiv 0, b(\\theta) \\equiv \\frac{1}{2} k^{2}\\), \\(h(x) \\equiv 0\\) ]. Then the \\(T_{n}\\) minimizing \\(\\Sigma \\rho\\left(x_{i}, T_{n}\\right)\\) is a consistent estimate of \\(\\theta_{0}\\).</p> <p>Under slightly more stringent conditions, it is also asymptotically normal. Assume for simplicity that \\(\\theta_{0}=0\\), and assume that the true underlying distribution function \\(F\\) has a density \\(F^{\\prime}\\) in some neighborhoods of the points \\(\\pm k\\), and that \\(F^{\\prime}\\) is continuous at these points. Assumptions ( \\(\\mathrm{N}-1\\) ), (N-2), (N-3) (ii), (iii), and (N-4) are obviously satisfied with \\(\\psi(x, \\theta)=\\) \\((\\partial / \\partial \\theta) \\rho(x, \\theta)\\). If</p> \\[ \\int_{-k}^{k} F(d x)-k F^{\\prime}(-k)-k F^{\\prime}(k)&gt;0 \\] <p>then (N-3) (i) is also satisfied. We can easily check that Corollary 3.2 is applicable; hence \\(T_{n}\\) is asymptotically normal.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#64-simultaneous-m-estimates-of-location-and-scale","title":"6.4 SIMULTANEOUS M-ESTIMATES OF LOCATION AND SCALE","text":"<p>In order to make an \\(M\\)-estimate of location scale invariant, we must couple it with an estimate of scale.</p> <p>If the underlying distribution \\(F\\) is symmetric, location estimates \\(T\\) and scale estimates \\(S\\) typically are asymptotically independent, and the asymptotic behavior of \\(T\\) depends on \\(S\\) only through the asymptotic value \\(S(F)\\). We can therefore afford to choose \\(S\\) on criteria other than low statistical variability.</p> <p>Consider the simultaneous maximum likelihood estimates of \\(\\theta\\) and \\(\\sigma\\) for a family of densities</p> \\[ \\frac{1}{\\sigma} f\\left(\\frac{x-\\theta}{\\sigma}\\right) \\] <p>that is, the values \\(\\hat{\\theta}\\) and \\(\\hat{\\sigma}\\) maximizing</p> \\[ \\prod_{i&lt;n} \\frac{1}{\\sigma} f\\left(\\frac{x_{i}-\\theta}{\\sigma}\\right) \\] <p>Evidently, these satisfy the following system of equations [with \\(\\psi(x)=\\) \\(-(d / d x) \\log f(x)]\\)</p> \\[ \\begin{gathered} \\sum \\psi\\left(\\frac{x_{i}-\\theta}{\\sigma}\\right)=0 \\\\ \\sum\\left[\\psi\\left(\\frac{x_{i}-\\theta}{\\sigma}\\right) \\frac{x_{i}-\\theta}{\\sigma}-1\\right]=0 \\end{gathered} \\] <p>We generalize this and call simultaneous M-estimate of location and scale any pair of statistics ( \\(T_{n}, S_{n}\\) ) determined by two equations of the form</p> \\[ \\begin{aligned} &amp; \\sum \\psi\\left(\\frac{x_{i}-T_{n}}{S_{n}}\\right)=0 \\\\ &amp; \\sum \\chi\\left(\\frac{x_{i}-T_{n}}{S_{n}}\\right)=0 \\end{aligned} \\] <p>Evidently, \\(T_{n}=T\\left(F_{n}\\right)\\) and \\(S_{n}=S\\left(F_{n}\\right)\\) can be expressed in terms of functionals \\(T\\) and \\(S\\), defined by</p> \\[ \\begin{aligned} &amp; \\int \\psi\\left(\\frac{x-T(F)}{S(F)}\\right) F(d x)=0 \\\\ &amp; \\int \\chi\\left(\\frac{x-T(F)}{S(F)}\\right) F(d x)=0 \\end{aligned} \\] <p>Neither \\(\\psi\\) nor \\(\\chi\\) need be determined by a probability density as in (4.3) and (4.4). In most cases, however, \\(\\psi\\) will be an odd and \\(\\chi\\) an even function.</p> <p>As before the influence functions can be found straightforwardly by inserting \\(F_{t}=(1-t) F+t \\delta_{x}\\) for \\(F\\) into (4.7) and (4.8), and then taking the derivative with respect to \\(t\\) at \\(t=0\\). We obtain that the two influence curves \\(I C(x ; F, T)\\) and \\(I C(x ; F, S)\\) satisfy the system of equations</p> \\[ I C(x ; F, T) \\int \\psi^{\\prime}(y) F(d x)+I C(x ; F, S) \\int \\psi^{\\prime}(y) y F(d x)=\\psi(y) S(F) \\] \\[ I C(x ; F, T) \\int \\chi^{\\prime}(y) F(d x)+I C(x ; F, S) \\int \\chi^{\\prime}(y) y F(d x)=\\chi(y) S(F) \\] <p>where \\(y\\) is short for \\(y=[x-T(F)] / S(F)\\). If \\(F\\) is symmetric, \\(\\psi\\) is odd, and \\(\\chi\\) is even, some integrals vanish for reasons of symmetry and there are considerable simplifications:</p> \\[ I C(x ; F, T)=\\frac{\\psi\\left(\\frac{x}{S(F)}\\right) S(F)}{\\int \\psi^{\\prime}\\left(\\frac{x}{S(F)}\\right) F(d x)} \\] \\[ I C(x ; F, S)=\\frac{\\chi\\left(\\frac{x}{S(F)}\\right) S(F)}{\\int \\chi^{\\prime}\\left(\\frac{x}{S(F)}\\right) \\frac{x}{S(F)} F(d x)} \\] <p>Example 4.1 Let</p> \\[ \\psi(x)=\\max [-k, \\min (k, x)] \\] <p>and</p> \\[ \\chi(x)=\\min \\left(c^{2}, x^{2}\\right)-\\beta \\] <p>where \\(0&lt;\\beta&lt;c^{2}\\). With \\(\\beta=\\beta(c)\\),</p> \\[ \\beta(c)=\\int \\min \\left(c^{2}, x^{2}\\right) \\Phi(d x) \\] <p>we obtain consistency of the scale estimate at the normal model. This example is a combination of the asymptotic minimax estimates of location (Section 4.6) and of scale (Section 5.7); \\(k\\) and \\(c=x_{1}\\) might be determined from (4.5.21) and (5.6.14), respectively. A simplified version of this estimate uses \\(c=k\\) [Huber (1964), p. 96 \"Proposal 2\"], that is,</p> \\[ \\chi(x)=\\psi(x)^{2}-\\beta(k) \\] <p>Example 4.2 Median and Median Absolute Deviation Let</p> \\[ \\begin{aligned} &amp; \\psi(x)=\\operatorname{sign}(x) \\\\ &amp; \\chi(x)=\\operatorname{sign}(|x|-1) \\end{aligned} \\] <p>A (formal) evaluation of (4.9) and (4.10) gives</p> \\[ I C(x ; F, T)=\\frac{\\operatorname{sign}(x-T(F))}{2 f(T(F))} \\] <p>and</p> \\[ I C(x ; F, S)=\\frac{\\operatorname{sign}(|x-T|-S)-\\frac{f(T+S)-f(T-S)}{f(T)} \\operatorname{sign}(x-T)}{2[f(T+S)+f(T-S)]} \\] <p>If \\(F\\) is symmetric, (4.20) simplifies to</p> \\[ I C(x ; F, S)=\\frac{\\operatorname{sign}(|x|-S(F))}{4 f(S(F))} \\]"},{"location":"research/note/lecture-note-automation/result/ocr_result/#existence-and-uniqueness-of-the-solutions-of-47-and-48","title":"Existence and Uniqueness of the solutions of (4.7) and (4.8)","text":"<p>We follow Scholz (1971). Assume that \\(\\psi\\) and \\(\\chi\\) are differentiable, that \\(\\psi^{\\prime}&gt;0\\), that \\(\\psi\\) has a zero at \\(x=0\\) and \\(\\chi\\) has a minimum at \\(x=0\\), and that \\(\\chi^{\\prime} / \\psi^{\\prime}\\) is strictly monotone. [In the particular case \\(\\chi(x)=\\psi(x)^{2}-\\beta\\), this last assumption follows from \\(\\psi^{\\prime}&gt;0\\) ]. \\(F\\) is indifferently either the true or the empirical distribution.</p> <p>The Jacobian of the map</p> \\[ (t, s) \\rightarrow\\left(\\int \\psi\\left(\\frac{x-t}{s}\\right) F(d x), \\int \\chi\\left(\\frac{x-t}{s}\\right) F(d x)\\right) \\] <p>is</p> \\[ -\\frac{1}{s}\\left\\{\\begin{array}{ll} \\int \\psi^{\\prime}(y) d F &amp; \\int y \\psi^{\\prime}(y) d F \\\\ \\int \\chi^{\\prime}(y) d F &amp; \\int y \\chi^{\\prime}(y) d F \\end{array}\\right\\} \\] <p>with \\(y=(x-t) / s\\). We define a new probability measure \\(F^{*}\\) by</p> \\[ F^{*}(d y)=\\frac{\\psi^{\\prime}(y)}{E_{F}\\left[\\psi^{\\prime}(y)\\right]} F(d x) \\] <p>then the Jacobian can be written as</p> \\[ -\\frac{1}{s} E_{F}\\left[\\psi^{\\prime}(y)\\right]\\left\\{\\begin{array}{cc} 1 &amp; E_{F^{*}}(y) \\\\ E_{F^{*}}\\left(\\frac{\\chi^{\\prime}}{\\psi^{\\prime}}\\right) &amp; E_{F^{*}} y\\left(\\frac{\\chi^{\\prime}}{\\psi^{\\prime}}\\right) \\end{array}\\right\\} \\] <p>Its determinant</p> \\[ \\left[\\frac{E_{F} \\psi^{\\prime}(y)}{s}\\right]^{2} \\operatorname{cov}_{F^{*}}\\left(y, \\frac{\\chi^{\\prime}}{\\psi^{\\prime}}\\right) \\] <p>is strictly positive unless \\(F\\) is concentrated at a single point. To prove this, let \\(f\\) and \\(g\\) be any two strictly monotone functions, and let \\(Y_{1}\\) and \\(Y_{2}\\) be two independent, identically distributed random variables. As \\(\\left[f\\left(Y_{1}\\right)-\\right.\\) \\(\\left.f\\left(Y_{2}\\right)\\right]\\left[g\\left(Y_{1}\\right)-g\\left(Y_{2}\\right)\\right]&gt;0\\) unless \\(Y_{1}=Y_{2}\\), we have</p> \\[ \\operatorname{cov}\\left[f\\left(Y_{1}\\right), g\\left(Y_{1}\\right)\\right]=\\frac{1}{2} E\\left\\{\\left[f\\left(Y_{1}\\right)-f\\left(Y_{2}\\right)\\right]\\left[g\\left(Y_{1}\\right)-g\\left(Y_{2}\\right)\\right]\\right\\}&gt;0 \\] <p>unless \\(P\\left(Y_{1}=Y_{2}\\right)=1\\). Thus as the diagonal elements of the Jacobian are strictly negative, and its determinant is strictly positive, we conclude [cf. Gale and Nikaid\u00f4 (1965), Theorem 4] that (4.22) is a one-to-one map.</p> <p>The existence of a solution now follows from the observations (1) that, for each fixed \\(s\\), the first component of (4.22) has a unique zero at some \\(t=t(s)\\) that depends continuously on \\(s\\), and (2) that the second component \\(\\int \\chi\\{[x-t(s)] / s\\} F(d x)\\) ranges from \\(\\chi(0)\\) to (at least) \\((1-\\eta) \\chi( \\pm \\infty)+\\eta \\chi(0)\\), where \\(\\eta\\) is the largest pointmass of \\(F\\), when \\(s\\) varies from \\(\\infty\\) to 0 . We now conclude from the intermediate value theorem for continuous functions that \\([T(F), T(S)]\\) exists uniquely, provided \\(\\chi(0)&lt;0&lt;\\chi( \\pm \\infty)\\) and \\(F\\) does not have pointmasses that are too large; the largest one should satisfy \\(\\eta&lt;\\chi( \\pm \\infty) /[\\chi( \\pm \\infty)-\\chi(0)]\\).</p> <p>The special case of Example 4.1 is not covered by this proof, as \\(\\psi\\) is not strictly monotone, but the result remains valid [approximate \\(\\psi\\) by strictly monotone functions; for a direct proof see Huber (1964), p. 98; cf. also Section 7.7].</p> <p>It is intuitively obvious (and easy to check rigorously) that the map \\(F \\rightarrow(T(F), S(F))\\) is not only well defined but also weakly continuous, provided \\(\\psi\\) and \\(\\chi\\) are bounded; hence \\(T\\) and \\(S\\) are qualitatively robust in Hampel's sense. The Glivenko-Cantelli theorem then implies consistency of \\(\\left(T_{n}, S_{n}\\right)\\). The monotonicity and differentiability properties of \\(\\psi\\) and \\(\\chi\\) make it relatively easy to check assumptions ( \\(\\mathrm{N}-1\\) ) to ( \\(\\mathrm{N}-4\\) ) of Section 6.3, and, since the map (4.22) is differentiable by assumption, \\(\\left(T_{n}, S_{n}\\right)\\) is asymptotically normal in virtue of Corollary 3.2. The special case of Example 4.1 is again not quite covered; if \\(F\\) puts pointmasses on the discontinuities of \\(\\psi^{\\prime}\\), asymptotic normality is destroyed just as in the case of location alone (Section 3.2), but for finite \\(n\\) the case is now milder, because the random fluctuations in the scale estimate smooth away these discontinuities.</p> <p>If \\(F\\) is symmetric, and \\(\\psi\\) and \\(\\chi\\) skew symmetric and symmetric, respectively, the location and scale estimates are uncorrelated for symmetry reasons, and hence asymptotically independent.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#65-m-estimates-with-preliminary-estimates-of-scale","title":"6.5 M-ESTIMATES WITH PRELIMINARY ESTIMATES OF SCALE","text":"<p>The simultaneous solution of two equations (4.5) and (4.6) is perhaps unnecessarily complicated. A somewhat simplified variant is an \\(M\\)-estimate of location with a preliminary estimate of scale: take any estimate \\(S_{n}=S\\left(F_{n}\\right)\\) of scale, and determine location from (4.5) or (4.7), respectively. If the influence function of the scale estimate is known, then the influence function of the location estimate can be determined from (4.9), or, in the symmetric case, simply from (4.11). Note that, in the symmetric case, only the limiting value \\(S(F)\\), but neither the influence function nor the asymptotic variance of \\(S\\), enters into the expression for the influence function of \\(T\\).</p> <p>Another, even simpler, variant is the so-called one-step \\(M\\)-estimate. Here, we start with some preliminary estimates \\(T_{0}(F)\\) and \\(S_{0}(F)\\) of location and scale, and then we solve (4.7) approximately for \\(T\\) by applying Newton's rule just once. Since the Taylor expansion of (4.7) with respect to \\(T\\) at \\(T_{0}=T_{0}(F)\\) begins with \\(\\int \\psi\\left(\\frac{x-T}{S_{0}}\\right) F(d x)=\\int \\psi\\left(\\frac{x-T_{0}}{S_{0}}\\right) F(d x)-\\frac{T-T_{0}}{S_{0}} \\int \\psi^{\\prime}\\left(\\frac{x-T_{0}}{S_{0}}\\right) F(d x)+\\cdots\\), this estimate can be formally defined by the functional</p> \\[ T(F)=T_{0}(F)+\\frac{S_{0} \\int \\psi\\left(\\frac{x-T_{0}}{S_{0}}\\right) F(d x)}{\\int \\psi^{\\prime}\\left(\\frac{x-T_{0}}{S_{0}}\\right) F(d x)} \\] <p>The influence function corresponding to (5.1) can be calculated straightforwardly, if those of \\(T_{0}\\) and \\(S_{0}\\) are known. In the general asymmetric case this leads to unpleasantly complicated expressions:</p> \\[ \\begin{aligned} I C(x ; F, T)= &amp; \\frac{S_{0}}{\\int \\psi^{\\prime}} \\psi-\\frac{S_{0} \\int \\psi}{\\left(\\int \\psi^{\\prime}\\right)^{2}} \\psi^{\\prime}+\\frac{\\int \\psi \\int \\psi^{\\prime \\prime}}{\\left(\\int \\psi^{\\prime}\\right)^{2}} I C\\left(x ; F, T_{0}\\right) \\\\ &amp; +\\left[\\frac{\\int \\psi}{\\int \\psi^{\\prime}}-\\frac{\\int y \\psi^{\\prime}}{\\int \\psi^{\\prime}}+\\frac{\\int \\psi \\int y \\psi^{\\prime \\prime}}{\\left(\\int \\psi^{\\prime}\\right)^{2}}\\right] I C\\left(x ; F, S_{0}\\right) \\end{aligned} \\] <p>where the argument of \\(\\psi, \\psi^{\\prime}\\), and \\(\\psi^{\\prime \\prime}\\) in all instances is \\(y=\\) \\(\\left[x-T_{0}(F)\\right] / S_{0}(F)\\), and all integrals are with respect to \\(d F\\).</p> <p>If we assume that \\(T_{0}\\) is translation invariant and odd,</p> \\[ \\begin{aligned} &amp; T\\left(F_{X+c}\\right)=T\\left(F_{X}\\right)+c \\\\ &amp; T\\left(F_{-X}\\right)=-T\\left(F_{X}\\right) \\end{aligned} \\] <p>that \\(\\psi\\) is odd, and that \\(F\\) is symmetric, then all terms except the first vanish, and the formula simplifies again to (4.11):</p> \\[ I C(x ; F, T)=\\frac{\\psi\\left(\\frac{x}{S_{0}(F)}\\right) S_{0}(F)}{\\int \\psi^{\\prime}\\left(\\frac{x}{S_{0}(F)}\\right) F(d x)} \\] <p>It is intuitively clear from the influence functions that the estimate with preliminary scale and the corresponding one-step estimate will both be asymptotically normal and asymptotically equivalent to each other if \\(T_{0}\\) is consistent. Asymptotic normality proofs utilizing one-step estimates as auxiliary devices are usually relatively straightforward to construct.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#66-quantitative-robustness-properties-of-simultaneous-estimates-for-location-and-scale","title":"6.6 QUANTITATIVE ROBUSTNESS PROPERTIES OF SIMULTANEOUS ESTIMATES FOR LOCATION AND SCALE","text":"<p>The breakdown properties of the estimates considered in the preceding two sections are mainly determined by the breakdown of the scale part. Thus they can differ considerably from those of fixed-scale \\(M\\)-estimates of location.</p> <p>To be specific consider first joint \\(M\\)-estimates, assume that both \\(\\psi\\) and \\(\\chi\\) are continuous, that \\(\\psi\\) is odd and \\(\\chi\\) is even, and that both are monotone increasing for positive arguments. We only consider \\(\\varepsilon\\)-contamination (the results for Prohorov- \\(\\varepsilon\\)-neighborhoods are the same). We regard scale as a nuisance parameter and concentrate on the location aspects.</p> <p>Let \\(\\varepsilon_{S}^{*}\\) and \\(\\varepsilon_{T}^{*}\\) be the infima of the set of \\(\\varepsilon\\)-values for which \\(S(F)\\), or \\(T(F)\\), respectively, can become infinitely large. We first note that \\(\\varepsilon_{S}^{*} \\leqslant \\varepsilon_{T}^{*}\\). Otherwise \\(T(F)\\) would break down while \\(S(F)\\) stays bounded; therefore we would have \\(\\varepsilon_{T}^{*}=0.5\\) as in the fixed-scale case, but \\(\\varepsilon_{S}^{*}&gt;0.5\\) is impossible (5.2.7). Scale breakdown by \"implosion,\" \\(S \\rightarrow 0\\), is uninteresting in the present context because then the location estimate is converted into the highly robust sample median.</p> <p>Now let \\(\\{F\\}\\) be a sequence of \\(\\varepsilon\\)-contaminated distributions, \\(F=\\) \\((1-\\varepsilon) F_{0}+\\varepsilon H\\), such that \\(T(F) \\rightarrow \\infty, S(F) \\rightarrow \\infty\\), and \\(\\varepsilon \\rightarrow \\varepsilon_{T}^{*}=\\varepsilon^{*}\\). Without loss of generality we assume that the limit</p> \\[ 0 \\leqslant \\lim \\frac{T(F)}{S(F)}=y \\leqslant \\infty \\] <p>exists (if necessary we pass to a subsequence). We write the defining equations (4.7) and (4.8) as</p> \\[ \\begin{aligned} &amp; (1-\\varepsilon) \\int \\psi\\left(\\frac{x-T}{S}\\right) F_{0}(d x)+\\varepsilon \\int \\psi\\left(\\frac{x-T}{S}\\right) H(d x)=0 \\\\ &amp; (1-\\varepsilon) \\int \\chi\\left(\\frac{x-T}{S}\\right) F_{0}(d x)+\\varepsilon \\int \\chi\\left(\\frac{x-T}{S}\\right) H(d x)=0 \\end{aligned} \\] <p>If we replace the coefficients of \\(\\varepsilon\\) by their upper bounds \\(\\psi(\\infty)\\) and \\(\\chi(\\infty)\\), respectively, we obtain from (6.2) and (6.3), respectively,</p> \\[ \\begin{aligned} &amp; (1-\\varepsilon) \\int \\psi\\left(\\frac{x-T}{S}\\right) F_{0}(d x)+\\varepsilon \\psi(\\infty) \\geqslant 0 \\\\ &amp; (1-\\varepsilon) \\int \\chi\\left(\\frac{x-T}{S}\\right) F_{0}(d x)+\\varepsilon \\chi(\\infty) \\geqslant 0 \\end{aligned} \\] <p>In the limit we have</p> \\[ \\begin{aligned} &amp; \\left(1-\\varepsilon^{*}\\right) \\psi(-y)+\\varepsilon^{*} \\psi(\\infty) \\geqslant 0 \\\\ &amp; \\left(1-\\varepsilon^{*}\\right) \\chi(-y)+\\varepsilon^{*} \\chi(\\infty) \\geqslant 0 \\end{aligned} \\] <p>hence using the symmetry and monotonicity properties of \\(\\psi\\) and \\(\\chi\\),</p> \\[ \\chi^{-1}\\left(-\\frac{\\varepsilon^{*}}{1-\\varepsilon^{*}} \\chi(\\infty)\\right) \\leqslant y \\leqslant \\psi^{-1}\\left(\\frac{\\varepsilon^{*}}{1-\\varepsilon^{*}} \\psi(\\infty)\\right) \\] <p>It follows that the solution \\(\\varepsilon_{0}\\) of</p> \\[ \\chi^{-1}\\left(-\\frac{\\varepsilon}{1-\\varepsilon} \\chi(\\infty)\\right)=\\psi^{-1}\\left(\\frac{\\varepsilon}{1-\\varepsilon} \\psi(\\infty)\\right) \\] <p>is a lower bound for \\(\\varepsilon^{*}\\) (assume for simplicity that \\(\\varepsilon_{0}\\) is unique). It is not difficult to check that this is also an upper bound for \\(\\varepsilon^{*}\\). Assume that \\(\\varepsilon\\) is small enough so that the solution \\([T(F), S(F)]\\) of (6.2)</p> <p>and (6.3) stays bounded for all \\(H\\). In particular, if we let \\(H\\) tend to a pointmass at \\(+\\infty,(6.2)\\) and (6.3) then converge to</p> \\[ \\begin{aligned} &amp; (1-\\varepsilon) \\int \\psi\\left(\\frac{x-T}{S}\\right) F_{0}(d x)+\\varepsilon \\psi(\\infty)=0 \\\\ &amp; (1-\\varepsilon) \\int \\chi\\left(\\frac{x-T}{S}\\right) F_{0}(d x)+\\varepsilon \\chi(\\infty)=0 \\end{aligned} \\] <p>Now let \\(\\varepsilon\\) increase until the solutions \\(T(F)\\) and \\(S(F)\\) of (6.8) and (6.9) begin to diverge. We can again assume that (6.1) holds for some \\(y\\). The limiting \\(\\varepsilon\\) must be at least as large as the breakdown point, and it will satisfy (6.4) and (6.5), with equality signs. It follows that the solution \\(\\varepsilon_{0}\\) of (6.7) is an upper bound for \\(\\varepsilon^{*}\\), and that it is the common breakdown point of \\(T\\) and \\(S\\).</p> <p>Example 6.1 Continuation of Example 4.1 In this case we have \\(\\psi(\\infty)=k\\),</p> \\[ \\psi^{-1}\\left[\\frac{\\varepsilon}{1-\\varepsilon} \\psi(\\infty)\\right]=\\frac{\\varepsilon}{1-\\varepsilon} k \\] <p>hence (6.7) can be written</p> \\[ \\left[\\left(\\frac{\\varepsilon}{1-\\varepsilon}\\right)^{2} k^{2}-\\beta(c)\\right]+\\frac{\\varepsilon}{1-\\varepsilon}\\left[c^{2}-\\beta(c)\\right]=0 \\] <p>If \\(c=k\\), the solution of (6.10) is simply</p> \\[ \\varepsilon^{*}=\\frac{\\beta(k)}{\\beta(k)+k^{2}} \\] <p>For symmetric contamination the variance of the location estimate breaks down \\([(v(\\varepsilon) \\rightarrow \\infty]\\) for</p> \\[ \\varepsilon^{* *}=\\frac{\\beta(k)}{k^{2}} \\] <p>These values should be compared quantitatively to the corresponding breakdown points \\(\\varepsilon^{*}=\\alpha\\) and \\(\\varepsilon^{* *}=2 \\alpha\\) of the \\(\\alpha\\)-trimmed mean. To facilitate this comparison, the following table of breakdown points (Exhibit 6.6.1) also gives the \"equivalent trimming rate\" \\(\\alpha_{\\Phi}=\\Phi(-k)\\), for which the corresponding \\(\\alpha\\)-trimmed mean has the same influence function and the same asymptotic performance at the normal model.</p> Example 6.1  \"Proposal 2\" Example 6.2 Trimmed Mean Equivalent for \\(\\Phi\\), \\(\\alpha=\\Phi(-k)\\) Scale:  Interquartile  Range Scale:  Median  Deviation \\(k\\) \\(\\varepsilon^{*}\\) \\(\\varepsilon^{* *}\\) \\(\\varepsilon^{*}\\) \\(\\varepsilon^{* *}\\) \\(\\varepsilon^{*}\\) \\(\\varepsilon^{* *}\\) \\(\\varepsilon^{*}=\\alpha\\) \\(\\varepsilon^{* *}=2 \\alpha\\) 3.0 0.100 0.111 0.001 0.003 2.5 0.135 0.156 0.006 0.012 2.0 0.187 0.230 0.023 0.046 1.7 0.227 0.294 0.045 0.090 1.5 0.257 0.346 0.067 0.134 1.4 0.273 0.375 0.25 0.5 0.5 0.5 0.081 0.162 1.3 0.290 0.407 0.097 0.194 1.2 0.307 0.441 0.115 0.230 1.1 0.324 0.478 0.136 0.272 1.0 0.340 0.516 0.159 0.318 0.7 0.392 0.645 0.242 0.484 <p>Exhibit 6.6.1 Breakdown points for the estimates of Examples 6.1 and 6.2, and for the trimmed mean with equivalent performance at the normal distribution.</p> <p>Also the breakdown of \\(M\\)-estimates with preliminary estimates of scale is governed by the breakdown of the scale part, but the situation is much simpler. The following example will suffice to illustrate this.</p> <p>Example 6.2 With the same \\(\\psi\\) as in Example 6.1, but with the interquartile range as scale [normalized such that \\(S(\\Phi)=1\\) ], we have \\(\\varepsilon^{*}=0.25\\) and \\(\\varepsilon^{* *}=0.5\\). For the symmetrized version \\(\\hat{S}\\) (the median absolute deviation, cf. Sections 5.1 and 5.3) breakdown is pushed up to \\(\\varepsilon^{*}=\\varepsilon^{* *}=0.5\\). See Exhibit 6.6.1.</p> <p>As a further illustration Exhibit 6.6.2 compares the suprema \\(v_{s}(\\varepsilon)\\) of the asymptotic variances for symmetric \\(\\varepsilon\\)-contamination, for various estimates whose finite sample properties had been investigated by Andrews et al. (1972). Among these estimates:</p> <ul> <li>H14, H10, and H07 are Huber's \"Proposal 2\" with \\(k=1.4,1.0\\), and 0.7 respectively; see Examples 4.1 and 6.1.</li> <li>A14, A10, and A07 have the same \\(\\psi\\) as the corresponding H-estimates, but use MAD/0.6745 as a preliminary estimate of scale (cf. Section 6.5).</li> </ul> <p></p> <ul> <li>25A, 21A, 17A, and 12A are descending Hampel estimates, with the constants \\((a, b, c)\\) given in parentheses [cf. Section 4.8, especially (4.8.8)]; they use MAD as a preliminary estimate of scale.</li> </ul>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#67-the-computation-of-m-estimates","title":"6.7 THE COMPUTATION OF M-ESTIMATES","text":"<p>We describe several variants, beginning with some where the median absolute deviation is used as an auxiliary estimate of scale.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#variant-1-modified-residuals-let","title":"Variant 1 Modified Residuals Let","text":"\\[ \\begin{aligned} &amp; T^{(0)}=\\operatorname{med}\\left\\{x_{i}\\right\\} \\\\ &amp; S^{(0)}=\\operatorname{med}\\left\\{\\left|x_{i}-T^{(0)}\\right|\\right\\} \\end{aligned} \\] <p>Perform at least one Newton step, that is, one iteration of</p> \\[ T^{(m+1)}=T^{(m)}+\\frac{\\frac{1}{n} \\sum \\psi\\left(\\frac{x_{i}-T^{(m)}}{S^{(0)}}\\right) S^{(0)}}{\\frac{1}{n} \\sum \\psi^{\\prime}\\left(\\frac{x_{i}-T^{(m)}}{S^{(0)}}\\right)} \\] <p>Compare Section 6.5 and note that the one-step estimate \\(T^{(1)}\\) is asymptotically \\((n \\rightarrow \\infty)\\) equivalent to the iteration limit \\(T^{(\\infty)}\\), provided that the underlying distribution is symmetric and \\(\\psi\\) is skew symmetric. The denominator in (7.3) is not very critical, and it might be replaced by a constant. If \\(0 \\leqslant \\psi^{\\prime} \\leqslant 1\\), then any constant denominator \\(&gt;\\frac{1}{2}\\) will give convergence (for a proof, see Section 7.8). However, if \\(\\psi\\) is piecewise linear, then (7.3) will lead to the exact solution of</p> \\[ \\sum \\psi\\left(\\frac{x_{i}-T}{S^{(0)}}\\right)=0 \\] <p>in a finite number of steps (if it converges at all). Variant 2 Modified Weights Let \\(T^{(0)}\\) and \\(S^{(0)}\\) be defined as above. Perform a few iterations of</p> \\[ T^{(m+1)}=\\frac{\\sum w_{i}^{(m)} x_{i}}{\\sum w_{i}^{(m)}} \\] <p>with</p> \\[ w_{i}^{(m)}=\\frac{\\psi\\left[\\left(x_{i}-T^{(m)}\\right) / S^{(0)}\\right]}{\\left(x_{i}-T^{(m)}\\right) / S^{(0)}} \\] <p>A convergence proof is also given in Section 7.8; the iteration limit \\(T^{(\\infty)}\\), of course, is a solution of (7.4).</p> <p>Variant 3 Joint M-Estimates of Location and Scale Assume that we want to solve the system</p> \\[ \\begin{aligned} &amp; \\sum \\psi\\left(\\frac{x_{i}-T}{S}\\right)=0 \\\\ &amp; \\sum \\psi^{2}\\left(\\frac{x_{i}-T}{S}\\right)=(n-1) \\beta \\end{aligned} \\] <p>with</p> \\[ \\beta=E_{\\Phi}\\left(\\psi^{2}\\right) \\] <p>and where \\(\\psi\\) is assumed to be skew symmetric and monotone, \\(0&lt;\\psi^{\\prime}&lt;1\\). Start with \\(T^{(0)}\\) and \\(S^{(0)}\\) as above. Let</p> \\[ \\begin{aligned} &amp; {\\left[S^{(m+1)}\\right]^{2}=\\frac{1}{(n-1) \\beta} \\sum \\psi^{2}\\left(\\frac{x_{i}-T^{(m)}}{S^{(m)}}\\right)\\left[S^{(m)}\\right]^{2}} \\\\ &amp; T^{(m+1)}=T^{(m)}+\\frac{\\frac{1}{n} \\sum \\psi\\left(\\frac{x_{i}-T^{(m)}}{S^{(m)}}\\right) S^{(m)}}{\\frac{1}{n} \\sum \\psi^{\\prime}\\left(\\frac{x_{i}-T^{(m)}}{S^{(m)}}\\right)} \\end{aligned} \\] <p>For a convergence proof [with a constant denominator in (7.10)] see Section 7.8.</p> <p>Variant 4 Joint M-Estimates of Location and Scale, Continued Assume that \\(\\psi(x)=\\max [-c, \\min (c, x)]\\). Let \\(m_{1}, m_{2}\\), and \\(m_{3}\\) be the number of observations satisfying \\(x_{i}&lt;T-c S, T-c S&lt;x_{i}&lt;T+c S\\), and \\(T+c S&lt;x_{i}\\),</p> <p>respectively. Then (7.7) and (7.8) can be written</p> \\[ \\begin{gathered} \\sum^{\\prime} x_{i}-m_{2} T+\\left(m_{3}-m_{1}\\right) c S=0 \\\\ \\sum^{\\prime}\\left(x_{i}-T\\right)^{2}+\\left(m_{1}+m_{3}\\right) c^{2} S^{2}-(n-1) \\beta S^{2}=0 \\end{gathered} \\] <p>Here, the primed summation sign indicates that the sum is extended only over the observations for which \\(\\left|x_{i}-T\\right|&lt;c S\\). If we determine \\(T\\) from (7.11) and insert it into (7.12), we obtain the equivalent system</p> \\[ \\begin{aligned} \\bar{x}^{\\prime} &amp; =\\frac{\\sum^{\\prime} x_{i}}{m_{2}} \\\\ S^{2} &amp; =\\frac{\\sum^{\\prime}\\left(x_{i}-\\bar{x}^{\\prime}\\right)^{2}}{(n-1) \\beta-\\left[m_{1}+m_{3}+\\frac{\\left(m_{3}-m_{1}\\right)^{2}}{m_{2}}\\right] c^{2}} \\\\ T &amp; =\\bar{x}^{\\prime}+c S \\frac{\\left(m_{3}-m_{1}\\right)}{m_{2}} \\end{aligned} \\] <p>These three last equations now are used to calculate \\(T\\) and \\(S\\). Assume that we have already determined \\(T^{(m)}\\) and \\(S^{(m)}\\). Find the corresponding partition of the sample according to \\(T^{(m)} \\pm c S^{(m)}\\), then evaluate (7.13) and (7.14) to find \\(S^{(m+1)}\\), and finally find \\(T^{(m+1)}\\) through (7.15), using \\(S^{(m+1)}\\).</p> <p>The convergence of this procedure has not yet been proved, and in fact, there are counterexamples for small values of \\(c\\). But in practice it converges extremely fast and reaches the exact solution in a finite number of steps.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#68-studentizing","title":"6.8 STUDENTIZING","text":"<p>As a matter of principle each estimate \\(T_{n}=T_{n}\\left(x_{1}, \\ldots, x_{n}\\right)\\) of any parameter \\(\\theta\\) should be accompanied by an estimate \\(D_{n}=D_{n}\\left(x_{1}, \\ldots, x_{n}\\right)\\) of its own variability. Since \\(T_{n}\\) will often be asymptotically normal, \\(D_{n}\\) should be standardized such that it estimates the (asymptotic) standard deviation of \\(T_{n}\\), that is</p> \\[ \\mathcal{C}\\left\\{\\sqrt{n}\\left[T_{n}-T(F)\\right]\\right\\} \\rightarrow \\mathbb{R}[0, A(F, T)] \\] <p>and</p> \\[ n D_{n}^{2} \\rightarrow A(F, T) \\] <p>Most likely \\(D_{n}\\) will be put to either of two uses: (1) For finding confidence intervals ( \\(T_{n}-c D_{n}, T_{n}+c D_{n}\\) ) for the unknown true parameter estimated by \\(T_{n}\\). (2) For finding (asymptotic) standard deviations for functions of \\(T_{n}\\) by the so-called \\(\\Delta\\)-method:</p> \\[ \\sigma\\left(h\\left(T_{n}\\right)\\right) \\approx\\left|h^{\\prime}\\left(T_{n}\\right)\\right| \\sigma\\left(T_{n}\\right) \\] <p>In Section 1.2 we have proposed standardizing an estimate \\(T\\) of \\(\\theta\\) such that it would be Fisher consistent at the model, that is, \\(T\\left(F_{\\theta}\\right)=\\theta\\), and otherwise defining the estimand in terms of the limiting value of the estimate.</p> <p>For \\(D\\) we do not have this freedom; the estimand is asymptotically fixed by (8.2). If \\(A\\left(F_{n}, T\\right) \\rightarrow A(F, T)\\), our estimate should therefore satisfy</p> \\[ \\sqrt{n} D_{n} \\approx A\\left(F_{n}, T\\right)^{1 / 2} \\] <p>and we might in fact define \\(D_{n}\\) by this relation, that is,</p> \\[ D_{n}^{2}=\\frac{1}{n(n-1)} \\sum I C\\left(x_{i}, F_{n}, T\\right)^{2} \\] <p>The factor \\(n-1\\) (instead of \\(n\\) ) was substituted to preserve equivalence with the classical formula for the estimated standard deviation of the sample mean.</p> <p>Almost equivalently, we can use the jackknife method (Section 1.5). In some cases both (8.5) and the jackknife fail, for instance for the sample median. In this particular case we can take recourse to the well-known nonparametric confidence intervals for the median, given by the interval between two selected order statistics ( \\(x_{(i)}, x_{(n+1-i)}\\) ). If we then divide \\(x_{(n+1-i)}-x_{(i)}\\) by a suitable constant \\(2 c\\), we may also get an estimate \\(D_{n}\\) satisfying (8.2). In view of the central limit theorem, the proper choice is, asymptotically,</p> \\[ c=\\Phi^{-1}\\left(\\frac{1}{2}+\\frac{1}{2} \\alpha\\right) \\] <p>where \\(\\alpha\\) is the level of the confidence interval.</p> <p>If \\(T_{n}\\) and \\(D_{n}\\) are jointly asymptotically normal, they will be asymptotically independent in the symmetric case (for reasons of symmetry, their covariance is 0 ). We can expect that the quotient</p> \\[ \\frac{T_{n}-T(F)}{D_{n}} \\] <p>will behave very much like a \\(t\\)-statistic, but with how many degrees of freedom?</p> <p>This question is tricky and probably does not have a satisfactory answer. The difficulties are connected with the following points: (1) we intend to use (8.5) not only for normal data; and (2) the answer is interesting only for relatively small sample sizes, where the asymptotic approximations are poor and depend very much on the actual underlying \\(F\\).</p> <p>The common opinion is that the appropriate number of degrees of freedom is somewhat smaller than the classical \\(n-1\\), but by how much is anybody's guess. Since we are typically interested in a 95 or \\(99 \\%\\) confidence interval, it is really the tail behavior of (8.7) that matters. For small \\(n\\) this is overwhelmingly determined by the density of \\(D_{n}\\) near 0 . Huber's approach (1970), which determined an equivalent number of degrees of freedom by matching the asymptotic moments of \\(D_{n}^{2}\\) with those of a \\(\\chi^{2}\\)-distribution, might therefore be rather misleading.</p> <p>All this notwithstanding, (8.5) and (8.7) work remarkably well for \\(M\\)-estimates; compare the extensive Monte Carlo study by Shorack (1976). [Shorack's definition and the use of his number of degrees of freedom \\(d f^{*}\\) in formula (5) are unsound \\(-d f^{*}\\) is not only unstable under small perturbations of \\(\\psi\\), but even gives wrong asymptotic results when used in (5). But for his favorite Hampel estimate, the difference between \\(d f^{*}\\) and \\(n-1\\) is negligible.] Example 8.1 For an \\(M\\)-estimate \\(T\\) of location we obtain, from (8.5) and the influence function (4.11),</p> \\[ n D_{n}^{2}=\\frac{\\frac{1}{n-1} \\sum \\psi\\left(\\frac{x_{i}-T_{n}}{S_{n}}\\right)^{2} S_{n}^{2}}{\\left[\\frac{1}{n} \\sum \\psi^{\\prime}\\left(\\frac{x_{i}-T_{n}}{S_{n}}\\right)\\right]^{2}} \\] <p>Example 8.2 In the case of the \\(\\alpha\\)-trimmed mean \\(\\bar{x}_{\\alpha}\\), an instructive, explicit comparison between the scatter estimates derived from the jack-</p> <p>knife and from the influence function is possible. Assume that the sample is ordered, \\(x_{1} \\leqslant x_{2} \\leqslant \\cdots \\leqslant x_{n}\\). We distinguish two cases: Case \\(\\boldsymbol{A} \\quad g-1 \\leqslant(n-1) \\alpha&lt;n \\alpha \\leqslant g, g\\) integral Then, with \\(p=g-n \\alpha\\), \\(q=g-(n-1) \\alpha\\), we have</p> \\[ (1-2 \\alpha) n \\bar{x}_{\\alpha, n}=p x_{g}+x_{g+1}+\\cdots+x_{n-g}+p x_{n-g+1} \\] <p>The jackknifed pseudo-observations can be represented as</p> \\[ T_{n i}^{*}=\\frac{1}{1-2 \\alpha}\\left(x_{i}^{W}-\\Delta\\right) \\] <p>where \\(\\left\\{x_{i}^{W}\\right\\}\\) is the \\(\\alpha^{\\prime}\\)-Winsorized sample [with \\(\\alpha^{\\prime}=\\alpha(n-1) / n\\) ]</p> \\[ \\begin{aligned} x_{i}^{W} &amp; =q x_{g}+(1-q) x_{g+1}, &amp; &amp; \\text { for } i \\leqslant g \\\\ &amp; =x_{i}, &amp; &amp; \\text { for } g&lt;i&lt;n-g+1 \\\\ &amp; =(1-q) x_{n-g}+q x_{n-g+1} &amp; &amp; \\text { for } i \\geqslant n-g+1 \\end{aligned} \\] <p>and</p> \\[ \\Delta=\\alpha\\left(x_{g}+x_{n-g+1}\\right) \\] <p>Thus</p> \\[ T_{n}^{*}=\\frac{1}{n} \\sum T_{n i}^{*}=\\bar{x}_{\\alpha, n}+\\frac{g(1-q)}{(1-2 \\alpha) n}\\left[-x_{g}+x_{g+1}+x_{n-g}-x_{n-g+1}\\right] \\] <p>and we obtain the jackknifed variance</p> \\[ n D_{n}^{2}=\\frac{1}{n-1} \\sum\\left(T_{n i}^{*}-T_{n}^{*}\\right)^{2}=\\frac{1}{n-1} \\frac{1}{(1-2 \\alpha)^{2}} \\sum\\left(x_{i}^{W}-\\bar{x}^{W}\\right)^{2} \\] <p>Case B \\((n-1) \\alpha=g-q \\leqslant g \\leqslant g+p=n \\alpha, g\\) integral Then</p> \\[ (1-2 \\alpha) n \\bar{x}_{\\alpha, n}=(1-p) x_{g+1}+x_{g+2}+\\cdots+x_{n-g-1}+(1-p) x_{n-g} \\] <p>Formulas (8.9) to (8.13) remain valid with the following changes:</p> \\[ \\Delta=q x_{g}+p x_{g+1}+p x_{n-g}+q x_{n-g+1} \\] <p>and in (8.12) for \\(T_{n}^{*}\\) the factor in front of the square bracket is changed into \\((n-g) q /(1-2 \\alpha) n\\).</p> <p>The influence function approach (8.5) works as follows. The influence function of the \\(\\alpha\\)-trimmed mean is given by (3.3.18). There is a question of taste whether we should define \\(F_{n}^{-1}(\\alpha)=x_{1 n \\alpha 1}=x_{g}\\) or, by linear interpolation, \\(F_{n}^{-1}(\\alpha)=p x_{g}+(1-p) x_{g+1}\\), with \\(g\\) and \\(p\\) as in Case A. For either choice we obtain the representation</p> \\[ n D_{n}^{2}=\\frac{n}{n-1} \\int I C_{n}^{2} d F_{n}=\\frac{1}{n-1} \\frac{1}{(1-2 \\alpha)^{2}} \\sum\\left(x_{i}^{W}-\\bar{x}^{W}\\right)^{2} \\] <p>where the Winsorizing parameter used in the definition of \\(x_{i}^{W}\\) is \\(g / n\\) or \\(\\alpha\\), respectively. The difference to the jackknifed variance clearly is negligible and has mostly to do with the fine print in the definition of the estimates and sample distribution functions. But obviously, the influence function approach, when available, is cheaper to calculate.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-7","title":"CHAPTER 7","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#regression","title":"Regression","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#71-general-remarks","title":"7.1 GENERAL REMARKS","text":"<p>Regression poses some peculiar and difficult robustness problems. Consider the following example. Assume that a straight line is to be fitted through six points, whose coordinates are given in Exhibit 7.1.1. A least squares fit (fit 1) yields the line shown in Exhibit 7.1.2a. A casual scan of the values in Exhibit 7.1.1 leaves the impression that everything is fine; in particular, none of the residuals \\(r_{i}=y_{i}-\\hat{y}_{i}\\) is exceptionally large when compared to the estimated standard deviation \\(\\hat{\\sigma}\\) of the observations. A closer scrutiny, in particular, a closer look at Exhibit 7.1.2a, may, however, lead to the suspicion that there could be something wrong either with point 1 (which has the largest residual), or, perhaps, with point 6 . If we drop point 6 from the fit, we obtain fit 2 (shown in Exhibit 7.1.2b). But, possibly, a linear model was inappropriate to start with, and we should have fitted a parabola (fit 3, Exhibit 7.1.2c). It is fairly clear that the available data do not suffice to distinguish between these three possibilities. Because of the low residual error \\(\\hat{\\sigma}\\), we might perhaps lean towards</p> Point \\(x\\) \\(y\\) Fit 1 Fit 2 Fit 3 \\(\\hat{y}\\) \\(y-\\hat{y}\\) \\(\\hat{y}\\) \\(y-\\hat{y}\\) \\(\\hat{y}\\) \\(y-\\hat{y}\\) 1 -4 2.48 0.39 2.09 2.04 0.44 2.23 0.25 2 -3 0.73 0.31 0.42 1.06 \\(-0.33\\) 0.99 \\(-0.26\\) 3 -2 \\(-0.04\\) 0.23 \\(-0.27\\) 0.08 \\(-0.12\\) \\(-0.09\\) \\(-0.13\\) 4 -1 \\(-1.44\\) 0.15 \\(-1.59\\) \\(-0.90\\) \\(-0.54\\) \\(-1.00\\) \\(-0.44\\) 5 0 \\(-1.32\\) 0.07 \\(-1.39\\) \\(-1.87\\) 0.55 \\(-1.74\\) 0.42 6 10 0. \\(-0.75\\) 0.75 \\(-11.64\\) (11.64) 0.01 \\(-0.01\\) e.s.d. \\(\\hat{\\sigma}=1.55\\) \\(\\hat{\\sigma}=0.55\\) \\(\\hat{\\sigma}=0.41\\) \\(r_{\\max } / \\hat{\\sigma}=1.35\\) \\(r_{\\max } / \\hat{\\sigma}=1.00\\) \\(r_{\\max } / \\hat{\\sigma}=1.08\\) <p>Exhibit 7.1.1</p> <p></p> <p>Exhibit 7.1.2 (a) Fit 1. (b) Fit 2. (c) Fit 3.</p> <p>the third variant. In actual fact the example is synthetic and the points have been generated by taking the line \\(y=-2-x\\), adding random normal errors (with mean 0 and standard error 0.6 ) to points 1 to 5 , and a gross error of 12 to point 6 . Thus fit 2 is the appropriate one, and it happens to hit the true line almost perfectly.</p> <p>In this case, only two parameters were involved, and a graph like Exhibit 7.1.2 helped to spot the potentially troublesome point number 6 , even if the corresponding residual was quite unobtrusive. But what can be done in more complicated multiparameter problems? The difficulty is of course that a gross error does not necessarily show up through a large residual; by causing an overall increase in the size of other residuals, it can even hide behind a veritable smokescreen. In order to disentangle the issues, we must: (1) Find analytical methods for identifying so-called leverage points, that is, points in the design space where an observation, by virtue of its position, has an overriding influence on the fit and in particular on its own fitted value. Such an observation may be the most important one in the sample (e.g., an isolated astronomical observation from remote antiquity), but on the other hand its value is difficult or impossible to crosscheck. (2) Find routine methods for robust estimation of regression coefficients when there are no leverage points. (3) Find estimation methods that work reasonably well and robustly also in the presence of moderately bad leverage points.</p> <p>Not surprisingly, the treatment of the last part is the least satisfactory of the three.</p> <p>We leave aside all issues connected with ridge regression, Stein estimation, and the like. These questions and robustness seem to be sufficiently orthogonal to each other that it should be possible to superimpose them without very serious interactions.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#72-the-classical-linear-least-squares-case","title":"7.2 THE CLASSICAL LINEAR LEAST SQUARES CASE","text":"<p>The main purpose of this section is to discuss and clarify some of the issues connected with the leverage points.</p> <p>Assume that \\(p\\) unknown parameters \\(\\theta_{1}, \\ldots, \\theta_{p}\\) are to be estimated from \\(n\\) observations \\(y_{1}, \\ldots, y_{n}\\) to which they are related by</p> \\[ y_{i}=\\sum_{j=1}^{p} x_{i j} \\theta_{j}+u_{i} \\] <p>where the \\(x_{i j}\\) are known coefficients and the \\(u_{i}\\) are independent random variables with (approximately) identical distributions. We also use matrix notation,</p> \\[ \\mathbf{y}=X \\boldsymbol{\\theta}+\\mathbf{u} \\] <p>Classically, the problem is solved by minimizing the sum of squares</p> \\[ \\sum_{i}\\left(y_{i}-\\sum x_{i j} \\theta_{j}\\right)^{2}=\\min ! \\] <p>or, equivalently, by solving the system of \\(p\\) equations obtained by differentiating (2.3),</p> \\[ \\sum_{i}\\left(y_{i}-\\sum x_{i k} \\theta_{k}\\right) x_{i j}=0 \\] <p>or, in matrix notation,</p> \\[ X^{T} X \\boldsymbol{\\theta}=X^{T} \\mathbf{y} \\] <p>We assume that \\(X\\) has full rank \\(p\\), so the solution can be written</p> \\[ \\hat{\\boldsymbol{\\theta}}=\\left(X^{T} X\\right)^{-1} X^{T} \\mathbf{y} \\] <p>and, in particular, the fitted values [the least squares estimates \\(\\hat{y}_{i}\\) of the expected values \\(E y_{i}=(X \\boldsymbol{\\theta})_{i}\\) of the observations] are given by</p> \\[ \\hat{\\mathbf{y}}=X\\left(X^{T} X\\right)^{-1} X^{T} \\mathbf{y}=H \\mathbf{y} \\] <p>with</p> \\[ H=X\\left(X^{T} X\\right)^{-1} X^{T} \\] <p>The matrix \\(H\\) is often called \"hat matrix\" (since it puts the hat on \\(\\mathbf{y}\\) ). We note that \\(H\\) is a symmetric \\(n \\times n\\) projection matrix, that is, \\(H H=H\\), and that it has \\(p\\) eigenvalues equal to 1 and \\(n-p\\) eigenvalues equal to 0 . Its diagonal elements, denoted by \\(h_{i}=h_{i i}\\), satisfy</p> \\[ 0 \\leqslant h_{i} \\leqslant 1 \\] <p>and the trace of \\(H\\) is</p> \\[ \\operatorname{tr}(H)=p \\] <p>We now assume that the errors \\(u_{i}\\) are independent and have a common distribution \\(F\\) with mean \\(E u_{i}=0\\) and variance \\(E u_{i}^{2}=\\sigma^{2}&lt;\\infty\\). Assume that our regression problem is imbedded in an infinite sequence of similar problems, such that the number \\(n\\) of observations, and possibly also the number \\(p\\) of parameters, tend to infinity; we suppress the index that gives the position of our problems in this sequence.</p> <p>Questions When is a fitted value \\(\\hat{y}_{i}\\) consistent, in the sense that</p> \\[ \\hat{y}_{i}-E\\left(y_{i}\\right) \\rightarrow 0 \\] <p>in probability? When are all fitted values consistent? Since \\(E \\mathbf{u}=0, \\hat{\\mathbf{y}}\\) is unbiased:</p> \\[ E \\hat{\\mathbf{y}}=E \\mathbf{y}=X \\boldsymbol{\\theta} \\] <p>We have</p> \\[ \\hat{y}_{i}=\\sum h_{i k} y_{k} \\] <p>and thus</p> \\[ \\operatorname{var}\\left(\\hat{y}_{i}\\right)=\\sum_{k} h_{i k}^{2} \\sigma^{2}=h_{i} \\sigma^{2} \\] <p>(note that \\(\\sum_{k} h_{i k}^{2}=h_{i}\\), since \\(H\\) is symmetric and idempotent). Hence by Chebyshev's inequality,</p> \\[ P\\left[\\left|\\hat{y}_{i}-E\\left(y_{i}\\right)\\right| \\geqslant \\varepsilon\\right] \\leqslant \\frac{h_{i} \\sigma^{2}}{\\varepsilon^{2}} \\] <p>and we have proved the sufficiency part of the following proposition. PROPOSITION 2.1 Assume that the errors \\(u_{i}\\) are independent with mean 0 and common variance \\(\\sigma^{2}&lt;\\infty\\). Then \\(\\hat{y}_{i}\\) is consistent iff \\(h_{i} \\rightarrow 0\\), and the fitted values \\(\\hat{y}_{i}\\) are all consistent iff</p> \\[ h=\\max _{1&lt;i&lt;n} h_{i} \\rightarrow 0 \\] <p>Proof We have to show necessity of the condition. This follows easily from</p> \\[ \\hat{y}_{i}-E \\hat{y}_{i}=h_{i} u_{i}+\\sum_{k \\neq i} h_{i k} u_{k} \\] <p>and the remark that, for independent random variables \\(X\\) and \\(Y\\), we have</p> \\[ \\begin{aligned} P(|X+Y| \\geqslant \\varepsilon) &amp; \\geqslant P(X \\geqslant \\varepsilon) P(Y \\geqslant 0)+P(X \\leqslant-\\varepsilon) P(Y&lt;0) \\\\ &amp; \\geqslant \\min [P(X \\geqslant \\varepsilon), P(X \\leqslant-\\varepsilon)] \\end{aligned} \\] <p>Thus</p> \\[ P\\left(\\left|\\hat{y}_{i}-E \\hat{y}_{i}\\right| \\geqslant \\varepsilon\\right) \\geqslant \\min \\left[P\\left(u_{i} \\geqslant \\frac{\\varepsilon}{h_{i}}\\right), P\\left(u_{i} \\leqslant-\\frac{\\varepsilon}{h_{i}}\\right)\\right] \\] <p>Note that \\(h=\\max h_{i} \\geqslant\\) ave \\(h_{i}=\\operatorname{tr}(H) / n=p / n\\); hence \\(h\\) cannot converge to 0 unless \\(p / n \\rightarrow 0\\).</p> <p>The following formulas are straightforward to establish (under the assumptions of the preceding proposition):</p> \\[ \\begin{aligned} \\operatorname{var} \\hat{y}_{i} &amp; =h_{i} \\sigma^{2} \\\\ \\operatorname{var}\\left(y_{i}-\\hat{y}_{i}\\right) &amp; =\\left(1-h_{i}\\right) \\sigma^{2} \\\\ \\operatorname{cov}\\left(\\hat{y}_{i}, \\hat{y}_{k}\\right) &amp; =h_{i k} \\sigma^{2} \\\\ \\operatorname{cov}\\left(y_{i}-\\hat{y}_{i}, y_{k}-\\hat{y}_{k}\\right) &amp; =\\left(\\delta_{i k}-h_{i k}\\right) \\sigma^{2} \\\\ \\operatorname{cov}\\left(\\hat{y}_{i}, y_{k}-\\hat{y}_{k}\\right) &amp; =0, \\quad \\text { for all } i, k \\end{aligned} \\] <p>Now let</p> \\[ \\hat{\\alpha}=\\sum a_{j} \\hat{\\theta}_{j}=\\mathbf{a}^{T} \\hat{\\boldsymbol{\\theta}} \\] <p>be the least squares estimate of an arbitrary linear combination \\(\\alpha=\\mathbf{a}^{T} \\boldsymbol{\\theta}\\). If \\(F\\) is normal, then \\(\\hat{\\alpha}\\) is automatically normal.</p> <p>Question Assume that \\(F\\) is not normal. Under which conditions is \\(\\hat{\\alpha}\\) asymptotically normal (as \\(p, n \\rightarrow \\infty\\) )?</p> <p>Without restricting generality we can choose the coordinate system in the parameter space such that \\(X^{T} X=I\\) is the \\(p \\times p\\) identity matrix. Further-</p> <p>more assume \\(\\mathbf{a}^{T} \\mathbf{a}=1\\). Then \\(\\hat{\\boldsymbol{\\theta}}=X^{T} \\mathbf{y}\\), and</p> \\[ \\hat{\\alpha}=\\mathbf{a}^{T} \\hat{\\boldsymbol{\\theta}}=\\mathbf{a}^{T} X^{T} \\mathbf{y}=\\mathbf{s}^{T} \\mathbf{y} \\] <p>with</p> \\[ \\mathbf{s}=X \\mathbf{a} \\] <p>and</p> \\[ \\mathbf{s}^{T} \\mathbf{s}=\\mathbf{a}^{T} X^{T} X \\mathbf{a}=\\mathbf{a}^{T} \\mathbf{a}=1 \\] <p>Thus</p> \\[ \\operatorname{var}(\\hat{\\alpha})=\\sigma^{2} \\] <p>PROPOSITION 2.2 \\(\\hat{\\alpha}\\) is asymptotically normal iff \\(\\max _{i}\\left|s_{i}\\right| \\rightarrow 0\\). Proof If \\(\\max _{i}\\left|s_{i}\\right| \\rightarrow 0\\), then \\(\\hat{\\alpha}\\) either does not have a limiting distribution at all, or, if it has, the limiting distribution can be written as a convolution of two parts one of which is \\(F\\) (apart from a scale factor); hence it cannot be normal [see, e.g., Feller (1966), p. 498]. If \\(\\gamma=\\max _{i}\\left|s_{i}\\right| \\rightarrow 0\\), then we can easily check Lindeberg's condition:</p> \\[ \\begin{aligned} \\frac{1}{\\sigma^{2}} \\sum_{i} E\\left\\{s_{i}^{2} u_{i}^{2} 1_{\\left\\{\\left|s_{i} u_{i}\\right|&gt;\\varepsilon \\sigma\\right\\}}\\right\\} &amp; &lt;\\frac{1}{\\sigma^{2}} \\sum_{i} s_{i}^{2} E\\left\\{u_{i}^{2} 1_{\\left\\{\\left|u_{i}\\right|&gt;\\varepsilon \\sigma / \\gamma\\right\\}}\\right\\} \\\\ &amp; =\\frac{1}{\\sigma^{2}} E\\left\\{u^{2} 1_{\\left\\{|u|&gt;\\varepsilon \\sigma / \\gamma\\right\\}}\\right\\} \\rightarrow 0 \\end{aligned} \\] <p>This finishes the proof of the proposition. Note that the Schwarz inequality gives</p> \\[ s_{i}^{2}=\\left(\\sum_{k} x_{i k} a_{k}\\right)^{2} \\leqslant \\sum_{k} x_{i k}^{2} \\sum_{k} a_{k}^{2}=h_{i} \\] <p>Hence we obtain, as a corollary, the following theorem. THEOREM 2.3 If \\(h=\\max _{i} h_{i} \\rightarrow 0\\), then all least squares estimates \\(\\hat{\\alpha}=\\) \\(\\sum a_{j} \\hat{\\theta}_{j}=\\mathbf{a}^{T} \\hat{\\boldsymbol{\\theta}}\\) are asymptotically normal. If not, then, in particular, some of the fitted values are not asymptotically normal.</p> <p>Proof The direct part is an immediate consequence of the preceding proposition. For the converse recall that \\(\\hat{y}_{i}=\\sum h_{i k} y_{k}\\). For each \\(n\\) choose \\(i\\) such that \\(h_{i}=h\\). Then the standardized sequence \\(\\left[\\left(\\hat{y}_{i}-E\\left(\\hat{y}_{i}\\right)\\right] / \\sqrt{h_{i}}\\right.\\) has expectation 0 and variance \\(\\sigma^{2}\\), but cannot be asymptotically normal.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#residuals-and-outliers","title":"Residuals and Outliers","text":"<p>The \\(i\\) th residual can be written</p> \\[ r_{i}=y_{i}-\\hat{y}_{i}=\\left(1-h_{i}\\right) y_{i}-\\sum_{k \\neq i} h_{i k} y_{k} \\] <p>Hence if \\(h_{i}\\) is close to 1 , a gross error in \\(y_{i}\\) will not necessarily show up in \\(r_{i}\\). But it might show up elsewhere, say in \\(r_{k}\\), if \\(h_{k i}\\) happens to be large. For instance, in the introductory example of Section 7.1 (fit 1), we have \\(h_{k}=0.936\\). Points with large \\(h_{i}\\) are, by definition, leverage points.</p> <p>We may say that \\(1 / h_{i}\\) is the equivalent number of observations entering into the determination of \\(\\hat{y}_{i}\\). We show later that, if \\(h_{i}=1 / k\\) and if we duplicate the \\(i\\) th row of \\(X\\) (make an additional observation there), then \\(h_{i}\\) is changed to \\(1 /(k+1)\\). In other words if \\(h_{i}\\) is large, it can easily be decreased by duplication or triplication of the observation \\(y_{i}\\). (In practice approximate duplication, i.e., observing under slightly varied conditions, is to be preferred over exact duplication, since this helps to avoid repetition of systematic errors.)</p> <p>We now work this out in detail. We have</p> \\[ H=X\\left(X^{T} X\\right)^{-1} X^{T} \\] <p>What happens if we add another row vector \\(\\mathbf{x}^{T}\\) to \\(X\\) :</p> \\[ \\tilde{X}=\\binom{X}{\\mathbf{x}^{T}} ? \\] <p>Without loss of generality we assume \\(X^{T} X=I\\); then</p> \\[ \\tilde{X}^{T} \\tilde{X}=I+\\mathbf{x x}^{T} \\] <p>We can easily check that</p> \\[ \\left(\\tilde{X}^{T} \\tilde{X}\\right)^{-1}=I-\\frac{\\mathbf{x x}^{T}}{1+\\mathbf{x}^{T} \\mathbf{x}} \\] <p>The modified hat matrix \\(\\tilde{H}\\) is also easy to work out:</p> \\[ \\begin{aligned} \\tilde{H} &amp; =\\tilde{X}\\left(\\tilde{X}^{T} \\tilde{X}\\right)^{-1} \\tilde{X}^{T} \\\\ &amp; =\\left(\\frac{X}{\\mathbf{x}^{T}}\\right)\\left(I-\\frac{\\mathbf{x} \\mathbf{x}^{T}}{1+\\mathbf{x}^{T} \\mathbf{x}}\\right)\\left(X^{T} \\mid \\mathbf{x}\\right) \\\\ &amp; =\\left(\\begin{array}{c|c} X X^{T}-\\frac{(X \\mathbf{x})(X \\mathbf{x})^{T}}{1+\\mathbf{x}^{T} \\mathbf{x}} &amp; \\frac{X \\mathbf{x}}{1+\\mathbf{x}^{T} \\mathbf{x}} \\\\ \\frac{(X \\mathbf{x})^{T}}{1+\\mathbf{x}^{T} \\mathbf{x}} &amp; \\frac{\\mathbf{x}^{T} \\mathbf{x}}{1+\\mathbf{x}^{T} \\mathbf{x}} \\end{array}\\right) \\end{aligned} \\] <p>Example 2.1 Duplication of a row, say row \\(n\\). Then (still assuming \\(\\left.X^{T} X=I\\right)\\) we have \\(\\mathbf{x}^{T} \\mathbf{x}=h_{n}\\), and thus</p> \\[ \\tilde{h}_{n+1}=\\frac{h_{n}}{1+h_{n}} \\] <p>Since there is no possibility of confusion, we omit the tilde on \\(h_{n+1}\\) from now on. In particular, if \\(h_{n}=1 / k\\), we obtain \\(h_{n+1}=1 /(k+1)\\). Example 2.2 Leaving out a row (say row \\(n+1\\), after we have added it): (1) With row \\(n+1\\) in we obtain, from (2.31),</p> \\[ \\operatorname{var}\\left(\\hat{y}_{n+1}\\right)=h_{n+1} \\sigma^{2}=\\frac{\\mathbf{x}^{T} \\mathbf{x}}{1+\\mathbf{x}^{T} \\mathbf{x}} \\sigma^{2} \\] <p>(2) With row \\(n+1\\) out, let \\(\\hat{\\alpha}_{n+1}\\) be the estimate of \\(E\\left(y_{n+1}\\right)\\) based on the remaining observations \\(y_{1}, \\ldots, y_{n}\\) :</p> \\[ \\begin{aligned} \\hat{\\alpha}_{n+1} &amp; =\\mathbf{x}^{T} \\hat{\\boldsymbol{\\theta}} \\\\ \\operatorname{var}\\left(\\hat{\\alpha}_{n+1}\\right) &amp; =\\mathbf{x}^{T} \\mathbf{x} \\sigma^{2}=\\frac{h_{n+1}}{1-h_{n+1}} \\sigma^{2} \\end{aligned} \\] <p>Note that \\(\\operatorname{var}\\left(\\hat{\\alpha}_{n+1}\\right)\\) is larger than \\(\\operatorname{var}\\left(y_{n+1}\\right)\\) if \\(h_{n+1}&gt;\\frac{1}{2}\\). We have</p> \\[ \\hat{y}_{n+1}=\\left(1-h_{n+1}\\right) \\hat{\\alpha}_{n+1}+h_{n+1} y_{n+1} \\] <p>that is the \\((n+1)\\) th fitted value is a convex linear combination of the \"predicted\" value \\(\\hat{\\alpha}_{n+1}\\) (which disregards \\(y_{n+1}\\) ) and the observation \\(y_{n+1}\\), with weights \\(1-h_{n+1}\\) and \\(h_{h+1}\\), respectively. This is shown by a look at the last row of the matrix \\(\\hat{H}\\).</p> <p>In terms of residuals the above formula reads</p> \\[ r_{n+1}=y_{n+1}-\\hat{y}_{n+1}=\\left(1-h_{n+1}\\right)\\left(y_{n+1}-\\hat{\\alpha}_{n+1}\\right) \\] <p>This relation is important; it connects the ordinary residual \\(y_{n+1}-\\hat{y}_{n+1}\\) with the residual \\(y_{n+1}-\\hat{\\alpha}_{n+1}\\) relative to the \"interpolated\" value \\(\\hat{\\alpha}_{n+1}\\) ignoring \\(y_{n+1}\\).</p> <p>Of course, all these relations hold for arbitrary indices, not only for \\(i=n+1\\).</p> <p>We may conclude from this discussion that the diagonal of the hat matrix contains extremely useful information. In particular, large values of \\(h_{i}\\) should serve as warning signals that the \\(i\\) th observation may have a decisive, yet hardly checkable, influence. Values \\(h_{i}&lt;0.2\\) appear to be safe, values between 0.2 and 0.5 are risky, and if we can control the design at all, we had better avoid values above 0.5 .</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#73-robustizing-the-least-squares-approach","title":"7.3 ROBUSTIZING THE LEAST SQUARES APPROACH","text":"<p>The classical equations (2.3) and (2.4) can be robustized in a straightforward way; instead of minimizing a sum of squares, we minimize a sum of less rapidly increasing functions of the residuals:</p> \\[ \\sum_{i=1}^{n} \\rho\\left(y_{i}-\\sum x_{i j} \\theta_{j}\\right)=\\min ! \\] <p>or, after taking derivatives, we solve</p> \\[ \\sum_{i=1}^{n} \\psi\\left(y_{i}-\\sum x_{i j} \\theta_{j}\\right) x_{i k}=0, \\quad k=1, \\ldots, p \\] <p>with \\(\\psi=\\rho^{\\prime}\\). If \\(\\rho\\) is convex, the two approaches are essentially equivalent; otherwise the selection of the \"best\" solution of (3.2) may create problems.</p> <p>We denote the \\(i\\) th residual by</p> \\[ r_{i}=r_{i}(\\theta)=y_{i}-\\sum_{j} x_{i j} \\theta_{j} \\] <p>Note that (3.2) can be viewed as a robustized version of the cross product</p> <p>of the residual vector \\(\\mathbf{r}\\) with the \\(k\\) th column vector of \\(X\\); the residuals \\(r_{i}\\) have been replaced by metrically Winsorized versions \\(\\psi\\left(r_{i}\\right)\\).</p> <p>Ordinarily, scale will not be known, so it will be necessary to make (3.2) scale invariant by introducing some estimate \\(s\\) of scale:</p> \\[ \\sum \\psi\\left(\\frac{r_{i}}{s}\\right) x_{i k}=0 \\] <p>Possibly we might use individual scales \\(s_{i}\\), and even more generally, we might use different functions \\(\\rho_{i}\\) and \\(\\psi_{i}\\) for different observations.</p> <p>We are acting under the assumption that the \\(x_{i j}\\) are known and error-free. If this is not so, it might be advisable to modify not only the residual vector \\(\\mathbf{r}\\), but also the column vectors \\(\\mathbf{x}_{j}\\) in order to gain robustness also with regard to errors in the coefficients \\(x_{i j}\\). There are several obvious proposals for doing so; they may look plausible, but they hitherto lack a sound theoretical underpinning. Conceivably they might do more harm (by introducing bias) than good.</p> <p>We obtain \\(R\\)-estimates of regression if we minimize, instead of (3.1),</p> \\[ \\sum_{i} a_{n}\\left(R_{i}\\right) r_{i}=\\min ! \\] <p>Here, \\(R_{i}\\) is the rank of \\(r_{i}\\) in \\(\\left(r_{1}, \\ldots, r_{n}\\right)\\), and \\(a_{n}(\\cdot)\\) is some monotone scores function satisfying \\(\\Sigma_{i} a_{n}(i)=0\\) [see Jaeckel (1972)]. Note, however, that these estimates are unable to estimate an additive main effect and thus do not contain estimates of location as particular cases. On the contrary the additive main effect has to be estimated by applying an estimate of location to the residuals.</p> <p>If we differentiate (3.5), which is a piecewise linear convex function of \\(\\boldsymbol{\\theta}\\), we obtain the following approximate equalities at the minimum:</p> \\[ \\sum_{i} a_{n}\\left(R_{i}\\right) x_{i k} \\approx 0, \\quad k=1, \\ldots, p \\] <p>These approximate equations in turn can be reconverted into a minimum problem, for example,</p> \\[ \\sum_{k}\\left|\\sum_{i} a_{n}\\left(r_{i}\\right) x_{i k}\\right|=\\min ! \\] <p>This last variant was investigated by Jure\u010dkov\u00e1 (1971), and asymptotic equivalence between (3.6) and (3.7) was shown by Jaeckel (1972). The task</p> <p>of solving (3.6) or (3.7) by linear programming techniques appears to be very formidable, however, unless \\(p\\) and \\(n\\) are quite small.</p> <p>All of the regression estimates allow one-step versions: start with some reasonably good preliminary estimate \\(\\theta^{*}\\), and then apply one step of Newton's method to (3.2), and so on, just as in the location case. A one-step \\(L\\)-estimate of regression has been investigated by Bickel (1973). However, in the regression case it is very difficult to find a good starting value. We know from the location case that the least squares estimate, that is, the sample mean, will not do for one-step estimates, and the analogue of the sample median, which gives an excellent starting point for location, would be the so-called \\(L_{1}\\)-estimate [corresponding to \\(\\rho(X)=|X|]\\), which itself may be harder to compute than most of the robust regression estimates we want to use.</p> <p>It appears that \\(M\\)-estimates offer enough flexibility and are by far the easiest to cope with, simultaneously, with regard to computation, asymptotic theory, and intuitive interpretation; moreover, the step from (2.3) to (3.1) is easily explainable to nonstatisticians also. We therefore restrict ourselves to \\(M\\)-estimates of regression.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#74-asymptotics-of-robust-regression-estimates","title":"7.4 ASYMPTOTICS OF ROBUST REGRESSION ESTIMATES","text":"<p>The obvious approach to asymptotics is: keep the number \\(p\\) of parameters fixed, let the number \\(n\\) of observations go to infinity. However, in practice \\(p\\) and \\(n\\) tend to become large simultaneously; in crystallography, where some of the largest least squares problems occur (with hundreds or thousands of parameters) we find the explicit recommendation that there should be at least five observations per parameter (Hamilton 1970). This suggests that a meaningful asymptotic theory should be in terms of \\(p / n \\rightarrow 0\\), or, perhaps better, in terms of \\(h=\\max h_{c} \\rightarrow 0\\). The point we make here, which is given some technical substantiation later, is that, if the asymptotic theory requires, say, \\(p^{3} / n \\rightarrow 0\\), and if it is able to give a useful approximation for \\(n=20\\) if \\(p=1\\), then, for \\(p=10\\), we would need \\(n=20,000\\) to get an equally good approximation! In an asymptotic theory that keeps \\(p\\) fixed such distinctions do not become visible at all.</p> <p>We begin with a short discussion of the overall regularity conditions. They separate into three parts, conditions on: the design matrix \\(X\\), the estimate, and the error laws.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#conditions-on-the-design-matrix-x","title":"Conditions on the Design Matrix \\(X\\)","text":"<p>\\(X\\) has full rank \\(p\\), and the diagonal elements of the hat matrix</p> \\[ H=X\\left(X^{T} X\\right)^{-1} X^{T} \\] <p>are assumed to be uniformly small:</p> \\[ \\max _{i&lt;i&lt;n} h_{i}=h \\ll 1 \\] <p>The precise order of smallness will be specified from case to case. Without loss of generality we choose the coordinate system in the parameter space such that the true parameter point is \\(\\boldsymbol{\\theta}^{0}=0\\), and such that \\(X^{T} X\\) is the \\(p \\times p\\) identity matrix.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#conditions-on-the-estimate","title":"Conditions on the Estimate","text":"<p>The function \\(\\rho\\) is assumed to be convex and nonmonotone and to possess bounded derivatives of sufficiently high order (approximately four). In particular, \\(\\psi(x)=(d / d x) \\rho(x)\\) should be continuous and bounded. Convexity of \\(\\rho\\) serves to guarantee equivalence between (3.1) and (3.2) and asymptotic uniqueness of the solution. If we are willing to forego this and are satisfied with local uniqueness, the convexity assumption can be omitted. Higher order derivatives are technically convenient, since they make Taylor expansions possible, but their existence does not seem to be essential for the results to hold.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#conditions-on-the-error-laws","title":"Conditions on the Error Laws","text":"<p>We assume that the errors \\(u_{i}\\) are independent, identically distributed, such that</p> \\[ E\\left[\\psi\\left(u_{i}\\right)\\right]=0 \\] <p>We require this in order that the expectation of (3.1) reaches its minimum and the expectation of (3.2) vanishes, at the true value \\(\\boldsymbol{\\theta}^{0}\\).</p> <p>The assumption of independence is a serious restriction. The assumption that the errors are identically distributed simplifies notations and calculations, but could easily be relaxed: \"random\" deviations (i.e., not related to the structure of \\(X\\) ) can be modeled by identical distributions (take the \"averaged\" cumulative distribution). Nonrandom deviations (e.g., changes</p> <p>in scale that depend on \\(X\\) in a systematic fashion) can be handled by a minimax approach if the deviations are small; if they are large, they transgress our notion of robustness.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#the-case-h-p2-rightarrow-0-and-h-p-rightarrow-0","title":"The Case \\(h p^{2} \\rightarrow 0\\) and \\(h p \\rightarrow 0\\)","text":"<p>A simple but rigorous treatment is possible if \\(h p^{2} \\rightarrow 0\\), or, with slightly weaker results, if \\(h p \\rightarrow 0\\). Note that this implies \\(p^{3} / n \\rightarrow 0\\) and \\(p^{2} / n \\rightarrow 0\\), respectively. Thus quite moderate values of \\(p\\) already lead to very large and impractical values for \\(n\\).</p> <p>The idea is to compare the zeros of two vector-valued random functions \\(\\boldsymbol{\\Phi}\\) and \\(\\boldsymbol{\\Psi}\\) of \\(\\boldsymbol{\\theta}\\) :</p> \\[ \\begin{aligned} &amp; \\Phi_{j}(\\boldsymbol{\\theta})=\\frac{-1}{E\\left(\\psi^{\\prime}\\right)} \\sum_{i} \\psi\\left(y_{i}-\\sum x_{i k} \\theta_{k}\\right) x_{i j} \\\\ &amp; \\Psi_{j}(\\boldsymbol{\\theta})=\\theta_{j}-\\frac{1}{E\\left(\\psi^{\\prime}\\right)} \\sum_{i} \\psi\\left(y_{i}\\right) x_{i j} \\end{aligned} \\] <p>The zero \\(\\tilde{\\boldsymbol{\\theta}}\\) of \\(\\boldsymbol{\\Phi}\\) is our estimate. The zero \\(\\tilde{\\boldsymbol{\\theta}}\\) of \\(\\boldsymbol{\\Psi}\\),</p> \\[ \\tilde{\\theta}_{j}=\\frac{1}{E\\left(\\psi^{\\prime}\\right)} \\sum_{i} \\psi\\left(y_{i}\\right) x_{i j} \\] <p>of course is not a genuine estimate, but according to Theorem 2.3 all linear combinations \\(\\hat{\\alpha}=\\sum a_{j} \\hat{\\theta}_{j}\\) are asymptotically normal if \\(h \\rightarrow 0\\). So we can prove asymptotic normality of \\(\\tilde{\\boldsymbol{\\theta}}\\) (or, better, of \\(\\hat{\\alpha}=\\sum a_{j} \\hat{\\theta}_{j}\\) ) by showing that the difference between \\(\\tilde{\\boldsymbol{\\theta}}\\) and \\(\\tilde{\\boldsymbol{\\theta}}\\) is small.</p> <p>Let \\(a_{j}\\) be indeterminate coefficients satisfying \\(\\sum a_{j}^{2}=1\\) and write for short</p> \\[ \\begin{aligned} &amp; s_{i}=\\sum x_{i j} a_{j} \\\\ &amp; t_{i}=\\sum x_{i j} \\theta_{j} \\end{aligned} \\] <p>Since \\(X^{T} X=I\\), we have</p> \\[ \\begin{aligned} &amp; \\|\\mathbf{t}\\|^{2}=(X \\boldsymbol{\\theta})^{T} X \\boldsymbol{\\theta}=\\|\\boldsymbol{\\theta}\\|^{2} \\\\ &amp; \\|\\mathbf{s}\\|^{2}=1 \\end{aligned} \\] <p>We expand \\(\\sum a_{j} \\Phi_{j}(\\boldsymbol{\\theta})\\) into a Taylor series with remainder term</p> \\[ \\sum a_{j} \\Phi_{j}(\\boldsymbol{\\theta})=\\frac{-1}{E\\left(\\psi^{\\prime}\\right)}\\left[\\sum \\psi\\left(y_{i}\\right) s_{i}-\\sum \\psi^{\\prime}\\left(y_{i}\\right) t_{i} s_{i}+\\frac{1}{2} \\sum \\psi^{\\prime \\prime}\\left(y_{i}-\\eta t_{i}\\right) t_{i}^{2} s_{i}\\right] \\] <p>with \\(0&lt;\\eta&lt;1\\). This can be rearranged to give</p> \\[ \\sum a_{j}\\left[\\Phi_{j}(\\boldsymbol{\\theta})-\\Psi_{j}(\\boldsymbol{\\theta})\\right]=\\sum_{j k} \\Delta_{j k} a_{j} \\theta_{k}-\\frac{1}{2 E\\left(\\psi^{\\prime}\\right)} \\sum_{i} \\psi^{\\prime \\prime}\\left(y_{i}-\\eta t_{i}\\right) t_{i}^{2} s_{i} \\] <p>where</p> \\[ \\Delta_{j k}=\\frac{1}{E\\left(\\psi^{\\prime}\\right)} \\sum_{i}\\left[\\psi^{\\prime}\\left(y_{i}\\right)-E \\psi^{\\prime}\\left(y_{i}\\right)\\right] x_{i j} x_{i k} \\] <p>We now intend to show that \\(\\boldsymbol{\\Phi}-\\boldsymbol{\\Psi}\\) is uniformly small in a neighborhood of \\(\\boldsymbol{\\theta}=0\\), or more precisely, that (4.12) is uniformly small on sets of the form</p> \\[ \\{(\\boldsymbol{\\theta}, \\mathbf{a}) \\mid\\|\\boldsymbol{\\theta}\\|^{2} \\leqslant K p,\\|\\mathbf{a}\\|=1\\} \\] <p>By the Schwarz inequality the first term on the right-hand side of (4.12) can be bounded as follows:</p> \\[ \\left(\\sum \\Delta_{j k} a_{j} \\theta_{k}\\right)^{2} \\leqslant \\sum \\Delta_{j k}^{2} \\sum a_{j}^{2} \\sum \\theta_{k}^{2}=\\sum \\Delta_{j k}^{2}\\|\\boldsymbol{\\theta}\\|^{2} \\] <p>We have</p> \\[ E\\left(\\sum \\Delta_{j k}^{2}\\right)=\\sum E\\left(\\Delta_{j k}^{2}\\right)=\\sum_{j k i} x_{i j}^{2} x_{i k}^{2} \\cdot \\frac{\\operatorname{var}\\left(\\psi^{\\prime}\\right)}{\\left(E \\psi^{\\prime}\\right)^{2}} \\] <p>and</p> \\[ \\sum_{j k i} x_{i j}^{2} x_{i k}^{2}=\\sum h_{i}^{2} \\leqslant \\max \\left(h_{i}\\right) \\sum h_{i}=h p \\] <p>Now let \\(\\delta&gt;0\\) be given. Markov's inequality then yields that there is a constant \\(K_{1}\\), namely</p> \\[ K_{1}=\\frac{\\operatorname{var}\\left(\\psi^{\\prime}\\right)}{\\left(E \\psi^{\\prime}\\right)^{2}} \\cdot \\frac{1}{\\delta} \\] <p>such that</p> \\[ P\\left\\{\\sum \\Delta_{j k}^{2} \\geqslant K_{1} h p\\right\\} \\leqslant \\delta \\] <p>We conclude that, with probability greater than \\(1-\\delta\\),</p> \\[ \\left(\\sum \\Delta_{j k} a_{j} \\theta_{k}\\right)^{2} \\leqslant K K_{1} h p^{2} \\] <p>holds simultaneously for all (a, \\(\\boldsymbol{\\theta}\\) ) in (4.14). Assume that \\(\\psi^{\\prime \\prime}\\) is bounded, say \\(\\left|\\psi^{\\prime \\prime}(x)\\right| \\leqslant 2\\left|E\\left(\\psi^{\\prime}\\right)\\right| M\\) for some \\(M\\); then</p> \\[ \\left|\\frac{1}{2 E\\left(\\psi^{\\prime}\\right)} \\sum \\psi^{\\prime \\prime}\\left(y_{i}-\\eta t_{i}\\right) t_{i}^{2} s_{i}\\right| \\leqslant M \\max \\left|s_{i}\\right| \\sum t_{i}^{2} \\leqslant M h^{1 / 2}\\|\\boldsymbol{\\theta}\\|^{2} \\] <p>[see (4.9) and recall that \\(s_{i}^{2} \\leqslant \\sum x_{i j}^{2} \\sum a_{j}^{2}=h_{i}\\) ]. If we put things together, we obtain that, with probability \\(&gt;1-\\delta\\), (4.12) is bounded in absolute value by</p> \\[ r=\\left[\\left(K K_{1}\\right)^{1 / 2}+M K\\right]\\left(h p^{2}\\right)^{1 / 2} \\] <p>and this uniformly on the set (4.14). Since the results hold simultaneously for all a with \\(\\|\\mathbf{a}\\|=1\\), we have in fact shown that, with probability greater than \\(1-\\delta\\),</p> \\[ \\|\\boldsymbol{\\Phi}(\\boldsymbol{\\theta})-\\boldsymbol{\\Psi}(\\boldsymbol{\\theta})\\| \\leqslant r, \\quad \\text { for }\\|\\boldsymbol{\\theta}\\|^{2} \\leqslant K p \\] <p>If \\(K\\) is chosen large enough, and since</p> \\[ E\\left(\\|\\tilde{\\boldsymbol{\\theta}}\\|^{2}\\right)=\\frac{E\\left(\\psi^{2}\\right)}{\\left[E\\left(\\psi^{\\prime}\\right)\\right]^{2}} p \\] <p>it follows from Markov's inequality that</p> \\[ P\\left\\{\\|\\tilde{\\boldsymbol{\\theta}}\\|^{2} \\leqslant K p / 4\\right\\} \\] <p>can be made arbitrarily large. Moreover, then</p> \\[ \\|\\Phi(\\boldsymbol{\\theta})-\\boldsymbol{\\theta}\\| \\leqslant\\|\\boldsymbol{\\Phi}(\\boldsymbol{\\theta})-\\boldsymbol{\\Psi}(\\boldsymbol{\\theta})\\|+\\|\\tilde{\\boldsymbol{\\theta}}\\| \\leqslant r+\\frac{1}{2}(K p)^{1 / 2} \\] <p>on the set \\(\\|\\boldsymbol{\\theta}\\|^{2} \\leqslant K p\\). If \\(h p \\rightarrow 0\\), then \\(r\\) can be made smaller than \\(\\frac{1}{2}(K p)^{1 / 2}\\), so that (4.26) implies</p> \\[ \\|\\boldsymbol{\\theta}-\\boldsymbol{\\Phi}(\\boldsymbol{\\theta})\\|&lt;(K p)^{1 / 2} \\] <p>on the set \\(\\|\\boldsymbol{\\theta}\\| \\leqslant(K p)^{1 / 2}\\). But this is precisely the premiss of Brouwer's fixed point theorem: we conclude that the map \\(\\boldsymbol{\\theta} \\rightarrow \\boldsymbol{\\theta}-\\boldsymbol{\\Phi}(\\boldsymbol{\\theta})\\) has a fixed point \\(\\tilde{\\boldsymbol{\\theta}}\\), which necessarily then is a zero of \\(\\boldsymbol{\\Phi}(\\boldsymbol{\\theta})\\), with \\(\\|\\tilde{\\boldsymbol{\\theta}}\\|&lt;(K p)^{1 / 2}\\).</p> <p>If we substitute \\(\\tilde{\\boldsymbol{\\theta}}\\) for \\(\\boldsymbol{\\theta}\\) into (4.23), we obtain</p> \\[ \\|\\tilde{\\boldsymbol{\\theta}}-\\tilde{\\boldsymbol{\\theta}}\\| \\leqslant r \\] <p>We thus obtain the following proposition.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#proposition-41","title":"PROPOSITION 4.1","text":"<p>(1) If \\(h p^{2} \\rightarrow 0\\), then</p> \\[ \\|\\tilde{\\boldsymbol{\\theta}}-\\tilde{\\boldsymbol{\\theta}}\\| \\rightarrow 0 \\] <p>in probability. (2) If \\(h p \\rightarrow 0\\), then</p> \\[ \\frac{\\|\\tilde{\\boldsymbol{\\theta}}-\\tilde{\\boldsymbol{\\theta}}\\|}{p^{1 / 2}} \\rightarrow 0 \\] <p>in probability. [Note that \\(\\left\\|\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{0}\\right\\| \\sim p^{1 / 2}\\) in view of (4.24).] Now let \\(\\hat{\\alpha}=\\sum a_{j} \\hat{\\theta}_{j}\\) and \\(\\tilde{\\alpha}=\\sum a_{j} \\tilde{\\theta}_{j}\\), with \\(\\|\\mathbf{a}\\|=1\\). Recall that \\(\\hat{\\alpha}\\) is the estimate to be investigated, while \\(\\tilde{\\alpha}\\) is a sum of independent random variables and is asymptotically normal if \\(h \\rightarrow 0\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#proposition-42","title":"PROPOSITION 4.2","text":"<p>(1) If \\(h p^{2} \\rightarrow 0\\), then</p> \\[ \\sup _{\\|\\mathbf{a}\\| \\rightarrow 1}|\\hat{\\alpha}-\\tilde{\\alpha}| \\rightarrow 0 \\] <p>in probability.</p> <p>(2) If \\(\\mathbf{a}\\) is chosen at random with respect to the invariant measure on the sphere \\(\\|\\mathbf{a}\\|=1\\), and if \\(h p \\rightarrow 0\\), then</p> \\[ \\hat{\\alpha}-\\bar{\\alpha} \\rightarrow 0 \\] <p>in probability. Both (1) and (2) imply that \\(\\hat{\\alpha}\\) is asymptotically normal. Proof (1) is an immediate consequence of part (1) of the preceding proposition; similarly, (2) follows from part (2) and the fact that the average of \\(|\\hat{\\alpha}-\\bar{\\alpha}|^{2}\\) over the unit sphere \\(\\|\\mathbf{a}\\|=1\\) is \\(\\|\\hat{\\boldsymbol{\\theta}}-\\hat{\\boldsymbol{\\theta}}\\|^{2} / p\\). Remark 1 In essence we have shown that \\(\\boldsymbol{\\Phi}(\\boldsymbol{\\theta})\\) is asymptotically linear in a neighborhood of the true parameter point \\(\\boldsymbol{\\theta}^{0}\\). Actually, the assumption that \\(\\boldsymbol{\\theta}^{0}=0\\) was used only once, namely, in (4.27). If \\(\\boldsymbol{\\theta}^{*}\\) is any estimate satisfying \\(\\left\\|\\boldsymbol{\\theta}^{*}-\\boldsymbol{\\theta}^{0}\\right\\|=O_{p}\\left(p^{1 / 2}\\right)\\), then we can show in the same way that just one step of Newton's method for solving \\(\\boldsymbol{\\Phi}(\\boldsymbol{\\theta})=0\\), with trial value \\(\\boldsymbol{\\theta}^{*}\\), leads to an estimate \\(\\hat{\\boldsymbol{\\theta}}^{*}\\) satisfying</p> \\[ \\left\\|\\hat{\\boldsymbol{\\theta}}^{*}-\\hat{\\boldsymbol{\\theta}}\\right\\| \\rightarrow 0, \\quad\\left\\|\\hat{\\boldsymbol{\\theta}}^{*}-\\hat{\\boldsymbol{\\theta}}\\right\\| \\rightarrow 0 \\] <p>in probability, provided \\(h p^{2} \\rightarrow 0\\). Remark 2 Recently Yohai and Maronna (1979) have improved this result and shown that \\(\\hat{\\alpha}\\) is asymptotically normal for arbitrary choices of a, assuming only \\(h p^{3 / 2} \\rightarrow 0\\), instead of \\(h p^{2} \\rightarrow 0\\). My conjecture is that \\(h p \\rightarrow 0\\) is sufficient for (4.32) to hold for arbitrary a, and that \\(h p^{1 / 2} \\rightarrow 0\\) is necessary, if either the distribution of the \\(u_{i}\\) or \\(\\rho\\) are allowed to be asymmetric. If both the distribution of the \\(u_{i}\\) and \\(\\rho\\) are symmetric, then perhaps already \\(h \\rightarrow 0\\) is sufficient, as in the classical least squares case.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#75-conjectures-and-empirical-results","title":"7.5 CONJECTURES AND EMPIRICAL RESULTS","text":"<p>An asymptotic theory that requires \\(h p^{2} \\rightarrow 0\\) (and hence \\(a\\) fortiori \\(p^{3} / n \\rightarrow 0\\) ) is for all practical purposes worthless and the situation is only a little more favorable for \\(h p \\rightarrow 0\\); already for a moderately large number of parameters we would need an impossibly large number of observations. Of course, my inability to prove theorems assuming only \\(h \\rightarrow 0\\) does not imply that then the robustized estimates fail to be consistent and asymptotically normal, but what if they should fail? In order to get some insight into what is going on, we may resort to asymptotic expansions. These are without remainder terms, so the results are nonrigorous, but they can be verified by Monte</p> <p>Carlo simulations. The expansions are reported in some detail in Huber (1973a); here we only summarize the salient points.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#the-question-of-bias","title":"The Question of Bias","text":"<p>Assume that either the distribution of the errors \\(u_{i}\\) or the function \\(\\rho\\), or both, are asymmetric. Then the parameters to be estimated are not intrinsically defined by symmetry considerations; we chose to fix them through the convention that \\(E \\psi\\left(u_{i}\\right)=0\\). Then, for instance, a location estimate \\(T_{n}\\), defined by</p> \\[ \\sum_{i=1}^{n} \\psi\\left(u_{i}-T_{n}\\right)=0 \\] <p>is asymptotically normal with mean 0 , but its distribution for finite \\(n\\) is asymmetric and not exactly centered at 0 .</p> <p>Take now the following simple regression design (which actually represents the worst possible case). Assume that we have \\(p\\) unknown parameters \\(\\theta_{1}, \\ldots, \\theta_{p}\\); we take \\(r\\) independent observations on each of them, and as an overall check we take one observation \\(y_{n}\\) of</p> \\[ \\sqrt{\\frac{r}{n-p}}\\left(\\theta_{1}+\\cdots+\\theta_{p}\\right) \\] <p>Here, \\(n=r p+1\\) is the total number of observations, and the corresponding hat matrix happens to be balanced, that is, all its diagonal elements \\(h_{i}\\) are equal to \\(p / n\\).</p> <p>It is intuitively obvious that any robust regression estimate of \\(\\left(\\theta_{1}, \\ldots, \\theta_{p}\\right)\\), for all practical purposes, is equivalent to estimating the \\(p\\) parameters separately from \\(\\sum_{1}^{r} \\psi\\left(y_{i}-\\hat{\\theta}_{1}\\right)=0\\), and so on, since the single observation \\(y_{n}\\) of the scaled sum should have only a negligible influence. So the predicted value of this last observation is</p> \\[ \\hat{\\alpha}_{n}=\\sqrt{\\frac{r}{n-p}}\\left(\\hat{\\theta}_{1}+\\cdots+\\hat{\\theta}_{p}\\right) \\] <p>where the \\(\\hat{\\theta}_{i}\\) have been estimated from the \\(r\\) observations of each separately. [The definition of \\(g\\) in Huber (1973a), p. 810, should read \\(g^{2}=r /\\) \\((n-p)\\).] But the distributions of the \\(\\hat{\\theta}_{i}\\) are slightly asymmetric and not quite centered at their \"true\" values, and if we work things out in detail, we find that \\(\\hat{\\alpha}_{n}\\) is affected by a bias of the order \\(p^{3 / 2} / n\\). Note that the</p> <p>asymptotic variance of \\(\\hat{\\alpha}_{n}\\) is of the order \\(p / n\\), so that the bias measured in units of the standard deviation is \\(p / n^{1 / 2}\\). The asymptotic behavior of the fitted value \\(\\hat{y}_{n}\\) is the same as that of \\(\\hat{\\alpha}_{n}\\), of course.</p> <p>In other words, if \\(h=p / n \\rightarrow 0\\), but \\(p^{3 / 2} / n \\rightarrow \\infty\\), it can happen that the residual \\(r_{n}=y_{n}-\\hat{y}_{n}\\) tends to infinity, not because of a gross error in \\(y_{n}\\), but because the small biases in the \\(\\hat{\\theta}_{i}\\) have added up to a large bias in \\(\\hat{y}_{n}\\) ! However, we should hasten to add that this bias of the order \\(\\sqrt{p} p / n\\) is asymptotically negligible against the bias</p> \\[ \\sqrt{\\frac{r}{n-p}} p \\delta \\approx \\sqrt{p} \\delta \\] <p>caused by a systematic error \\(+\\delta\\) in all the observations. Moreover, the quantitative aspects are such that it is far from easy to verify the effect by Monte Carlo simulation; with \\(p / n=\\frac{1}{2}\\) we need \\(p \\cong 100\\) to make the bias of \\(\\hat{y}_{n}\\) approximately equal to \\(\\left(\\operatorname{var} \\hat{y}_{n}\\right)^{1 / 2}\\), and this with highly asymmetric error distributions ( \\(x^{2}\\) with two to four degrees of freedom).</p> <p>From these remarks we derive the following practical conclusions: (1) The biases caused by asymmetric error distributions exist and can cause havoc within the asymptotic theory but, for most practical purposes, they will be so small that they can be neglected. (2) The biases are largest in situations that should also be avoided for another reason (robustness of design), namely, situations where the estimand is interpolated between observations that are widely separated in the design space-as in our example where \\(\\alpha=\\Sigma \\theta_{i}\\) is estimated from observed values for the individual \\(\\theta_{i}\\). In such cases relatively minor deviations from the linear model may cause large deviations in the fitted values.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#76-asymptotic-covariances-and-their-estimation","title":"7.6 ASYMPTOTIC COVARIANCES AND THEIR ESTIMATION","text":"<p>The covariance matrix of the classical least squares estimate \\(\\boldsymbol{\\theta}_{\\mathrm{LS}}\\) is traditionally estimated by</p> \\[ \\operatorname{cov}\\left(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\right) \\approx \\frac{1}{n-p}\\left(\\sum r_{i}^{2}\\right)\\left(X^{T} X\\right)^{-1} \\] <p>By what should this be replaced in the robustized case?</p> <p>The limiting expression for the covariance of the robust estimate, derived from Proposition 4.1, is</p> \\[ \\operatorname{cov}(\\hat{\\boldsymbol{\\theta}})=\\frac{E(\\psi)^{2}}{\\left(E \\psi^{\\prime}\\right)^{2}}\\left(X^{T} X\\right)^{-1} \\] <p>which can be translated straightforwardly into the estimate</p> \\[ \\operatorname{cov}(\\hat{\\boldsymbol{\\theta}}) \\approx \\frac{(1 / n) \\sum \\psi\\left(r_{i}\\right)^{2}}{\\left[(1 / n) \\sum \\psi^{\\prime}\\left(r_{i}\\right)\\right]^{2}}\\left(X^{T} X\\right)^{-1} \\] <p>If we want to recapture the classical formula (6.1) in the classical case \\([\\psi(x)=x]\\), we should multiply the right-hand side of (6.3) by \\(n /(n-p)\\), and perhaps some other corrections of the order \\(h=p / n\\) are needed. Also the matrix \\(X^{T} X\\) should perhaps be replaced by something like the matrix</p> \\[ W_{j k}=\\sum \\psi^{\\prime}\\left(r_{i}\\right) x_{i j} x_{i k} \\] <p>The second, and perhaps even more important, goal of the asymptotic expansions mentioned in the preceding sections is to find proposals for correction terms of the order \\(h\\).</p> <p>The general expressions are extremely unwieldy, but in the balanced case (i.e., \\(h_{i}=h=p / n\\) ), with symmetric error distributions and skew symmetric \\(\\psi\\), assuming that \\(1 \\ll p \\ll n\\), and if we neglect terms of the orders \\(h^{2}=(p / n)^{2}\\) or \\(1 / n\\), the following three expressions all are unbiased estimates of \\(\\operatorname{cov}(\\hat{\\boldsymbol{\\theta}})\\) :</p> \\[ \\begin{gathered} K^{2} \\frac{[1 /(n-p)] \\sum \\psi\\left(r_{i}\\right)^{2}}{\\left[(1 / n) \\sum \\psi^{\\prime}\\left(r_{i}\\right)\\right]^{2}}\\left(X^{T} X\\right)^{-1} \\\\ K \\frac{[1 /(n-p)] \\sum \\psi\\left(r_{i}\\right)^{2}}{(1 / n) \\sum \\psi^{\\prime}\\left(r_{i}\\right)} W^{-1} \\\\ K^{-1} \\frac{1}{n-p} \\sum \\psi\\left(r_{i}\\right)^{2} W^{-1}\\left(X^{T} X\\right) W^{-1} \\end{gathered} \\] <p>The correction factors are expressed in terms of</p> \\[ K=1+\\frac{p}{n} \\frac{\\operatorname{var}\\left(\\psi^{\\prime}\\right)}{\\left(E \\psi^{\\prime}\\right)^{2}} \\] <p>In practice \\(E\\left(\\psi^{\\prime}\\right)\\) and \\(\\operatorname{var}\\left(\\psi^{\\prime}\\right)\\) are unknown and will be estimated by</p> \\[ \\begin{gathered} E\\left(\\psi^{\\prime}\\right) \\approx m=\\frac{1}{n} \\sum \\psi^{\\prime}\\left(r_{i}\\right) \\\\ \\operatorname{var}\\left(\\psi^{\\prime}\\right) \\approx \\frac{1}{n} \\sum\\left[\\psi^{\\prime}\\left(r_{i}\\right)-m\\right]^{2} \\end{gathered} \\] <p>In the special case</p> \\[ \\psi(x)=\\min [c, \\max (-c, x)] \\] <p>(6.8) simplifies to</p> \\[ K=1+\\frac{p}{n} \\frac{1-m}{m} \\] <p>where \\(m\\) is the relative frequency of the residuals satisfying \\(-c&lt;r_{i}&lt;c\\). Note that, in the classical case, all three expressions (6.5), (6.6), and (6.7) reduce to (6.1).</p> <p>In the simple location case ( \\(p=1, x_{i j}=1\\) ), the three expressions agree exactly if we put \\(K=1\\) (the derivation of \\(K\\) neglected terms of the order \\(1 / n\\) anyhow).</p> <p>For details and a comparison with Monte Carlo results, see Huber (1973a). (For normal errors, the agreement between the expansions and the Monte Carlo results was excellent up to \\(p / n=\\frac{1}{4}\\); for Cauchy errors excellent up to \\(p / n=\\frac{1}{16}\\), and still tolerable for \\(p / n=\\frac{1}{8}\\).)</p> <p>NOTE Since \\(\\hat{\\boldsymbol{\\theta}}\\) can also be characterized formally as the solution of the weighted least squares problem</p> \\[ \\sum_{i} w_{i} r_{i} x_{i j}=0, \\quad j=1, \\ldots, p \\] <p>with weights \\(w_{i}=\\psi\\left(r_{i}\\right) / r_{i}\\) depending on the sample, a further variant to \\(X^{T} X\\) and (6.4), namely,</p> \\[ \\sum w_{i} x_{i j} x_{i k} \\] <p>looks superficially attractive, together with</p> \\[ \\frac{1}{n-p} \\sum w_{i} r_{i}^{2} \\] <p>in place of</p> \\[ \\frac{1}{n-p} \\sum \\psi\\left(r_{i}\\right)^{2} \\] <p>However, (6.14) is not robust in general [ \\(w_{i} r_{i}^{2}=\\psi\\left(r_{i}\\right) r_{i}\\) is unbounded unless \\(\\psi\\) is redescending] and is not a consistent estimate of \\(E\\left(\\psi^{2}\\right)\\). So we should be strongly advised against the use of (6.14).</p> <p>It would be feasible to use the suitably scaled matrix (6.13), namely</p> \\[ \\frac{\\sum w_{i} x_{i j} x_{i k}}{(1 / n) \\sum w_{i}} \\] <p>in place of \\(X^{T} X\\), just as we did with \\(W\\) in (6.6) and (6.7), but the bias correction factors then seem to become discouragingly complicated.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#77-concomitant-scale-estimates","title":"7.7 CONCOMITANT SCALE ESTIMATES","text":"<p>For the sake of simplicity, we have so far assumed that scale was known and fixed. In practice we would have to estimate also a scale parameter \\(\\sigma\\), and we would have to solve</p> \\[ \\sum \\psi\\left(\\frac{y_{i}-\\sum x_{i j} \\theta_{j}}{\\sigma}\\right) x_{i j}=0, \\quad j=1, \\ldots, p \\] <p>in place of (3.2). This introduces some technical complications similar to those in Chapter 6 , but does not change the asymptotic results. The reason is that, if we have some estimate for which the fitted values \\(\\hat{y}_{i}\\) are consistent, we can estimate scale \\(\\hat{\\sigma}\\) consistently from the corresponding residuals \\(r_{i}=y_{i}-\\hat{y}_{i}\\), and then use this \\(\\hat{\\sigma}\\) in (7.1) for calculating the final estimate \\(\\hat{\\theta}\\).</p> <p>In practice we calculate the estimates \\(\\hat{\\boldsymbol{\\theta}}\\) and \\(\\hat{\\sigma}\\) by simultaneous iterations (which may cause difficulties with the convergence proofs).</p> <p>Which scale estimate should we use? In the simple location case the unequivocal answer is given by the results of the Princeton study (Andrews et al., 1972); the estimates using the median absolute deviation, that is</p> \\[ \\hat{\\sigma}=\\operatorname{med}\\left\\{\\left|r_{i}\\right|\\right\\} \\] <p>when expressed in terms of residuals relative to the sample median, fared best. This result is theoretically underpinned by the facts that the median</p> <p>absolute deviation (1) is minimax with respect to bias (Section 5.7), and (2) has the highest possible breakdown point \\(\\left(\\varepsilon^{*}=\\frac{1}{2}\\right)\\).</p> <p>In regression the case for the median absolute residual (7.2) is less well founded. First, it is not feasible to calculate it beforehand (the analogue to the sample median, the \\(L_{1}\\)-estimate, may take more time to calculate than our intended estimate \\(\\hat{\\boldsymbol{\\theta}}\\) ). Second, we still lack a convergence proof for procedures simultaneously iterating (7.1) and (7.2) (the empirical evidence is, however, good).</p> <p>For the following we assume, somewhat more generally, that \\(\\hat{\\theta}\\) and \\(\\hat{\\sigma}\\) are estimated by solving the simultaneous equations</p> \\[ \\begin{gathered} \\sum \\psi\\left(\\frac{y_{i}-f_{i}(\\boldsymbol{\\theta})}{\\sigma}\\right) \\frac{\\partial f_{i}(\\boldsymbol{\\theta})}{\\partial \\theta_{j}}=0, \\quad j=1, \\ldots, p \\\\ \\sum \\chi\\left(\\frac{y_{i}-f_{i}(\\boldsymbol{\\theta})}{\\sigma}\\right)=0 \\end{gathered} \\] <p>for \\(\\boldsymbol{\\theta}\\) and \\(\\sigma\\); the functions \\(f_{i}\\) are not necessarily linear. Note that this contains, in particular: (1) Maximum likelihood estimation. Assume that the observations have a probability density of the form</p> \\[ \\frac{1}{\\sigma} g\\left(\\frac{y_{i}-f_{i}(\\boldsymbol{\\theta})}{\\sigma}\\right) \\] <p>then (7.3) and (7.4) give the maximum likelihood estimates if</p> \\[ \\begin{aligned} &amp; \\psi(x)=-\\frac{g^{\\prime}(x)}{g(x)} \\\\ &amp; \\chi(x)=x \\psi(x)-1 \\end{aligned} \\] <p>(2) Median absolute residuals as the scale estimate, if</p> \\[ \\chi(x)=\\operatorname{sign}(|x|-1) \\] <p>Some problems with existence and convergence proofs arise when \\(\\psi\\) and \\(\\chi\\) are totally unrelated. For purely technical reasons we therefore introduce the following minimum problem:</p> \\[ Q(\\boldsymbol{\\theta}, \\sigma)=\\frac{1}{n} \\sum \\rho\\left(\\frac{y_{i}-f_{i}(\\boldsymbol{\\theta})}{\\sigma}\\right) \\sigma=\\min ! \\] <p>where \\(\\rho\\) is a convex function that has a strictly positive minimum at 0 . If we take partial derivatives of (7.9) with respect to \\(\\theta_{j}\\) and \\(\\sigma\\), we obtain the following characterization of the minimum:</p> \\[ \\begin{aligned} \\sum \\psi\\left(\\frac{y_{i}-f_{i}(\\boldsymbol{\\theta})}{\\sigma}\\right) \\frac{\\partial f_{i}}{\\partial \\theta_{j}} &amp; =0 \\\\ \\sum \\chi\\left(\\frac{y_{j}-f_{j}(\\boldsymbol{\\theta})}{\\sigma}\\right) &amp; =0 \\end{aligned} \\] <p>with</p> \\[ \\begin{aligned} &amp; \\psi(x)=\\rho^{\\prime}(x) \\\\ &amp; \\chi(x)=x \\psi(x)-\\rho(x) \\end{aligned} \\] <p>Note that \\(\\chi^{\\prime}(x)=x \\psi^{\\prime}(x)\\) then is negative for \\(x \\leqslant 0\\) and positive for \\(x \\geqslant 0\\); hence \\(\\chi\\) has an absolute minimum at \\(x=0\\), namely, \\(\\chi(0)=-\\rho(0)&lt;0\\).</p> <p>In particular, with</p> \\[ \\begin{aligned} \\rho(x) &amp; =\\frac{1}{2} x^{2}+\\frac{1}{2} \\beta, &amp; &amp; \\text { for }|x|&lt;c \\\\ &amp; =c\\left|x \\left\\lvert\\,-\\frac{1}{2} c^{2}+\\frac{1}{2} \\beta\\right.,\\right. &amp; &amp; \\text { for }|x| \\geqslant c \\end{aligned} \\] <p>we obtain</p> \\[ \\begin{aligned} \\psi(x) &amp; =-c, &amp; &amp; \\text { for } x \\leqslant-c \\\\ &amp; =x, &amp; &amp; \\text { for }-c&lt;x&lt;c \\\\ &amp; =c, &amp; &amp; \\text { for } x \\geqslant c \\end{aligned} \\] <p>and</p> \\[ \\chi(x)=\\frac{1}{2}\\left[\\psi(x)^{2}-\\beta\\right] \\] <p>Note that this is a \\(\\psi, \\chi\\) pair suggested by minimax considerations both for location and for scale (cf. Example 6.4.1), and that both \\(\\psi\\) and \\(\\chi\\) are bounded [whereas, with the maximum likelihood approach, the \\(\\chi\\) corresponding to a monotone \\(\\psi\\) would always be unbounded; cf. (7.7)].</p> <p>If the \\(f_{i}\\) are linear, then \\(Q(\\boldsymbol{\\theta}, \\sigma)\\) in fact is a convex function not only of \\(\\boldsymbol{\\theta}\\), but of \\((\\boldsymbol{\\theta}, \\sigma)\\). In order to demonstrate this, we assume that \\((\\boldsymbol{\\theta}, \\sigma)\\)</p> <p>depends linearly on some real parameter \\(t\\) and calculate the second derivative with respect to \\(t\\) of the summands of (7.9):</p> \\[ q_{i}(t)=\\rho\\left(\\frac{y_{i}-f_{i}(\\theta)}{\\sigma}\\right) \\sigma \\] <p>Denote differentiation with respect to \\(t\\) by a superscript dot, then (omitting the index \\(i\\) )</p> \\[ \\dot{q}=\\rho\\left(\\frac{y-f}{\\sigma}\\right) \\dot{\\sigma}+\\rho^{\\prime}\\left(\\frac{y-f}{\\sigma}\\right)\\left(-\\frac{y-f}{\\sigma} \\dot{\\sigma}-\\dot{f}\\right) \\] <p>and</p> \\[ \\ddot{q}=\\rho^{\\prime \\prime}\\left(\\frac{y-f}{\\sigma}\\right)\\left(\\frac{y-f}{\\sigma} \\dot{\\sigma}+\\dot{f}\\right)^{2} \\frac{1}{\\sigma} \\geqslant 0 \\] <p>Thus \\(Q\\) is convex. If \\(\\rho\\) is not twice differentiable, the result still holds (prove this by approximating \\(\\rho\\) differentiably).</p> <p>Assume now that</p> \\[ 0&lt;\\lim _{|x| \\rightarrow \\infty} \\frac{\\rho(x)}{|x|}=c \\leqslant \\infty \\] <p>If \\(c&lt;\\infty, Q\\) can be extended by continuity:</p> \\[ Q(\\boldsymbol{\\theta}, 0)=c \\frac{1}{n} \\sum\\left|y_{i}-f_{i}(\\boldsymbol{\\theta})\\right| \\] <p>Hence the limiting case \\(\\sigma=0\\) corresponds to \\(L_{1}\\)-estimation. Of course, on the boundary \\(\\sigma=0\\), the characterization of the minimum by (7.10) and (7.11) breaks down, but in any case, the set of solutions \\((\\boldsymbol{\\theta}, \\sigma)\\) of (7.9) is a convex subset of \\((p+1)\\)-space. Often it reduces to a single point. For this it suffices, for instance, that \\(\\rho\\) is strictly convex, that the \\(f_{i}\\) are linear, and that the columns of the design matrix \\(x_{i, j}=\\partial f_{i} / \\partial \\theta_{j}\\) and the residual vector \\(x_{i}-f_{i}\\) are linearly independent (that is, the design matrix has full rank, and there is no exact solution with vanishing residuals). Then also \\(Q\\) is strictly convex [cf. (7.19)], and the solution \\((\\boldsymbol{\\theta}, \\sigma)\\) is necessarily unique.</p> <p>Even if \\(\\rho\\) is not strictly convex everywhere, but contains a strictly convex piece, the solution is usually unique when \\(n / p\\) is large (because then enough residuals will fall into the strictly convex region of \\(\\rho\\) for the above argument to carry through).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#78-computation-of-regression-m-estimates","title":"7.8 COMPUTATION OF REGRESSION M-ESTIMATES","text":"<p>We now describe some simple algorithms. They alternate between improving trial values for \\(\\hat{\\boldsymbol{\\theta}}\\) and \\(\\hat{\\sigma}\\), and they decrease (7.9). We prefer to write the latter expression in the form</p> \\[ Q(\\boldsymbol{\\theta}, \\sigma)=\\frac{1}{n} \\sum\\left[\\rho_{0}\\left(\\frac{y_{i}-f_{i}(\\boldsymbol{\\theta})}{\\sigma}\\right)+a\\right] \\sigma \\] <p>where \\(\\rho_{0}(0)=0\\) and \\(a&gt;0\\). The equations (7.10) and (7.11) can then be written</p> \\[ \\begin{gathered} \\frac{1}{n} \\sum \\psi_{0}\\left(\\frac{r_{i}}{\\sigma}\\right) \\frac{\\partial f_{i}}{\\partial \\theta_{j}}=0 \\\\ \\frac{1}{n} \\sum \\chi_{0}\\left(\\frac{r_{i}}{\\sigma}\\right)=a \\end{gathered} \\] <p>with</p> \\[ \\begin{aligned} &amp; \\psi_{0}(x)=\\rho_{0}^{\\prime}(x) \\\\ &amp; \\chi_{0}(x)=x \\psi_{0}(x)-\\rho_{0}(x) \\end{aligned} \\] <p>Note that \\(\\chi_{0}\\) has an absolute minimum at \\(x=0\\), namely, \\(\\chi_{0}(0)=0\\). We assume throughout that \\(\\psi_{0}\\) and \\(\\chi_{0}\\) are continuous.</p> <p>In order to obtain consistency of the scale estimate at the normal model and to recapture the classical estimates for the classical choice \\(\\rho_{0}(x)=\\frac{1}{2} x^{2}\\), we propose to take</p> \\[ a=\\frac{n-p}{n} E_{\\theta}\\left(\\chi_{0}\\right) \\]"},{"location":"research/note/lecture-note-automation/result/ocr_result/#the-scale-step","title":"The Scale Step","text":"<p>Let \\(\\boldsymbol{\\theta}^{(m)}\\) and \\(\\sigma^{(m)}\\) be trial values for \\(\\boldsymbol{\\theta}\\) and \\(\\sigma\\), and put \\(r_{i}=y_{i}-f_{i}\\left(\\boldsymbol{\\theta}^{(m)}\\right)\\). Define</p> \\[ \\left(\\sigma^{(m+1)}\\right)^{2}=\\frac{1}{n a} \\sum \\chi_{0}\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right)\\left(\\sigma^{(m)}\\right)^{2} \\] <p>Remarks For the classical choice \\(\\rho_{0}(x)=\\frac{1}{2} x^{2}\\), with \\(a\\) as in (8.6), we obtain</p> \\[ \\left(\\sigma^{(m+1)}\\right)^{2}=\\frac{1}{n-p} \\sum r_{i}^{2} \\] <p>For the choice (7.14) we obtain</p> \\[ \\left(\\sigma^{(m+1)}\\right)^{2}=\\frac{1}{(n-p) \\beta} \\sum \\psi\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right)^{2}\\left(\\sigma^{(m)}\\right)^{2} \\] <p>with</p> \\[ \\beta=E_{\\Phi}\\left(\\psi^{2}\\right) \\] <p>In the latter case we may say that \\(\\sigma^{(m+1)}\\) is an ordinary variance estimate (8.8), but calculated from metrically Winsorized residuals</p> \\[ \\begin{aligned} r_{i}^{w}=\\psi\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right) \\sigma^{(m)} &amp; =-c \\sigma^{(m)}, &amp; &amp; \\text { for } r_{i}&lt;-c \\sigma^{(m)} \\\\ =r_{i}, &amp; &amp; \\text { for }\\left|r_{i}\\right| \\leqslant c \\sigma^{(m)} \\\\ =c \\sigma^{(m)}, &amp; &amp; \\text { for } r_{i}&gt;c \\sigma^{(m)} \\end{aligned} \\] <p>and corrected for bias by the factor \\(\\beta\\). LEMMA 8.1 Assume that \\(\\rho_{0} \\geqslant 0\\) is convex, that \\(\\rho_{0}(0)=0\\), and that \\(\\rho_{0}(x) / x\\) is convex for \\(x&lt;0\\), concave for \\(x&gt;0\\). Then</p> \\[ Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)-Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m+1)}\\right) \\geqslant \\frac{a\\left(\\sigma^{(m+1)}-\\sigma^{(m)}\\right)^{2}}{\\sigma^{(m)}} \\] <p>In particular, unless (8.3) is already satisfied, \\(Q\\) is strictly decreased. Proof The idea is to construct a simple \"comparison function\" \\(U(\\sigma)\\) that agrees with \\(Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma\\right)\\) at \\(\\sigma=\\sigma^{(m)}\\), that lies wholly above \\(Q\\left(\\boldsymbol{\\theta}^{(m)}, \\cdot\\right)\\), and that reaches its minimum at \\(\\sigma^{(m+1)}\\), namely,</p> \\[ U(\\sigma)=Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)+a\\left(\\sigma-\\sigma^{(m)}\\right)+\\frac{1}{n} \\sum \\chi_{0}\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right)\\left[\\frac{\\left(\\sigma^{(m)}\\right)^{2}}{\\sigma}-\\sigma^{(m)}\\right] \\] <p>Obviously, \\(U\\left(\\sigma^{(m)}\\right)=Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)\\). The derivatives with respect to \\(\\sigma\\) are</p> \\[ \\begin{aligned} U^{\\prime}(\\sigma) &amp; =-\\frac{1}{n} \\sum \\chi_{0}\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right)\\left(\\frac{\\sigma^{(m)}}{\\sigma}\\right)^{2}+a \\\\ Q^{\\prime}\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma\\right) &amp; =-\\frac{1}{n} \\sum \\chi_{0}\\left(\\frac{r_{i}}{\\sigma}\\right)+a \\end{aligned} \\] <p>hence they agree at \\(\\sigma=\\sigma^{(m)}\\). Define</p> \\[ f(z)=U\\left(\\frac{1}{z}\\right)-Q\\left(\\boldsymbol{\\theta}^{(m)}, \\frac{1}{z}\\right), \\quad z&gt;0 \\] <p>This function is convex, since it can be written</p> \\[ f(z)=-\\frac{1}{n} \\sum \\frac{\\rho_{0}\\left(z r_{i}\\right)}{z}+b_{0}+b_{1} z \\] <p>with some constants \\(b_{0}\\) and \\(b_{1}\\); it has a horizontal tangent at \\(z=1 / \\sigma^{(m)}\\), and it vanishes there. It follows that \\(f(z) \\geqslant 0\\) for all \\(z&gt;0\\); hence</p> \\[ U(\\sigma) \\geqslant Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma\\right) \\] <p>for all \\(\\sigma&gt;0\\). Note that \\(U\\) reaches its minimum at \\(\\sigma^{(m+1)}\\). A simple calculation, using (8.7) to eliminate \\(\\Sigma \\chi_{0}\\), gives</p> \\[ \\begin{aligned} U\\left(\\sigma^{(m+1)}\\right) &amp; =Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)+a\\left(\\sigma^{(m+1)}-\\sigma^{(m)}\\right)+a\\left(\\frac{\\sigma^{(m+1)}}{\\sigma^{(m)}}\\right)^{2}\\left[\\frac{\\left(\\sigma^{(m)}\\right)^{2}}{\\sigma^{(m+1)}}-\\sigma^{(m)}\\right] \\\\ &amp; =Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)-a \\frac{\\left(\\sigma^{(m+1)}-\\sigma^{(m)}\\right)^{2}}{\\sigma^{(m)}} \\end{aligned} \\] <p>The assertion of the lemma now follows from (8.18). For the location step, we have two variants: one modifies the residuals, the other the weights.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#the-location-step-with-modified-residuals","title":"The Location Step with Modified Residuals","text":"<p>Let \\(\\boldsymbol{\\theta}^{(m)}\\) and \\(\\sigma^{(m)}\\) be trial values for \\(\\boldsymbol{\\theta}\\) and \\(\\sigma\\). Put</p> \\[ \\begin{aligned} r_{i} &amp; =y_{i}-f_{i}\\left(\\boldsymbol{\\theta}^{(m)}\\right) \\\\ r_{i}^{*} &amp; =\\psi\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right) \\sigma^{(m)} \\\\ x_{i k} &amp; =\\frac{\\partial}{\\partial \\theta_{k}} f_{i}\\left(\\boldsymbol{\\theta}^{(m)}\\right) \\end{aligned} \\] <p>Solve</p> \\[ \\sum\\left(r_{i}^{*}-\\sum_{k} x_{i k} \\tau_{k}\\right)^{2}=\\min ! \\] <p>for \\(\\tau\\), that is, determine the solution \\(\\tau=\\hat{\\tau}\\) of</p> \\[ X^{T} X \\tau=X^{T} \\mathbf{r}^{*} \\] <p>Put</p> \\[ \\theta^{(m+1)}=\\theta^{(m)}+q \\hat{\\tau} \\] <p>where \\(0&lt;q&lt;2\\) is an arbitrary relaxation factor. Remark Except that the residuals \\(r_{i}\\) have been replaced by their metrically Winsorized versions \\(r_{i}^{*}\\), this is just the ordinary iterative GaussNewton step one uses to solve nonlinear least squares problems (if the \\(f_{i}\\) are linear, it gives the least squares solution in one step).</p> <p>LEMMA 8.2 Assume that \\(\\rho_{0} \\geqslant 0, \\rho_{0}(0)=0,0 \\leqslant \\rho_{0}^{\\prime \\prime} \\leqslant 1\\), and that the \\(f_{i}\\) are linear. Without loss of generality choose the coordinate system such that \\(X^{T} X=I\\). Then</p> \\[ \\begin{aligned} Q\\left(\\theta^{(m)}, \\sigma^{(m)}\\right)-Q\\left(\\theta^{(m+1)}, \\sigma^{(m)}\\right) &amp; \\geqslant \\frac{q(2-q)}{2 \\sigma^{(m)} n} \\sum_{j}\\left(\\sum_{i} r_{i}^{*} x_{i j}\\right)^{2} \\\\ &amp; =\\frac{q(2-q)}{2 \\sigma^{(m)} n}\\|\\hat{\\tau}\\|^{2} \\\\ &amp; =\\frac{2-q}{2 \\sigma^{(m)} n q}\\left\\|\\theta^{(m+1)}-\\theta^{(m)}\\right\\|^{2} \\end{aligned} \\] <p>In particular, unless (8.2) is already satisfied, \\(Q\\) is strictly decreased. Proof As in the scale step, we use a comparison function that agrees with \\(Q\\) at \\(\\theta^{(m)}\\), that lies wholly above \\(Q\\), and that reaches its minimum at \\(\\theta^{(m+1)}\\). Put</p> \\[ W(\\tau)=Q\\left(\\theta^{(m)}, \\sigma^{(m)}\\right)+\\frac{1}{2 \\sigma^{(m)} n} \\sum\\left[\\left(r_{i}^{*}-\\sum_{k} x_{i k} \\tau_{k}\\right)^{2}-\\left(r_{i}^{*}\\right)^{2}\\right] \\] <p>The functions \\(W(\\tau)\\) and \\(Q\\left(\\boldsymbol{\\theta}^{(m)}+\\tau, \\sigma^{(m)}\\right)\\) then have the same value and the same first derivative at \\(\\tau=0\\), as we can easily check. The matrix of second order derivatives of the difference,</p> \\[ \\frac{\\partial^{2}}{\\partial \\tau_{j} \\partial \\tau_{k}}\\left[W(\\tau)-Q\\left(\\boldsymbol{\\theta}^{(m)}+\\tau, \\sigma^{(m)}\\right)\\right]=\\frac{1}{\\sigma^{(m)} n} \\sum_{i}\\left[1-\\psi^{\\prime}\\left(\\frac{r_{i}-\\Sigma x_{i j} \\tau_{i}}{\\sigma^{(m)}}\\right)\\right] x_{i j} x_{i k} \\] <p>is positive semidefinite; hence</p> \\[ W(\\tau) \\geqslant Q\\left(\\boldsymbol{\\theta}^{(m)}+\\tau, \\sigma^{(m)}\\right) \\] <p>for all \\(\\tau\\). The minimum of \\(W(\\tau)\\) occurs at \\(\\hat{\\tau}=X^{T} \\tau^{*}\\), and we easily check that it has the value</p> \\[ W(\\hat{\\tau})=Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)-\\frac{1}{2 \\sigma^{(m)} n}\\|\\hat{\\tau}\\|^{2} \\] <p>As a function of \\(q\\),</p> \\[ W(q \\hat{\\tau})-Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right) \\] <p>is quadratic, vanishes at \\(q=0\\), has a minimum at \\(q=1\\), and for reasons of symmetry must vanish again at \\(q=2\\). Hence we obtain, by quadratic interpolation,</p> \\[ W(q \\hat{\\tau})-Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)=-\\frac{q(2-q)}{2 \\sigma^{(m)} n}\\|\\hat{\\tau}\\| \\] <p>and the assertion of the lemma now follows from (8.28). Remark The relaxation factor \\(q\\) had originally been introduced because theoretical considerations had indicated that \\(q \\approx 1 / E \\psi^{\\prime} \\geqslant 1\\) should give faster convergence than \\(q=1\\). The empirical experience shows hardly any difference.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#the-location-step-with-modified-weights","title":"The Location Step with Modified Weights","text":"<p>Instead of (8.2) we can equivalently write</p> \\[ \\sum w_{i} r_{i} \\frac{\\partial f_{i}}{\\partial \\theta_{j}}=0, \\quad j=1, \\ldots, p \\] <p>with weights depending on the current residuals \\(r_{i}\\), determined by</p> \\[ w_{i}=\\frac{\\psi\\left(r_{i} / \\sigma^{(m)}\\right)}{r_{i} / \\sigma^{(m)}} \\] <p>Let \\(\\boldsymbol{\\theta}^{(m)}\\) and \\(\\sigma^{(m)}\\) be trial values, then find \\(\\boldsymbol{\\theta}^{(m+1)}\\) by solving the weighted least squares problem (8.31), that is, find the solution \\(\\tau=\\hat{\\tau}\\) of</p> \\[ X^{T} W X \\tau=X^{T} W \\tau \\] <p>where \\(W\\) is the diagonal matrix with diagonal elements \\(w_{i}\\), and put</p> \\[ \\boldsymbol{\\theta}^{(m+1)}=\\boldsymbol{\\theta}^{(m)}+\\hat{\\tau} \\] <p>LEMMA 8.3 (Dutter 1975) Assume that \\(\\rho_{0}\\) is convex and symmetric, that \\(\\psi(x) / x\\) is bounded and monotone decreasing for \\(x&gt;0\\), and that the \\(f_{i}\\) are linear. Then, for \\(\\sigma&gt;0\\), we have \\(Q\\left(\\theta^{(m+1)}, \\sigma^{(m)}\\right)&lt;Q\\left(\\theta^{(m)}, \\sigma^{(m)}\\right)\\), unless \\(\\theta^{(m)}\\) already minimizes \\(Q\\left(\\cdot, \\sigma^{(m)}\\right)\\). The decrease in \\(Q\\) exceeds that of the corresponding modified residuals step. Proof To simplify notation assume \\(\\sigma^{(m)}=1\\). We also use a comparison function \\(U\\) here, and we define it as follows:</p> \\[ U(\\boldsymbol{\\theta})=\\sum_{i} U_{i}\\left(y_{i}-f_{i}(\\boldsymbol{\\theta})\\right) \\] <p>where each \\(U_{i}\\) is a quadratic function</p> \\[ U_{i}(x)=a_{i}+\\frac{1}{2} b_{i} x^{2} \\] <p>with \\(a_{i}\\) and \\(b_{i}\\) determined such that</p> \\[ U_{i}(x) \\geqslant \\rho_{0}(x), \\quad \\text { for all } x \\] <p>and</p> \\[ U_{i}\\left(r_{i}\\right)=\\rho_{0}\\left(r_{i}\\right) \\] <p>with \\(r_{i}=y_{i}-f_{i}\\left(\\boldsymbol{\\theta}^{(m)}\\right)\\); see Exhibit 7.8.1. These conditions imply that \\(U_{i}\\) and \\(\\rho\\) have a common tangent at \\(r_{i}\\) :</p> \\[ U_{i}^{\\prime \\prime}\\left(r_{i}\\right)=b_{i} r_{i}=\\psi_{0}\\left(r_{i}\\right) \\] <p></p> <p>Exhibit 7.8.1 hence</p> \\[ b_{i}=\\frac{\\psi\\left(r_{i}\\right)}{r_{i}}=w_{i} \\] <p>and</p> \\[ a_{i}=\\rho_{0}\\left(r_{i}\\right)-\\frac{1}{2} r_{i} \\psi\\left(r_{i}\\right) \\] <p>We have to check that (8.37) holds. Write \\(r\\) instead of \\(r_{i}\\). The difference</p> \\[ \\begin{aligned} z(x) &amp; =U_{i}(x)-\\rho_{0}(x) \\\\ &amp; =\\rho_{0}(r)-\\frac{1}{2} r \\psi_{0}(r)+\\frac{1}{2} \\frac{\\psi_{0}(r)}{r} x^{2}-\\rho_{0}(x) \\end{aligned} \\] <p>satisfies</p> \\[ \\begin{aligned} z(r) &amp; =z(-r)=0 \\\\ z^{\\prime}(r) &amp; =z^{\\prime}(-r)=0 \\end{aligned} \\] <p>and</p> \\[ z^{\\prime}(x)=\\frac{\\psi(r)}{r} x-\\psi(x) \\] <p>Since \\(\\psi(x) / x\\) is decreasing for \\(x&gt;0\\), this implies that</p> \\[ \\begin{aligned} z^{\\prime}(x) &amp; &lt;0, &amp; &amp; \\text { for } 0&lt;x \\leqslant r \\\\ &amp; \\geqslant 0, &amp; &amp; \\text { for } x \\geqslant r \\end{aligned} \\] <p>hence \\(z(x) \\geqslant z(r)=0\\) for \\(x \\geqslant 0\\), and the same holds for \\(x \\leqslant 0\\) because of symmetry.</p> <p>In view of (8.40) we have</p> \\[ U(\\boldsymbol{\\theta})=\\frac{1}{2} \\sum w_{i}\\left[y_{i}-f_{i}(\\boldsymbol{\\theta})\\right]^{2}+\\text { const. } \\] <p>and this is, of course, minimized by \\(\\boldsymbol{\\theta}^{(m+1)}\\). This proves the first part of the lemma. The second part follows from the remark that, if we had used comparison functions of the form</p> \\[ U_{i}^{*}(x)=a_{i}+c_{i} x+\\frac{1}{2} x^{2} \\] <p>instead of (8.36), we would have recaptured the proof of Lemma 8.2, and that</p> \\[ U_{i}^{*}(x) \\geqslant U_{i}(x), \\quad \\text { for all } x \\] <p>provided \\(0 \\leqslant \\rho^{\\prime \\prime} \\leqslant 1\\) (if necessary rescale \\(\\psi\\) to achieve this). Hence</p> \\[ W(\\tau) \\geqslant U\\left(\\boldsymbol{\\theta}^{(m)}+\\tau\\right) \\geqslant \\rho\\left(\\boldsymbol{\\theta}^{(m)}+\\tau\\right) \\] <p>In fact the same argument shows that \\(U\\) is the best possible quadratic comparison function.</p> <p>Remark 1 If we omit the convexity assumption and only assume that \\(\\rho(x)\\) increases for \\(x&gt;0\\), the above proof still goes through and shows that the modified weights algorithm converges to a (local) minimum if the scale is kept fixed.</p> <p>Remark 2 The second part of Lemma 8.3 implies that the modified weights approach should give a faster convergence than the modified residuals approach. However the empirically observed convergence rates show only small differences. Since the modified residuals approach (for</p> <p>linear \\(f_{i}\\) ) can use the same matrices over all iterations, it even seems to have a slight advantage in total computing costs [cf. Dutter (1977a, b)].</p> <p>If we alternate between location and scale steps (using either of the two versions for the location step), we obtain a sequence \\(\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)\\), which is guaranteed to decrease \\(Q\\) at each step. We now want to prove that the sequence converges toward a solution of (8.2) and (8.3).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#theorem-84","title":"THEOREM 8.4","text":"<p>(1) The sequence \\(\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)\\) has at least one accumulation point \\((\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})\\). (2) Every accumulation point \\((\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})\\) with \\(\\hat{\\sigma}&gt;0\\) is a solution of (8.2) and (8.3) and minimizes (8.1).</p> <p>Proof The sets of the form</p> \\[ A_{b}=\\{(\\boldsymbol{\\theta}, \\sigma) \\mid \\sigma \\geqslant 0, Q(\\boldsymbol{\\theta}, \\sigma) \\leqslant b\\} \\] <p>are compact. First, they are obviously closed, since \\(Q\\) is continuous. We have \\(\\sigma \\leqslant b / a\\) on \\(A_{b}\\). Since the \\(f_{i}\\) are linear and the matrix of the \\(x_{i j}=\\partial f_{i} / \\partial \\theta_{j}\\) is assumed to have full rank, \\(\\|\\boldsymbol{\\theta}\\|\\) must also be bounded (otherwise at least one of the \\(f_{i}(\\theta)\\) would be unbounded on \\(A_{b}\\); hence \\(\\sigma \\rho\\left\\{\\left[y_{i}-f_{i}(\\theta)\\right] / \\sigma\\right\\}\\) would be unbounded). Compactness of the sets \\(A_{b}\\) obviously implies (1). To prove (2), assume \\(\\hat{\\sigma}&gt;0\\) and let \\(\\left(\\boldsymbol{\\theta}^{\\left(m_{i}\\right)}, \\sigma^{\\left(m_{i}\\right)}\\right)\\) be a subsequence converging toward \\((\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})\\). Then</p> \\[ Q\\left(\\boldsymbol{\\theta}^{\\left(m_{i}\\right)}, \\sigma^{\\left(m_{i}\\right)}\\right) \\geqslant Q\\left(\\boldsymbol{\\theta}^{\\left(m_{i}\\right)}, \\sigma^{\\left(m_{i}+1\\right)}\\right) \\geqslant Q\\left(\\boldsymbol{\\theta}^{\\left(m_{i+1}\\right)}, \\sigma^{\\left(m_{i+1}\\right)}\\right) \\] <p>(see Lemma 8.1); the two outer members of this inequality tend to \\(Q(\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})\\); hence (see Lemmas 8.2 and 8.3)</p> \\[ Q\\left(\\boldsymbol{\\theta}^{\\left(m_{i}\\right)}, \\sigma^{\\left(m_{i}\\right)}\\right)-Q\\left(\\boldsymbol{\\theta}^{\\left(m_{i}\\right)}, \\sigma^{\\left(m_{i}+1\\right)}\\right) \\geqslant a \\frac{\\left(\\sigma^{\\left(m_{i}+1\\right)}-\\sigma^{\\left(m_{i}\\right)}\\right)^{2}}{\\sigma^{\\left(m_{i}\\right)}} \\] <p>converges to 0 . In particular, it follows that</p> \\[ \\left(\\frac{\\sigma^{\\left(m_{i}+1\\right)}}{\\sigma^{\\left(m_{i}\\right)}}\\right)^{2}=\\frac{1}{n a} \\sum \\chi_{0}\\left(\\frac{y_{i}-f_{i}\\left(\\boldsymbol{\\theta}^{\\left(m_{i}\\right)}\\right)}{\\sigma^{\\left(m_{i}\\right)}}\\right) \\] <p>converges to 1 ; hence in the limit</p> \\[ \\frac{1}{n} \\sum \\chi_{0}\\left(\\frac{y_{i}-f_{i}(\\hat{\\theta})}{\\hat{\\sigma}}\\right)=a \\] <p>Thus (8.3) is satisfied. In the same way, we obtain from Lemma 8.2 that</p> \\[ Q\\left(\\boldsymbol{\\theta}^{\\left(m_{i}\\right)}, \\sigma^{\\left(m_{i}\\right)}\\right)-Q\\left(\\theta^{\\left(m_{i}+1\\right)}, \\sigma^{\\left(m_{i}\\right)}\\right) \\geqslant \\frac{q(2-q)}{2 \\sigma^{\\left(m_{i}\\right)} n} \\sum_{j}\\left(\\sum_{i} r_{i}^{*} x_{i j}\\right)^{2} \\] <p>tends to 0 ; in particular</p> \\[ \\sum_{i} r_{i}^{*} x_{i j}=\\sigma^{\\left(m_{i}\\right)} \\sum_{i} \\psi\\left(\\frac{y_{i}-f_{i}\\left(\\theta^{\\left(m_{i}\\right)}\\right)}{\\sigma^{\\left(m_{i}\\right)}}\\right) x_{i j} \\rightarrow 0 \\] <p>Hence in the limit</p> \\[ \\sum \\psi\\left(\\frac{y_{i}-f_{i}(\\hat{\\theta})}{\\hat{\\sigma}}\\right) x_{i j}=0 \\] <p>and thus (8.2) also holds. In view of the convexity of \\(Q\\), every solution of (8.2) and (8.3) minimizes (8.1).</p> <p>We now intend to give conditions sufficient for there to be no accumulation points with \\(\\hat{\\sigma}=0\\). The main condition is one ensuring that the maximum number \\(p^{\\prime}\\) of residuals that can be made simultaneously 0 is not too big; assume that \\(\\chi_{0}\\) is symmetric and bounded, and that</p> \\[ n-p^{\\prime}&gt;(n-p) \\frac{E_{\\Phi}\\left(\\chi_{0}\\right)}{\\chi_{0}(\\infty)} \\] <p>Note that \\(p^{\\prime}=p\\) with probability 1 if the error distribution is absolutely continuous with respect to Lebesgue measure, so (8.51) is then automatically satisfied, since \\(E_{\\Phi}\\left(\\chi_{0}\\right)&lt;\\max \\left(\\chi_{0}\\right)=\\chi_{0}(\\infty)\\).</p> <p>We assume that the iteration is started with \\(\\left(\\theta^{(0)}, \\sigma^{(0)}\\right)\\) where \\(\\sigma^{(0)}&gt;0\\). Then \\(\\sigma^{(m)}&gt;0\\) for all finite \\(m\\). Moreover, we note that, for all \\(m,\\left(\\theta^{(m)}, \\sigma^{(m)}\\right)\\) then is contained in the compact set \\(A_{b}\\), with \\(b=Q\\left(\\theta^{(0)}, \\sigma^{(0)}\\right)\\). Hence it suffices to restrict \\((\\boldsymbol{\\theta}, \\sigma)\\) to \\(A_{b}\\) for all the following arguments.</p> <p>Clearly, (8.51) is equivalent to the following: for sufficiently small \\(\\sigma\\) we have</p> \\[ \\frac{1}{n} \\sum \\chi_{0}\\left(\\frac{r_{i}}{\\sigma}\\right)&gt;a=\\frac{n-p}{n} E_{\\Phi}\\left(\\chi_{0}\\right) \\] <p>This is strengthened in the following lemma. LEMMA 8.5 Assume that (8.51) holds. Then there is a \\(\\sigma_{0}&gt;0\\) and a \\(d&gt;1\\) such that for all \\((\\boldsymbol{\\theta}, \\sigma) \\in A_{b}\\) with \\(\\sigma \\leqslant \\sigma_{0}\\)</p> \\[ \\frac{1}{n} \\sum \\chi_{0}\\left(\\frac{r_{i}}{\\sigma}\\right) \\geqslant d^{2} a \\] <p>Proof For each \\(\\boldsymbol{\\theta}\\) order the corresponding residuals according to increasing absolute magnitude, and let \\(h(\\boldsymbol{\\theta})=\\left|r_{\\left(p^{\\prime}+1\\right)}\\right|\\) be the \\(\\left(p^{\\prime}+1\\right)\\) st smallest. Then \\(h(\\boldsymbol{\\theta})\\) is a continuous (in fact piecewise linear) strictly positive function. Since \\(A_{b}\\) is compact, the minimum \\(h_{0}\\) of \\(h(\\boldsymbol{\\theta})\\) is attained and hence must be strictly positive. It follows that</p> \\[ \\frac{1}{n} \\sum \\chi_{0}\\left(\\frac{r_{i}}{\\sigma}\\right) \\geqslant \\frac{n-p^{\\prime}}{n} \\chi_{0}\\left(\\frac{h_{0}}{\\sigma}\\right) \\] <p>In the limit \\(\\sigma \\rightarrow 0\\) the right-hand side becomes</p> \\[ \\frac{n-p^{\\prime}}{n} \\chi_{0}(\\infty)&gt;\\frac{n-p}{n} E_{\\Phi}\\left(\\chi_{0}\\right)=a \\] <p>in view of (8.51). Clearly strict inequality must already hold for some nonzero \\(\\sigma_{0}\\), and the assertion of the lemma follows.</p> <p>PROPOSITION 8.6 Assume (8.51), that \\(\\chi_{0}\\) is symmetric and bounded, and that \\(\\sigma^{(0)}&gt;0\\). Then the sequence \\(\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right.\\) ) cannot have an accumulation point on the boundary \\(\\sigma=0\\). Proof Lemma 8.5 implies that \\(\\sigma^{(m+1)} \\geqslant d \\sigma^{(m)}\\). It follows that the sequence \\(\\sigma^{(m)}\\) cannot indefinitely stay below \\(\\sigma_{0}\\) and that there must be infinitely many \\(m\\) for which \\(\\sigma^{(m)}&gt;\\sigma_{0}\\). Hence \\(\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)\\) has an accumulation point \\((\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})\\) with \\(\\hat{\\sigma}&gt;0\\), and, by Theorem 8.4, \\((\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})\\) minimizes \\(Q(\\boldsymbol{\\theta}, \\sigma)\\). It follows from (8.51) that, on the boundary, \\(Q(\\boldsymbol{\\theta}, 0)&gt;Q(\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})=b_{0}\\). Furthermore \\(\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)\\) ultimately stays in \\(A_{b_{0}+\\varepsilon}\\) for every \\(\\varepsilon&gt;0\\), and, for sufficiently small \\(\\varepsilon, A_{b_{0}+\\varepsilon}\\) does not intersect the boundary.</p> <p>THEOREM 8.7 Assume (8.51). Then, with the location step using modified residuals, the sequence \\(\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right.\\) ) always converges to some solution of (8.2) and (8.3).</p> <p>Proof If the solution \\((\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})\\) of the minimum problem (8.1), or of the simultaneous equations (8.2) and (8.3), is unique, then Theorem 8.4 and Proposition 8.6 together imply that \\((\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})\\) must be the unique accumulation point of the sequence, and there is nothing to prove. Assume now that the (necessarily convex) solution set \\(S\\) contains more than one point.</p> <p>A look at Exhibit 7.8.2 helps us to understand the following arguments; the diagram shows \\(S\\) and some of the surfaces \\(Q(\\boldsymbol{\\theta}, \\sigma)=\\) const.</p> <p>Clearly, for \\(m \\rightarrow \\infty, Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right) \\rightarrow \\inf Q(\\boldsymbol{\\theta}, \\sigma)\\), that is, \\(\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)\\) converge to the set \\(S\\). The idea is to demonstrate that the iteration steps succeeding \\(\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)\\) will have to stay inside an approximately conical region (the shaded region in the picture). With increasing \\(m\\) the base of the respective cone will get smaller and smaller, and since each cone is contained in the preceding one, this will imply convergence. The details of the proof are messy; we only sketch the main idea.</p> <p>We standardize the coordinate system such that \\(X^{T} X=2 a n I\\). Then</p> \\[ \\begin{aligned} &amp; \\Delta \\theta_{j}=\\frac{1}{2 a n} \\sum \\psi_{0}\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right) x_{i j} \\sigma^{(m)} \\\\ &amp; \\Delta \\sigma \\cong \\frac{1}{2} \\sigma^{(m)}\\left[\\left(\\frac{\\sigma^{(m+1)}}{\\sigma^{(m)}}\\right)^{2}-1\\right]=\\frac{\\sigma^{(m)}}{2 a}\\left[\\frac{1}{n} \\sum x_{0}\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right)-a\\right] \\end{aligned} \\] <p></p> <p>Exhlbit 7.8.2</p> <p>whereas the gradient of \\(Q\\) is given by</p> \\[ \\begin{aligned} g_{j} &amp; =\\frac{\\partial}{\\partial \\theta_{j}} Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)=-\\frac{1}{2} \\sum \\psi_{0}\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right) x_{i j}, \\quad j=1, \\ldots, p \\\\ g_{p+1} &amp; =\\frac{\\partial}{\\partial \\sigma} Q\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)=-\\frac{1}{n} \\sum \\chi_{0}\\left(\\frac{r_{i}}{\\sigma^{(m)}}\\right)-a \\end{aligned} \\] <p>In other words in this particular coordinate system, the step</p> \\[ \\begin{aligned} &amp; \\Delta \\theta_{j}=-\\frac{\\sigma^{(m)}}{2 a} g_{j} \\\\ &amp; \\Delta \\sigma=-\\frac{\\sigma^{(m)}}{2 a} g_{p+1} \\end{aligned} \\] <p>is in the direction of the negative gradient at the point \\(\\left(\\boldsymbol{\\theta}^{(m)}, \\sigma^{(m)}\\right)\\). It is not known to me whether this theorem remains true with the location step with modified weights.</p> <p>Of course, the algorithms described so far have to be supplemented with a stopping rule, for example, stop iterations when the shift of every linear combination \\(\\alpha=\\mathbf{a}^{T} \\boldsymbol{\\theta}\\) is smaller than \\(\\varepsilon\\) times its own estimated standard deviation [using (6.5)], with \\(\\varepsilon=0.001\\) or the like. Our experience is that on the average this will need about 10 iterations [for \\(\\rho\\) as in (7.14), with \\(c=1.5\\) ], with relatively little dependence on \\(p\\) and \\(n\\).</p> <p>If \\(\\psi\\) is piecewise linear, it is possible to devise algorithms that reach the exact solution in a finite (and usually small, mostly under 10) number of iterations, if they converge at all: partition the residuals according to the linear piece of \\(\\psi\\) on which they sit and determine the algebraically exact solution under the assumption that the partitioning of the residuals stays the same for the new parameter values. If this assumption turns out to be true, we have found the exact solution \\((\\hat{\\boldsymbol{\\theta}}, \\hat{\\sigma})\\); otherwise iterate. In the one-dimensional location case, this procedure seems to converge without fail; in the general regression case, some elaborate safeguards against singular matrices and other mishaps are needed. See Huber (1973a) and Dutter (1975, 1977a, b).</p> <p>As starting values \\(\\left(\\boldsymbol{\\theta}^{(0)}, \\sigma^{(0)}\\right)\\) we usually take the ordinary least squares estimate, despite its known poor properties [cf. Andrews et al. (1972), for the simple location case].</p> <p>Descending \\(\\psi\\)-functions are tricky, especially when the starting values for the iterations are nonrobust. Residuals that are accidentally large because of the poor starting parameters then may stay large forever</p> <p>because they exert zero pull. It is therefore preferable to start with a monotone \\(\\psi\\), iterate to death, and then append a few ( 1 or 2 ) iterations with the nonmonotone \\(\\psi\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#79-moderate-leverage-points","title":"7.9 MODERATE LEVERAGE POINTS","text":"<p>We now return to the example of Section 7.1 and to the formulas derived in Section 7.2. In particular, we recall that in the classical least squares case, with \\(\\operatorname{var}\\left(y_{i}\\right)=\\sigma^{2}\\),</p> \\[ \\begin{aligned} \\operatorname{var}\\left(\\hat{y}_{i}\\right) &amp; =h_{i} \\sigma^{2} \\\\ \\operatorname{var}\\left(y_{i}-\\hat{y}_{i}\\right) &amp; =\\left(1-h_{i}\\right) \\sigma^{2} \\\\ \\operatorname{var}\\left(\\hat{\\alpha}_{i}\\right) &amp; =\\frac{h_{i}}{1-h_{i}} \\sigma^{2} \\\\ y_{i}-\\hat{y}_{i} &amp; =\\left(1-h_{i}\\right)\\left(y_{i}-\\hat{\\alpha}_{i}\\right) \\\\ \\operatorname{var}\\left(y_{i}-\\hat{\\alpha}_{i}\\right) &amp; =\\frac{1}{1-h_{i}} \\sigma^{2} \\end{aligned} \\] <p>Here \\(\\hat{y}_{i}\\) denotes the fitted value, and \\(\\hat{\\alpha}_{i}\\) the \"interpolated\" value, estimated without using \\(y_{i}\\).</p> <p>If we robustize least squares by using (7.1), that is,</p> \\[ \\sum \\psi\\left(\\frac{y_{i}-\\hat{y}_{i}}{\\sigma}\\right) x_{i j}=0, \\quad j=1, \\ldots, p \\] <p>with \\(\\psi\\) as in (7.15), for instance, and if point \\(i\\) happens to be a leverage point with a high \\(h_{i}\\), then \\(y_{i}\\) can be grossly aberrant, but \\(\\left(y_{i}-\\hat{y}_{i}\\right) / \\sigma\\) will still remain on the linear part of \\(\\psi\\). This is, of course, undesirable.</p> <p>We can try to correct this by cutting down the overall influence of observation \\(i\\) by introducing a weight factor \\(\\gamma\\); we can shorten the linear part of \\(\\psi\\) by cutting down scale by a factor \\(\\delta\\); and we can do both ( \\(\\gamma\\) and \\(\\delta\\), of course, depend on \\(h_{i}\\) and possibly other variables). This means that we replace the term \\(\\psi\\left[\\left(y_{i}-\\hat{y}_{i}\\right) / \\sigma\\right]\\) in (9.6) by</p> \\[ \\gamma \\psi\\left(\\frac{y_{i}-\\hat{y}_{i}}{\\delta \\sigma}\\right) \\] <p>We claim there are strong heuristic arguments in favor of choosing \\(\\gamma=\\delta\\), and somewhat weaker ones suggest choosing</p> \\[ \\gamma=\\delta=\\sqrt{1-h_{i}} \\] <p>These arguments run as follows. Assume first that \\(y_{i}\\) and the interpolated value \\(\\hat{\\alpha}_{i}\\) differ only slightly, so that the residual sits on the linear part of \\(\\psi\\). Then (9.7) means, essentially, that the \\(i\\) th observation is treated with weight \\(\\gamma / \\delta\\). Clearly, if \\(h_{i}\\) is small, we should have \\(\\gamma \\approx \\delta \\approx 1\\). On the other hand if \\(h_{i}\\) is large, say \\(h_{i}&gt;\\frac{1}{2}\\), then according to (9.3) a \"good\" observation \\(y_{i}\\) is more accurate than \\(\\hat{\\alpha}_{i}\\). If the underlying distribution is a moderately contaminated normal one ( \\(\\varepsilon=1\\) to \\(10 \\%\\) ), then the likelihood is quite high that \\(y_{i}\\) is a \"good\" observation if it does not deviate too much from \\(\\hat{\\alpha}_{i}\\). But then we would not want the extrapolated value \\(\\hat{\\alpha}_{i}\\) to take precedence over \\(y_{i}\\), that is, we would not want to downweight \\(y_{i}\\). [Note that \\(\\hat{\\alpha}_{i}\\) is more influential than \\(y_{i}\\) anyway, cf. (2.36).] Thus we are induced to put \\(\\gamma=\\delta\\).</p> <p>Now imagine that, for most observations, the residuals sit on the linear part of \\(\\psi\\), so that, essentially, the parameters are determined by ordinary least squares. Move one observation \\(y_{i}\\) from \\(-\\infty\\) to \\(+\\infty\\), and, for each value of \\(y_{i}\\), let \\(y_{i}^{*}\\) be the corresponding observational value for which the least squares solution would agree with the robustized version based on (9.7). Using (9.4) we find that</p> \\[ \\begin{aligned} y_{i}^{*} &amp; =y_{i}, &amp; &amp; \\text { for }\\left|y_{i}-\\hat{\\alpha}_{i}\\right| \\leqslant \\frac{\\delta}{1-h_{i}} c \\sigma \\\\ &amp; =\\hat{\\alpha}_{i} \\pm \\frac{\\delta}{1-h_{i}} c \\sigma, &amp; &amp; \\text { for }\\left|y_{i}-\\hat{\\alpha}_{i}\\right| \\geqslant \\frac{\\delta}{1-h_{i}} c \\sigma \\end{aligned} \\] <p>In view of (9.5) it would seem natural to choose \\(\\delta=\\sqrt{1-h_{i}}\\), so that the changeover in (9.9) would be related to a natural measure of scale of \\(\\left|y_{i}-\\hat{\\alpha}_{i}\\right|\\).</p> <p>In other words we propose to modify (9.6) to</p> \\[ \\sum_{i} \\sigma \\sqrt{1-h_{i}} \\psi\\left\\{\\frac{y_{i}-\\hat{y}_{i}}{\\sigma \\sqrt{1-h_{i}}}\\right\\} x_{i j}=0 \\] <p>with scale \\(\\sigma\\) determined simultaneously from</p> \\[ \\sum_{i}\\left[\\sqrt{1-h_{i}} \\psi\\left\\{\\frac{y_{i}-\\hat{y_{i}}}{\\sigma \\sqrt{1-h_{i}}}\\right\\}\\right]^{2}=(n-p) E_{\\Phi} \\psi^{2} \\] <p>Equivalently, this corresponds to minimizing, simultaneously with respect to \\(\\boldsymbol{\\theta}\\) and \\(\\sigma\\), the expression</p> \\[ \\sum \\rho\\left\\{\\frac{y_{i}-\\hat{y_{i}}}{\\sigma \\sqrt{1-h_{i}}}\\right\\}\\left(1-h_{i}\\right) \\sigma \\] <p>with \\(\\rho\\) as in (7.14) and \\(\\beta=E_{\\Phi} \\psi^{2}\\). Computationally, this does not introduce any new problems; instead of modifying the residual \\(r_{i}=y_{i}-\\hat{y_{i}}\\) to \\(r_{i}^{*}= \\pm c \\sigma\\) whenever \\(\\left|r_{i}\\right|&gt;c \\sigma\\), we now modify it to \\(r_{i}^{*}= \\pm \\sqrt{1-h_{i}} c \\sigma\\) whenever \\(\\left|r_{i}\\right|&gt;\\sqrt{1-h_{i}} c \\sigma\\) (cf. the location step with modified residuals).</p> <p>If we look at these matters quantitatively, then it is clear that, for \\(h_{i} \\leqslant 0.2\\), the change from (9.6) to (9.10) is hardly noticeable (and the effort hardly worthwhile); on the other hand, for \\(h_{i} \\geqslant 0.8\\), it may not give a good enough protection from the ill effects of outliers.</p> <p>Therefore, several researchers have proposed more drastic downweighting of leverage points in order to bound, simultaneously for all possible positions in the design space, the influence of any observational value toward any estimable quantity. This is an outgrowth of Hampel's approach (cf. Section 11.1). Much of the material is unpublished; the most systematic treatment so far, together with a comprehensive account of earlier work done by Hampel, Mallows, Schweppe, and others, can be found in Krasker and Welsch (1980). The last-named authors find estimates that are asymptotically efficient at the model, subject to an overall bound on the gross-error sensitivity, both with regard to value and position.</p> <p>However, some basic difficulties remain unresolved. The most fundamental one probably is that we are confronted with a small sample problem - the fitted value at a high leverage point is essentially determined by a single observation. Therefore, we cannot really rely on tools derived from asymptotic considerations, like the influence function, and we should check everything against insights gained from finite sample theory (cf. Chapter 10). Some preliminary explorations along these lines seem to suggest that the Krasker-Welsch approach may be overly pessimistic with regard to outliers at leverage points, but not pessimistic enough with regard</p> <p>to small systematic errors at nonleverage points, and, somewhat obnoxiously, the first effect becomes particularly severe when there are no real leverage points (i.e., when max \\(h_{i}\\) is small, but large against \\(p / n\\) ). Clearly, much work still remains to be done in this area.</p> <p>To avoid possible misunderstandings, we should add that the preceding discussion has little to do with robustness relative to outliers among the independent variables (the rows of \\(X\\) ). Although several studies have been devoted to this important problem in the past few years, it does not seem that an adequate conceptual penetration has been achieved. In particular, unless we have an (approximate) model underlying the generation of the rows of \\(X\\), the concept of robustness would seem to be ill defined. In some cases a treatment via robust covariance/correlation matrices would seem to make more sense than the regression approach.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#710-analysis-of-variance","title":"7.10 ANALYSIS OF VARIANCE","text":"<p>Geometrically speaking, analysis of variance is concerned with nested models, say a larger \\(p\\)-parameter model and a smaller \\(q\\)-parameter model, \\(q&lt;p\\), and with orthogonal projections of the observational vector \\(y\\) into the linear subspaces \\(V_{q} \\subset V_{p}\\) spanned by the columns of the respective design matrices; see Exhibit 7.10.1. Let \\(\\hat{y}_{(p)}\\) and \\(\\hat{y}_{(q)}\\) be the fitted values. </p> <p>Exhibit 7.10.1</p> <p>If the experimental errors are independent normal with (say) unit variance, then the differences squared,</p> \\[ \\left\\|\\mathbf{y}-\\hat{\\mathbf{y}}_{(q)}\\right\\|^{2}, \\quad\\left\\|\\mathbf{y}-\\hat{\\mathbf{y}}_{(p)}\\right\\|^{2}, \\quad\\left\\|\\hat{\\mathbf{y}}_{(p)}-\\hat{\\mathbf{y}}_{(q)}\\right\\|^{2} \\] <p>are \\(\\chi^{2}\\)-distributed with \\(n-q, n-p, p-q\\) degrees of freedom, and the latter two are independent, so that</p> \\[ \\frac{1 /(p-q)\\left\\|\\hat{y}_{(p)}-\\hat{y}_{(q)}\\right\\|^{2}}{1 /(n-p)\\left\\|\\mathbf{y}-\\hat{y}_{(p)}\\right\\|^{2}} \\] <p>has an \\(F\\)-distribution, on which we can then base a test of the adequacy of the smaller model.</p> <p>What of this can be salvaged if the errors are no longer normal? Of course, the distributional assumptions behind (10.1) are then violated, and worse, the power of the tests may be severely impaired.</p> <p>If we try to improve by estimating \\(\\hat{y}_{(p)}\\) and \\(\\hat{y}_{(q)}\\) robustly, then these two quantities at least will be asymptotically normal under fairly general assumptions (cf. Sections 7.4 and 7.5). Since the projections are no longer orthogonal, but defined in a somewhat complicated nonlinear fashion, we do not obtain the same result by first projecting to \\(V_{p}\\), then to \\(V_{q}\\) as by directly projecting to \\(V_{q}\\) (even though the two results are asymptotically equivalent). For the sake of internal consistency, when more than two nested models are concerned, the former variant (project via \\(V_{p}\\) ) is preferable.</p> <p>It follows from Proposition 4.1 that, under suitable regularity conditions, \\(\\left\\|\\hat{y}_{(p)}-\\hat{y}_{(q)}\\right\\|^{2}\\) for the robust estimates still is asymptotically \\(\\chi^{2}\\), when suitably scaled, with \\(p-q\\) degrees of freedom. The denominator of (10.1), however, is nonrobust and useless as it stands. We must replace it by something that is a robust and consistent estimate of the expected value of the numerator. The obvious choice for the denominator, suggested by (6.5), is of course</p> \\[ \\frac{1}{n-p} \\frac{K^{2} \\sum \\psi\\left(r_{i} / \\sigma\\right)^{2} \\sigma^{2}}{\\left[(1 / n) \\sum \\psi^{\\prime}\\left(r_{i} / \\sigma\\right)\\right]^{2}} \\] <p>where</p> \\[ r_{i}=y_{i}-f_{i}\\left(\\hat{\\boldsymbol{\\theta}}_{(p)}\\right) \\] <p>Since the asymptotic approximations will not work very well unless \\(p / n\\) is</p> <p>reasonably small (say \\(p / n \\leqslant 0.2\\) ), and since \\(p \\geqslant 2, n-p\\) will be much larger than \\(p-q\\), and the numerator</p> \\[ \\frac{1}{p-q}\\left\\|\\hat{y}_{(p)}-\\hat{y}_{(q)}\\right\\|^{2} \\] <p>will always be much more variable than (10.2). Thus the quotient of (10.3) divided by (10.2),</p> \\[ \\frac{\\frac{1}{p-q}\\left\\|\\hat{y}_{(p)}-\\hat{y}_{(q)}\\right\\|^{2}}{\\frac{1}{n-p} \\frac{K^{2} \\sum \\psi\\left(r_{i} / \\sigma\\right)^{2} \\sigma^{2}}{\\left[(1 / n) \\sum \\psi^{\\prime}\\left(r_{i} / \\sigma\\right)\\right]^{2}}} \\] <p>will be approximated quite well by a \\(\\chi^{2}\\)-variable with \\(p-q\\) degrees of freedom, divided by \\(p-q\\), and presumably even better by an \\(F\\)-distribution with \\(p-q\\) degrees of freedom in the numerator and \\(n-p\\) degrees in the denominator. We might argue that the latter value-but not the factor \\(n-p\\) occurring in (10.4)-should be lowered somewhat; however, since the exact amount depends on the underlying distribution and is not known anyway, we may just as well stick to the classical value \\(n-p\\).</p> <p>Thus we end up with the following proposal for doing analysis of variance. Unfortunately, it is only applicable when there is a considerable excess of observations over parameters, say \\(p / n \\leqslant 0.2\\). First, fit the largest model under consideration, giving \\(\\hat{y}_{(p)}\\). Make sure that there are no leverage points (an erroneous observation at a leverage point of the larger model may cause an erroneous rejection of the smaller model), or at least, be aware of the danger. Then estimate the dispersion of the \"unit weight\" fitted value by (10.2). Estimate the parameters of smaller models using \\(\\hat{y}_{(p)}\\) (not y ) by ordinary least squares. Then proceed in the classical fashion [but replace \\([1 /(n-p)]\\left\\|\\mathrm{y}-\\mathrm{y}_{(p)}\\right\\|^{2}\\) by (10.2)].</p> <p>Incidentally, the above procedure can also be described as follows. Let</p> \\[ r_{k}^{*}=\\frac{K \\psi\\left(r_{k} / \\sigma\\right) \\sigma}{\\frac{1}{n} \\sum \\psi^{\\prime}\\left(r_{i} / \\sigma\\right)} \\] <p>Put</p> \\[ \\mathbf{y}^{*}=\\hat{y}_{(p)}+\\mathbf{r}^{*} \\] <p>Then proceed classically, using the pseudo-observations of \\(\\mathbf{y}^{*}\\) instead of \\(y\\).</p> <p>At first sight the following approach also might look attractive. First, fit the largest model, yielding \\(\\hat{\\mathbf{y}}_{(p)}\\). This amounts to an ordinary weighted least squares fit with modified weights (8.32). Now freeze the weights \\(w_{i}\\) and proceed in the classical fashion, using \\(y_{i}\\) and the same weights \\(w_{i}\\) for all models. However, this gives improper (inconsistent) values for the denominator of (10.1), and for monotone \\(\\psi\\)-functions it is not even outlier resistant.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-8","title":"CHAPTER 8","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#robust-covariance-and-correlation-matrices","title":"Robust Covariance and Correlation Matrices","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#81-general-remarks","title":"8.1 GENERAL REMARKS","text":"<p>The classical covariance and correlation matrices are used for a variety of different purposes. We mention just a few:</p> <ul> <li>They (or better: the associated ellipsoids) give a simple description of the overall shape of a pointcloud in \\(p\\)-space. This is an important aspect with regard to discriminant analysis as well as principal component and factor analysis.</li> <li>They allow us to calculate variances in arbitrary directions: \\(\\operatorname{var}\\left(\\mathbf{a}^{\\boldsymbol{T}} \\mathbf{x}\\right)\\) \\(=\\mathbf{a}^{T} \\operatorname{cov}(\\mathbf{x}) \\mathbf{a}\\).</li> <li>For a multivariate Gaussian distribution, the sample covariance matrix, together with the sample mean, is a sufficient statistic.</li> <li>They can be used for tests of independence.</li> </ul> <p>Unfortunately, sample covariance matrices are excessively sensitive to outliers. All too often a principal component or factor analysis \"explains\" a structure that, on closer scrutiny, has been created by a mere one or two outliers (cf. Exhibit 8.1.1).</p> <p>The robust alternative approaches can be roughly classified into the following three categories: (1) Robust estimation of the individual matrix elements of the covariance/correlation matrix. (2) Robust estimation of variances in sufficiently many selected directions (to which a quadratic form is then fitted). (3) Direct (maximum likelihood) estimation of the shape matrix of some elliptical distribution.</p> <p>The third of these approaches is affinely invariant; the first one certainly is not. The second one is somewhere in between, depending on how the</p> <p></p> <p>Exhibit 8.1.1 From Devlin, Gnanadesikan, and Kettenring (1979); see also Chen, H., Gnanadesikan, R., and Kettenring, J. R. (1974), with permission of the authors. directions are selected. For example, we can choose them in relation to the coordinate axes and determine the matrix elements as in (1), or we can mimic an eigenvector/eigenvalue determination and find the direction with the smallest or largest robust variance, leading to an orthogonally invariant approach.</p> <p>The coordinate dependent approaches are more germane to the estimation of correlation matrices (which are coordinate dependent anyway); the affinely invariant ones are better matched to covariance matrices.</p> <p>Exhibits 8.1.1 and 8.1.2 illustrate the severity of the effects. Exhibit 8.1.1 shows a principal component analysis of 14 economic characteristics of 29 chemical companies, namely the projection of the data on the plane of the first 2 components. The sample correlation between the two principal components is zero, as it should be, but there is a maverick company in the bottom right-hand corner, invalidating the analysis.</p> <p>Exhibit 8.1.2 compares the influence of outliers on classical and on robust covariance estimates. The solid lines show ellipses derived from the classical sample covariance, theoretically containing \\(80 \\%\\) of the total normal mass; the dotted lines derive from the maximum likelihood estimate based on (10.26) with \\(\\kappa=2\\) and correspond to the ellipse \\(|\\mathbf{y}|=b=2\\),</p> <p> (a)  (b)</p> <p>Exhibit 8.1.2</p> <p>which, asymptotically, also contains about \\(80 \\%\\) of the total mass, if the underlying distribution is normal. The observations in Exhibit 8.1.2a are a random sample of size 18 from a bivariate normal distribution with covariance matrix</p> \\[ \\left(\\begin{array}{cc} 1 &amp; 0.9 \\\\ 0.9 &amp; 1 \\end{array}\\right) \\] <p>In Exhibit 8.1.2b, two contaminating observations with covariance matrix</p> \\[ \\left(\\begin{array}{cc} 4 &amp; -3.6 \\\\ -3.6 &amp; 4 \\end{array}\\right) \\] <p>were added to the sample.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#82-estimation-of-matrix-elements-through-robust-variances","title":"8.2 ESTIMATION OF MATRIX ELEMENTS THROUGH ROBUST VARIANCES","text":"<p>This approach is based on the following identity, valid for square integrable random variables \\(X\\) and \\(Y\\) :</p> \\[ \\operatorname{cov}(X, Y)=\\frac{1}{4 a b}[\\operatorname{var}(a X+b Y)-\\operatorname{var}(a X-b Y)] \\] <p>It has been utilized by Gnanadesikan and Kettenring (1972). Assume that \\(S\\) is a robust scale functional; we write for short \\(S(X)=\\) \\(S\\left(F_{X}\\right)\\) and assume that</p> \\[ S(a X+b)=|a| S(X) \\] <p>If we replace \\(\\operatorname{var}(\\cdot)\\) by \\(S(\\cdot)^{2}\\), then (2.1) is turned into the definition of a robust alternative \\(C(X, Y)\\) to the covariance between \\(X\\) and \\(Y\\) :</p> \\[ C(X, Y)=\\frac{1}{4 a b}\\left[S(a X+b Y)^{2}-S(a X-b Y)^{2}\\right] \\] <p>The constants \\(a\\) and \\(b\\) can be chosen arbitrarily, but (2.3) will have awkward and unstable properties if \\(a X\\) and \\(b Y\\) are on an entirely different scale. Gnanadesikan and Kettenring therefore recommend to take, for \\(a\\) and \\(b\\), the inverses of some robust scale estimates for \\(X\\) and \\(Y\\), respectively; for example, take</p> \\[ \\begin{aligned} &amp; a=\\frac{1}{S(X)} \\\\ &amp; b=\\frac{1}{S(Y)} \\end{aligned} \\] <p>Then</p> \\[ \\frac{1}{4}\\left[S(a X+b Y)^{2}-S(a X-b Y)^{2}\\right] \\] <p>becomes a kind of robust correlation. However, it is not necessarily confined to the interval \\([-1,+1]\\), and the expression</p> \\[ R^{*}(X, Y)=\\frac{S(a X+b Y)^{2}-S(a X-b Y)^{2}}{S(a X+b Y)^{2}+S(a X-b Y)^{2}} \\] <p>will therefore be preferable to (2.5) as a definition of a robust correlation coefficient. \"Covariances\" then can be reconstructed as</p> \\[ C^{*}(X, Y)=R^{*}(X, Y) S(X) S(Y) \\] <p>It is convenient to standardize \\(S\\) such that \\(S(X)=1\\) if \\(X\\) is normal \\(\\mathfrak{R}(0,1)\\). Then if the joint distribution of \\((X, Y)\\) is bivariate normal, we have</p> \\[ C(X, Y)=C^{*}(X, Y)=\\operatorname{cov}(X, Y) \\] <p>Proof (2.8) Note that \\(a X \\pm b Y\\) is normal with variance</p> \\[ a^{2} \\operatorname{var}(X) \\pm 2 a b \\operatorname{cov}(X, Y)+b^{2} \\operatorname{var}(Y) \\] <p>Now (2.8) follows from (2.2). If \\(X\\) and \\(Y\\) are independent, but not necessarily normal, and if the distribution of either \\(X\\) or \\(Y\\) is symmetric, then, clearly, \\(C(X, Y)=\\) \\(C^{*}(X, Y)=0\\).</p> <p>Now let \\(S_{n}(X)\\) and \\(C_{n}(X, Y)\\) be the finite sample versions based on \\(\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\). We can expect that the asymptotic distribution of \\(\\sqrt{n}\\left[C_{n}(X, Y)-C(X, Y)\\right]\\) will be normal, but already for a normal parent distribution we obtain some quite complicated expressions. For a nonnormal parent the situation seems to become almost intractably messy.</p> <p>This approach has another and even more serious drawback: when it is applied to the components of a \\(p\\)-vector \\(\\mathbf{X}=\\left(X_{1}, \\ldots, X_{p}\\right)\\), it does not automatically produce a positive definite robust correlation or covariance matrix \\(\\left[C\\left(X_{i}, X_{j}\\right)\\right]\\) and thus these matrices may cause both computational and conceptual trouble (the shape ellipsoid may be a hyperboloid!). The schemes proposed by Devlin et al. (1975) to enforce positive definiteness would seem to be very difficult to analyze theoretically.</p> <p>There is an intriguing and, as far as I know, not yet explored variant of this approach, that avoids this drawback. It directly determines the eigenvalues \\(\\lambda_{i}\\) and eigenvectors \\(\\mathbf{u}_{i}\\) of a robust covariance matrix; namely find</p> <p>that unit vector \\(\\mathbf{u}_{1}\\) for which \\(\\lambda_{1}=S\\left(\\mathbf{u}_{1}^{T} \\mathbf{X}\\right)^{2}\\) is maximal (or minimal), then do the same for unit vectors \\(\\mathbf{u}_{2}\\) orthogonal to \\(\\mathbf{u}_{1}\\), and so on. This will automatically give a positive definite matrix.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#83-estimation-of-matrix-elements-through-robust-correlation","title":"8.3 ESTIMATION OF MATRIX ELEMENTS THROUGH ROBUST CORRELATION","text":"<p>This approach is based on a remarkable distribution-free property of the ordinary sample correlation coefficient</p> \\[ r_{n}(\\mathbf{x}, \\mathbf{y})=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\left[\\sum\\left(x_{i}-\\bar{x}\\right)^{2} \\cdot \\sum\\left(y_{i}-\\bar{y}\\right)^{2}\\right]^{1 / 2}} \\] <p>THEOREM 3.1 If the two vectors \\(\\mathbf{x}^{T}=\\left(x_{1}, \\ldots, x_{n}\\right)\\) and \\(\\mathbf{y}^{T}=\\left(y_{1}, \\ldots, y_{n}\\right)\\) are independent, and either the distribution of \\(\\mathbf{y}\\) or the distribution of \\(\\mathbf{x}\\) is invariant under permutation of the components, then</p> \\[ \\begin{aligned} &amp; E\\left(r_{n}\\right)=0 \\\\ &amp; E\\left(r_{n}^{2}\\right)=\\frac{1}{n-1} \\end{aligned} \\] <p>Proof It suffices to calculate the above expectations conditionally, \\(\\mathbf{x}\\) given, and \\(\\mathbf{y}\\) given up to a random permutation.</p> <p>Despite this distribution-free result, \\(r_{n}\\) obviously is not robust-one single, sufficiently bad outlying pair ( \\(x_{i}, y_{i}\\) ) can shift \\(r_{n}\\) to any value in \\((-1,+1)\\).</p> <p>But the following is a remedy. Replace \\(r_{n}(\\mathbf{x}, \\mathbf{y})\\) by \\(r_{n}(\\mathbf{u}, \\mathbf{v})\\), where \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are computed from \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), respectively, according to certain quite general rules given below. The first two of the following five requirements are essential; the others add some convenience: (1) \\(\\mathbf{u}\\) is computed from \\(\\mathbf{x}\\) and \\(\\mathbf{v}\\) from \\(\\mathbf{y}: \\mathbf{u}=\\Psi(\\mathbf{x}), \\mathbf{v}=\\Xi(\\mathbf{y})\\). (2) \\(\\Psi\\) and \\(\\Xi\\) commute with permutations of the components of \\(\\mathbf{x}, \\mathbf{u}\\) and \\(\\mathbf{y}, \\mathbf{v}\\). (3) \\(\\Psi\\) and \\(\\Xi\\) preserve a monotone ordering of the components of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). (4) \\(\\Psi=\\Xi\\). (5) \\(\\forall a&gt;0, \\forall b, \\exists a_{1}&gt;0, \\exists b_{1}, \\forall \\mathbf{x} \\Psi(a \\mathbf{x}+b)=a_{1} \\Psi(\\mathbf{x})+b_{1}\\).</p> <p>Of these requirements, (1) and (2) ensure that \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) still satisfy the assumptions of Theorem 3.1 if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) do. If (3) holds, then perfect rank correlations are preserved. Finally (4) and (5) together imply that correla-</p> <p>tions \\(\\pm 1\\) are preserved. In the following two examples, all five requirements hold.</p> <p>Example 3.1 Let</p> \\[ u_{i}=a\\left(R_{i}\\right) \\] <p>where \\(R_{i}\\) is the rank of \\(x_{i}\\) in \\(\\left(x_{1}, \\ldots, x_{n}\\right)\\) and \\(a(\\cdot)\\) is some monotone scores function. The choice \\(a(i)=i\\) gives the classical Spearman rank correlation between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).</p> <p>Example 3.2 Let \\(T\\) and \\(S\\) be arbitrary estimates of location and scale satisfying</p> \\[ \\begin{aligned} &amp; T(a \\mathbf{x}+b)=a T(\\mathbf{x})+b \\\\ &amp; S(a \\mathbf{x}+b)=|a| S(\\mathbf{x}) \\end{aligned} \\] <p>let \\(\\psi\\) be a monotone function, and put</p> \\[ u_{i}=\\psi\\left(\\frac{x_{i}-T}{S}\\right) \\] <p>For example, \\(S\\) could be the median absolute deviation, and \\(T\\) the \\(M\\)-estimate determined by</p> \\[ \\sum \\psi\\left(\\frac{x_{i}-T}{S}\\right)=0 \\] <p>The choice \\(\\psi(x)=\\operatorname{sign}(x)\\) and \\(T=\\operatorname{med}\\left\\{x_{i}\\right\\}\\) gives the so-called quadrant correlation.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#some-properties-of-such-modified-correlations","title":"Some Properties of Such Modified Correlations","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#minimax-bias","title":"Minimax Bias","text":"<p>Let \\(G\\) and \\(H\\) be centrosymmetric distributions in \\(\\mathbb{R}^{2}\\), and assume that \\((X, Y)\\) is distributed according to the mixture</p> \\[ F=(1-\\varepsilon) G+\\varepsilon H \\] <p>Then the (ordinary) correlation coefficient \\(\\rho_{F}\\) of \\(\\psi(X)\\) and \\(\\psi(Y)\\) satisfies</p> \\[ (1-\\eta) \\rho_{G}-\\eta \\leqslant \\rho_{F} \\leqslant(1-\\eta) \\rho_{G}+\\eta \\] <p>The bounds are sharp, with</p> \\[ \\frac{\\eta}{1-\\eta}=\\frac{\\varepsilon}{1-\\varepsilon} \\cdot \\frac{\\sup \\psi^{2}}{E_{G}\\left(\\psi^{2}\\right)} \\] <p>Thus \\(\\eta\\) is made smallest if \\(\\psi(x)=\\operatorname{sign}(x)\\); in other words the quadrant correlation is asymptotically minimax with respect to bias. This is analogous to the minimax property of the sample median (Section 4.2).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#tests-for-independence","title":"Tests for Independence","text":"<p>Take the following test problem. Hypothesis: the probability law behind \\(\\left(X^{*}, Y^{*}\\right)\\) is</p> \\[ \\begin{aligned} &amp; X^{*}=X+\\delta \\cdot Z \\\\ &amp; Y^{*}=Y+\\delta \\cdot Z_{1} \\end{aligned} \\] <p>where \\(X, Y, Z\\), and \\(Z_{1}\\) are independent symmetric random variables, with \\(Z\\) and \\(Z_{1}\\) being bounded and having the same distribution. Assume \\(\\operatorname{var}(Z)=\\operatorname{var}\\left(Z_{1}\\right)=1 ; \\delta\\) is a small number.</p> <p>The alternative is the same, except that \\(Z=Z_{1}\\). According to the Neyman-Pearson lemma, the most powerful tests are based on the test statistic</p> \\[ \\prod_{i} \\frac{h_{A}\\left(x_{i}, y_{i}\\right)}{h_{H}\\left(x_{i}, y_{i}\\right)} \\] <p>where \\(h_{H}\\) and \\(h_{A}\\) are the densities of \\(\\left(X^{*}, Y^{*}\\right)\\) under the hypothesis and the alternative, respectively. If \\(f\\) and \\(g\\) are the densities of \\(X\\) and \\(Y\\), respectively, we have</p> \\[ \\begin{aligned} &amp; h_{H}(x, y)=E\\left[f(x-\\delta Z) g\\left(y-\\delta Z_{1}\\right)\\right] \\\\ &amp; h_{A}(x, y)=E[f(x-\\delta Z) g(y-\\delta Z)] \\end{aligned} \\] <p>and thus</p> \\[ \\frac{h_{A}(x, y)}{h_{H}(x, y)}=1+\\frac{\\operatorname{cov}[f(x-\\delta Z), g(y-\\delta Z)]}{E[f(x-\\delta Z)] E[g(y-\\delta Z)]} \\] <p>If \\(f\\) and \\(g\\) can be expanded into a Taylor series</p> \\[ f(x-\\delta Z)=f(x)-\\delta Z f^{\\prime}(x)+\\frac{1}{2} \\delta^{2} Z^{2} f^{\\prime \\prime}(x)-\\cdots \\] <p>we obtain that</p> \\[ \\frac{h_{A}(x, y)}{h_{H}(x, y)}=1+\\delta^{2} \\frac{f^{\\prime}(x)}{f(x)} \\frac{g^{\\prime}(y)}{g(y)}+O\\left(\\delta^{4}\\right) \\] <p>so, asymptotically for \\(\\delta \\rightarrow 0\\), the most powerful test will be based on the test statistic</p> \\[ T_{n}=\\sum \\psi\\left(x_{i}\\right) \\chi\\left(y_{i}\\right) \\] <p>where</p> \\[ \\begin{aligned} &amp; \\psi(x)=\\frac{f^{\\prime}(x)}{f(x)} \\\\ &amp; \\chi(x)=\\frac{g^{\\prime}(x)}{g(x)} \\end{aligned} \\] <p>If we standardize (3.15) by dividing it by its (estimated) standard deviation, then we obtain a robust correlation of the form suggested in Example 3.2.</p> <p>Under the hypothesis the test statistic (3.15) has expectation 0 and variance</p> \\[ E_{H}\\left(T_{n}^{2}\\right)=n E\\left(\\psi^{2}\\right) E\\left(\\chi^{2}\\right) \\] <p>Under the alternative the expectation is</p> \\[ E_{A}\\left(T_{n}\\right)=\\delta^{2} n E\\left(\\psi^{\\prime}\\right) E\\left(\\chi^{\\prime}\\right) \\] <p>while the variance stays the same (neglecting higher order terms in \\(\\delta\\) ). It follows that the asymptotic power of the test can be measured by the variance ratio</p> \\[ \\frac{\\left[E_{A}\\left(T_{n}\\right)\\right]^{2}}{\\operatorname{var}_{A}\\left(T_{n}\\right)} \\approx n \\delta^{4} \\frac{\\left[E\\left(\\psi^{\\prime}\\right)\\right]^{2}\\left[E\\left(\\chi^{\\prime}\\right)\\right]^{2}}{E\\left(\\psi^{2}\\right) E\\left(\\chi^{2}\\right)} \\] <p>This holds also if \\(\\psi\\) and \\(\\chi\\) are not related to \\(f\\) and \\(g\\) by (3.16) and (3.17). [For a rigorous treatment of such problems under less stringent regularity conditions, see H\u00e1jek and \u0160id\u00e1k (1967), p. 75ff.].</p> <p>A glance at (3.20) shows that there is a close formal analogy to problems in estimation of location. For instance, if the distributions of \\(X^{*}\\) and \\(Y^{*}\\)</p> <p>vary over some sets and we would like to maximize the minimum asymptotic power of a test for independence, we have to find the distributions \\(f\\) and \\(g\\) minimizing Fisher information for location (!).</p> <p>Of course, this also bears directly on correlation estimates, since in most cases it will be desirable to optimize the estimates so that they are best for nearly independent variables.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#a-particular-choice-for-psi","title":"A Particular Choice for \\(\\psi\\)","text":"<p>Let</p> \\[ \\begin{aligned} &amp; \\psi_{c}(x)=2 \\Phi\\left(\\frac{x}{c}\\right)-1, \\quad \\text { for } c&gt;0 \\\\ &amp; \\psi_{0}(x)=\\operatorname{sign}(x) \\end{aligned} \\] <p>where \\(\\Phi\\) is the standard normal cumulative. PROPOSITION 3.2 If \\((X, Y)\\) is bivariate normal with mean 0 and covariance matrix</p> \\[ \\left(\\begin{array}{cc} 1 &amp; \\beta \\\\ \\beta &amp; 1 \\end{array}\\right) \\] <p>then</p> \\[ E\\left[\\psi_{c}(X) \\psi_{c}(Y)\\right]=\\frac{2}{\\pi} \\arcsin \\left(\\frac{\\beta}{1+c^{2}}\\right) \\] <p>Proof We first treat the special case \\(c=0\\). We may represent \\(Y\\) as \\(Y=\\beta X-\\sqrt{1-\\beta^{2}} Z\\) where \\(X\\) and \\(Z\\) are independent standard normal. We have</p> \\[ E\\left[\\psi_{0}(X) \\psi_{0}(Y)\\right]=4 P\\{X&gt;0, Y&gt;0\\}-1 \\] <p>Now</p> \\[ \\begin{aligned} P\\{X&gt;0, Y&gt;0\\} &amp; =P\\left\\{X&gt;0, \\beta X-\\sqrt{1-\\beta^{2}} Z&gt;0\\right\\} \\\\ &amp; =\\int_{\\beta x-\\sqrt{1-\\beta^{2}} z&gt;0} \\frac{1}{2 \\pi} e^{-\\left(x^{2}+z^{2}\\right) / 2} d x d z \\end{aligned} \\] <p></p> <p>Exhibit 8.3.1 is the integral of the bivariate normal density over the shaded sector in Exhibit 8.3.1. The slope of the boundary line is \\(\\beta / \\sqrt{1-\\beta^{2}}\\); thus</p> \\[ \\varphi=\\arcsin \\beta \\] <p>and it follows that</p> \\[ P(X&gt;0, Y&gt;0)=\\frac{1}{4}+\\frac{1}{2 \\pi} \\arcsin \\beta \\] <p>Thus the special case is proved. With regard to the general case, we first note that</p> \\[ E\\left[\\psi_{c}(X) \\psi_{c}(Y)\\right]=4 E\\left[\\Phi\\left(\\frac{X}{c}\\right) \\Phi\\left(\\frac{Y}{c}\\right)\\right]-1 \\] <p>If we introduce two auxiliary random variables \\(Z_{1}\\) and \\(Z_{2}\\) that are standard normal and independent of \\(X\\) and \\(Y\\), we can write</p> \\[ \\begin{aligned} E\\left[\\Phi\\left(\\frac{X}{c}\\right) \\Phi\\left(\\frac{Y}{c}\\right)\\right] &amp; =E\\left[P^{X}\\left(X-c Z_{1}&gt;0\\right) P^{Y}\\left(Y-c Z_{2}&gt;0\\right)\\right] \\\\ &amp; =P\\left\\{X-c Z_{1}&gt;0, Y-c Z_{2}&gt;0\\right\\} \\end{aligned} \\] <p>where \\(P^{X}\\) and \\(P^{Y}\\) denote conditional probabilities, given \\(X\\) and \\(Y\\), respectively. But since the correlation of \\(X-c Z_{1}\\) and \\(Y-c Z_{2}\\) is \\(\\beta /\\left(1+c^{2}\\right)\\), the general case now follows from the special one.</p> <p>NOTE 1 This theorem exhibits a choice of \\(\\psi\\) for which we can recapture the original correlation of \\(X\\) and \\(Y\\) from that of \\(\\psi(X)\\) and \\(\\psi(Y)\\) in a particularly simple way. However, if this transformation is applied to the elements of a sample covariance/correlation matrix, it in general destroys positive definiteness. So we may prefer to work with the covariances of \\(\\psi(X)\\) and \\(\\psi(Y)\\), even though they are biased.</p> <p>NOTE 2 If \\(T_{X, n}\\) and \\(T_{Y, n}\\) are the location estimates determined through</p> \\[ \\begin{aligned} &amp; \\sum \\psi\\left(x_{i}-T_{X}\\right)=0 \\\\ &amp; \\sum \\psi\\left(y_{i}-T_{Y}\\right)=0 \\end{aligned} \\] <p>then the correlation \\(\\rho(\\psi(X), \\psi(Y))\\) can be interpreted as the (asymptotic) correlation between the two location estimates \\(T_{X, n}\\) and \\(T_{Y, n}\\). (Heuristic argument: use the influence function to write</p> \\[ \\begin{aligned} &amp; T_{X, n} \\cong \\frac{1}{n} \\sum \\frac{\\psi\\left(x_{i}\\right)}{E\\left(\\psi^{\\prime}\\right)} \\\\ &amp; T_{Y, n} \\cong \\frac{1}{n} \\sum \\frac{\\psi\\left(y_{i}\\right)}{E\\left(\\psi^{\\prime}\\right)} \\end{aligned} \\] <p>assuming without loss of generality that the limiting values of \\(T_{X, n}\\) and \\(T_{Y, n}\\) are 0 . Thus</p> \\[ \\operatorname{cov}\\left(T_{X, n}, T_{Y, n}\\right) \\cong \\frac{1}{n} \\frac{E[\\psi(X) \\psi(Y)]}{\\left[E\\left(\\psi^{\\prime}\\right)\\right]^{2}} \\] <p>The (relative) efficiency of these covariance/correlation estimates is the square of that of the corresponding location estimates, so the efficiency loss may be quite severe. For instance, assume that the correlation \\(\\rho\\) in Proposition 3.2 is small. Then</p> \\[ \\rho\\left(\\psi_{c}(X), \\psi_{c}(Y)\\right) \\approx \\beta \\frac{1}{\\left(1+c^{2}\\right) \\arcsin \\left[1 /\\left(1+c^{2}\\right)\\right]} \\] <p>and</p> \\[ \\rho\\left(\\psi_{0}(X), \\psi_{0}(Y)\\right) \\approx \\beta \\cdot \\frac{2}{\\pi} \\] <p>Thus if we are testing \\(\\rho(X, Y)=0\\) against \\(\\rho(X, Y)=\\beta=\\beta_{0} / \\sqrt{n}\\), for sample size \\(n\\), then the asymptotic efficiency of \\(r_{n}\\left(\\psi_{c}(X), \\psi_{c}(Y)\\right)\\) relative to \\(r_{n}(X, Y)\\) is</p> \\[ \\left[\\left(1+c^{2}\\right) \\arcsin \\left(\\frac{1}{1+c^{2}}\\right)\\right]^{-2} \\] <p>For \\(c=0\\) this amounts to \\(4 / \\pi^{2} \\approx 0.41\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#84-an-affinely-invariant-approach","title":"8.4 AN AFFINELY INVARIANT APPROACH","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#maximum-likelihood-estimates","title":"Maximum Likelihood Estimates","text":"<p>Let \\(f(\\mathbf{x})=f(|\\mathbf{x}|)\\) be a spherically symmetric probability density in \\(\\mathbb{R}^{p}\\). We apply general nondegenerate affine transformations \\(\\mathbf{x} \\rightarrow V(\\mathbf{x}-\\mathbf{t})\\) to obtain a \\(p\\)-dimensional location and scale family of \"elliptic\" densities</p> \\[ f(\\mathbf{x} ; \\mathbf{t}, V)=|\\operatorname{det} V| f(|V(\\mathbf{x}-\\mathbf{t})|) \\] <p>The problem is to estimate the vector \\(\\mathbf{t}\\) and the matrix \\(V\\) from \\(n\\) observations of \\(\\mathbf{x}\\).</p> <p>Evidently, \\(V\\) is not uniquely identifiable (it can be multiplied by an arbitrary orthogonal matrix from the left), but \\(V^{T} V\\) is. We can enforce uniqueness of \\(V\\) by suitable side conditions, for example by requiring that it be positive definite symmetric, or that it be lower triangular with a positive diagonal. Mostly, we adopt the latter convention; it is the most convenient one for numerical calculations, but the other is more convenient for some proofs.</p> <p>The maximum likelihood estimate of \\((\\mathbf{t}, V)\\) is obtained by maximizing</p> \\[ \\log (\\operatorname{det} V)+\\operatorname{ave}\\{\\log f(|V(\\mathbf{x}-\\mathbf{t})|)\\} \\] <p>where ave \\(\\{\\cdot\\}\\) denotes the average taken over the sample. A necessary condition for a maximum is that (4.2) remain stationary under infinitesimal variations of \\(\\mathbf{t}\\) and \\(V\\). So we let \\(\\mathbf{t}\\) and \\(V\\) depend differentiably on a dummy parameter and take the derivative (denoted by a superscribed dot). We obtain the condition</p> \\[ \\operatorname{tr}(S)+\\operatorname{ave}\\left\\{\\frac{f^{\\prime}(|\\mathbf{y}|)}{f(|\\mathbf{y}|)} \\cdot \\frac{\\mathbf{y}^{T} S \\mathbf{y}}{|\\mathbf{y}|}\\right\\}-\\operatorname{ave}\\left\\{\\frac{f^{\\prime}(|\\mathbf{y}|)}{f(|\\mathbf{y}|)} \\frac{\\mathbf{t}^{T} V^{T} \\mathbf{y}}{|\\mathbf{y}|}\\right\\}=0 \\] <p>with the abbreviations</p> \\[ \\begin{aligned} &amp; \\mathbf{y}=V(\\mathbf{x}-\\mathbf{t}) \\\\ &amp; S=\\dot{V} V^{-1} \\end{aligned} \\] <p>Since this should hold for arbitrary infinitesimal variations \\(\\dot{\\mathbf{t}}\\) and \\(\\dot{V}\\), (4.3) can be rewritten into the set of simultaneous matrix equations</p> \\[ \\begin{aligned} \\operatorname{ave}\\{w(|y|) y\\} &amp; =0 \\\\ \\operatorname{ave}\\left\\{w(|y|) y y^{T}-I\\right\\} &amp; =0 \\end{aligned} \\] <p>where \\(I\\) is the \\(p \\times p\\) identity matrix, and</p> \\[ w(|y|)=-\\frac{f^{\\prime}(|y|)}{|y| f(|y|)} \\] <p>Example 4.1 Let</p> \\[ f(|x|)=(2 \\pi)^{-p / 2} \\exp \\left(\\frac{-|\\mathbf{x}|^{2}}{2}\\right) \\] <p>be the standard normal density. Then \\(w \\equiv 1\\), and (4.6) and (4.7) can equivalently be written as</p> \\[ \\begin{aligned} \\mathbf{t} &amp; =\\operatorname{ave}\\{\\mathbf{x}\\} \\\\ \\left(V^{T} V\\right)^{-1} &amp; =\\operatorname{ave}\\left\\{(\\mathbf{x}-\\mathbf{t})(\\mathbf{x}-\\mathbf{t})^{T}\\right\\} \\end{aligned} \\] <p>In this case \\(\\left(V^{T} V\\right)^{-1}\\) is the ordinary covariance matrix of \\(\\mathbf{x}\\) (the sample one if the average is taken over the sample, the true one if the average is taken over the distribution).</p> <p>More generally, we call \\(\\left(V^{T} V\\right)^{-1}\\) a pseudo-covariance matrix of \\(\\mathbf{x}\\), if \\(\\mathbf{t}\\) and \\(V\\) are determined from any set of equations</p> \\[ \\begin{aligned} \\operatorname{ave}\\{w(|y|) y\\} &amp; =0 \\\\ \\operatorname{ave}\\left\\{u(|y|) \\frac{y y^{T}}{|y|^{2}}-v(|y|) I\\right\\} &amp; =0 \\end{aligned} \\] <p>with \\(\\mathbf{y}=V(\\mathbf{x}-\\mathbf{t})\\), and where \\(u, v\\) and \\(w\\) are arbitrary functions.</p> <p>Note that (4.11) determines location \\(t\\) as a weighted mean</p> \\[ \\mathbf{t}=\\frac{\\operatorname{ave}\\{w(|\\mathbf{y}|) \\mathbf{x}\\}}{\\operatorname{ave}\\{w(|\\mathbf{y}|)\\}} \\] <p>with weights \\(w(|\\mathbf{y}|)\\) depending on the sample. Similarly, the pseudo-covariance can be written as a kind of scaled weighted covariance</p> \\[ \\left(V^{T} V\\right)^{-1}=\\frac{\\operatorname{ave}\\left\\{s(|\\mathbf{y}|)(\\mathbf{x}-\\mathbf{t})(\\mathbf{x}-\\mathbf{t})^{T}\\right\\}}{\\operatorname{ave}\\{s(|\\mathbf{y}|)\\}} \\cdot \\frac{\\operatorname{ave}\\{s(|\\mathbf{y}|)\\}}{\\operatorname{ave}\\{v(|\\mathbf{y}|)\\}} \\] <p>with weights</p> \\[ s(|\\mathbf{y}|)=\\frac{u(|\\mathbf{y}|)}{|\\mathbf{y}|^{2}} \\] <p>depending on the sample. The choice \\(v=s\\) then looks particularly attractive, since it make the scale factor in (4.14) disappear.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#85-estimates-determined-by-implicit-equations","title":"8.5 ESTIMATES DETERMINED BY IMPLICIT EQUATIONS","text":"<p>This section shows that (4.12), with arbitrary functions \\(u\\) and \\(v\\), is in some sense the most general form of an implicit equation determining \\(\\left(V^{T} V\\right)^{-1}\\).</p> <p>In order to simplify the discussion, we assume that location is known and fixed to be \\(\\mathbf{t}=0\\). Then we can write (4.12) in the form</p> \\[ \\operatorname{ave}\\{\\Psi(V \\mathbf{x})\\}=0 \\] <p>with</p> \\[ \\Psi(\\mathbf{x})=s(|\\mathbf{x}|) \\mathbf{x} \\mathbf{x}^{T}-v(|\\mathbf{x}|) I \\] <p>where \\(s\\) is as in (4.15). Is this the most general form of \\(\\Psi\\) ? Let us take a sufficiently smooth, but otherwise arbitrary function \\(\\Psi\\) from \\(\\mathbb{R}^{p}\\) into the space of symmetric \\(p \\times p\\) matrices. This gives us the proper number of equations for the \\(p(p+1) / 2\\) unique components of \\(\\left(V^{T} V\\right)^{-1}\\).</p> <p>We determine a matrix \\(V\\) such that</p> \\[ \\operatorname{ave}\\{\\Psi(V \\mathbf{x})\\}=0 \\] <p>where the average is taken with respect to a fixed (true or sample) distribution of \\(\\mathbf{x}\\).</p> <p>Let us assume that \\(\\Psi\\) and the distribution of \\(\\mathbf{x}\\) are such that (5.3) has at least one solution \\(V\\), that if \\(S\\) is an arbitrary orthogonal matrix, \\(S V\\) is also a solution, and that all solutions lead to the same pseudo-covariance matrix</p> \\[ C_{\\mathbf{x}}=\\left(V^{T} V\\right)^{-1} \\] <p>This uniqueness assumption implies at once that \\(C_{\\mathbf{x}}\\) transforms in the same way under linear transformations \\(B\\) as the classical covariance matrix</p> \\[ C_{B \\mathbf{x}}=B C_{\\mathbf{x}} B^{T} \\] <p>Now let \\(S\\) be an arbitrary orthogonal transformation and define</p> \\[ \\Psi_{S}(\\mathbf{x})=S^{T} \\Psi(S \\mathbf{x}) S \\] <p>The transformed function \\(\\Psi_{S}\\) determines a new pseudo-covariance \\(\\left(W^{T} W\\right)^{-1}\\) through the solution \\(W\\) of</p> \\[ \\operatorname{ave}\\left\\{\\Psi_{S}(W \\mathbf{x})\\right\\}=\\operatorname{ave}\\left\\{S^{T} \\Psi(S W \\mathbf{x}) S\\right\\}=0 \\] <p>Evidently, this is solved by \\(W=S^{T} V\\), where \\(V\\) is any solution of (5.3), and thus</p> \\[ W^{T} W=V^{T} S S^{T} V=V^{T} V \\] <p>It follows that \\(\\Psi\\) and \\(\\Psi_{S}\\) determine the same pseudo-covariances. We now form</p> \\[ \\bar{\\Psi}(\\mathbf{x})=\\underset{S}{\\operatorname{ave}}\\left\\{\\Psi_{S}(\\mathbf{x})\\right\\} \\] <p>by averaging over \\(S\\) (using the invariant measure on the orthogonal group). Evidently, every solution of (5.3) still solves \\(\\operatorname{ave}\\{\\bar{\\Psi}(V \\mathbf{x})\\}=0\\), but, of course, the uniqueness postulated in (5.4) may have been destroyed by the averaging process.</p> <p>Clearly, \\(\\bar{\\Psi}\\) is invariant under orthogonal transformations in the sense that</p> \\[ \\bar{\\Psi}_{S}(\\mathbf{x})=S^{T} \\bar{\\Psi}(S \\mathbf{x}) S=\\bar{\\Psi}(\\mathbf{x}) \\] <p>or equivalently,</p> \\[ \\bar{\\Psi}(S \\mathbf{x}) S=S \\bar{\\Psi}(\\mathbf{x}) \\] <p>Now let \\(\\mathbf{x} \\neq 0\\) be an arbitrary fixed vector, then (5.9) shows that the matrix \\(\\widetilde{\\Psi}(\\mathbf{x})\\) commutes with all orthogonal matrices \\(S\\) that keep \\(\\mathbf{x}\\) fixed. This implies that the restriction of \\(\\widetilde{\\Psi}(\\mathbf{x})\\) to the subspace of \\(\\mathbb{R}^{p}\\) orthogonal to \\(\\mathbf{x}\\) must be a multiple of the identity. Moreover, for every \\(S\\) that keeps \\(\\mathbf{x}\\) fixed, we have</p> \\[ S \\widetilde{\\Psi}(\\mathbf{x}) \\mathbf{x}=\\widetilde{\\Psi}(\\mathbf{x}) \\mathbf{x} \\] <p>hence \\(S\\) also keeps \\(\\widetilde{\\Psi}(\\mathbf{x}) \\mathbf{x}\\) fixed, which therefore must be a multiple of \\(\\mathbf{x}\\). It follows that \\(\\widetilde{\\Psi}(\\mathbf{x})\\) can be written in the form</p> \\[ \\widetilde{\\Psi}(\\mathbf{x})=s(\\mathbf{x}) \\mathbf{x} \\mathbf{x}^{T}-v(\\mathbf{x}) I \\] <p>with some scalar-valued functions \\(s\\) and \\(v\\). Because of (5.8) \\(s\\) and \\(v\\) depend on \\(\\mathbf{x}\\) only through \\(|\\mathbf{x}|\\), and we conclude that \\(\\widetilde{\\Psi}\\) is of the form (5.2).</p> <p>Global uniqueness, as postulated in (5.4), is a rather severe requirement. The arguments carry through in all essential respects with the much weaker local uniqueness requirement that there be a neighborhood of \\(C_{\\mathbf{x}}\\) that contains no other solutions besides \\(C_{\\mathbf{x}}\\). For the symmetrized version (5.2) a set of sufficient conditions for local uniqueness is outlined at the end of Section 8.7.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#86-existence-and-uniqueness-of-solutions","title":"8.6 EXISTENCE AND UNIQUENESS OF SOLUTIONS","text":"<p>The following existence and uniqueness results are due to Maronna (1976) and Sch\u00f6nholzer (1979). The results are not entirely satisfactory with regard to joint estimation of \\(\\mathbf{t}\\) and \\(V\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#the-scatter-estimate-v","title":"The Scatter Estimate \\(V\\)","text":"<p>Assume first that location is fixed \\(\\mathbf{t}=0\\). Existence is proved constructively by defining an iterative process converging to a solution \\(V\\) of (4.12). The iteration step from \\(V_{m}\\) to \\(V_{m+1}=h\\left(V_{m}\\right)\\) is defined as follows:</p> \\[ \\left(V_{m+1}^{T} V_{m+1}\\right)^{-1}=\\frac{\\operatorname{ave}\\left\\{s\\left(\\left|V_{m} \\mathbf{x}\\right|\\right) \\mathbf{x} \\mathbf{x}^{T}\\right\\}}{\\operatorname{ave}\\left\\{v\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}} \\] <p>If the process converges, then the limit \\(V\\) satisfies (4.14) and hence solves (4.12). If (6.1) is used for actual computation, then it is convenient to assume that the matrices \\(V_{m}\\) are lower triangular with a positive diagonal; for the proofs below, it is more convenient to take them as positive definite</p> <p>symmetric. Clearly, the choice does not matter-both sides of (6.1) are unchanged if \\(V_{m}\\) and \\(V_{m+1}\\) are multiplied by arbitrary orthogonal matrices from the left.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#assumptions_4","title":"ASSUMPTIONS","text":"<p>(E-1) The function \\(s(r)\\) is monotone decreasing and \\(s(r)&gt;0\\) for \\(r&gt;0\\). (E-2) The function \\(v(r)\\) is monotone increasing, and \\(v(r)&gt;0\\) for \\(r \\geqslant 0\\). (E-3) \\(u(r)=r^{2} s(r)\\), and \\(v(r)\\) are bounded and continuous. (E-4) \\(u(0) / v(0)&lt;p\\). For any hyperplane \\(H\\) in the sample space (i.e., \\(\\operatorname{dim}(H)=p-1\\) ), let</p> \\[ P(H)=\\operatorname{ave}\\left\\{1_{[\\mathbf{x} \\in H]}\\right\\} \\] <p>be the probability of \\(H\\), or the fraction of observations lying in \\(H\\), respectively (depending on whether we work with the true or with the sample distribution). (E-5) (i) For all hyperplanes \\(H, P(H)&lt;1-p v(\\infty) / u(\\infty)\\). (ii) For all hyperplanes \\(H, P(H) \\leqslant 1 / p\\).</p> <p>LEMMA 6.1 If (E-1), (E-2), (E-3), and (E-5)(i) are satisfied, and if there is an \\(r_{0}&gt;0\\) such that</p> \\[ \\frac{\\operatorname{ave}\\left\\{u\\left(r_{0}|\\mathbf{x}|\\right)\\right\\}}{\\operatorname{ave}\\left\\{v\\left(r_{0}|\\mathbf{x}|\\right)\\right\\}}&lt;1 \\] <p>then \\(h\\) has a fixed point \\(V\\).</p> <p>Proof Let \\(\\mathbf{z}\\) be an arbitrary vector. Then with \\(V_{0}=r_{0} I\\) we obtain, from (6.1) and (6.2),</p> \\[ \\begin{aligned} \\mathbf{z}^{T}\\left(V_{1}^{T} V_{1}\\right)^{-1} \\mathbf{z} &amp; =\\frac{\\operatorname{ave}\\left\\{s\\left(r_{0}|\\mathbf{x}|\\right)\\left(\\mathbf{z}^{T} \\mathbf{x}\\right)^{2}\\right\\}}{\\operatorname{ave}\\left\\{v\\left(r_{0}|\\mathbf{x}|\\right)\\right\\}} \\\\ &amp; \\leqslant \\frac{|\\mathbf{z}|^{2}}{r_{0}^{2}} \\frac{\\operatorname{ave}\\left\\{u\\left(r_{0}|\\mathbf{x}|\\right)\\right\\}}{\\operatorname{ave}\\left\\{v\\left(r_{0}|\\mathbf{x}|\\right)\\right\\}} \\leqslant \\frac{|\\mathbf{z}|^{2}}{r_{0}^{2}} \\end{aligned} \\] <p>Hence</p> \\[ \\left(V_{1}^{T} V_{1}\\right)^{-1}&lt;\\frac{1}{r_{0}^{2}} I \\] <p>(where \\(A&lt;B\\) means that \\(B-A\\) is positive semidefinite). Thus</p> \\[ r_{0} I&lt;V_{1}=h\\left(r_{0} I\\right) \\] <p>It follows from (E-1) and (E-2) that \\(V_{m+1}=h\\left(V_{m}\\right)\\) defines an increasing sequence</p> \\[ r_{0} I=V_{0}&lt;V_{1}&lt;V_{2}&lt;\\cdots \\] <p>So it suffices to show that the sequence \\(V_{m}\\) is bounded from above in order to prove convergence \\(V_{m} \\rightarrow V\\). Continuity (E-3) then implies that \\(V\\) satisfies (4.14).</p> <p>Let</p> \\[ H=\\left\\{\\mathbf{z}|\\lim | V_{m} \\mathbf{z} \\mid&lt;\\infty\\right\\} \\] <p>\\(H\\) is a vector space. Assume first that \\(H\\) is a proper subspace of \\(\\mathbb{R}^{p}\\). Since \\(V_{m}&lt;V_{m+1}\\), we have</p> \\[ I&gt;V_{m} V_{m+1}^{-2} V_{m}=\\frac{\\operatorname{ave}\\left\\{s\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\left(V_{m} \\mathbf{x}\\right)\\left(V_{m} \\mathbf{x}\\right)^{T}\\right\\}}{\\operatorname{ave}\\left\\{v\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}} \\] <p>Taking the trace on both sides gives</p> \\[ p \\geqslant \\frac{\\operatorname{ave}\\left\\{u\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}}{\\operatorname{ave}\\left\\{v\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}} \\geqslant \\frac{\\operatorname{ave}\\left\\{u\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}}{v(\\infty)} \\] <p>Since \\(\\left|V_{m} \\mathbf{x}\\right| \\uparrow \\infty\\) for all \\(\\mathbf{x} \\notin H\\), we obtain from the monotone convergence theorem</p> \\[ p \\geqslant[1-P(H)] \\frac{u(\\infty)}{v(\\infty)} \\] <p>which contradicts (E-5)(i). Hence \\(H=\\mathbb{R}^{p}\\), but this is only possible if \\(V_{m}\\) stays bounded (note that the trace must converge).</p> <p>Remark Assumption (6.2) serves to guarantee the existence of a starting matrix \\(V_{0}\\), such that \\(h\\left(V_{0}\\right)&gt;V_{0}\\). Assume, for instance, that \\(s(0)&gt;0\\), then (6.2) is satisfied for all sufficiently small \\(r_{0}\\). In the limit \\(r_{0} \\rightarrow 0\\), we obtain that \\(\\left(V_{1}^{T} V_{1}\\right)^{-1}\\) then is a multiple of the ordinary covariance matrix.</p> <p>PROPOSITION 6.2 Assume (E-1) to (E-5). Then \\(h\\) has a fixed point \\(V\\). Proof If \\(s\\) is bounded, the existence of a fixed point follows from Lemma 6.1. If \\(s\\) is unbounded, we choose an \\(r_{1}&gt;0\\) and replace \\(s\\) by \\(\\tilde{s}\\) :</p> \\[ \\begin{aligned} \\tilde{s}(r) &amp; =s\\left(r_{1}\\right), &amp; &amp; \\text { for } r&lt;r_{1} \\\\ &amp; =s(r), &amp; &amp; \\text { for } r \\geqslant r_{1} \\end{aligned} \\] <p>Let \\(\\tilde{h}\\) be the function defined by (6.1) with \\(\\tilde{s}\\) in place of \\(s\\). Lemma 6.1 then implies that \\(\\tilde{h}\\) has a fixed point \\(\\tilde{V}\\). Since \\(s \\geqslant \\tilde{s}\\), it follows that, for all \\(V\\), \\(h(V)&lt;\\tilde{h}(V)\\). Hence \\(h(\\tilde{V})&lt;\\tilde{h}(\\tilde{V})=\\tilde{V}\\), and it follows from (E-1) and (E-2) that \\(V_{m+1}=h\\left(V_{m}\\right)\\) defines a decreasing sequence</p> \\[ \\tilde{V}=V_{0}&gt;V_{1}&gt;V_{2}&gt;\\cdots \\] <p>So it suffices to show that \\(V=\\lim V_{m}\\) is nonsingular in order to prove that it is a fixed point of \\(h\\).</p> <p>As in the proof of Lemma 6.1, we find</p> \\[ I&lt;\\frac{\\operatorname{ave}\\left\\{s\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\left(V_{m} \\mathbf{x}\\right)\\left(V_{m} \\mathbf{x}\\right)^{T}\\right\\}}{\\operatorname{ave}\\left\\{v\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}} \\] <p>and, taking the trace,</p> \\[ p \\leqslant \\frac{\\operatorname{ave}\\left\\{u\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}}{\\operatorname{ave}\\left\\{v\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}} \\leqslant \\frac{\\operatorname{ave}\\left\\{u\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}}{v(0)} \\] <p>We conclude that not all eigenvalues of \\(V_{m}\\) can converge to 0 , because otherwise</p> \\[ p \\leqslant \\frac{\\lim \\operatorname{ave}\\left\\{u\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)\\right\\}}{v(0)}=\\frac{u(0)}{v(0)} \\] <p>by the monotone convergence theorem, which would contradict (E-4). Now assume that \\(\\mathbf{q}_{m}\\) and \\(\\mathbf{z}_{m}\\) are unit-length eigenvectors of \\(V_{m}\\) belonging to the largest and smallest eigenvalues \\(\\lambda_{m}\\) and \\(\\mu_{m}\\) of \\(V_{m}\\), respectively. If</p> <p>we multiply (6.5) from the left and right with \\(\\mathbf{z}_{m}^{T}\\) and \\(\\mathbf{z}_{m}\\), respectively, we obtain</p> \\[ 1 \\leqslant \\frac{\\operatorname{ave}\\left\\{s\\left(\\left|V_{m} \\mathbf{x}\\right|\\right) \\mu_{m}^{2}\\left(\\mathbf{z}_{m}^{T} \\mathbf{x}\\right)^{2}\\right\\}}{\\operatorname{ave} v\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)} \\] <p>Since the largest eigenvalues converge monotonely \\(\\lambda_{m} \\downarrow \\lambda&gt;0\\), we obtain</p> \\[ \\begin{aligned} G_{m, r} &amp; =\\left\\{\\mathbf{x} \\| V_{m} \\mathbf{x} \\mid \\leqslant r\\right\\} \\subset\\left\\{\\mathbf{x} \\mid \\lambda_{m}\\left|\\mathbf{h}_{m}^{T} \\mathbf{x}\\right| \\leqslant r\\right\\} \\\\ &amp; \\subset\\left\\{\\mathbf{x}\\left\\|\\mathbf{h}_{m}^{T} \\mathbf{x}\\right| \\leqslant \\frac{r}{\\lambda}\\right\\}=H_{m, r} \\end{aligned} \\] <p>with \\(G_{m, r}\\) and \\(H_{m, r}\\) defined by (6.8). Assumption (E-5)(ii) implies that, for each \\(\\varepsilon&gt;0\\), there is a \\(r_{1}&gt;0\\) such that</p> \\[ P\\left\\{H_{m, r}\\right\\} \\leqslant \\frac{1}{p}+\\varepsilon, \\quad \\text { for } r \\leqslant r_{1} \\] <p>Furthermore (E-4) implies that we can choose \\(b&gt;0\\) and \\(\\varepsilon&gt;0\\) such that</p> \\[ \\frac{u(0)+b}{v(0)}\\left(\\frac{1}{p}+\\varepsilon\\right)&lt;1 \\] <p>If \\(r_{0}&lt;r_{1}\\) is chosen such that \\(u(r) \\leqslant u(0)+b\\) for \\(r \\leqslant r_{0}\\), then (6.7) to (6.9) imply</p> \\[ \\begin{aligned} 1 &amp; \\leqslant \\frac{1}{v(0)}\\left[\\operatorname{ave}\\left\\{1_{G_{m, r}}(\\mathbf{x}) u\\left(\\left|V_{m} \\mathbf{x}\\right|\\right)+1_{G_{m, r}^{T}}(\\mathbf{x}) s\\left(\\left|V_{m} \\mathbf{x}\\right|\\right) \\mu_{m}^{2}\\left(\\mathbf{z}_{m}^{T} \\mathbf{x}\\right)^{2}\\right\\}\\right] \\\\ &amp; \\leqslant \\frac{1}{v(0)}[u(0)+b]\\left(\\frac{1}{p}+\\varepsilon\\right)+\\operatorname{ave}\\left\\{\\frac{1}{v(0)} \\min \\left[s\\left(r_{0}\\right) \\mu_{m}^{2}|\\mathbf{x}|^{2}, u(\\infty)\\right]\\right\\} \\end{aligned} \\] <p>If \\(\\lim \\mu_{m}=0\\), then the last summand tends to 0 by the dominated convergence theorem; this leads to a contradiction in view of (6.10). Hence \\(\\lim \\mu_{m}&gt;0\\) and the proposition is proved.</p> <p>Uniqueness of the fixed point can be proved under the following assumptions.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#assumptions_5","title":"ASSUMPTIONS","text":"<p>(U-1) \\(s(r)\\) is decreasing. (U-2) \\(u(r)=r^{2} s(r)\\) is continuous and increasing, and \\(u(r)&gt;0\\) for \\(r&gt;0\\).</p> <p>(U-3) \\(v(r)\\) is continuous and decreasing, and \\(v(r) \\geqslant 0, v(r)&gt;0\\) for \\(0 \\leqslant r&lt;r_{0}\\). (U-4) For all hyperplanes \\(H \\subset \\mathbb{R}^{p}, P(H)&lt;\\frac{1}{2}\\). Remark In view of (E-3), we can prove simultaneous existence and uniqueness only if \\(v=\\) constant (as in the ML case).</p> <p>PROPOSITION 6.3 Assume ( \\(\\mathrm{U}-1\\) ) to ( \\(\\mathrm{U}-4\\) ). If \\(V\\) and \\(V_{1}\\) are two fixed points of \\(h\\), then there is a real number \\(\\lambda\\) such that \\(V_{1}=\\lambda V\\), and</p> \\[ u(|V \\mathbf{x}|)=u(\\lambda|V \\mathbf{x}|), \\quad v(|V \\mathbf{x}|)=v(\\lambda|V \\mathbf{x}|) \\] <p>for almost all \\(\\mathbf{x}\\). In particular, \\(\\lambda=1\\) if either \\(u\\) or \\(v\\) is strictly monotone. We first prove a special case: LEMMA 6.4 Proposition 6.3 holds if either \\(V&gt;V_{1}\\) or \\(V&lt;V_{1}\\). Proof of the Lemma We may assume without loss of generality that \\(V_{1}=I\\). Assume \\(V&gt;I\\) (the case \\(V&lt;I\\) is proved in the same way). Then</p> \\[ \\underbrace{\\operatorname{ave}\\left\\{u(|V \\mathbf{x}|) \\frac{(V \\mathbf{x})(V \\mathbf{x})^{T}}{|V \\mathbf{x}|^{2}}\\right\\}}_{\\operatorname{ave}\\{v(|V \\mathbf{x}|)\\}}=\\frac{\\operatorname{ave}\\left\\{u(|\\mathbf{x}|) \\frac{\\mathbf{x} \\mathbf{x}^{T}}{|\\mathbf{x}|^{2}}\\right\\}}{\\operatorname{ave}\\{v(|\\mathbf{x}|)\\}}=I \\] <p>If we take the trace, we obtain</p> \\[ \\frac{\\operatorname{ave}\\{u(|V \\mathbf{x}|)\\}}{\\operatorname{ave}\\{v(|V \\mathbf{x}|)\\}}=\\frac{\\operatorname{ave}\\{u(|\\mathbf{x}|)\\}}{\\operatorname{ave}\\{v(|\\mathbf{x}|)\\}}=p \\] <p>In view of (U-2) and (U-3), we must have</p> \\[ \\begin{aligned} &amp; \\operatorname{ave}\\{u(|V \\mathbf{x}|)\\}=\\operatorname{ave}\\{u(|\\mathbf{x}|)\\} \\\\ &amp; \\operatorname{ave}\\{v(|V \\mathbf{x}|)\\}=\\operatorname{ave}\\{v(|\\mathbf{x}|)\\} \\end{aligned} \\] <p>Because of \\(V&gt;I\\) this implies</p> \\[ \\begin{aligned} &amp; u(|V \\mathbf{x}|)=u(|\\mathbf{x}|) \\\\ &amp; v(|V \\mathbf{x}|)=v(|\\mathbf{x}|) \\end{aligned} \\] <p>for almost all \\(\\mathbf{x}\\).</p> <p>If either \\(u\\) or \\(v\\) is strictly monotone, this already forces \\(V=I\\). In view of (6.14) it follows from (6.11) that</p> \\[ \\operatorname{ave}\\left\\{u(|\\mathbf{x}|)\\left[\\frac{(V \\mathbf{x})(V \\mathbf{x})^{T}}{|V \\mathbf{x}|^{2}}-\\frac{\\mathbf{x} \\mathbf{x}^{T}}{|\\mathbf{x}|^{2}}\\right]\\right\\}=0 \\] <p>Now let \\(\\mathbf{z}\\) be an eigenvector belonging to the largest eigenvalue \\(\\lambda\\) of \\(V\\). Then (6.15) implies</p> \\[ \\operatorname{ave}\\left\\{u(|\\mathbf{x}|)\\left(\\frac{\\lambda^{2}}{|V \\mathbf{x}|^{2}}-\\frac{1}{|\\mathbf{x}|^{2}}\\right)\\left(\\mathbf{z}^{T} \\mathbf{x}\\right)^{2}\\right\\}=0 \\] <p>The expression inside the curly parentheses of (6.16) is \\(&gt;0\\) unless either: (1) \\(\\mathbf{x}\\) is an eigenvector of \\(V\\), to the eigenvalue \\(\\lambda\\); or (2) \\(\\mathbf{x}\\) is orthogonal to \\(\\mathbf{z}\\).</p> <p>If \\(V=\\lambda I\\), the lemma is proved. If \\(V \\neq \\lambda I\\), then (U-4) implies that the union of the \\(\\mathbf{x}\\)-sets (1) and (2) has a total mass less than 1 , which leads to a contradiction.</p> <p>Proof of the Proposition Assume the fixed points are \\(V\\) and \\(I\\), and neither \\(V&lt;I\\) nor \\(V&gt;I\\). Choose \\(0&lt;r&lt;1\\) so that \\(r I&lt;V\\). Because of (U-2) and (U-3) we have</p> \\[ \\begin{aligned} h(r I)^{-2} &amp; =\\frac{\\operatorname{ave}\\left\\{u(r|\\mathbf{x}|) \\frac{\\mathbf{x} \\mathbf{x}^{T}}{|\\mathbf{x}|^{2}}\\right\\}}{\\operatorname{ave}\\{v(r|\\mathbf{x}|)\\}} \\cdot \\frac{1}{r^{2}} \\\\ &amp; &lt;\\frac{\\operatorname{ave}\\left\\{u(|\\mathbf{x}|) \\frac{\\mathbf{x} \\mathbf{x}^{T}}{|\\mathbf{x}|^{2}}\\right\\}}{\\operatorname{ave}\\{v(|\\mathbf{x}|)\\}} \\cdot \\frac{1}{r^{2}}=\\frac{1}{r^{2}} I \\end{aligned} \\] <p>or</p> \\[ r I&lt;h(r I) \\] <p>It follows from \\(r I&lt;I\\) and \\(r I&lt;V\\) that \\(V_{1}=\\lim h^{m}(r I)\\) is a fixed point with \\(V_{1}&lt;I\\) and \\(V_{1}&lt;V\\). Then both pairs \\(V_{1}, I\\) and \\(V_{1}, V\\) satisfy the assumptions of Lemma 6.4, so \\(V_{1}, I\\), and \\(V\\) are scalar multiples of each others. This contradicts the assumption that neither \\(V&lt;I\\) nor \\(V&gt;I\\), and the proposition is proved.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#the-location-estimate-mathbft","title":"The Location Estimate \\(\\mathbf{t}\\)","text":"<p>If \\(V\\) is kept fixed, say \\(V=I\\), existence and uniqueness of the location estimate \\(\\mathbf{t}\\) are easy to establish, provided \\(\\psi(r)=w(r) r\\) is monotone increasing for positive \\(r\\). Then there is a convex function</p> \\[ \\rho(\\mathbf{x})=\\rho(|\\mathbf{x}|)=\\int_{0}^{|\\mathbf{x}|} \\psi(r) d r \\] <p>such that \\(\\mathbf{t}\\) can equivalently be defined as minimizing</p> \\[ Q(\\mathbf{t})=\\operatorname{ave}\\{\\rho(|\\mathbf{x}-\\mathbf{t}|)\\} \\] <p>We only treat the sample case, so we do not have to worry about the possible nonexistence of the distribution average. Thus the set of solutions \\(\\mathbf{t}\\) is nonempty and convex, and if there is at least one observation \\(\\mathbf{x}\\) such that \\(\\rho^{\\prime \\prime}(|\\mathbf{x}-\\mathbf{t}|)&gt;0\\), the solution is in fact unique.</p> <p>Proof We shall show that \\(Q\\) is strictly convex. Assume that \\(\\mathbf{z} \\in \\mathbb{R}^{p}\\) depends linearly on a parameter \\(s\\), and take derivatives with respect to \\(s\\) (denoted by a superscript dot). Then</p> \\[ \\rho(|\\mathbf{z}|)^{`}=\\frac{\\rho^{\\prime}(|\\mathbf{z}|)}{|\\mathbf{z}|} \\mathbf{z}^{T} \\dot{\\mathbf{z}} \\] <p>and</p> \\[ \\rho(|\\mathbf{z}|)^{`}=\\frac{\\rho^{\\prime \\prime}(|\\mathbf{z}|)}{|\\mathbf{z}|^{2}}\\left(\\mathbf{z}^{T} \\dot{\\mathbf{z}}\\right)^{2}+\\frac{\\rho^{\\prime}(|\\mathbf{z}|)}{|\\mathbf{z}|^{3}}\\left[\\left(\\mathbf{z}^{T} \\mathbf{z}\\right)\\left(\\dot{\\mathbf{z}}^{T} \\dot{\\mathbf{z}}\\right)-\\left(\\mathbf{z}^{T} \\dot{\\mathbf{z}}\\right)^{2}\\right] \\geqslant 0 \\] <p>since \\(\\rho^{\\prime} \\geqslant 0,\\left(\\mathbf{z}^{T} \\dot{\\mathbf{z}}\\right)^{2} \\leqslant\\left(\\mathbf{z}^{T} \\mathbf{z}\\right)\\left(\\dot{\\mathbf{z}}^{T} \\dot{\\mathbf{z}}\\right)\\), and \\(\\rho^{\\prime \\prime}(r)=\\psi^{\\prime}(r) \\geqslant 0\\). Hence \\(\\rho(|\\mathbf{z}|)\\) is convex as a function of \\(\\mathbf{z}\\). Moreover, if \\(\\rho^{\\prime \\prime}(|\\mathbf{z}|)&gt;0\\) and \\(\\rho^{\\prime}(|\\mathbf{z}|)&gt;0\\), then \\(\\rho\\) is strictly convex at the point \\(\\mathbf{z}\\) : if the variation \\(\\dot{\\mathbf{z}}\\) is orthogonal to \\(\\mathbf{z}\\), then</p> \\[ \\rho(|\\mathbf{z}|)^{`}=\\frac{\\rho^{\\prime}(|\\mathbf{z}|)}{|\\mathbf{z}|}\\left(\\dot{\\mathbf{z}}^{T} \\dot{\\mathbf{z}}\\right)&gt;0 \\] <p>and otherwise</p> \\[ \\rho(|\\mathbf{z}|)^{`} \\geqslant \\frac{\\rho^{\\prime \\prime}(|\\mathbf{z}|)}{|\\mathbf{z}|^{2}}\\left(\\mathbf{z}^{T} \\dot{\\mathbf{z}}\\right)^{2}&gt;0 \\] <p>In fact \\(\\rho^{\\prime \\prime}(r)&gt;0, \\rho^{\\prime}(r)=0\\) can only happen at \\(r=0\\), and \\(\\mathbf{z}=0\\) is a point of strict convexity, as we verify easily by a separate argument. Hence \\(Q\\) is strictly convex, which implies uniqueness.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#joint-estimation-of-mathbft-and-v","title":"Joint Estimation of \\(\\mathbf{t}\\) and \\(V\\)","text":"<p>Joint existence of solutions \\(\\mathbf{t}\\) and \\(V\\) is then also easy to establish, if we do not mind somewhat restrictive regularity conditions. Assume that, for each fixed \\(\\mathbf{t}\\), there is a unique solution \\(V_{t}\\) of (4.12), which depends continuously on \\(\\mathbf{t}\\), and that for each fixed \\(V\\) there is a unique solution \\(\\mathbf{t}(V)\\) of (4.11), which depends continuously on \\(V\\). It follows from (4.13) that \\(\\mathbf{t}(V)\\) is always contained in the convex hull \\(H\\) of the observations. Thus the continuous function \\(\\mathbf{t} \\rightarrow \\mathbf{t}\\left(V_{t}\\right)\\) maps \\(H\\) into itself and hence has a fixed point by Brouwer's theorem. The corresponding pair ( \\(\\mathbf{t}, V_{t}\\) ) obviously solves (4.11) and (4.12). Uniqueness of the fixed point so far has been proved only under the assumption that the distribution of the \\(\\mathbf{x}\\) has a center of symmetry; in the sample distribution case this is of course very unrealistic [cf. Maronna (1976)].</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#87-influence-functions-and-qualitative-robustness","title":"8.7 INFLUENCE FUNCTIONS AND QUALITATIVE ROBUSTNESS","text":"<p>Our estimates \\(\\mathbf{t}\\) and \\(V\\), defined through (4.11) and (4.12) with the help of averages over the sample distribution, clearly can be regarded as functionals \\(\\mathbf{t}(F)\\) and \\(V(F)\\) of some underlying distribution \\(F\\). The estimates are vector and matrix valued; the influence functions, measuring changes of \\(\\mathbf{t}\\) and \\(V\\) under infinitesimal changes of \\(F\\), clearly are vector and matrix valued too.</p> <p>Without loss of generality we can choose the coordinate system such that \\(\\mathbf{t}(F)=0\\) and \\(V(F)=I\\). We assume that \\(F\\) is (at least) centrosymmetric. In order to find the influence functions, we have to insert \\(F_{s}=(1-s) F+s \\delta_{\\mathbf{x}}\\) into the defining equations and take the derivative with respect to \\(s\\) at \\(s=0\\); we denote it by a superscript dot.</p> <p>We first take (4.11). The procedure just outlined gives</p> \\[ \\begin{gathered} -\\operatorname{ave}_{F}\\left\\{\\frac{w^{\\prime}(|\\mathbf{y}|)}{|\\mathbf{y}|}\\left(\\mathbf{y}^{T} \\dot{\\mathbf{t}}\\right) \\mathbf{y}+w(|\\mathbf{y}|) \\dot{\\mathbf{t}}\\right\\} \\\\ +\\operatorname{ave}_{F}\\left\\{\\frac{w^{\\prime}(|\\mathbf{y}|)}{|\\mathbf{y}|}\\left(\\mathbf{y}^{T} \\dot{V} \\mathbf{y}\\right) \\mathbf{y}+w(|\\mathbf{y}|) \\dot{V} \\mathbf{y}\\right\\}+w(|\\mathbf{x}|) \\mathbf{x}=0 \\end{gathered} \\] <p>The second term (involving \\(\\dot{V}\\) ) averages to 0 if \\(F\\) is centrosymmetric. There is a considerable further simplification if \\(F\\) is spherically symmetric [or, at least, if the conditional covariance matrix of \\(\\mathbf{y} /|\\mathbf{y}|\\), given \\(|\\mathbf{y}|\\), equals \\((1 / p) I\\) for all \\(|y|]\\), since then \\(E\\left\\{\\left(\\mathbf{y}^{T} \\mathbf{i}\\right) \\mathbf{y}| | \\mathbf{y} \\mid\\right\\}=(1 / p)|\\mathbf{y}|^{2} \\mathbf{i}\\). So (7.1) becomes</p> \\[ -\\operatorname{ave}_{F}\\left\\{\\frac{1}{p} w^{\\prime}(|\\mathbf{y}|)|\\mathbf{y}|+w(|\\mathbf{y}|)\\right\\} \\mathbf{i}+w(|\\mathbf{x}|) \\mathbf{x}=0 \\] <p>Hence the influence function for location is</p> \\[ I C(\\mathbf{x} ; F, \\mathbf{t})=\\frac{w(|\\mathbf{x}|) \\mathbf{x}}{\\operatorname{ave}_{F}\\left\\{w(|\\mathbf{y}|)+\\frac{1}{p} w^{\\prime}(|\\mathbf{y}|)|\\mathbf{y}| \\right\\}} \\] <p>Similarly, differentiation of (4.12) gives,</p> \\[ \\begin{aligned} &amp; \\operatorname{ave}_{F}\\left\\{\\frac{u^{\\prime}(|\\mathbf{y}|)|\\mathbf{y}|-2 u(|\\mathbf{y}|)}{|\\mathbf{y}|^{4}}\\left(\\mathbf{y}^{T} \\dot{V} \\mathbf{y}\\right)\\left(\\mathbf{y} \\mathbf{y}^{T}\\right)+\\frac{u(|\\mathbf{y}|)}{|\\mathbf{y}|^{2}}\\left(\\dot{V} \\mathbf{y} \\mathbf{y}^{T}+\\mathbf{y} \\mathbf{y}^{T} \\dot{V}^{T}\\right)\\right. \\\\ &amp; \\left.-\\frac{v^{\\prime}(|\\mathbf{y}|)}{|\\mathbf{y}|}\\left(\\mathbf{y}^{T} \\dot{V} \\mathbf{y}\\right) I\\right\\} \\\\ &amp; -\\operatorname{ave}_{F}\\left\\{\\frac{u^{\\prime}(|\\mathbf{y}|)|\\mathbf{y}|-2 u(|\\mathbf{y}|)}{|\\mathbf{y}|^{4}}\\left(\\mathbf{y}^{T} \\mathbf{i}\\right)\\left(\\mathbf{y} \\mathbf{y}^{T}\\right)+\\frac{u(|\\mathbf{y}|)}{|\\mathbf{y}|^{2}}\\left(\\mathbf{i} \\mathbf{y}^{T}+\\mathbf{y} \\mathbf{i}^{T}\\right)\\right. \\\\ &amp; \\left.-\\frac{v^{\\prime}(|\\mathbf{y}|)}{|\\mathbf{y}|}\\left(\\mathbf{y}^{T} \\mathbf{i}\\right) I\\right\\}+\\left\\{u(|\\mathbf{x}|) \\frac{\\mathbf{x} \\mathbf{x}^{T}}{|\\mathbf{x}|^{2}}-v(|\\mathbf{x}|) I\\right\\}=0 \\end{aligned} \\] <p>The second term (involving \\(\\dot{\\mathbf{i}}\\) ) averages to 0 if \\(F\\) is centrosymmetric. It is convenient to split (7.3) into two equations. We first take the trace of (7.3) and divide it by \\(p\\). This gives</p> \\[ \\operatorname{ave}_{F}\\left\\{\\left[\\frac{1}{p} u^{\\prime}(|\\mathbf{y}|)-v^{\\prime}(|\\mathbf{y}|)\\right] \\frac{\\mathbf{y}^{T} \\dot{V} \\mathbf{y}}{|\\mathbf{y}|}\\right\\}+\\left\\{\\frac{1}{p} u(|\\mathbf{x}|)-v(|\\mathbf{x}|)\\right\\}=0 \\] <p>If we now subtract (7.4) from the diagonal of (7.3), we obtain</p> \\[ \\begin{aligned} &amp; \\operatorname{ave}_{F}\\left\\{\\frac{u^{\\prime}(|\\mathbf{y}|)}{|\\mathbf{y}|}\\left[\\frac{\\mathbf{y y}^{T}}{|\\mathbf{y}|^{2}}-\\frac{1}{p} I\\right]\\left(\\mathbf{y}^{T} \\dot{V} \\mathbf{y}\\right)\\right. \\\\ &amp; \\left.\\quad+\\frac{u(|\\mathbf{y}|)}{|\\mathbf{y}|^{4}}\\left[|\\mathbf{y}|^{2}\\left(\\dot{V} \\mathbf{y} \\mathbf{y}^{T}+\\mathbf{y} \\mathbf{y}^{T} \\dot{V}^{T}\\right)-2\\left(\\mathbf{y}^{T} \\dot{V} \\mathbf{y}\\right) \\mathbf{y} \\mathbf{y}^{T}\\right]\\right\\} \\\\ &amp; +u(|\\mathbf{x}|)\\left(\\frac{\\mathbf{x} \\mathbf{x}^{T}}{|\\mathbf{x}|^{2}}-\\frac{1}{p} I\\right)=0 \\end{aligned} \\] <p>If \\(F\\) is spherically symmetric, the averaging process can be carried one step further. From (7.4) we then obtain [with \\(\\dot{W}=\\frac{1}{2}\\left(\\dot{V}+\\dot{V}^{T}\\right)\\) ]</p> \\[ \\operatorname{ave}_{F}\\left\\{\\left[\\frac{1}{p} u^{\\prime}(|\\mathbf{y}|)-v^{\\prime}(|\\mathbf{y}|)\\right]|\\mathbf{y}|\\right\\} \\frac{1}{p} \\operatorname{tr}(\\dot{W})+\\left[\\frac{1}{p} u(|\\mathbf{x}|)-v(|\\mathbf{x}|)\\right]=0 \\] <p>and from \\((7.5)\\)</p> \\[ \\begin{gathered} \\frac{2}{p+2} \\operatorname{ave}_{F}\\left\\{u(|\\mathbf{y}|)+\\frac{1}{p} u^{\\prime}(|\\mathbf{y}|)|\\mathbf{y}|\\right\\}\\left[\\dot{W}-\\frac{1}{p} \\operatorname{tr}(\\dot{W})\\right] \\\\ +u(|\\mathbf{x}|)\\left(\\frac{\\mathbf{x} \\mathbf{x}^{T}}{|\\mathbf{x}|^{2}}-\\frac{1}{p} I\\right)=0 \\end{gathered} \\] <p>[Cf. Section 8.10, after (10.15), for this averaging process.] Clearly, only the symmetric part \\(\\dot{W}\\) of the influence function \\(\\dot{V}=I C(\\mathbf{x} ; F, V)\\) matters and is determinable. We obtain it in explicit form from (7.6) and (7.7) as</p> \\[ \\begin{gathered} \\frac{1}{p} \\operatorname{tr}(\\dot{W})=-\\frac{\\frac{1}{p} u(|\\mathbf{x}|)-v(|\\mathbf{x}|)}{\\operatorname{ave}_{F}\\left\\{\\left[\\frac{1}{p} u^{\\prime}(|\\mathbf{y}|)-v^{\\prime}(|\\mathbf{y}|)\\right]|\\mathbf{y}|\\right\\}} \\\\ \\dot{W}-\\frac{1}{p} \\operatorname{tr}(\\dot{W}) I=-\\frac{p+2}{2} \\frac{u(|\\mathbf{x}|)\\left(\\frac{\\mathbf{x} \\mathbf{x}^{T}}{|\\mathbf{x}|^{2}}-\\frac{1}{p}\\right)}{\\operatorname{ave}_{F}\\left\\{u(|\\mathbf{y}|)+\\frac{1}{p} u^{\\prime}(|\\mathbf{y}|)|\\mathbf{y}|\\right\\}} \\end{gathered} \\] <p>The influence function of the pseudo-covariance is, clearly,</p> \\[ I C\\left(\\mathbf{x} ; F,\\left(V^{T} V\\right)^{-1}\\right)=-2 \\dot{W} \\] <p>(assuming throughout that the coordinate system is matched so that \\(V=I\\) ). It can be seen from (7.2) and (7.8) that the influence functions are bounded if and only if the functions \\(w(r) r, u(r)\\), and \\(v(r)\\) are bounded [and the denominators of (7.2) and (7.8) are not equal to 0 ].</p> <p>Qualitative robustness, that is, essentially the continuity of the functionals \\(\\mathbf{t}(F)\\) and \\(V(F)\\), is difficult to discuss for the simple reason that we do not yet know for which \\(F\\) these functionals are uniquely defined. However, they are so for elliptical distributions of the type (4.1), and by the implicit function theorem we can then conclude that the solutions are still well defined in some neighborhood. This involves a careful discussion of the influence functions, not only at the model distribution (which is spherically symmetric by assumption), but also in some neighborhood of it. That is, we have to argue directly with (7.1) and (7.3), instead of the simpler (7.2) and (7.8).</p> <p>Thus we are in good shape if the denominators in (7.2) and (7.8) are strictly positive and if \\(w, w r, w^{\\prime} r, w^{\\prime} r^{2}, u, u / r, u^{\\prime}, u^{\\prime} r, v, v^{\\prime}\\), and \\(v^{\\prime} r\\) are bounded and continuous, because then the influence function is stable at the model distribution, and we can use (2.5.8) to conclude that a small change in \\(F\\) induces only a small change in the values of the functionals.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#88-consistency-and-asymptotic-normality","title":"8.8 CONSISTENCY AND ASYMPTOTIC NORMALITY","text":"<p>The estimates \\(\\mathbf{t}\\) and \\(V\\) are consistent and asymptotically normal under relatively mild assumptions, and proofs can be found along the lines of Sections 6.2 and 6.3. While the consistency proof is complicated [the main problem being caused by the fact that we have a simultaneous location-scale problem, where assumptions (A-5) or (B-4) fail], asymptotic normality can be proved straightforwardly by verifying assumptions ( \\(\\mathrm{N}-1\\) ) to ( \\(\\mathrm{N}-4\\) ). Of course, this imposes some regularity conditions on \\(w, u\\), and \\(v\\) and the underlying distribution. Note in particular that there will be trouble if \\(u(r) / r\\) is unbounded and there is a pointmass at the origin. For details see Maronna (1976) and Sch\u00f6nholzer (1979).</p> <p>The asymptotic variances and covariances of the estimates coincide with those of their influence functions, and thus can easily be derived from (7.2) and (7.8). For symmetry reasons location and covariance estimates are asymptotically uncorrelated, and hence asymptotically independent.</p> <p>The location components \\(\\hat{t}_{j}\\) are asymptotically independent, with asymptotic variance</p> \\[ n \\operatorname{var}\\left(\\hat{t}_{j}\\right)=\\frac{p^{-1} E[w(|\\mathbf{x}|)|\\mathbf{x}|]^{2}}{\\left\\{E\\left[w(|\\mathbf{x}|)+p^{-1} w^{\\prime}(|\\mathbf{x}|)|\\mathbf{x}|]\\right\\}^{2}} \\] <p>The asymptotic variances and covariances of the components of \\(\\hat{V}\\) can be described as follows (we assume that \\(V\\) is lower triangular):</p> \\[ \\begin{gathered} n \\operatorname{var}\\left(\\frac{1}{p} \\operatorname{tr} \\hat{V}\\right)=\\frac{E\\left\\{\\left[p^{-1} u(|\\mathbf{x}|)-v(|\\mathbf{x}|)\\right]^{2}\\right\\}}{\\left\\{E\\left[p^{-1} u^{\\prime}(|\\mathbf{x}|)|\\mathbf{x}|-v^{\\prime}(|\\mathbf{x}|)|\\mathbf{x}|]\\right\\}^{2}} \\\\ n \\operatorname{var}\\left(\\hat{V}_{j j}-p^{-1} \\operatorname{tr} \\hat{V}\\right)=\\frac{(p-1)(p-2)}{2 p^{2}} \\lambda \\\\ n E\\left[\\left(\\hat{V}_{j j}-p^{-1} \\operatorname{tr} \\hat{V}\\right)\\left(\\hat{V}_{k k}-p^{-1} \\operatorname{tr} \\hat{V}\\right)\\right]=\\frac{p+2}{2 p^{2}} \\lambda, \\quad \\text { for } j \\neq k \\\\ n \\operatorname{var}\\left(\\hat{V}_{j k}\\right)=\\frac{p+2}{p} \\lambda, \\quad \\text { for } j&gt;k \\end{gathered} \\] <p>with</p> \\[ \\lambda=\\frac{E\\left[u(|\\mathbf{x}|)^{2}\\right]}{\\left\\{E\\left[(1 / p) u^{\\prime}(|\\mathbf{x}|)|\\mathbf{x}|+u(|\\mathbf{x}|)\\right]\\right\\}^{2}} \\] <p>All other asymptotic covariances between \\(p^{-1} \\operatorname{tr}(\\hat{V}), \\hat{V}_{j j}-p^{-1} \\operatorname{tr} \\hat{V}\\), and \\(\\hat{V}_{j k}\\) are 0 .</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#89-breakdown-point","title":"8.9 BREAKDOWN POINT","text":"<p>Let us agree that breakdown occurs when at least one solution of (4.12) misbehaves. Then the breakdown point (with regard to centrosymmetric \\(\\varepsilon\\)-contamination) is always</p> \\[ \\varepsilon^{*}&lt;\\frac{1}{p} \\] <p>This bound is conjectured to be sharp [if we allow asymmetric contamination, then the sharp bound is conjectured to be \\(1 /(p+1)]\\).</p> <p>The demonstration follows an idea of W. Stahel (personal communication). Let \\(G\\) and \\(H\\) be centrosymmetric, but not spherically symmetric, distributions in \\(\\mathbb{R}^{p}\\), centered at 0 , and put</p> \\[ F=(1-\\varepsilon) G+\\varepsilon H \\] <p>Assume that \\(|\\mathbf{x}|\\) has the same distribution under \\(G\\) and \\(H\\), hence also under \\(F\\).</p> <p>We assume that the conditional covariance matrix of \\(\\mathbf{x} /|\\mathbf{x}|\\), given \\(|\\mathbf{x}|\\), is diagonal under both \\(G\\) and \\(H\\), namely, with diagonal vector</p> \\[ \\left(0, \\frac{1}{p-1}, \\ldots, \\frac{1}{p-1}\\right) \\] <p>under \\(G\\), with diagonal vector \\((1,0,0, \\ldots, 0)\\) under \\(H\\). For instance, we may take \\(G\\) to be the distribution of \\(\\left(0, z_{2}, \\ldots, z_{p}\\right)\\), where \\(z_{2}, \\ldots, z_{p}\\) are independent standard normal, and \\(H\\) to be the distribution of \\(\\left(z_{1}, 0, \\ldots, 0\\right)\\), where \\(z_{1}\\) has a \\(\\chi\\)-distribution with \\(p-1\\) degrees of freedom. For \\(\\varepsilon=1 / p\\), the conditional covariance matrix of \\(\\mathbf{x} /|\\mathbf{x}|\\), given \\(|\\mathbf{x}|\\), under \\(F\\) is diagonal with diagonal vector \\((1 / p, \\ldots, 1 / p)\\).</p> <p>Now let \\(\\bar{F}\\) be the spherically symmetric distribution obtained by averaging \\(F\\) over the orthogonal group. For both \\(F\\) and \\(\\bar{F}\\), the radial distribution (i.e. the distribution of \\(|\\mathbf{x}|\\) ) then is a \\(\\chi\\)-distribution with \\(p-1\\) degrees of freedom. Clearly, any covariance estimate defined by a relation of the form (4.12), viewed as a functional, then will be the same for \\(F\\) and for \\(\\bar{F}\\), namely a certain multiple of the identity matrix.</p> <p>We interpret this result that a symmetric \\(\\varepsilon\\)-contamination on the \\(x_{1}\\)-axis, with \\(\\varepsilon=1 / p\\), can cause breakdown.</p> <p>A breakdown point \\(\\varepsilon^{*} \\leqslant 1 / p\\) is disappointingly low in high dimension. A possible way out may be the following. Estimate first some location \\(\\mathbf{t}\\) and pseudo-covariance \\(\\left(V^{T} V\\right)^{-1}\\). Then make a search for outliers in the space of \\(y\\)-vectors \\([\\mathbf{y}=V(\\mathbf{x}-\\mathbf{t})]\\). If the type of breakdown encountered in this section is at all typical (and there is little doubt that it is), the \"good\" points \\(\\mathbf{y}\\) will form a flat disk, while the \"bad\" points that have caused breakdown will stick out roughly in the axial direction of the disk. A univariate robust scale estimate (e.g., the median absolute deviation) should therefore show a well defined minimum in this direction.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#810-least-informative-distributions","title":"8.10 LEAST INFORMATIVE DISTRIBUTIONS","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#location","title":"Location","text":"<p>Consider the family of distributions</p> \\[ f(\\mathbf{x} ; \\mathbf{t}, I)=f(|\\mathbf{x}-\\mathbf{t}|), \\quad \\mathbf{x}, \\mathbf{t} \\in \\mathbb{R}^{p} \\] <p>where \\(f\\) belongs to some convex set \\(\\mathscr{F}\\) of densities. Assume that \\(\\mathbf{t}\\) depends differentiably on some real parameter \\(\\theta\\). Then Fisher information with respect to \\(\\theta\\) is</p> \\[ \\begin{aligned} I(f) &amp; =E_{\\mathbf{t}}\\left\\{\\left[\\frac{\\partial}{\\partial \\theta} \\log f(|\\mathbf{x}-\\mathbf{t}|)\\right]^{2}\\right\\} \\\\ &amp; =E\\left\\{\\left[\\frac{f^{\\prime}(|\\mathbf{x}|)}{f(|\\mathbf{x}|)} \\frac{\\mathbf{i}^{T} \\mathbf{x}}{|\\mathbf{x}|}\\right]^{2}\\right\\} \\\\ &amp; =E\\left\\{\\left[\\frac{f^{\\prime}(|\\mathbf{x}|)}{f(|\\mathbf{x}|)}\\right]^{2}\\right\\} \\cdot \\frac{|\\mathbf{t}|^{2}}{p} \\end{aligned} \\] <p>We now intend to find a \\(f_{0} \\in \\mathscr{F}\\) minimizing \\(I(f)\\). Clearly, this is done by minimizing</p> \\[ E\\left\\{\\left[\\frac{f^{\\prime}(|\\mathbf{x}|)}{f(|\\mathbf{x}|)}\\right]^{2}\\right\\}=C_{p} \\int_{0}^{\\infty}\\left[\\frac{f^{\\prime}(r)}{f(r)}\\right]^{2} f r^{p-1} d r \\] <p>where \\(C_{p}\\) denotes the surface area of the unit sphere in \\(\\mathbb{R}^{p}\\). This immediately leads to the variational condition</p> \\[ \\int_{0}^{\\infty}\\left[-\\left(\\frac{f^{\\prime}}{f}\\right)^{2} r^{p-1}-2\\left(\\frac{f^{\\prime}}{f} r^{p-1}\\right)^{\\prime}\\right] \\delta f d r \\geqslant 0 \\] <p>subject to the side condition</p> \\[ \\int r^{p-1} \\delta f d r=0 \\] <p>or, with some Lagrange multiplier \\(\\gamma\\),</p> \\[ 4 \\gamma r^{p-1}-\\left(\\frac{f^{\\prime}}{f}\\right)^{2} r^{p-1}-2\\left(\\frac{f^{\\prime}}{f} r^{p-1}\\right)^{\\prime}=0 \\] <p>on the set of \\(r\\)-values where \\(f\\) can be varied freely; the equality sign should be replaced by \\(\\geqslant 0\\) on the set where \\(\\delta f \\geqslant 0\\).</p> <p>With \\(u=\\sqrt{f}\\) we obtain the linear differential equation</p> \\[ u^{\\prime \\prime}+\\frac{p-1}{r} u^{\\prime}-\\gamma u=0 \\] <p>valid on the set where \\(f\\) can be freely varied. Example 10.1 Let \\(\\mathscr{F}\\) be the set of spherically symmetric \\(\\varepsilon\\)-contaminated normal distributions in \\(\\mathbb{R}^{3}\\). Then (10.7) has the particular solution</p> \\[ u(r)=\\frac{e^{-\\sqrt{\\gamma} r}}{r} \\] <p>Since \\(f_{0}\\) and \\(f_{0}^{\\prime} / f_{0}\\) should be continuous, we obtain after some calculations</p> \\[ \\begin{aligned} f_{0}(r) &amp; =a e^{-r^{2} / 2}, &amp; &amp; \\text { for } r \\leqslant r_{0} \\\\ &amp; =b \\frac{e^{-c r}}{r^{2}}, &amp; &amp; \\text { for } r \\geqslant r_{0} \\end{aligned} \\] <p>with</p> \\[ \\begin{aligned} &amp; a=(1-\\varepsilon)(2 \\pi)^{-3 / 2} \\\\ &amp; b=(1-\\varepsilon)(2 \\pi)^{-3 / 2} r_{0}^{2} e^{\\left(r_{0}^{2} / 2\\right)-2} \\\\ &amp; c=2 \\sqrt{\\gamma}=r_{0}-\\frac{2}{r_{0}} \\end{aligned} \\] <p>and thus</p> \\[ \\begin{aligned} -\\frac{f_{0}^{\\prime}(r)}{f_{0}(r)} &amp; =r, &amp; &amp; \\text { for } r \\leqslant r_{0} \\\\ &amp; =c+\\frac{2}{r}, &amp; &amp; \\text { for } r \\geqslant r_{0} \\end{aligned} \\] <p>The constants \\(r_{0}\\) and \\(\\varepsilon\\) are related by the requirement that \\(f_{0}\\) be a probability density:</p> \\[ C_{p} \\int f_{0}(r) r^{p-1} d r=1 \\] <p>In particular, we must have \\(c&gt;0\\), and hence \\(r_{0}&gt;\\sqrt{2}\\); the limiting case \\(c=0\\) corresponds to \\(r_{0}=\\sqrt{2}\\) and \\(\\varepsilon=1\\).</p> <p>It can be seen from the nonmonotonicity of (10.11) that \\(-\\log f_{0}(|x|)\\) is not a convex function of \\(\\mathbf{x}\\). Hence, in general, the maximum likelihood estimate of location need not be unique, and there are some troubles with consistency proofs when \\(\\varepsilon\\) is large.</p> <p>For our present purposes location is but a nuisance parameter, and it is hardly worthwhile to bother with complicated estimates of location. We therefore prefer to work with a simple monotone approximation to (10.11), of the form</p> \\[ \\begin{aligned} w(r) r &amp; =r, &amp; &amp; \\text { for } r&lt;r_{0} \\\\ &amp; =r_{0}, &amp; &amp; \\text { for } r \\geqslant r_{0} \\end{aligned} \\] <p>compare (4.6).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#covariance","title":"Covariance","text":"<p>We now consider the family of distributions</p> \\[ f(\\mathbf{x} ; 0, V)=|\\operatorname{det} V| f(|V \\mathbf{x}|), \\quad \\mathbf{x} \\in \\mathbb{R}^{p} \\] <p>We assume that \\(V\\) depends differentiably on some real parameter \\(\\theta\\). Then Fisher information with respect to \\(\\theta\\) at \\(V=V_{0}=I\\) is</p> \\[ \\begin{aligned} I(f) &amp; =E\\left\\{\\left[\\frac{\\partial}{\\partial \\theta} \\log f(\\mathbf{x} ; 0, V)\\right]^{2}\\right\\} \\\\ &amp; =E\\left\\{\\left[\\operatorname{tr} \\dot{V}+\\frac{f^{\\prime}(|\\mathbf{x}|)}{f(|\\mathbf{x}|)} \\frac{\\mathbf{x}^{T} \\dot{V} \\mathbf{x}}{|\\mathbf{x}|}\\right]^{2}\\right\\} \\end{aligned} \\] <p>Because of symmetry it suffices to treat this special case. In order to simplify (10.15) we first take the conditional expectation, given \\(|\\mathbf{x}|\\), that is we average over the uniform distribution on the spheres \\(|\\mathbf{x}|=\\) const. The conditional averages of \\(\\mathbf{x}^{T} \\dot{V} \\mathbf{x}\\) and \\(\\left(\\mathbf{x}^{T} \\dot{V} \\mathbf{x}\\right)^{2}\\) are \\(\\beta|\\mathbf{x}|^{2}\\) and \\(\\gamma|\\mathbf{x}|^{4}\\), respectively, with \\(\\beta=(1 / p) \\operatorname{tr} \\dot{V}\\) and</p> \\[ \\gamma=\\frac{1}{p(p+2)}\\left[(\\operatorname{tr} \\dot{V})^{2}+2 \\sum_{j, k} \\dot{V}_{j k}^{2}\\right] \\] <p>if we assume (without loss of generality) that \\(\\dot{V}\\) is symmetric. The easiest</p> <p>way to prove this is to show that, for reasons of symmetry and homogeneity, the averages must be proportional to \\(|\\mathbf{x}|^{2}\\) and \\(|\\mathbf{x}|^{4}\\), respectively, and then to determine the proportionality constants in the special case where \\(\\mathbf{x}\\) is \\(p\\)-variate standard normal and \\(V\\) is diagonal. Thus if we put</p> \\[ u(r)=-\\frac{f^{\\prime}(r)}{f(r)} r \\] <p>we have</p> \\[ \\begin{aligned} I(f) &amp; =E\\left\\{\\left[u(|\\mathbf{x}|) \\frac{\\mathbf{x}^{T} \\dot{V} \\mathbf{x}}{|\\mathbf{x}|^{2}}-p \\beta\\right]^{2}\\right\\} \\\\ &amp; =E\\left[\\gamma u(|\\mathbf{x}|)^{2}-2 p \\beta u(|\\mathbf{x}|)+p^{2} \\beta^{2}\\right] \\\\ &amp; =\\gamma E\\left[u(|\\mathbf{x}|)^{2}\\right]-p^{2} \\beta^{2} \\end{aligned} \\] <p>Hence in order to minimize \\(I(f)\\) over \\(\\mathscr{F}\\), it suffices to minimize</p> \\[ \\begin{aligned} J(f) &amp; =E\\left[u(|\\mathbf{x}|)^{2}\\right]=C_{p} \\int_{0}^{\\infty} u(r)^{2} r^{p-1} f d r \\\\ &amp; =C_{p} \\int_{0}^{\\infty} \\frac{f^{\\prime}(r)^{2}}{f(r)} r^{p+1} d r \\end{aligned} \\] <p>A standard variational argument gives</p> \\[ \\delta J(f)=C_{p} \\int_{0}^{\\infty}\\left(-u^{2}+2 p u+2 r u^{\\prime}\\right) r^{p-1} \\delta f d r \\] <p>Together with the side condition \\(C_{p} \\int r^{p-1} \\delta f d r=0\\), we obtain that the \\(u\\) corresponding to the minimizing \\(f_{0}\\) should satisfy</p> \\[ 2 r u^{\\prime}+2 p u-u^{2}=c \\] <p>for those \\(r\\) where \\(f_{0}\\) can be varied freely, or</p> \\[ -2 r u^{\\prime}+(u-p)^{2}=p^{2}-c=\\kappa^{2} \\] <p>for some constant \\(\\kappa\\).</p> <p>For our purposes we only need the constant solutions corresponding to \\(u^{\\prime}=0\\). Thus</p> \\[ u=p \\pm \\kappa \\] <p>In particular, let</p> \\[ \\mathscr{G}=\\left\\{f \\mid f(r)=(1-\\varepsilon) \\varphi(r)+\\varepsilon h(r), h \\in \\mathscr{M}_{\\tau}\\right\\} \\] <p>be the set of all spherically symmetric contaminated normal densities, with</p> \\[ \\varphi(r)=(2 \\pi)^{-p / 2} e^{-r^{2} / 2} \\] <p>and \\(\\mathscr{M}_{\\tau}\\) being the set of all spherically symmetric probability densities in \\(\\mathbb{R}^{p}\\).</p> <p>Then we verify easily that \\(J(f)\\), and thus \\(I(f)\\), are minimized by choosing</p> \\[ \\begin{aligned} u(r)=-\\frac{f_{0}^{\\prime}(r)}{f_{0}(r)} r &amp; =a^{2}, &amp; &amp; \\text { for } 0 \\leqslant r \\leqslant a \\\\ &amp; =r^{2}, &amp; &amp; \\text { for } a \\leqslant r \\leqslant b \\\\ &amp; =b^{2}, &amp; &amp; \\text { for } b \\leqslant r \\end{aligned} \\] <p>and thus</p> \\[ \\begin{array}{rlr} f_{0}(r) &amp; =(1-\\varepsilon) \\varphi(a)\\left(\\frac{a}{r}\\right)^{a^{2}}, &amp; &amp; \\text { for } 0 \\leqslant r \\leqslant a \\\\ &amp; =(1-\\varepsilon) \\varphi(r), &amp; &amp; \\text { for } a \\leqslant r \\leqslant b \\\\ &amp; =(1-\\varepsilon) \\varphi(b)\\left(\\frac{b}{r}\\right)^{b^{2}} &amp; &amp; \\text { for } b \\leqslant r . \\end{array} \\] <p>The constants \\(a\\) and \\(b\\) satisfy</p> \\[ \\begin{aligned} &amp; a=\\sqrt{(p-\\kappa)^{+}} \\\\ &amp; b=\\sqrt{p+\\kappa} \\end{aligned} \\] <p>and \\(\\kappa&gt;0\\) has to be determined such that the total mass of \\(f_{0}\\) is 1 , or</p> <p>equivalently, that</p> \\[ \\begin{aligned} C_{p} &amp; {\\left[\\varphi(a) \\int_{0}^{a}\\left(\\frac{a}{r}\\right)^{a^{2}} r^{p-1} d r+\\int_{a}^{b} \\varphi(r) r^{p-1} d r+\\varphi(b) \\int_{b}^{\\infty}\\left(\\frac{b}{r}\\right)^{b^{2}} r^{p-1} d r\\right] } \\\\ &amp; =\\frac{1}{1-\\varepsilon} \\end{aligned} \\] <p>The maximum likelihood estimate of pseudo-covariance for \\(f_{0}\\) can be described by (4.12), with \\(u\\) as in (10.25), and \\(v \\equiv 1\\). It has the following minimax property. Let \\(\\mathscr{T}_{\\varepsilon} \\subset \\mathscr{T}\\) be that subset for which it is a consistent estimate of the identity matrix. Then it minimizes the supremum over \\(\\mathscr{T}_{\\varepsilon}\\) of the asymptotic variances (8.2) to (8.6).</p> <p>If \\(\\kappa&lt;p\\) and hence \\(a&gt;0\\), the least informative density \\(f_{0}\\) is highly unrealistic in view of its singularity at the origin. In other words the corresponding minimax estimate appears to protect against an unlikely contingency. Moreover, if the underlying distribution happens to put a pointmass at the origin (or, if in the course of a computation, a sample point happens to coincide with the current trial value \\(\\mathbf{t}\\) ), (4.12) or (4.14) is not well defined.</p> <p>If we separate the scale aspects (information contained in \\(|\\mathbf{y}|\\) ) from the directional aspects (information contained in \\(\\mathbf{y} /|\\mathbf{y}|\\) ), then it appears that values \\(a&gt;0\\) are beneficial with regard to the former aspects only-they help to prevent breakdown by \"implosion,\" caused by inliers. The limiting scale estimate for \\(\\kappa \\rightarrow 0\\) is, essentially, the median absolute deviation \\(\\operatorname{med}\\{|\\mathbf{x}|\\}\\), and we have already commented upon its good robustness properties in the one-dimensional case. Also the indeterminacy of (4.12) at \\(\\mathbf{y}=0\\) only affects the directional, but not the scale aspects.</p> <p>With regard to the directional aspects, a value \\(u(0) \\neq 0\\) is distinctly awkward. To give some intuitive insight into what is going on, we note that, for the maximum likelihood estimates \\(\\hat{\\mathbf{t}}\\) and \\(\\hat{\\mathbf{V}}\\), the linearly transformed quantities \\(\\mathbf{y}=\\hat{V}(\\mathbf{x}-\\hat{\\mathbf{t}})\\) possess the following property (cf. Exhibit 8.10.1): if the sample points with \\(|\\mathbf{y}|&lt;a\\) and those with \\(|\\mathbf{y}|&gt;b\\) are moved radially outward and inward to the spheres \\(|\\mathbf{y}|=a\\) and \\(|\\mathbf{y}|=b\\), respectively, while the points with \\(a \\leqslant|\\mathbf{y}| \\leqslant b\\) are left where they are, then the sample thus modified has the (ordinary) covariance matrix \\(I\\).</p> <p>A value \\(\\mathbf{y}\\) very close to the origin clearly does not give any directional information; in fact \\(\\mathbf{y} /|\\mathbf{y}|\\) changes randomly under small random changes of \\(\\mathbf{t}\\). We should therefore refrain from moving points to the sphere with radius \\(a\\) when they are close to the origin, but we should like to retain the scale information contained in them. This can be achieved by letting \\(u\\) decrease to 0 as \\(r \\rightarrow 0\\), and simultaneously changing \\(v\\) so that the trace of</p> <p></p> <p>Exhlbit 8.10.1 From Huber (1977a), with permission of the publisher. (4.12) is unchanged. For instance, we might change (10.25) by putting</p> \\[ u(r)=\\frac{a^{2}}{r_{0}} r, \\quad \\text { for } r \\leqslant r_{0}&lt;a \\] <p>and</p> \\[ \\begin{aligned} v(r) &amp; =\\left(1-\\frac{a^{2}}{p}\\right)+\\frac{a^{2}}{p r_{0}} r, &amp; &amp; \\text { for } r \\leqslant r_{0} \\\\ &amp; =1, &amp; &amp; \\text { for } r \\geqslant r_{0} \\end{aligned} \\] <p>Unfortunately, this will destroy the uniqueness proofs of Section 8.6. It usually is desirable to standardize the scale part of these estimates such that we obtain the correct asymptotic values at normal distributions. This is best done by applying a correction factor \\(\\tau\\) at the very end, as follows.</p> <p>Example 10.2 With the \\(u\\) defined in (10.25) we have, for standard normal observations \\(\\mathbf{x}\\),</p> \\[ \\begin{aligned} E[u(\\tau \\mid \\mathbf{x} \\mid)]= &amp; a^{2} \\chi^{2}\\left(p, \\frac{a^{2}}{\\tau^{2}}\\right)+b^{2}\\left[1-\\chi^{2}\\left(p, \\frac{b^{2}}{\\tau^{2}}\\right)\\right] \\\\ &amp; +\\tau^{2} p\\left[\\chi^{2}\\left(p+2, \\frac{b^{2}}{\\tau^{2}}\\right)-\\chi^{2}\\left(p+2, \\frac{a^{2}}{\\tau^{2}}\\right)\\right] \\end{aligned} \\] Mass of \\(F_{0}\\) Below \\(a\\) Above \\(b\\) \\(\\varepsilon\\) \\(p\\) \\(\\kappa\\) \\(a=\\sqrt{(p-\\kappa)^{+}}\\) \\(b=\\sqrt{p+\\kappa}\\) \\(\\tau^{2}\\) 0.01 1 4.1350 0 0.0332 1.0504 2 5.2573 0 0.0363 1.0305 3 6.0763 0 0.0380 1.0230 5 7.3433 0 0.0401 1.0164 10 9.6307 0.0000 0.0426 1.0105 20 12.9066 0.0038 0.0440 1.0066 50 19.7896 0.0133 0.0419 1.0030 100 27.7370 0.0187 0.0395 1.0016 0.05 1 2.2834 0 0.1165 1.1980 2 3.0469 0 0.1262 1.1165 3 3.6045 0 0.1313 1.0873 5 4.4751 0.0087 0.1367 1.0612 10 6.2416 0.0454 0.1332 1.0328 20 8.8237 0.0659 0.1263 1.0166 50 13.9670 0.0810 0.1185 1.0067 100 19.7634 0.0877 0.1141 1.0033 0.10 1 1.6086 0 0.1957 1.3812 2 2.2020 0 0.2101 1.2161 3 2.6635 0.0445 0.2141 1.1539 5 3.4835 0.0912 0.2072 1.0908 10 5.0051 0.1198 0.1965 1.0441 20 7.1425 0.1352 0.1879 1.0216 50 11.3576 0.1469 0.1797 1.0086 100 16.0931 0.1523 0.1754 1.0043 0.25 1 0.8878 0.2135 0.3604 1.9470 2 1.3748 0.2495 0.3406 1.3598 3 1.7428 0.2582 0.3311 1.2189 5 2.3157 0.2657 0.3216 1.1220 10 3.3484 0.2730 0.3122 1.0577 20 4.7888 0.2782 0.3059 1.0281 50 7.6232 0.2829 0.3004 1.0110 100 10.8052 0.2854 0.2977 1.0055 <p>Exhibit 8.10.2 From Huber (1977a), with permission of the publisher.</p> <p>where \\(\\chi^{2}(p, \\cdot)\\) is the cumulative \\(\\chi^{2}\\)-distribution with \\(p\\) degrees of freedom. So we determine \\(\\tau\\) from \\(E[u(\\tau \\mid \\mathbf{x} \\mid)]=p\\), and then we multiply the pseudocovariance \\(\\left(V^{T} V\\right)^{-1}\\) found from (4.12) by \\(\\tau^{2}\\). Some numerical results are summarized in Exhibit 8.10.2.</p> <p>Some further remarks are needed on the question of spherical symmetry. First, we should point out that the assumption of spherical symmetry is not needed when minimizing Fisher information. Note that Fisher information is a convex function of \\(f\\), so by taking averages over the orthogonal group we obtain (by Jensen's inequality)</p> \\[ I(\\operatorname{ave}\\{f\\}) \\leqslant \\operatorname{ave}\\{I(f)\\} \\] <p>where ave \\(\\{f\\}=\\bar{f}\\) is a spherically symmetric density. So instead of minimizing \\(I(f)\\) for spherically symmetric \\(f\\), we might minimize ave \\(\\{I(f)\\}\\) for more general \\(f\\); the minimum will occur at a spherically symmetric \\(f\\).</p> <p>Second, we might criticize the approach for being restricted to a framework of elliptic densities (with the exception of Section 8.9).</p> <p>Such a symmetry assumption is reasonable if we are working with genuinely long-tailed \\(p\\)-variate distributions. But, for instance, in the framework of the gross error model, typical outliers will be generated by a process distinct from that of the main family and hence will have quite a different covariance structure. For example, the main family may consist of a tight and narrow ellipsoid with only a few principal axes significantly different from zero, while there is a diffuse and roughly spherical cloud of outliers. Or it might be the outliers that show a structure and lie along some well-defined lower dimensional subspaces, and so on. Of course, in an affinely invariant framework, the two situations are not really distinguishable.</p> <p>But we do not seem to have the means to attack such multidimensional separation problems directly, unless we possess some prior information. The estimates developed in Sections 8.4 ff . are useful just because they are able to furnish an unprejudiced estimate of the overall shape of the principal part of a pointcloud, from which a more meaningful analysis of its composition might start off.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#811-some-notes-on-computation","title":"8.11 SOME NOTES ON COMPUTATION","text":"<p>Unfortunately, so far we have neither a really fast, nor a demonstrably convergent, procedure for calculating simultaneous estimates of location and scatter.</p> <p>A relatively simple and straightforward approach can be constructed from (4.13) and (4.14): (1) Starting values For example, let</p> \\[ \\begin{aligned} \\mathbf{t} &amp; :=\\operatorname{ave}\\{\\mathbf{x}\\} \\\\ \\sum &amp; :=\\operatorname{ave}\\left\\{(\\mathbf{x}-\\mathbf{t})(\\mathbf{x}-\\mathbf{t})^{T}\\right\\} \\end{aligned} \\] <p>be the classical estimates. Take the Choleski decomposition \\(\\Sigma=\\) \\(B B^{T}\\), with \\(B\\) lower triangular, and put</p> \\[ V:=B^{-1} \\] <p>Then alternate between scatter steps and location steps, as follows. (2) Scatter step With \\(\\mathbf{y}=V(\\mathbf{x}-\\mathbf{t})\\) let</p> \\[ C:=\\frac{\\operatorname{ave}\\left\\{s(|\\mathbf{y}|) \\mathbf{y} \\mathbf{y}^{T}\\right\\}}{\\operatorname{ave}\\{v(|\\mathbf{y}|)\\}} \\] <p>Take the Choleski decomposition \\(C=B B^{T}\\) and put</p> \\[ \\begin{aligned} W: &amp; =B^{-1} \\\\ V: &amp; =W V \\end{aligned} \\] <p>(3) Location step With \\(\\mathbf{y}=V(\\mathbf{x}-\\mathbf{t})\\) let</p> \\[ \\begin{aligned} &amp; \\mathbf{h}:=\\frac{\\operatorname{ave}\\{w(|\\mathbf{y}|)(\\mathbf{x}-\\mathbf{t})\\}}{\\operatorname{ave}\\{w(|\\mathbf{y}|)\\}} \\\\ &amp; \\mathbf{t}:=\\mathbf{t}+\\mathbf{h} \\end{aligned} \\] <p>(4) Termination rule Stop iterating when both \\(\\|W-I\\|&lt;\\varepsilon\\) and \\(\\|V \\mathbf{h}\\|\\) \\(&lt;\\delta\\), for some predetermined tolerance levels, for example \\(\\varepsilon=\\delta=\\) \\(10^{-3}\\).</p> <p>Note that this algorithm attempts to improve the numerical properties by avoiding the possibly poorly conditioned matrix \\(V^{T} V\\).</p> <p>If either \\(\\mathbf{t}\\) or \\(V\\) is kept fixed, it is not difficult to show that the algorithm converges under fairly general assumptions. A convergence proof for fixed \\(\\mathbf{t}\\) is contained in the proof of Lemma 6.1.</p> <p>For fixed \\(V\\) convergence of the location step can easily be proved if \\(w(r)\\) is monotone decreasing and \\(w(r) r\\) is monotone increasing. Assume for simplicity that \\(V=I\\) and let \\(\\rho(r)\\) be an indefinite integral of \\(w(r) r\\). Then</p> <p>\\(\\rho(|\\mathbf{x}-\\mathbf{t}|)\\) is convex as a function of \\(\\mathbf{t}\\), and minimizing \\(\\operatorname{ave}\\{\\rho(|\\mathbf{x}-\\mathbf{t}|)\\}\\) is equivalent to solving (4.11).</p> <p>As in Section 7.8 we define comparison functions. Let \\(r_{i}=\\left|\\mathbf{y}_{i}\\right|=\\left|\\mathbf{x}_{i}-\\mathbf{t}^{(m)}\\right|\\), where \\(\\mathbf{t}^{(m)}\\) is the current trial value and the index \\(i\\) denotes the \\(i\\) th observation. Define the comparison functions \\(u_{i}\\) such that</p> \\[ \\begin{aligned} &amp; u_{i}(r)=a_{i}+\\frac{1}{2} b_{i} r^{2} \\\\ &amp; u_{i}\\left(r_{i}\\right)=\\rho\\left(r_{i}\\right) \\\\ &amp; u_{i}^{\\prime}\\left(r_{i}\\right)=\\rho^{\\prime}\\left(r_{i}\\right)=w\\left(r_{i}\\right) r_{i} \\end{aligned} \\] <p>The last condition implies \\(b_{i}=w\\left(r_{i}\\right)\\); hence</p> \\[ u_{i}(r)=\\rho\\left(r_{i}\\right)+\\frac{1}{2} w\\left(r_{i}\\right)\\left(r^{2}-r_{i}^{2}\\right) \\] <p>and, since \\(w\\) is monotone decreasing, we have</p> \\[ \\begin{aligned} {\\left[u_{i}(r)-\\rho(r)\\right]^{\\prime}=\\left[w\\left(r_{i}\\right)-w(r)\\right] r \\leqslant 0, } &amp; \\text { for } r \\leqslant r_{i} \\\\ &gt; &amp; 0, \\quad \\text { for } r \\geqslant r_{i} \\end{aligned} \\] <p>Hence</p> \\[ u_{i}(r) \\geqslant \\rho(r), \\quad \\text { for all } r \\] <p>Minimizing</p> \\[ \\operatorname{ave}\\left(u_{i}\\left(\\left|\\mathbf{x}_{i}-\\mathbf{t}\\right|\\right)\\right) \\] <p>is equivalent to performing one location step, from \\(\\mathbf{t}^{(m)}\\) to \\(\\mathbf{t}^{(m+1)}\\); hence \\(\\operatorname{ave}\\{\\rho(|\\mathbf{x}-\\mathbf{t}|)\\}\\) is strictly decreased, unless \\(\\mathbf{t}^{(m)}=\\mathbf{t}^{(m+1)}\\) already is a solution, and convergence towards the minimum is now easily proved.</p> <p>Convergence has not been proved yet when \\(\\mathbf{t}\\) and \\(V\\) are estimated simultaneously.</p> <p>The speed of convergence of the location step is satisfactory, but not so that of the more expensive scatter step (most of the work is spent in building up the matrix \\(C\\) ).</p> <p>Some supposedly faster procedures have been proposed by Maronna (1976) and Huber (1977a). The former tried to speed up the scatter step by overrelaxation (in our notation, the Choleski decomposition would be applied to \\(C^{2}\\) instead of \\(C\\), so the step is roughly doubled). The latter proposed using a modified Newton approach instead (with the Hessian</p> <p>matrix replaced by its average over the spheres \\(|\\mathbf{y}|=\\) const.). But neither of these proposals performed very well in our numerical experiments (Maronna's too often led to oscillatory behavior; Huber's did not really improve the overall speed). A straightforward Newton approach is out of the question because of the high number of variables.</p> <p>The most successful method so far (with an improvement slightly better than two in overall speed) turned out to be a variant of the conjugate gradient method, using explicit second derivatives. The idea behind it is as follows. Assume that a function \\(f(\\mathbf{z}), \\mathbf{z} \\in \\mathbb{R}^{n}\\), is to be minimized, and assume that \\(\\mathbf{z}^{(m)}:=\\mathbf{z}^{(m-1)}+\\mathbf{h}^{(m-1)}\\) was the last iteration step. If \\(\\mathbf{g}^{(m)}\\) is the gradient of \\(f\\) at \\(\\mathbf{z}^{(m)}\\), then approximate the function</p> \\[ F\\left(t_{1}, t_{2}\\right)=f\\left(\\mathbf{z}^{(m)}+t_{1} \\mathbf{g}^{(m)}+t_{2} \\mathbf{h}^{(m-1)}\\right) \\] <p>by a quadratic function \\(Q\\left(t_{1}, t_{2}\\right)\\) having the same derivatives up to order two at \\(t_{1}=t_{2}=0\\), find the minimum of \\(Q\\), say at \\(\\hat{t}_{1}\\) and \\(\\hat{t}_{2}\\), and put \\(\\mathbf{h}^{(m)}:=\\hat{t}_{1} \\mathbf{g}^{(m)}+\\hat{t}_{2} \\mathbf{h}^{(m-1)}\\) and \\(\\mathbf{z}^{(m+1)}:=\\mathbf{z}^{(m)}+\\mathbf{h}^{(m)}\\). The first and second derivatives of \\(F\\) should be determined analytically.</p> <p>If \\(f\\) itself is quadratic, the procedure is algebraically equivalent to the standard descriptions of the conjugate gradient method and reaches the true minimum in \\(n\\) steps (where \\(n\\) is the dimension of \\(\\mathbf{z}\\) ). Its advantage over the more customary versions that determine \\(\\mathbf{h}^{(m)}\\) recursively (FletcherPowell, etc.) is that it avoids instabilities due to accumulation of errors caused by (1) deviation of \\(f\\) from a quadratic function, and (2) rounding (in essence the usual recursive determination of \\(\\mathbf{h}^{(m)}\\) amounts to numerical differentiation).</p> <p>In our case we start from the maximum likelihood problem (4.2) and assume that we have to minimize</p> \\[ Q=-\\log (\\operatorname{det} V)-\\operatorname{ave}\\{\\log f(|V(\\mathbf{x}-\\mathbf{t})|\\} \\] <p>We write \\(V(\\mathbf{x}-\\mathbf{t})=W \\mathbf{y}\\) with \\(\\mathbf{y}=V_{0}(\\mathbf{x}-\\mathbf{t}) ; \\mathbf{t}\\) and \\(V_{0}\\) will correspond to the current trial values. We assume that \\(W\\) is lower triangular and depends linearly on two real parameters \\(s_{1}\\) and \\(s_{2}\\) :</p> \\[ W=I+s_{1} U_{1}+s_{2} U_{2} \\] <p>where \\(U_{1}\\) and \\(U_{2}\\) are lower triangular matrices. If</p> \\[ Q(W)=-\\log (\\operatorname{det} W)-\\log \\left(\\operatorname{det} V_{0}\\right)-\\operatorname{ave}\\{\\log f(|W \\mathbf{y}|)\\} \\] <p>is differentiated with respect to a linear parameter in \\(W\\), we obtain</p> \\[ \\dot{Q}(W)=-\\operatorname{tr}\\left(\\dot{W} W^{-1}\\right)+\\operatorname{ave}\\left\\{s(|W y|)(W y)^{T}(\\dot{W} y)\\right\\} \\] <p>with</p> \\[ s(r)=-\\frac{f^{\\prime}(r)}{r f(r)} \\] <p>At \\(s_{1}=s_{2}=0\\) this gives</p> \\[ \\begin{aligned} &amp; \\dot{Q}(I)=\\operatorname{ave}\\left\\{s(|y|) y^{T} \\dot{W} y\\right\\}-\\operatorname{tr}(\\dot{W}) \\\\ &amp; \\dot{Q}(I)=\\operatorname{ave}\\left\\{\\frac{s^{\\prime}(|y|)}{|y|}\\left(y^{T} \\dot{W} y\\right)\\left(y^{T} \\dot{W} y\\right)+s(|y|)(\\dot{W} y)^{T}(\\dot{W} y)\\right\\}+\\operatorname{tr}(\\dot{W} \\dot{W}) \\end{aligned} \\] <p>In particular, if we calculate the partial derivatives of \\(Q\\) with respect to the \\(p(p+1) / 2\\) elements of \\(W\\), we obtain from the above that the gradient \\(U_{1}\\) can be naturally identified with the lower triangle of</p> \\[ U_{1}=\\operatorname{ave}\\left\\{s(|y|) y y^{T}\\right\\}-I \\] <p>The idea outlined before is now implemented as follows, in such a way that we can always work near the identity matrix and take advantage of the corresponding simpler formulas and better conditioned matrices.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#cg-iteration-step-for-scatter","title":"CG-Iteration Step for Scatter","text":"<p>Let \\(\\mathbf{t}\\) and \\(V\\) be the current trial values and write \\(\\mathbf{y}=V(\\mathbf{x}-\\mathbf{t})\\). Let \\(U_{1}\\) be lower triangular such that</p> \\[ U_{1}:=\\operatorname{ave}\\left\\{s(|y|) y y^{T}\\right\\}-I \\] <p>(ignoring the upper triangle of the right-hand side). In the first iteration step let \\(j=k=1\\); in all following steps let \\(j\\) and \\(k\\) take the values 1 and 2 ; let</p> \\[ \\begin{aligned} a_{j k} &amp; =\\operatorname{tr}\\left(U_{j} U_{k}\\right)+\\operatorname{ave}\\left\\{\\frac{s^{\\prime}(|y|)}{|y|}\\left(y^{T} U_{j} y\\right)\\left(y^{T} U_{k} y\\right)+s(|y|)\\left(U_{j} y\\right)^{T}\\left(U_{k} y\\right)\\right\\} \\\\ b_{j} &amp; =-\\operatorname{tr}\\left(U_{j}\\right)+\\operatorname{ave}\\left\\{s(|y|)\\left(y^{T} U_{j} y\\right)\\right\\} \\end{aligned} \\] <p>[then \\(Q(W) \\cong Q(I)+\\sum b_{j} s_{j}+\\frac{1}{2} \\sum a_{j k} s_{j} s_{k}\\) ]. Solve</p> \\[ \\sum_{k} a_{j k} s_{k}+b_{j}=0 \\] <p>for \\(s_{1}\\) and \\(s_{2}\\left(s_{2}=0\\right.\\) in the first step). Put</p> \\[ U_{2}:=s_{1} U_{1}+s_{2} U_{2} \\] <p>Cut \\(U_{2}\\) down by a fudge factor if \\(U_{2}\\) is too large; for example, let \\(U_{2}:=c U_{2}\\), with</p> \\[ c=\\frac{1}{\\max (1,2 d)} \\] <p>where \\(d\\) is the maximal absolute diagonal element of \\(U_{2}\\). Put</p> \\[ \\begin{aligned} W: &amp; =I+U_{2} \\\\ V: &amp; =W V \\end{aligned} \\] <p>Empirically, with \\(p\\) up to 20 [i.e., up to \\(p=20\\) parameters for location and \\(p(p+1) / 2=210\\) parameters for scatter] the procedure showed a smooth convergence down to essentially machine accuracy.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-9","title":"CHAPTER 9","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#robustness-of-design","title":"Robustness of Design","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#91-general-remarks","title":"9.1 GENERAL REMARKS","text":"<p>We already have encountered two design-related problems. The first was concerned with leverage points (Sections 7.1 and 7.2), the second with subtle questions of bias (Section 7.5). In both cases we had single observations sitting at isolated points in the design space, and the difficulty was, essentially, that these observations were not cross-checkable.</p> <p>There are many considerations entering into a design. From the point of view of robustness, the most important requirement is to have enough redundancy so that everything can be cross-checked. In this little chapter we give another example of this sort; it illuminates the surprising fact that deviations from linearity that are too small to be detected are already large enough to tip the balance away from the \"optimal\" designs, which assume exact linearity and put the observations on the extreme points of the observable range, toward the \"naive\" ones, which distribute the observations more or less evenly over the entire design space (and thus allow us to check for linearity).</p> <p>One simple example should suffice to illustrate the point; it is taken from Huber (1975). See Sacks and Ylvisaker (1978), as well as Bickel and Herzberg (1979), for interesting further developments.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#92-minimax-global-fit","title":"9.2 MINIMAX GLOBAL FIT","text":"<p>Assume that \\(f\\) is an approximately linear function defined in the interval \\(I=\\left[-\\frac{1}{2}, \\frac{1}{2}\\right]\\). It should be approximated by a linear function as accurately as possible; we choose mean square error as our measure of disagreement:</p> \\[ \\int[f(x)-\\alpha-\\beta x]^{2} d x \\] <p>All integrals are over the interval \\(I\\). Clearly, (2.1) is minimized for</p> \\[ \\alpha_{0}=\\int f(x) d x, \\quad \\beta_{0}=\\frac{\\int x f(x) d x}{\\int x^{2} d x} \\] <p>and the minimum value of (2.1) is denoted by</p> \\[ Q_{f}=\\int\\left[f(x)-\\alpha_{0}-\\beta_{0} x\\right]^{2} d x \\] <p>Assume now that the values of \\(f\\) are only observable with some measurement errors. Assume that we can observe \\(f\\) at \\(n\\) freely chosen points \\(x_{1}, \\ldots, x_{n}\\) in the interval \\(I\\), and that the observed values are</p> \\[ y_{i}=f\\left(x_{i}\\right)+u_{i} \\] <p>where the \\(u_{i}\\) are independent normal \\(\\mathfrak{R}\\left(0, \\sigma^{2}\\right)\\). Our original problem is thus turned into the following: find estimates \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) for the coefficients of a linear function, based on the \\(y_{i}\\), such that the expected mean square error</p> \\[ Q=E\\left\\{\\int\\left[f(x)-\\hat{\\alpha}-\\hat{\\beta} x\\right]^{2} d x\\right\\} \\] <p>is least possible. \\(Q\\) can be decomposed into a constant part, a bias part, and a variance part:</p> \\[ Q=Q_{f}+Q_{b}+Q_{v} \\] <p>where \\(Q_{f}\\) depends on \\(f\\) alone [see (2.3)], where</p> \\[ Q_{b}=\\left(\\alpha_{1}-\\alpha_{0}\\right)^{2}+\\frac{\\left(\\beta_{1}-\\beta_{0}\\right)^{2}}{12} \\] <p>with</p> \\[ \\alpha_{1}=E(\\hat{\\alpha}), \\quad \\beta_{1}=E(\\hat{\\beta}) \\] <p>and where</p> \\[ Q_{v}=\\operatorname{var}(\\hat{\\alpha})+\\frac{\\operatorname{var}(\\hat{\\beta})}{12} \\] <p>It is convenient to characterize the design by the design measure</p> \\[ \\xi=\\frac{1}{n} \\sum \\delta_{x_{i}} \\] <p>where \\(\\delta_{x}\\) denotes the pointmass 1 at \\(x\\). We allow arbitrary probability measures for \\(\\xi\\) [in practice they have to be approximated by a measure of the form (2.10)].</p> <p>For the sake of simplicity we consider only the traditional linear estimates</p> \\[ \\hat{\\alpha}=\\frac{1}{n} \\sum y_{i}, \\quad \\hat{\\beta}=\\frac{\\sum x_{i} y_{i}}{\\sum x_{i}^{2}} \\] <p>based on a symmetric design \\(x_{1}, \\ldots, x_{n}\\). For fixed \\(x_{1}, \\ldots, x_{n}\\) and a linear \\(f\\), these are, of course, the optimal estimates. The restriction to symmetric designs is inessential and can be removed at the cost of some complications; the restriction to linear estimates is more serious and certainly awkward from a point of view of theoretical purity.</p> <p>Then we obtain the following explicit representation of (2.5):</p> \\[ \\begin{aligned} Q(f, \\xi) &amp; =E\\left\\{\\int\\left[f(x)-\\hat{\\alpha}-\\hat{\\beta} x\\right]^{2} d x\\right\\}=Q_{f}+Q_{b}+Q_{v} \\\\ &amp; =Q_{f}+\\left[\\left(\\alpha_{1}-\\alpha_{0}\\right)^{2}+\\frac{\\left(\\beta_{1}-\\beta_{0}\\right)^{2}}{12}\\right]+\\frac{\\sigma^{2}}{n}\\left(1+\\frac{1}{12 \\gamma}\\right) \\end{aligned} \\] <p>with</p> \\[ \\begin{gathered} \\alpha_{1}=E(\\hat{\\alpha})=\\int f(x) d \\xi \\\\ \\beta_{1}=E(\\hat{\\beta})=\\frac{\\int x f(x) d \\xi}{\\int x^{2} d \\xi} \\\\ \\gamma=\\int x^{2} d \\xi \\end{gathered} \\] <p>If \\(f\\) is exactly linear, then \\(Q_{f}=Q_{b}=0\\), and (2.12) is minimized by maximizing \\(\\gamma\\), that is, by putting all mass of \\(\\xi\\) on the extreme points \\(\\pm \\frac{1}{2}\\). Note that the uniform design (where \\(\\xi\\) has the density \\(m \\equiv 1\\) ) corresponds to \\(\\gamma=\\int x^{2} d x=\\frac{1}{12}\\), whereas the \"optimal\" design (all mass on \\(\\pm \\frac{1}{2}\\) ), has \\(\\gamma=\\frac{1}{4}\\).</p> <p>Assume now that the response curve \\(f\\) is only approximately linear, say \\(Q_{f} \\leqslant \\eta\\), where \\(\\eta&gt;0\\) is a small number, and assume that the Statistician plays a game against Nature, with loss function \\(Q(f, \\xi)\\).</p> <p>THEOREM 2.1 The game with loss function \\(Q(f, \\xi), f \\in \\mathscr{F}_{\\eta}=\\left\\{f \\mid Q_{f} \\leqslant \\eta\\right\\}\\), has a saddlepoint \\(\\left(f_{0}, \\xi_{0}\\right)\\) :</p> \\[ Q\\left(f, \\xi_{0}\\right) \\leqslant Q\\left(f_{0}, \\xi_{0}\\right) \\leqslant Q\\left(f_{0}, \\xi\\right) \\] <p>The design measure \\(\\xi_{0}\\) has a density of the form \\(m_{0}(x)=\\left(a x^{2}+b\\right)^{+}\\), and \\(f_{0}\\) is proportional to \\(m_{0}\\) (except that an arbitrary linear function can be added to it).</p> <p>The dependence of \\(\\left(f_{0}, \\xi_{0}\\right)\\) on \\(\\eta\\) can be described in parametric form, with everything depending on the parameter \\(\\gamma\\). If \\(\\frac{1}{12} \\leqslant \\gamma \\leqslant \\frac{3}{20}\\), then \\(\\xi_{0}\\) has the density</p> \\[ m_{0}(x)=1+\\frac{5}{4}(12 \\gamma-1)\\left(12 x^{2}-1\\right) \\] <p>and</p> \\[ f_{0}(x)=\\left(12 x^{2}-1\\right) \\varepsilon \\] <p>with</p> \\[ \\varepsilon^{2}=\\frac{\\sigma^{2}}{n} \\cdot \\frac{1}{2(12 \\gamma)^{2}(12 \\gamma-1)} \\] <p>and</p> \\[ \\eta=\\frac{4}{5} \\varepsilon^{2} \\] <p>If \\(\\frac{3}{20} \\leqslant \\gamma \\leqslant \\frac{1}{4}\\), the solution is much more complicated, and we better change the parameter to \\(c \\in[0,1)\\), with no direct interpretation of \\(c\\). Then</p> \\[ \\begin{aligned} m_{0}(x) &amp; =\\frac{3}{(1+2 c)(1-c)^{2}}\\left(4 x^{2}-c^{2}\\right)^{+} \\\\ \\gamma &amp; =\\frac{3+6 c+4 c^{2}+2 c^{3}}{20(1+2 c)} \\\\ f_{0}(x) &amp; =\\left[m_{0}(x)-1\\right] \\varepsilon \\\\ \\varepsilon^{2} &amp; =\\frac{125(1-c)^{3}(1+2 c)^{5}}{72\\left(3+6 c+4 c^{2}+2 c^{3}\\right)^{2}\\left(1+3 c+6 c^{2}+5 c^{3}\\right)} \\\\ \\eta &amp; =\\frac{25(1-c)^{2}(1+2 c)^{3}}{18\\left(3+6 c+4 c^{2}+2 c^{3}\\right)^{2}} \\end{aligned} \\] <p>In the limit \\(\\gamma=\\frac{1}{4}, c=1\\), the solution degenerates and \\(m_{0}\\) puts pointmasses \\(\\frac{1}{2}\\) at each of the points \\(\\pm \\frac{1}{2}\\).</p> <p>Proof We first keep \\(\\xi\\) fixed and assume it has a density \\(m\\). Then \\(Q(f, \\xi)\\) is maximized by maximizing the bias term</p> \\[ Q_{b}=\\left(\\alpha_{1}-\\alpha_{0}\\right)^{2}+\\frac{\\left(\\beta_{1}-\\beta_{0}\\right)^{2}}{12} \\] <p>Without loss of generality we normalize \\(f\\) such that \\(\\alpha_{0}=\\beta_{0}=0\\). Thus we have to maximize</p> \\[ Q_{b}=\\left(\\int f m d x\\right)^{2}+\\frac{\\left(\\int x f m d x\\right)^{2}}{12 \\gamma^{2}} \\] <p>under the side conditions</p> \\[ \\begin{aligned} \\int f d x &amp; =0 \\\\ \\int x f d x &amp; =0 \\\\ \\int f^{2} d x &amp; =\\eta \\end{aligned} \\] <p>A standard variational argument now shows that the maximizing \\(f\\) must be of the form</p> \\[ f=A \\cdot(m-1)+B \\cdot(m-12 \\gamma) x \\] <p>for some Lagrange multipliers \\(A\\) and \\(B\\). The multipliers have already been adjusted such that this \\(f\\) satisfies the side conditions (2.26) and (2.27). If we insert \\(f\\) into (2.25) and (2.28), we find that we have to maximize</p> \\[ Q_{b}=A^{2}\\left[\\int(m-1)^{2} d x\\right]^{2}+B^{2} \\frac{\\left[\\int(m-12 \\gamma)^{2} x^{2} d x\\right]^{2}}{12 \\gamma^{2}} \\] <p>under the side condition</p> \\[ A^{2} \\int(m-1)^{2} d x+B^{2} \\int(m-12 \\gamma)^{2} x^{2} d x=\\eta \\] <p>This is a linear programming problem (linear in \\(A^{2}\\) and \\(B^{2}\\) ), and the maximum is clearly reached on the boundary \\(A^{2}=0\\) or \\(B^{2}=0\\). According as the upper or the lower inequality holds in</p> \\[ \\int(m-1)^{2} d x \\leqq \\frac{1}{12 \\gamma^{2}} \\int(m-12 \\gamma)^{2} x^{2} d x \\] <p>either \\(B\\) or \\(A\\) is zero; it turns out that in all interesting cases the upper inequality applies, so \\(B=\\beta_{1}=0\\) (this verification is left to the reader). Thus if we solve for \\(A^{2}\\) in (2.31) and insert the solution into (2.30), we obtain an explicit expression for \\(\\sup Q_{b}\\) and hence</p> \\[ \\sup _{f} Q(f, \\xi)=\\eta+\\eta \\int(m-1)^{2} d x+\\frac{\\sigma^{2}}{n}\\left(1+\\frac{1}{12 \\gamma}\\right) \\] <p>We now minimize this under the side conditions</p> \\[ \\begin{gathered} \\int m d x=1 \\\\ \\int x^{2} m d x=\\gamma \\end{gathered} \\] <p>and obtain that</p> \\[ m_{0}(x)=\\left(a x^{2}+b\\right)^{+} \\] <p>for some Lagrange multipliers \\(a\\) and \\(b\\). We verify easily that, for \\(\\frac{1}{12}&lt;\\gamma&lt;\\frac{3}{20}\\), both \\(a\\) and \\(b\\) are \\(\\geqslant 0\\). For \\(\\frac{3}{20}&lt;\\gamma&lt;\\frac{1}{4}\\) we have \\(b&lt;0\\). Finally, we minimize over \\(\\gamma\\), which leads to (2.16) to (2.24).</p> <p>These results need some interpretation and discussion. First, with any minimax procedure there is the question of whether it is too pessimistic and perhaps safeguards only against some very unlikely contingency. This is not the case here; an approximately quadratic disturbance in \\(f\\) is perhaps the one most likely to occur, so (2.17) makes very good sense. But perhaps \\(f_{0}\\) corresponds to such a glaring nonlinearity that nobody in his right mind would want to fit a straight line anyway?</p> <p>To answer this in an objective fashion, we have to construct a most powerful test for distinguishing \\(f_{0}\\) from a straight line.</p> <p>If \\(\\xi\\) is an arbitrary fixed symmetric design, then the most powerful test is based on the test statistic</p> \\[ Z=\\sum y_{i}\\left[f_{0}\\left(x_{i}\\right)-\\bar{f}_{0}\\right] \\] <p>where</p> \\[ \\bar{f}_{0}=\\frac{1}{n} \\sum f_{0}\\left(x_{i}\\right) \\] <p>with \\(f_{0}\\) as in (2.17). Under the hypothesis, \\(E(Z)=0 ; \\operatorname{var}(Z)\\) is the same under the hypothesis and the alternative. We then obtain the signal-to-noise or variance ratio</p> \\[ \\frac{(E Z)^{2}}{\\operatorname{var}(Z)}=\\frac{1}{\\sigma^{2}} \\sum\\left[f_{0}\\left(x_{i}\\right)-\\bar{f}_{0}\\right]^{2}=\\frac{n}{\\sigma^{2}} \\int\\left[f_{0}(x)-\\alpha_{1}\\right]^{2} d \\xi \\] <p>Proof (2.37) We test the hypothesis that \\(f(x) \\equiv \\overline{f_{0}}\\) against the alternative that \\(f(x)=f_{0}(x)\\). The most powerful test is given by the Neyman-Pearson lemma; the logarithm of the likelihood ratio \\(\\Pi\\left[p_{1}\\left(x_{i}\\right) / p_{0}\\left(x_{i}\\right)\\right]\\) is</p> \\[ \\begin{aligned} -\\frac{1}{2} \\sum\\left[\\frac{y_{i}-f_{0}\\left(x_{i}\\right)}{\\sigma}\\right]^{2} &amp; +\\frac{1}{2} \\sum\\left(\\frac{y_{i}-\\bar{f}_{0}}{\\sigma}\\right)^{2} \\\\ &amp; =\\frac{1}{\\sigma^{2}}\\left\\{\\sum y_{i}\\left[f_{0}\\left(x_{i}\\right)-\\bar{f}_{0}\\right]-\\frac{1}{2} \\sum\\left[f_{0}\\left(x_{i}\\right)-\\bar{f}_{0}\\right]^{2}\\right\\} \\end{aligned} \\] <p>In particular, the best design for such a test, giving the highest variance ratio, puts one-half of the observations at \\(x=0\\), and one-quarter at each of the endpoints \\(x= \\pm \\frac{1}{2}\\). The variance ratio is then</p> \\[ \\frac{(E Z)^{2}}{\\operatorname{var}(Z)}=\\frac{9}{4} \\frac{n \\varepsilon^{2}}{\\sigma^{2}} \\] <p>The uniform design \\((m \\equiv 1)\\) gives a variance ratio</p> \\[ \\frac{(E Z)^{2}}{\\operatorname{var}(Z)}=\\frac{4}{5} \\frac{n \\varepsilon^{2}}{\\sigma^{2}} \\] <p>and, finally, the minimax design \\(\\xi_{0}\\) yields</p> \\[ \\frac{(E Z)^{2}}{\\operatorname{var}(Z)}=\\left[\\frac{4}{5}+\\frac{4}{7}(12 \\gamma-1)-(12 \\gamma-1)^{2}\\right] \\frac{n \\varepsilon^{2}}{\\sigma^{2}} \\] <p>Exhibit 9.2.1 gives some numerical values for these variance ratios. Note that: (1) according to (2.18), \\(n \\varepsilon^{2} / \\sigma^{2}\\) is a function of \\(\\gamma\\) alone; and (2) the</p> Variance Ratios \\(\\gamma\\) \\(\\frac{n \\varepsilon^{2}}{\\sigma^{2}}\\) \"Best\"  (2.40) \"Uniform\"  (2.41) \"Minimax \\(\\xi_{0}\\) \"  (2.42) Quotient  (2.42)/(2.41) \\(m_{0}(0)\\) 0.085 24.029 54.066 19.223 19.488 1.014 0.975 0.090 5.358 12.056 4.287 4.497 1.049 0.900 0.095 2.748 6.183 2.198 2.364 1.076 0.825 0.100 1.736 3.906 1.389 1.518 1.093 0.750 0.105 1.211 2.725 0.969 1.067 1.101 0.675 0.110 0.897 2.018 0.717 0.790 1.101 0.600 0.115 0.691 1.555 0.553 0.603 1.091 0.525 0.120 0.548 1.233 0.438 0.470 1.072 0.450 0.125 0.444 1.000 0.356 0.371 1.045 0.375 0.130 0.367 0.825 0.294 0.296 1.008 0.300 0.135 0.307 0.691 0.246 0.237 0.962 0.225 0.140 0.261 0.586 0.208 0.189 0.908 0.150 0.145 0.223 0.502 0.179 0.151 0.844 0.075 0.150 0.193 0.434 0.154 0.119 0.771 0. <p>Exhihit 9.2.1 Variance ratios for tests of linearity against a quadratic alternative. minimax and the uniform design have very similar variance ratios. To give an idea of the shape of the minimax design, its minimal density \\(m_{0}(0)\\) is also shown.</p> <p>From this exhibit we can, for instance, infer that, if \\(\\gamma \\geqslant 0.095\\) and if we use either the uniform or the minimax design, we are not able to see the nonlinearity of \\(f_{0}\\) with any degree of certainty, since the two-sided Neyman -Pearson test with level \\(10 \\%\\) does not even achieve \\(50 \\%\\) power (see Exhibit 9.2.2).</p> <p>To give another illustration let us now take that value of \\(\\varepsilon\\) for which the uniform design ( \\(m \\equiv 1\\) ), minimizing the bias term \\(Q_{b}\\), and the \"optimal\" design, minimizing the variance term \\(Q_{v}\\) by putting all mass on the extreme points of \\(I\\), have the same efficiency. As</p> \\[ \\begin{aligned} &amp; Q\\left(f_{0}, \\text { uni }\\right)=\\int f_{0}^{2} d x+2 \\frac{\\sigma^{2}}{n} \\\\ &amp; Q\\left(f_{0}, \\text { opt }\\right)=\\int f_{0}^{2} d x+(2 \\varepsilon)^{2}+\\frac{4}{3} \\frac{\\sigma^{2}}{n} \\end{aligned} \\] <p>we obtain equality for</p> \\[ \\varepsilon^{2}=\\frac{1}{6} \\frac{\\sigma^{2}}{n} \\] <p>and the variance ratio (2.41) then is</p> \\[ \\frac{(E Z)^{2}}{\\operatorname{var}(Z)}=\\frac{2}{15} \\] Variance Ratio Level \\(\\alpha\\) 1.0 2.0 3.0 4.0 5.0 6.0 9.0 0.01 0.058 0.123 0.199 0.282 0.367 0.450 0.664 0.02 0.093 0.181 0.276 0.372 0.464 0.549 0.750 0.05 0.170 0.293 0.410 0.516 0.609 0.688 0.851 0.10 0.264 0.410 0.535 0.639 0.723 0.790 0.912 0.20 0.400 0.556 0.675 0.764 0.830 0.879 0.957 <p>Exhlbit 9.2.2 Power of two-sided tests, in function of the level and the variance ratio.</p> <p>A variance ratio of about 4 is needed to obtain approximate power \\(50 \\%\\) with a \\(5 \\%\\) test (see Exhibit 9.2.2). Hence (2.46) can be interpreted as follows. Even if the pooled evidence of up to 30 experiments similar to the one under consideration suggests that \\(f_{0}\\) is linear, the uniform design may still be better than the \"optimal\" one and may lead to a smaller expected mean square error!</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#93-minimax-slope","title":"9.3 MINIMAX SLOPE","text":"<p>Conceivably, the situation might be different when we are only interested in estimating the slope \\(\\beta\\). The expected square error in this case is</p> \\[ \\begin{aligned} Q(f, \\xi) &amp; =E\\left(\\hat{\\beta}-\\beta_{0}\\right)^{2}=\\left(\\beta_{1}-\\beta_{0}\\right)^{2}+\\operatorname{var}(\\hat{\\beta}) \\\\ &amp; =\\left(\\frac{\\int x f(x) d x}{\\gamma}\\right)^{2}+\\frac{1}{\\gamma} \\frac{\\sigma^{2}}{n} \\end{aligned} \\] <p>if we standardize \\(f\\) such that \\(\\alpha_{0}=\\beta_{0}=0\\) (using the notation of the preceding section).</p> <p>The game with loss function (3.1) is easy to solve by variational methods similar to those used in the preceding section. For the Statistician the minimax design \\(\\xi_{0}\\) has density</p> \\[ m_{0}(x)=\\frac{1}{(1-2 a)^{2}}\\left(1-\\frac{a^{2}}{x^{2}}\\right)^{+} \\] <p>for some \\(0&lt;a&lt;\\frac{1}{2}\\), and for Nature the minimax strategy is</p> \\[ f_{0}(x) \\sim\\left[m_{0}(x)-12 \\gamma\\right] x \\] <p>We do not work out the details, but we note that \\(f_{0}\\) is crudely similar to a cubic function.</p> <p>For the following heuristics we therefore use a more manageable, and perhaps even more realistic, cubic \\(f\\) :</p> \\[ f(x)=\\left(20 x^{3}-3 x\\right) \\varepsilon \\] <p>This \\(f\\) satisfies \\(\\int f d x=\\int x f d x=0\\) and</p> \\[ \\int f(x)^{2} d x=\\frac{1}{3} \\varepsilon^{2} \\] <p>We now repeat the argumentation used in the last paragraphs of Section 9.2.</p> <p>How large should \\(\\varepsilon\\) be in order that the uniform design and the \"optimal\" design are equally efficient in terms of the risk function (3.1)? As</p> \\[ \\begin{aligned} &amp; Q(f, \\text { uni })=12 \\frac{\\sigma^{2}}{n} \\\\ &amp; Q(f, \\text { opt })=(4 \\varepsilon)^{2}+4 \\frac{\\sigma^{2}}{n} \\end{aligned} \\] <p>we obtain equality if</p> \\[ \\varepsilon^{2}=\\frac{\\sigma^{2}}{2 n} \\] <p>The most powerful test between a linear \\(f\\) and (3.4) has the variance ratio</p> \\[ \\frac{(E Z)^{2}}{\\operatorname{var}(Z)}=\\frac{1}{7} \\frac{n \\varepsilon^{2}}{\\sigma^{2}} \\] <p>If we insert (3.8) this becomes equal to \\(\\frac{1}{14}\\). Thus the situation is even worse than at the end of Section 9.2: even if the pooled evidence of up to 50 experiments similar to the one under consideration suggests that \\(f_{0}\\) is linear, the uniform design (which minimizes bias for a not necessarily linear \\(f\\) ) may still be better than the \"optimal\" design (which minimizes variance, assuming that \\(f\\) is exactly linear)!</p> <p>We conclude from these examples that the so-called optimum design theory (minimizing variance, assuming that the model is exactly correct) is meaningless in a robustness context; we should try rather to minimize bias, assuming that the model is only approximately correct. This had already been recognized by Box and Draper (1959), p. 622: \"The optimal design in typical situations in which both variance and bias occur is very nearly the same as would be obtained if variance were ignored completely and the experiment designed so as to minimize bias alone.\"</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-10","title":"CHAPTER 10","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#exact-finite-sample-results","title":"Exact Finite Sample Results","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#101-general-remarks","title":"10.1 GENERAL REMARKS","text":"<p>Assume that our data contain \\(1 \\%\\) gross errors. Then it makes a tremendous conceptual difference whether the sample size is 1000 or 5 . In the former case each sample will contain around 10 grossly erroneous values, while in the latter, 19 out of 20 samples are good. In particular, it is not at all clear whether conclusions derived from an asymptotic theory remain valid for small samples. Many people are willing to take a \\(5 \\%\\) risk (remember the customary levels of statistical tests and confidence intervals!), and possibly, if we are applying a nonrobust optimal procedure, the gains on the good samples might more than offset the losses caused by an occasional bad sample, especially if we are using a realistic (i.e., bounded) loss function.</p> <p>The main purpose of this chapter is to show that this is not so. We shall find exact, finite sample minimax estimates of location, which, surprisingly, have the same structure as the asymptotically minimax \\(M\\)-estimates found in Chapter 4, and they are even quantitatively comparable.</p> <p>These estimates are derived from minimax robust tests, and thus we have to develop a theory of robust tests.</p> <p>We begin with a discussion of the structure of some of the neighborhoods used to describe approximately specified probabilities; the goal would be to ultimately develop a kind of interval arithmetics for probability measures (e.g., in Bayesian framework, how we step from an approximate prior to an approximate posterior distribution). It appears that alternating capacities of order two, and occasionally of infinite order, are the appropriate tools in these contexts.</p> <p>If (and essentially only if) the inaccuracies can be formulated in terms of alternating capacities of order two, the minimax tests have a very simple structure.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#102-lower-and-upper-probabilities-and-capacities","title":"10.2 LOWER AND UPPER PROBABILITIES AND CAPACITIES","text":"<p>Let \\(\\mathscr{N}\\) be the set of all probability measures on some measurable space \\((\\Omega, \\mathscr{E})\\). We single out four classes of subsets \\(\\mathscr{P} \\subset \\mathscr{N}\\) : those representable through (1) upper expectations, (2) upper probabilities, (3) alternating capacities of order two, (4) alternating capacities of infinite order. Each class contains the following one.</p> <p>Formally, our treatment is restricted to finite sets \\(\\Omega\\), even though all the concepts and a majority of the results are valid for much more general spaces. But if we consider the more general spaces, the important conceptual aspects are buried under a mass of technical complications of a measure theoretic and topological nature.</p> <p>Let \\(\\mathscr{P} \\subset \\mathscr{N}\\) be an arbitrary nonempty subset. We define the lower and the upper expectation induced by \\(\\mathscr{P}\\) as</p> \\[ E_{*}(X)=\\inf _{\\mathscr{P}} \\int X d P, \\quad E^{*}(X)=\\sup _{\\mathscr{P}} \\int X d P \\] <p>and similarly, the lower and the upper probability induced by \\(\\mathscr{P}\\) as</p> \\[ v_{*}(A)=\\inf _{\\mathscr{P}} P(A), \\quad v^{*}(A)=\\sup _{\\mathscr{P}} P(A) \\] <p>\\(E_{*}\\) and \\(E^{*}\\) are nonlinear functionals conjugate to each other in the sense that</p> \\[ E_{*}(X)=-E^{*}(-X) \\] <p>and</p> \\[ v_{*}(A)=1-v^{*}\\left(A^{c}\\right) \\] <p>Conversely, we may start with an arbitrary pair of conjugate functionals ( \\(E_{*}, E^{*}\\) ) or set functions ( \\(v_{*}, v^{*}\\) ) satisfying (2.3) or (2.4), respectively, and define sets \\(\\mathscr{P}\\) by</p> \\[ \\begin{aligned} \\mathscr{P} &amp; =\\left\\{P \\in \\mathscr{N} \\mid \\int X d P \\geqslant E_{*}(X) \\text { for all } X\\right\\} \\\\ &amp; =\\left\\{P \\in \\mathscr{N} \\mid \\int X d P \\leqslant E^{*}(X) \\text { for all } X\\right\\} \\end{aligned} \\] <p>or</p> \\[ \\begin{aligned} \\mathscr{P} &amp; =\\left\\{P \\in \\mathscr{M} \\mid P(A) \\geqslant v_{*}(A) \\text { for all } A\\right\\} \\\\ &amp; =\\left\\{P \\in \\mathscr{M} \\mid P(A) \\leqslant v^{*}(A) \\text { for all } A\\right\\} \\end{aligned} \\] <p>respectively. We note that (2.1), followed by (2.5), does not in general restore \\(\\mathscr{P}\\); nor does (2.5), followed by (2.1), restore ( \\(E_{*}, E^{*}\\) ). But from the second round on, matters stabilize. We say that \\(\\mathscr{P}\\) and ( \\(E_{*}, E^{*}\\) ) represent each other if they mutually induce each other through (2.1) and (2.5).</p> <p>Similarly, we say that \\(\\mathscr{P}\\) and ( \\(v_{*}, v^{*}\\) ) represent each other if they mutually induce each other through (2.2) and (2.6).</p> <p>Obviously, it suffices to look at one member of the respective pairs \\(\\left(E_{*}, E^{*}\\right)\\) and \\(\\left(v_{*}, v^{*}\\right)\\), say \\(E^{*}\\) and \\(v^{*}\\).</p> <p>These notions immediately provoke a few questions: (1) What conditions must ( \\(E_{*}, E^{*}\\) ) satisfy so that it is representable by some \\(\\mathscr{P}\\) ? What conditions must \\(\\mathscr{P}\\) satisfy so that it is representable by some \\(\\left(E_{*}, E^{*}\\right)\\) ? (2) What conditions must ( \\(v_{*}, v^{*}\\) ) satisfy so that it is representable by some \\(\\mathscr{P}\\) ? What conditions must \\(\\mathscr{P}\\) satisfy so that it is representable by some \\(\\left(v_{*}, v^{*}\\right)\\) ? The answer to (1) is very simple. We first note that every representable \\(\\mathscr{P}\\) is closed and convex (since we are working with finite sets \\(\\Omega, \\mathscr{P}\\) can be identified with a subset of the simplex \\(\\left\\{\\left(p_{1}, \\ldots, p_{n}\\right) \\mid \\sum p_{i}=1, p_{i} \\geqslant 0\\right\\}\\), so there is a unique natural topology). On the other hand every representable \\(E^{*}\\) is monotone,</p> \\[ X \\leqslant Y \\Rightarrow E^{*}(X) \\leqslant E^{*}(Y) \\] <p>positively affinely homogeneous,</p> \\[ E^{*}(a X+b)=a E^{*}(X)+b, \\quad a, b \\in \\mathbf{R}, \\quad a \\geqslant 0 \\] <p>and subadditive,</p> \\[ E^{*}(X+Y) \\leqslant E^{*}(X)+E^{*}(Y) \\] <p>\\(E_{*}\\) satisfies the same conditions (2.7) and (2.8), but is superadditive,</p> \\[ E_{*}(X+Y) \\geqslant E_{*}(X)+E_{*}(Y) \\] <p>PROPOSITION 2.1 \\(\\mathscr{P}\\) is representable by an upper expectation \\(E^{*}\\) iff it is closed and convex. Conversely, (2.7), (2.8) and (2.9) are necessary and sufficient for representability of \\(E^{*}\\).</p> <p>Proof Assume that \\(\\mathscr{P}\\) is convex and closed, and define \\(E^{*}\\) by (2.1). \\(E^{*}\\) represents \\(\\mathscr{P}\\) if we can show that, for every \\(Q \\notin \\mathscr{P}\\), there is an \\(X\\) and a real number \\(c\\) such that, for all \\(P \\in \\mathscr{P}, \\int X d P \\leqslant c&lt;f X d Q\\); their existence is in fact guaranteed by one of the well-known separation theorems for convex sets.</p> <p>Now assume that \\(E^{*}\\) is monotone, positively affinely homogeneous, and subadditive. It suffices to show that for every \\(X_{0}\\) there is a probability measure \\(P\\) such that, for all \\(X, \\int X d P \\leqslant E^{*}(X)\\), and \\(\\int X_{0} d P=E^{*}\\left(X_{0}\\right)\\). Because of (2.8) we can assume without any loss of generality that \\(E^{*}\\left(X_{0}\\right)=1\\). Let \\(U=\\left\\{X \\mid E^{*}(X)&lt;1\\right\\}\\). It follows from (2.7) and (2.8) that \\(U\\) is open: with \\(X\\) it also contains all \\(Y\\) such that \\(Y&lt;X+\\varepsilon\\), for \\(\\varepsilon=1-E^{*}(X)\\). Moreover, (2.9) implies that \\(U\\) is convex. Since \\(X_{0} \\notin U\\), there is a linear functional \\(\\lambda\\) separating \\(X_{0}\\) from \\(U\\) :</p> \\[ \\lambda(X)&lt;\\lambda\\left(X_{0}\\right), \\quad \\text { for all } X \\in U \\] <p>With \\(X=0\\) this implies in particular that \\(\\lambda\\left(X_{0}\\right)\\) is strictly positive, and we may normalize \\(\\lambda\\) such that \\(\\lambda\\left(X_{0}\\right)=1=E^{*}\\left(X_{0}\\right)\\). Thus we may write (2.11) as</p> \\[ E^{*}(X)&lt;1 \\Rightarrow \\lambda(X)&lt;1 \\] <p>In view of (2.7) and (2.8), we have</p> \\[ X \\leqslant 0 \\Rightarrow E^{*}(X) \\leqslant E^{*}(0)=0 \\] <p>hence (2.12) implies that, for all \\(c&gt;0, X \\geqslant 0\\), we have</p> \\[ c \\lambda(X)=-\\lambda(-c X)&gt;-1 \\] <p>thus \\(\\lambda(X) \\geqslant-1 / c\\). Hence \\(\\lambda\\) is a positive functional. Moreover, we claim that \\(\\lambda(1)=1\\). First, it follows from (2.12) that \\(\\lambda(c)&lt;1\\) for \\(c&lt;1\\); hence \\(\\lambda(1) \\leqslant 1\\). On the other hand with \\(c&gt;1\\) we have \\(E^{*}\\left(2 X_{0}-c\\right)=2-c&lt;1\\); hence \\(\\lambda\\left(2 X_{0}-c\\right)=2-c \\lambda(1)&lt;1\\), or \\(\\lambda(1)&gt;1 / c\\) for all \\(c&gt;1\\); hence \\(\\lambda(1)=1\\). It follows now from (2.8) and (2.12) that, for all \\(c\\),</p> \\[ E^{*}(X)&lt;c \\Rightarrow \\lambda(X)&lt;c \\] <p>hence \\(\\lambda(X) \\leqslant E^{*}(X)\\) for all \\(X\\), and the probability measure \\(P(A)=\\lambda\\left(1_{A}\\right)\\) is the one we are looking for.</p> <p>Question (2) is trickier. We note first that every representable ( \\(v_{*}, v^{*}\\) ) will satisfy</p> \\[ \\begin{gathered} v_{*}(\\phi)=v^{*}(\\phi)=0, \\quad v_{*}(\\Omega)=v^{*}(\\Omega)=1 \\\\ A \\subset B \\Rightarrow v_{*}(A) \\leqslant v_{*}(B), \\quad v^{*}(A) \\leqslant v^{*}(B) \\\\ v_{*}(A \\cup B) \\geqslant v_{*}(A)+v_{*}(B), \\quad \\text { for } A \\cap B=\\phi \\\\ v^{*}(A \\cup B) \\leqslant v^{*}(A)+v^{*}(B) \\end{gathered} \\] <p>But these conditions are not sufficient for ( \\(v_{*}, v^{*}\\) ) to be representable, as the following counterexample shows.</p> <p>Example 2.1 Let \\(\\Omega\\) have cardinality \\(|\\Omega|=4\\), and assume that \\(v_{*}(A)\\) and \\(v^{*}(A)\\) depend only on the cardinality of \\(A\\), according to the following table:</p> \\(A\\) 0 1 2 3 4 \\(v_{*}\\) 0 0 \\(\\frac{1}{2}\\) \\(\\frac{1}{2}\\) 1 \\(v^{*}\\) 0 \\(\\frac{1}{2}\\) \\(\\frac{1}{2}\\) 1 1 <p>Then \\(\\left(v_{*}, v^{*}\\right)\\) satisfies the above necessary conditions, but there is only a single additive set function between \\(v_{*}\\) and \\(v^{*}\\), namely \\(P(A)=|A| / 4\\); hence \\(\\left(v_{*}, v^{*}\\right)\\) is not representable.</p> <p>Let \\(\\mathscr{P}\\) be any collection of subsets of \\(\\Omega\\), and let \\(v_{*}: \\mathscr{D} \\rightarrow \\mathbb{R}_{+}\\)be an arbitrary nonnegative set function. Let</p> \\[ \\mathscr{P}=\\left\\{P \\in \\mathscr{M} \\mid P(A) \\geqslant v_{*}(A) \\text { for all } A \\in \\mathscr{P}\\right\\} \\] <p>Dually, \\(\\mathscr{P}\\) can also be characterized as</p> \\[ \\mathscr{P}=\\left\\{P \\in \\mathscr{M} \\mid P(B) \\leqslant v^{*}(B) \\text { for all } B \\text { with } B^{c} \\in \\mathscr{P}\\right\\} \\] <p>where \\(v^{*}(B)=1-v_{*}\\left(B^{c}\\right)\\). LEMMA 2.2 The set \\(\\mathscr{P}\\) of (2.17) is not empty iff the following condition holds: whenever</p> \\[ \\sum a_{i} 1_{A_{i}} \\leqslant 1, \\quad a_{i} \\geqslant 0, A_{i} \\in \\mathscr{P} \\] <p>then</p> \\[ \\sum a_{i} v_{*}\\left(A_{i}\\right) \\leqslant 1 \\] <p>Proof The necessity of the condition is obvious. The sufficiency follows from the next lemma.</p> <p>We define functionals</p> \\[ E_{*}(X)=\\sup \\left\\{\\sum a_{i} v_{*}\\left(A_{i}\\right)-a \\mid \\sum a_{i} 1_{A_{i}}-a \\leqslant X, a_{i} \\geqslant 0, A_{i} \\in \\mathscr{D}\\right\\} \\] <p>and \\(E^{*}(X)=-E_{*}(-X)\\), or</p> \\[ E^{*}(X)=\\inf \\left\\{\\sum b_{i} v^{*}\\left(B_{i}\\right)-b \\mid \\sum b_{i} 1_{B_{i}}-b \\geqslant X, b_{i} \\geqslant 0, B_{i}^{c} \\in \\mathscr{D}\\right\\} \\] <p>Put</p> \\[ \\begin{array}{ll} v_{* 0}(A)=E_{*}\\left(1_{A}\\right), &amp; \\text { for } A \\subset \\Omega \\\\ v^{* 0}(A)=E^{*}\\left(1_{A}\\right), &amp; \\text { for } A \\subset \\Omega \\end{array} \\] <p>Clearly, \\(v_{*} \\leqslant v_{* 0}\\) and \\(v^{* 0} \\leqslant v^{*}\\); we verify easily that we obtain the same functionals \\(E_{*}\\) and \\(E^{*}\\) if we replace \\(v_{*}\\) and \\(v^{*}\\) by \\(v_{* 0}\\) and \\(v^{* 0}\\) and \\(\\mathscr{D}\\) by \\(2^{\\Omega}\\) in (2.19) and (2.20).</p> <p>LEMMA 2.3 Let \\(\\mathscr{P}\\) be given by (2.17). If \\(\\mathscr{P}\\) is empty, then \\(E_{*}(X)=\\infty\\) and \\(E^{*}(X)=-\\infty\\) identically for all \\(X\\). Otherwise \\(E_{*}\\) and \\(E^{*}\\) coincide with the lower/upper expectations (2.1) defined by \\(\\mathscr{P}\\), and \\(v_{* 0}\\) and \\(v^{* 0}\\) with the lower/upper probabilities (2.2).</p> <p>Proof We note first that \\(E_{*}(X) \\geqslant 0\\) if \\(X \\geqslant 0\\), and that either \\(E_{*}(0)=0\\), or else \\(E_{*}(X)=\\infty\\) for all \\(X\\). In the latter case \\(\\mathscr{P}\\) is empty (this follows from the necessity part of Lemma 2.2 which has already been proved). In the former case we verify easily that \\(E_{*}\\left(E^{*}\\right)\\) is monotone, positively affinely homogeneous, and superadditive (subadditive, respectively). The definitions imply at once that \\(\\mathscr{P}\\) is contained in the nonempty set \\(\\tilde{\\mathscr{P}}\\) induced by \\(\\left(E_{*}, E^{*}\\right):\\)</p> \\[ \\mathscr{P} \\subset \\tilde{\\mathscr{P}}=\\left\\{P \\in \\mathscr{M} \\mid E_{*}(X) \\leqslant \\int X d P \\leqslant E^{*}(X) \\text { for all } X\\right\\} \\] <p>But on the other hand it follows from \\(v_{*}(A) \\leqslant v_{* 0}(A)\\) and \\(v^{* 0}(A) \\leqslant v^{*}(A)\\) that \\(\\mathscr{P} \\supset \\tilde{\\mathscr{P}}\\); hence \\(\\mathscr{P}=\\tilde{\\mathscr{P}}\\). The assertion of the lemma follows.</p> <p>The sufficiency of the condition in Lemma 2.2 follows at once from the remark that it is equivalent to \\(E_{*}(0) \\leqslant 0\\).</p> <p>PROPOSITION 2.4 (Wolf 1977) A set function \\(v^{*}\\) on \\(\\oplus=2^{\\Omega}\\) is representable by some \\(\\mathscr{P}\\) iff it has the following property: whenever</p> \\[ 1_{A} \\leqslant \\sum a_{i} 1_{A_{i}}-a, \\quad \\text { with } a_{i} \\geqslant 0 \\] <p>then</p> \\[ v^{*}(A) \\leqslant \\sum a_{i} v^{*}\\left(A_{i}\\right)-a \\] <p>The following weaker set of conditions is in fact sufficient: \\(v^{*}\\) is monotone \\(v^{*}(\\phi)=0, v^{*}(\\Omega)=1\\), and (2.24) holds for all decompositions</p> \\[ 1_{A}=\\sum a_{i} 1_{A_{i}} \\] <p>where \\(a_{i}&gt;0\\) when \\(A_{i} \\neq \\Omega\\), and where \\(\\left(1_{A_{1}}, \\ldots, 1_{A_{k}}\\right)\\) is linearly independent. Proof If \\(\\oplus=2^{\\Omega}\\), then \\(v^{*}=v^{* 0}\\) is a necessary and sufficient condition for \\(v^{*}\\) to be representable; this follows immediately from Lemma 2.3. If we spell this out, we obtain (2.23) and (2.24). As (2.23) involves an uncountable infinity of conditions, it is not easy to verify; in the second version (2.25) the number of conditions is still uncomfortably large, but finite [the \\(a_{i}\\) are uniquely determined if the system \\(\\left(1_{A_{1}}, \\ldots, 1_{A_{k}}\\right)\\) is linearly independent].</p> <p>To prove the sufficiency of the second set of conditions, assume to the contrary that (2.24) holds for all decompositions (2.25), but fails for some (2.23). We may assume that we have equality in (2.23)-if not, we can achieve it by decreasing some \\(a_{i}\\) or \\(A_{i}\\), or increasing \\(a\\), on the right-hand side of (2.23). We thus can write (2.23) in the form (2.25), but \\(\\left(1_{A_{1}}, \\ldots, 1_{A_{k}}\\right)\\) then must be linearly dependent. Let \\(k\\) be least possible; then all \\(a_{i} \\neq 0\\), \\(A_{i} \\neq \\phi\\), and \\(a_{i}&gt;0\\) if \\(A_{i} \\neq \\Omega\\). Assume that \\(\\sum c_{i} 1_{A_{i}}=0\\), not all \\(c_{i}=0\\); then \\(1_{A}=\\sum\\left(a_{i}+\\lambda c_{i}\\right) A_{i}\\), for all \\(\\lambda\\). Let \\(\\left[\\lambda_{0}, \\lambda_{1}\\right]\\) be the interval of \\(\\lambda\\)-values for which \\(a_{i}+\\lambda c_{i} \\geqslant 0\\) for all \\(A_{i} \\neq \\Omega\\); clearly, it contains 0 in its interior. Evidently \\(\\sum\\left(a_{i}+\\lambda c_{i}\\right) v^{*}\\left(A_{i}\\right)\\) is a linear function of \\(\\lambda\\), and thus reaches its minimum at one of the endpoints \\(\\lambda_{0}\\) or \\(\\lambda_{1}\\). There, (2.24) is also violated, but \\(k\\) is decreased by at least one. But \\(k\\) was minimal, which leads to a contradiction.</p> <p>This proposition gives at least a partial answer to question (2). Note that, in general, several distinct closed convex sets \\(\\mathscr{P}\\) induce the same \\(v_{*}\\) and \\(v^{*}\\). The set given by (2.6) is the largest among them. Correspondingly, there will be several upper expectations \\(E^{*}\\) inducing \\(v^{*}\\) through \\(v^{*}(A)=E^{*}\\left(1_{A}\\right)\\); (2.20) is the largest one of them, and (2.19) is the smallest lower expectation inducing \\(v_{*}\\).</p> <p>For a given \\(v_{*}\\) and \\(v^{*}\\), there is no simple way to construct the corresponding (extremal) pair \\(E_{*}\\) and \\(E^{*}\\); we can do it either through (2.6) and (2.1) or through (2.19) and (2.20), but either way some awkward suprema and infima are involved.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#2-monotone-and-2-alternating-capacities","title":"2-Monotone and 2-Alternating Capacities","text":"<p>The situation is simplified if \\(v_{*}\\) and \\(v^{*}\\) are a monotone capacity of order two and an alternating capacity of order two, respectively (or short: 2-monotone, 2-alternating), that is, if \\(v_{*}\\) and \\(v^{*}\\), apart from the obvious conditions</p> \\[ \\begin{gathered} v_{*}(\\phi)=v^{*}(\\phi)=0, \\quad v_{*}(\\Omega)=v^{*}(\\Omega)=1 \\\\ A \\subset B \\Rightarrow v_{*}(A) \\leqslant v_{*}(B), \\quad v^{*}(A) \\leqslant v^{*}(B) \\end{gathered} \\] <p>satisfy</p> \\[ \\begin{gathered} v_{*}(A \\cup B)+v_{*}(A \\cap B) \\geqslant v_{*}(A)+v_{*}(B) \\\\ v^{*}(A \\cup B)+v^{*}(A \\cap B) \\leqslant v^{*}(A)+v^{*}(B) \\end{gathered} \\] <p>This seemingly slight strengthening of the assumptions (2.13) to (2.16) has dramatic effects.</p> <p>Assume \\(v^{*}\\) satisfies (2.26) and (2.27), and define a functional \\(E^{*}\\) through</p> \\[ E^{*}(X)=\\int_{0}^{\\infty} v^{*}\\{X&gt;t\\} d t, \\quad \\text { for } X \\geqslant 0 \\] <p>Then \\(E^{*}\\) is monotone and positively affinely homogeneous, as we verify easily; with the help of (2.8) it can be extended to all \\(X\\). [Note that, if the construction (2.30) is applied to a probability measure, we obtain the expectation:</p> \\[ \\int_{0}^{\\infty} P\\{X&gt;t\\} d t=\\int X d P, \\quad \\text { for } X \\geqslant 0 .] \\] <p>Similarly, define \\(E_{*}\\), with \\(v_{*}\\) in place of \\(v^{*}\\). PROPOSITION 2.5 The functional \\(E^{*}\\), defined by (2.30), is subadditive iff \\(v^{*}\\) satisfies (2.29). [Similarly, \\(E_{*}\\) is superadditive iff \\(v_{*}\\) satisfies (2.28)].</p> <p>Proof Assume that \\(E^{*}\\) is subadditive, then</p> \\[ E^{*}\\left(1_{A}+1_{B}\\right)=v^{*}(A \\cup B)+v^{*}(A \\cap B) \\] <p>and</p> \\[ E^{*}\\left(1_{A}\\right)+E^{*}\\left(1_{B}\\right)=v^{*}(A)+v^{*}(B) \\] <p>Hence if \\(E^{*}\\) is subadditive, (2.29) holds. The other direction is more difficult to establish. We first note that (2.29) is equivalent to</p> \\[ E^{*}(X \\vee Y)+E^{*}(X \\wedge Y) \\leqslant E^{*}(X)+E^{*}(Y), \\quad \\text { for } X, Y \\geqslant 0 \\] <p>where \\(X \\vee Y\\) and \\(X \\wedge Y\\) stand for the pointwise supremum and infimum of the two functions \\(X\\) and \\(Y\\). This follows at once from</p> \\[ \\begin{aligned} &amp; \\{X&gt;t\\} \\cup\\{Y&gt;t\\}=\\{X \\vee Y&gt;t\\} \\\\ &amp; \\{X&gt;t\\} \\cap\\{Y&gt;t\\}=\\{X \\wedge Y&gt;t\\} \\end{aligned} \\] <p>Since \\(\\Omega\\) is a finite set, \\(X\\) is a vector \\(\\mathbf{x}=\\left(x_{1}, \\ldots, x_{n}\\right)\\), and \\(E^{*}\\) is a function of \\(n\\) real variables. The proposition now follows from the following lemma.</p> <p>LEMMA 2.6 (Choquet) If \\(f\\) is a positively homogeneous function on \\(\\mathbb{R}_{+}^{n}\\)</p> \\[ f(c \\mathbf{x})=c f(\\mathbf{x}), \\quad \\text { for } c \\geqslant 0 \\] <p>satisfying</p> \\[ f(\\mathbf{x} \\vee \\mathbf{y})+f(\\mathbf{x} \\wedge \\mathbf{y}) \\leqslant f(\\mathbf{x})+f(\\mathbf{y}) \\] <p>then \\(f\\) is subadditive:</p> \\[ f(\\mathbf{x}+\\mathbf{y}) \\leqslant f(\\mathbf{x})+f(\\mathbf{y}) \\] <p>Proof Assume that \\(f\\) is twice continuously differentiable for \\(\\mathbf{x} \\neq 0\\). Let</p> \\[ \\begin{aligned} &amp; \\mathbf{a}=\\left(x_{1}+h_{1}, x_{2}, \\ldots, x_{n}\\right) \\\\ &amp; \\mathbf{b}=\\left(x_{1}, x_{2}+h_{2}, \\ldots, x_{n}+h_{n}\\right) \\end{aligned} \\] <p>with \\(h_{i} \\geqslant 0\\); then \\(\\mathbf{a} \\vee \\mathbf{b}=\\mathbf{x}+\\mathbf{h}, \\mathbf{a} \\wedge \\mathbf{b}=\\mathbf{x}\\). If we expand (2.33) into a power series in the \\(h_{i}\\), we find that the second order terms must satisfy</p> \\[ \\sum_{j \\neq 1} f_{x_{i} x_{j}} h_{1} h_{j} \\leqslant 0 \\] <p>hence</p> \\[ f_{x_{1} x_{j}} \\leqslant 0, \\quad \\text { for } \\quad j \\neq 1 \\] <p>and more generally</p> \\[ f_{x_{i} x_{j}} \\leqslant 0, \\quad \\text { for } \\quad i \\neq j \\] <p>Differentiate (2.32) with respect to \\(x_{j}\\) :</p> \\[ c f_{x_{i}}(c \\mathbf{x})=c f_{x_{j}}(\\mathbf{x}) \\] <p>divide by \\(c\\), and then differentiate with respect to \\(c\\) :</p> \\[ \\sum_{i} x_{i} f_{x_{i} x_{j}}=0 \\] <p>If \\(F\\) denotes the sum of the second order terms in the Taylor expansion of \\(f\\) at \\(\\mathbf{x}\\), we obtain thus</p> \\[ 2 F=-\\sum_{i \\neq j} x_{i} x_{j} f_{x_{i} x_{j}}\\left(\\frac{d x_{i}}{x_{i}}-\\frac{d x_{j}}{x_{j}}\\right)^{2} \\geqslant 0 \\] <p>It follows that \\(f\\) is convex, and because of (2.32), this is equivalent with being subadditive.</p> <p>If \\(f\\) is not twice continuously differentiable, we must approximate it in a suitable fashion.</p> <p>In view of Proposition 2.1, we thus obtain that \\(E^{*}\\) is the upper expectation induced by the set</p> \\[ \\begin{aligned} \\mathscr{P} &amp; =\\left\\{P \\in \\mathscr{M} \\mid \\int X d P \\leqslant E^{*}(X) \\text { for all } X\\right\\} \\\\ &amp; =\\left\\{P \\in \\mathscr{M} \\mid P(A) \\leqslant v^{*}(A) \\text { for all } A\\right\\} \\end{aligned} \\] <p>Hence every 2-alternating \\(v^{*}\\) is representable, and the corresponding maximal upper expectation is given by (2.30). In particular, (2.30) implies that, for any monotone sequence \\(A_{1} \\subset A_{2} \\subset \\cdots \\subset A_{k}\\), it is possible to find a probability \\(Q \\leqslant v^{*}\\) such that, for all \\(i\\), simultaneously \\(Q\\left(A_{i}\\right)=v^{*}\\left(A_{i}\\right)\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#monotone-and-alternating-capacities-of-infinite-order","title":"Monotone and Alternating Capacities of Infinite Order","text":"<p>Consider the following generalized gross error model: let ( \\(\\Omega^{\\prime}, \\mathscr{Q}^{\\prime}, P^{\\prime}\\) ) be some probability space, assign to each \\(\\omega^{\\prime} \\in \\Omega^{\\prime}\\) a nonempty subset \\(T\\left(\\omega^{\\prime}\\right) \\subset \\Omega\\),</p> <p>and put</p> \\[ \\begin{aligned} &amp; v_{*}(A)=P^{\\prime}\\left\\{\\omega^{\\prime} \\mid T\\left(\\omega^{\\prime}\\right) \\subset A\\right\\} \\\\ &amp; v^{*}(A)=P^{\\prime}\\left\\{\\omega^{\\prime} \\mid T\\left(\\omega^{\\prime}\\right) \\cap A \\neq \\phi\\right\\} \\end{aligned} \\] <p>We can easily check that \\(v_{*}\\) and \\(v^{*}\\) are conjugate set functions. The interpretation is that, instead of the ideal but unobservable outcome \\(\\omega^{\\prime}\\) of the random experiment, the statistician is shown an arbitrary (not necessarily randomly chosen) element of \\(T\\left(\\omega^{\\prime}\\right)\\). Clearly, \\(v_{*}(A)\\) and \\(v^{*}(A)\\) are lower and upper bounds for the probability that the statistician is shown an element of \\(A\\).</p> <p>It is intuitively clear that \\(v_{*}\\) and \\(v^{*}\\) are representable; it is easy to check that they are 2 -monotone and 2 -alternating, respectively. In fact a much stronger statement is true: they are monotone (alternating) of infinite order. We do not define this notion here, but refer the reader to Choquet's fundamental paper (1953/54); by a theorem of Choquet, a capacity is monotone/alternating of infinite order iff it can be generated in the forms (2.35) and (2.36), respectively.</p> <p>Example 2.2 Let \\(Y\\) and \\(U\\) be two independent real random variables; the first has the idealized distribution \\(P_{0}\\), and the second takes two values \\(\\delta \\geqslant 0\\) and \\(+\\infty\\) with probability \\(1-\\varepsilon\\) and \\(\\varepsilon\\), respectively. Let \\(T\\) be the intervalvalued set function defined by</p> \\[ T\\left(\\omega^{\\prime}\\right)=\\left[Y\\left(\\omega^{\\prime}\\right)-U\\left(\\omega^{\\prime}\\right), Y\\left(\\omega^{\\prime}\\right)+U\\left(\\omega^{\\prime}\\right)\\right] \\] <p>Then with probability \\(\\geqslant 1-\\varepsilon\\), the statistician is shown a value \\(x\\) that is accurate within \\(\\delta\\), that is, \\(\\left|x-Y\\left(\\omega^{\\prime}\\right)\\right| \\leqslant \\delta\\), and with probability \\(\\leqslant \\varepsilon\\), he is shown a value containing a gross error.</p> <p>The generalized gross error model, using monotone and alternating set functions of infinite order, was introduced by Strassen (1964). There has been a considerable literature on set valued stochastic processes \\(T\\left(\\omega^{\\prime}\\right)\\) in recent years; in particular, see Harding and Kendall (1974) and Matheron (1975). In a statistical context monotone/alternating capacities of infinite order were used by Dempster (1968) and Shafer (1976). The following example shows another application of such capacities [taken from Huber (1973b)].</p> <p>Example 2.3 Let \\(\\alpha_{0}\\) be a probability distribution (the idealized prior) on a finite parameter space \\(\\Theta\\). The gross error or \\(\\varepsilon\\)-contamination model</p> \\[ \\mathscr{P}=\\left\\{\\alpha \\mid \\alpha=(1-\\varepsilon) \\alpha_{0}+\\varepsilon \\alpha_{1}, \\alpha_{1} \\in \\mathscr{N}\\right\\} \\] <p>can be described by an alternating capacity of infinite order, namely,</p> \\[ \\begin{aligned} \\sup _{\\alpha \\in \\mathscr{Y}} \\alpha(A) &amp; =(1-\\varepsilon) \\alpha_{0}(A)+\\varepsilon, &amp; &amp; \\text { for } A \\neq \\phi \\\\ &amp; =0, &amp; &amp; \\text { for } A=\\phi \\end{aligned} \\] <p>Let \\(p(x \\mid \\theta)\\) be the conditional probability of observing \\(x\\), given that \\(\\theta\\) is true; \\(p(x \\mid \\theta)\\) is assumed to be accurately known. Let</p> \\[ \\beta(\\theta \\mid x)=\\frac{p(x \\mid \\theta) \\alpha(\\theta)}{\\sum_{\\theta} p(x \\mid \\theta) \\alpha(\\theta)} \\] <p>be the posterior distribution of \\(\\theta\\), given that \\(x\\) has been observed; let \\(\\beta_{0}(\\theta \\mid x)\\) be the posterior calculated with the prior \\(\\alpha_{0}\\).</p> <p>The inaccuracy in the prior is transmitted to the posterior:</p> \\[ \\begin{aligned} v^{*}(A) &amp; =\\sup _{\\alpha \\in \\mathscr{Y}} \\beta(A \\mid x)=\\frac{\\beta_{0}(A \\mid x)+s(A)}{1+s(A)} \\\\ v_{*}(A) &amp; =\\inf _{\\alpha \\in \\mathscr{Y}} \\beta(A \\mid x)=\\frac{\\beta_{0}(A \\mid x)}{1+s\\left(A^{\\varepsilon}\\right)} \\end{aligned} \\] <p>where</p> \\[ \\begin{aligned} s(A) &amp; =\\frac{\\varepsilon}{1-\\varepsilon} \\frac{\\sup _{\\theta \\in A} p(x \\mid \\theta)}{\\sum_{\\theta} p(x \\mid \\theta) \\alpha_{0}(\\theta)}, &amp; &amp; \\text { for } A \\neq \\phi \\\\ &amp; =0, &amp; &amp; \\text { for } A=\\phi \\end{aligned} \\] <p>Then \\(s\\) satisfies \\(s(A \\cup B)=\\max (s(A), s(B))\\) and is alternating of infinite order. I do not know the exact order of \\(v^{*}\\) (it is at least 2-alternating).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#103-robust-tests","title":"10.3 ROBUST TESTS","text":"<p>The classical probability ratio test between two simple hypotheses \\(P_{0}\\) and \\(P_{1}\\) is not robust: a single factor \\(p_{1}\\left(x_{i}\\right) / p_{0}\\left(x_{i}\\right)\\), equal or almost equal to 0 or \\(\\infty\\), may upset the test statistic \\(\\prod_{1}^{n} p_{1}\\left(x_{i}\\right) / p_{0}\\left(x_{i}\\right)\\). This danger can be averted by censoring the single factors, that is by replacing the test statistic</p> <p>by \\(\\Pi_{1}^{n} \\pi\\left(x_{i}\\right)\\), where \\(\\pi\\left(x_{i}\\right)=\\max \\left\\{c^{\\prime}, \\min \\left[c^{\\prime \\prime}, p_{1}\\left(x_{i}\\right) / p_{0}\\left(x_{i}\\right)\\right]\\right\\}\\), with \\(0&lt;c^{\\prime}&lt;c^{\\prime \\prime}\\) \\(&lt;\\infty\\).</p> <p>Somewhat surprisingly, it turns out that this test possesses exact finite sample minimax properties for a wide variety of models: tests of the above structure are minimax for testing between composite hypotheses \\(\\mathscr{P}_{0}\\) and \\(\\mathscr{P}_{1}\\), where \\(\\mathscr{P}_{j}\\) is a neighborhood of \\(P_{j}\\) in \\(\\varepsilon\\)-contamination, or total variation, and so on.</p> <p>In principle \\(P_{0}\\) and \\(P_{1}\\) can be arbitrary probability measures on arbitrary measurable spaces [cf. Huber (1965)]. But in order to prepare the ground for Section 10.5, from now on we assume that they are probability distributions on the real line. In fact very little generality is lost this way, since almost everything admits a reinterpretation in terms of the real random variable \\(p_{1}(X) / p_{0}(X)\\), under various distributions of \\(X\\).</p> <p>Let \\(P_{0}\\) and \\(P_{1}, P_{0} \\neq P_{1}\\), be two probability measures on the real line. Let \\(p_{0}\\) and \\(p_{1}\\) be their densities with respect to some measure \\(\\mu\\) (e.g., \\(\\mu=P_{0}+P_{1}\\) ), and assume that the likelihood ratio \\(p_{1}(x) / p_{0}(x)\\) is almost surely (with respect to \\(\\mu\\) ) equal to a monotone function \\(c(x)\\).</p> <p>Let \\(\\mathscr{O}\\) be the set of all probability measures on the real line, let \\(0 \\leqslant \\varepsilon_{0}, \\varepsilon_{1}, \\delta_{0}, \\delta_{1}&lt;1\\) be some given numbers, and let</p> \\[ \\begin{aligned} &amp; \\mathscr{P}_{0}=\\left\\{Q \\in \\mathscr{O} \\mid Q\\{X&lt;x\\} \\geqslant\\left(1-\\varepsilon_{0}\\right) P_{0}\\{X&lt;x\\}-\\delta_{0} \\text { for all } x\\right\\} \\\\ &amp; \\mathscr{P}_{1}=\\left\\{Q \\in \\mathscr{O} \\mid Q\\{X&gt;x\\} \\geqslant\\left(1-\\varepsilon_{1}\\right) P_{1}\\{X&gt;x\\}-\\delta_{1} \\text { for all } x\\right\\} \\end{aligned} \\] <p>We assume that \\(\\mathscr{P}_{0}\\) and \\(\\mathscr{P}_{1}\\) are disjoint (i.e., that \\(\\varepsilon_{j}\\) and \\(\\delta_{j}\\) are sufficiently small).</p> <p>It may help to visualize \\(\\mathscr{P}_{0}\\) as the set of distribution functions lying above the solid line \\(\\left(1-\\varepsilon_{0}\\right) P_{0}(x)-\\delta_{0}\\) in Exhibit 10.3.1 and \\(\\mathscr{P}_{1}\\) as the set of distribution functions lying below the dotted line \\(\\left(1-\\varepsilon_{1}\\right) P_{1}(x)+\\varepsilon_{1}+\\delta_{1}\\). As before \\(P\\{\\cdot\\}\\) denotes the set function, \\(P(\\cdot)\\) the corresponding distribution function: \\(P(x)=P\\{(-\\infty, x)\\}\\).</p> <p>Now let \\(\\varphi\\) be any (randomized) test between \\(\\mathscr{P}_{0}\\) and \\(\\mathscr{P}_{1}\\), rejecting \\(\\mathscr{P}_{j}\\) with conditional probability \\(\\varphi_{j}(\\mathbf{x})\\) given that \\(\\mathbf{x}=\\left(x_{1}, \\ldots, x_{n}\\right)\\) has been observed. </p> <p>Exhlbit 10.3.1</p> <p>Assume that a loss \\(L_{j}&gt;0\\) is incurred if \\(\\mathscr{P}_{j}\\) is falsely rejected; then the expected loss, or risk, is</p> \\[ R\\left(Q_{j}^{\\prime}, \\varphi\\right)=L_{j} E_{Q_{j}^{\\prime}}\\left(\\varphi_{j}\\right) \\] <p>if \\(Q_{j}^{\\prime} \\in \\mathscr{P}_{j}\\) is the true underlying distribution. The problem is to find a minimax test, that is, to minimize</p> \\[ \\max _{j=0,1} \\sup _{Q_{j}^{\\prime} \\in \\mathscr{P}_{j}^{\\prime}} R\\left(Q_{i}^{\\prime}, \\varphi\\right) \\] <p>These minimax tests happen to have quite a simple structure in our case. There is a least favorable pair \\(Q_{0} \\in \\mathscr{P}_{0}, Q_{1} \\in \\mathscr{P}_{1}\\), such that, for all sample sizes, the probability ratio tests \\(\\varphi\\) between \\(Q_{0}\\) and \\(Q_{1}\\) satisfy</p> \\[ R\\left(Q_{j}^{\\prime}, \\varphi\\right) \\leqslant R\\left(Q_{j}, \\varphi\\right), \\quad \\text { for } Q_{j}^{\\prime} \\in \\mathscr{P}_{j} \\] <p>Thus in view of the Neyman-Pearson lemma, the probability ratio tests between \\(Q_{0}\\) and \\(Q_{1}\\) form an essentially complete class of minimax tests between \\(\\mathscr{P}_{0}\\) and \\(\\mathscr{P}_{1}\\). The pair \\(Q_{0}, Q_{1}\\) is not unique, in general, but the probability ratio \\(d Q_{1} / d Q_{0}\\) is essentially unique; as already mentioned it will be a censored version of \\(d P_{1} / d P_{0}\\).</p> <p>It is in fact quite easy to guess such a pair \\(Q_{0}, Q_{1}\\). The successful conjecture is that there are two numbers \\(x_{0}&lt;x_{1}\\), such that the \\(Q_{j}(\\cdot)\\) coincide with the respective boundaries between \\(x_{0}\\) and \\(x_{1}\\); in particular, their densities thus will satisfy</p> \\[ q_{j}(x)=\\left(1-\\varepsilon_{j}\\right) p_{j}(x), \\quad \\text { for } x_{0} \\leqslant x \\leqslant x_{1} \\] <p>On \\(\\left(-\\infty, x_{0}\\right)\\) and on \\(\\left(x_{1}, \\infty\\right)\\), we expect the likelihood ratios to be constant, and we try densities of the form</p> \\[ q_{j}(x)=a p_{0}(x)+b p_{1}(x) \\] <p>The various internal consistency requirements, in particular that</p> \\[ \\begin{array}{ll} Q_{0}(x)=\\left(1-\\varepsilon_{0}\\right) P_{0}(x)-\\delta_{0}, &amp; \\text { for } x_{0} \\leqslant x \\leqslant x_{1} \\\\ Q_{1}(x)=\\left(1-\\varepsilon_{1}\\right) P_{1}(x)+\\varepsilon_{1}+\\delta_{1}, &amp; \\text { for } x_{0} \\leqslant x \\leqslant x_{1} \\end{array} \\] <p>now lead easily to the following explicit formulas (we skip the step-by-step derivation, just stating the final results and then checking them).</p> <p>Put</p> \\[ \\begin{array}{ll} v^{\\prime}=\\frac{\\varepsilon_{1}+\\delta_{1}}{1-\\varepsilon_{1}}, &amp; v^{\\prime \\prime}=\\frac{\\varepsilon_{0}+\\delta_{0}}{1-\\varepsilon_{0}} \\\\ w^{\\prime}=\\frac{\\delta_{0}}{1-\\varepsilon_{0}}, &amp; w^{\\prime \\prime}=\\frac{\\delta_{1}}{1-\\varepsilon_{1}} \\end{array} \\] <p>It turns out to be somewhat more convenient to characterize the middle interval between \\(x_{0}\\) and \\(x_{1}\\) in terms of \\(c(x)\\) than in terms of the \\(x\\) themselves: \\(c^{\\prime}&lt;c(x)&lt;1 / c^{\\prime \\prime}\\) for some constants \\(c^{\\prime}\\) and \\(c^{\\prime \\prime}\\), which are determined later. Since \\(c(x)\\) need not be continuous or strictly monotone, the two variants are not entirely equivalent.</p> <p>If both \\(v^{\\prime}&gt;0\\) and \\(v^{\\prime \\prime}&gt;0\\), we define \\(Q_{0}\\) and \\(Q_{1}\\) by their densities, as follows. Denote the three regions \\(c(x) \\leqslant c^{\\prime}, c^{\\prime}&lt;c(x)&lt;1 / c^{\\prime \\prime}\\), and \\(1 / c^{\\prime \\prime} \\leqslant\\) \\(c(x)\\) by \\(I_{-}, I_{0}\\), and \\(I_{+}\\), respectively. Then</p> \\[ \\begin{aligned} q_{0}(x) &amp; =\\frac{1-\\varepsilon_{0}}{v^{\\prime}+w^{\\prime} c^{\\prime}}\\left[v^{\\prime} p_{0}(x)+w^{\\prime} p_{1}(x)\\right], &amp; &amp; \\text { on } I_{-} \\\\ &amp; =\\left(1-\\varepsilon_{0}\\right) p_{0}(x), &amp; &amp; \\text { on } I_{0} \\\\ &amp; =\\frac{\\left(1-\\varepsilon_{0}\\right) c^{\\prime \\prime}}{v^{\\prime \\prime}+w^{\\prime \\prime} c^{\\prime \\prime}}\\left[w^{\\prime \\prime} p_{0}(x)+v^{\\prime \\prime} p_{1}(x)\\right], &amp; &amp; \\text { on } I_{+} \\\\ q_{1}(x) &amp; =\\frac{\\left(1-\\varepsilon_{1}\\right) c^{\\prime}}{v^{\\prime}+w^{\\prime} c^{\\prime}}\\left[v^{\\prime} p_{0}(x)+w^{\\prime} p_{1}(x)\\right], &amp; &amp; \\text { on } I_{-} \\\\ &amp; =\\left(1-\\varepsilon_{1}\\right) p_{1}(x), &amp; &amp; \\text { on } I_{0} \\\\ &amp; =\\frac{\\left(1-\\varepsilon_{1}\\right)}{v^{\\prime \\prime}+w^{\\prime \\prime} c^{\\prime \\prime}}\\left[w^{\\prime \\prime} p_{0}(x)+v^{\\prime \\prime} p_{1}(x)\\right], &amp; &amp; \\text { on } I_{+} \\end{aligned} \\] <p>If, say, \\(v^{\\prime}=0\\), then \\(w^{\\prime \\prime}=0\\), and the above formulas simplify to</p> \\[ \\begin{aligned} q_{0}(x) &amp; =\\left(1-\\varepsilon_{0}\\right) \\frac{1}{c^{\\prime}} p_{1}(x), &amp; &amp; \\text { on } I_{-} \\\\ &amp; =\\left(1-\\varepsilon_{0}\\right) p_{0}(x), &amp; &amp; \\text { on } I_{0} \\\\ &amp; =\\left(1-\\varepsilon_{0}\\right) c^{\\prime \\prime} p_{1}(x), &amp; &amp; \\text { on } I_{+} \\\\ q_{1}(x) &amp; =p_{1}(x), &amp; &amp; \\text { for all } x \\end{aligned} \\] <p>It is evident from (3.6) [and (3.7)] that the likelihood ratio has the postulated form</p> \\[ \\begin{aligned} \\pi(x)=\\frac{q_{1}(x)}{q_{0}(x)} &amp; =\\frac{1-\\varepsilon_{1}}{1-\\varepsilon_{0}} c^{\\prime}, &amp; &amp; \\text { on } I_{-} \\\\ &amp; =\\frac{1-\\varepsilon_{1}}{1-\\varepsilon_{0}} c(x), &amp; &amp; \\text { on } I_{0} \\\\ &amp; =\\frac{1-\\varepsilon_{1}}{1-\\varepsilon_{0}} \\cdot \\frac{1}{c^{\\prime \\prime}}, &amp; &amp; \\text { on } I_{+} \\end{aligned} \\] <p>Moreover, since \\(p_{1}(x) / p_{0}(x)=c(x)\\) is monotone, (3.6) implies</p> \\[ \\begin{array}{ll} q_{0}(x) \\leqslant\\left(1-\\varepsilon_{0}\\right) p_{0}(x), &amp; \\text { on } I_{-} \\\\ q_{0}(x) \\geqslant\\left(1-\\varepsilon_{0}\\right) p_{0}(x), &amp; \\text { on } I_{+} \\end{array} \\] <p>and dual relations hold for \\(q_{1}\\). In view of (3.9) we have \\(Q_{j} \\in \\mathscr{P}_{j}\\), with \\(Q_{j}(\\cdot)\\) touching the boundary between \\(x_{0}\\) and \\(x_{1}\\) if four relations hold, the first of which is</p> \\[ \\int_{c(x)&lt;c^{\\prime}}\\left[\\left(1-\\varepsilon_{0}\\right) p_{0}(x)-q_{0}(x)\\right] d \\mu=\\delta_{0} \\] <p>The other three are obtained by interchanging left and right, and the roles of \\(P_{0}\\) and \\(P_{1}\\).</p> <p>If we insert (3.6) into (3.10), we obtain the equivalent condition</p> \\[ \\int\\left[c^{\\prime} p_{0}(x)-p_{1}(x)\\right]^{+} d \\mu=v^{\\prime}+w^{\\prime} c^{\\prime} \\] <p>Of the other three relations, one coincides with (3.11), the other two with</p> \\[ \\int\\left[c^{\\prime \\prime} p_{1}(x)-p_{0}(x)\\right]^{+} d \\mu=v^{\\prime \\prime}+w^{\\prime \\prime} c^{\\prime \\prime} \\] <p>We must now show that (3.11) and (3.12) have solutions \\(c^{\\prime}\\) and \\(c^{\\prime \\prime}\\), respectively. Evidently, it suffices to discuss (3.11).</p> <p>If \\(v^{\\prime}=0\\), we have the trivial solution \\(c^{\\prime}=0\\) (and perhaps also some others). Let us exclude this case and put</p> \\[ f(z)=\\frac{\\int\\left(z p_{0}-p_{1}\\right)^{+} d \\mu}{v^{\\prime}+w^{\\prime} z}, \\quad z \\geqslant 0 \\] <p>We have to find a \\(z\\) such that \\(f(z)=1\\). Let \\(\\Delta \\geqslant 0\\), then</p> \\[ f(z+\\Delta)-f(z)=\\frac{\\Delta \\int_{E}\\left(v^{\\prime}+w^{\\prime} c\\right) p_{0} d \\mu+\\int_{E^{\\prime}}\\left(v^{\\prime}+w z\\right)(z+\\Delta-c) p_{0} d \\mu}{\\left(v^{\\prime}+w^{\\prime} z\\right)\\left[v^{\\prime}+w^{\\prime}(z+\\Delta)\\right]} \\] <p>with</p> \\[ E=\\{x \\mid c(x) \\leqslant z\\}, \\quad E^{\\prime}=\\{x \\mid z&lt;c(x) \\leqslant z+\\Delta\\} \\] <p>Hence</p> \\[ 0 \\leqslant f(z+\\Delta)-f(z) \\leqslant \\frac{\\Delta}{v^{\\prime}+w^{\\prime}(z+\\Delta)} \\] <p>and it follows that \\(f\\) is monotone increasing and continuous. As \\(z \\rightarrow \\infty, f(z) \\rightarrow 1 / w^{\\prime}\\), and as \\(z \\rightarrow 0, f(z) \\rightarrow 0\\). Thus there is a solution \\(c^{\\prime}\\) for which \\(f\\left(c^{\\prime}\\right)=1\\), provided \\(w^{\\prime}&lt;1\\). (Note that \\(w^{\\prime} \\geqslant 1\\) implies \\(\\mathscr{P}_{0}=\\mathscr{P R}\\); hence \\(\\mathscr{P}_{0} \\cap \\mathscr{P}_{1}=\\phi\\) ensures \\(\\left.w^{\\prime}&lt;1.\\right)\\)</p> <p>It can be seen from (3.11) and (3.14) that \\(f(z)\\) is strictly monotone for</p> \\[ z&gt;c_{1}=\\text { ess. } \\inf c(x) \\] <p>Since \\(f(z)=0\\) for \\(0 \\leqslant z \\leqslant c_{1}\\), the solution \\(c^{\\prime}\\) is unique. We can write the likelihood ratio between \\(Q_{0}\\) and \\(Q_{1}\\) in the form</p> \\[ \\frac{q_{1}(x)}{q_{0}(x)}=\\frac{1-\\varepsilon_{1}}{1-\\varepsilon_{0}} \\tilde{\\pi}(x) \\] <p>with</p> \\[ \\begin{aligned} \\tilde{\\pi}(x) &amp; =c^{\\prime}, &amp; &amp; \\text { on } I_{-} \\\\ &amp; =c(x), &amp; &amp; \\text { on } I_{0} \\\\ &amp; =1 / c^{\\prime \\prime}, &amp; &amp; \\text { on } I_{+} \\end{aligned} \\] <p>(assuming that \\(c^{\\prime}&lt;1 / c^{\\prime \\prime}\\) ).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#lemma-31","title":"LEMMA 3.1","text":"\\[ \\begin{array}{ll} Q_{0}^{\\prime}(\\tilde{\\pi}&lt;t) \\geqslant Q_{0}(\\tilde{\\pi}&lt;t), &amp; \\text { for } Q_{0}^{\\prime} \\in \\mathscr{P}_{0} \\\\ Q_{1}^{\\prime}(\\tilde{\\pi}&lt;t) \\leqslant Q_{1}(\\tilde{\\pi}&lt;t), &amp; \\text { for } Q_{1}^{\\prime} \\in \\mathscr{P}_{1} \\end{array} \\] <p>Proof These relations are trivially true for \\(t \\leqslant c^{\\prime}\\) and for \\(t&gt;1 / c^{\\prime \\prime}\\). For \\(c^{\\prime}&lt;t&lt;1 / c^{\\prime \\prime}\\) they boil down to the inequalities in (3.1).</p> <p>In other words among all distributions in \\(\\mathscr{P}_{0}, \\tilde{\\pi}\\) is stochastically largest for \\(Q_{0}\\), and among all distributions in \\(\\mathscr{P}_{1}, \\tilde{\\pi}\\) is stochastically smallest for \\(Q_{1}\\).</p> <p>THEOREM 3.2 For any sample size \\(n\\) and any level \\(\\alpha\\), the NeymanPearson test of level \\(\\alpha\\) between \\(Q_{0}\\) and \\(Q_{1}\\), namely</p> \\[ \\begin{aligned} \\varphi(\\mathbf{x}) &amp; =1, &amp; &amp; \\text { for } \\quad \\Pi_{1}^{n} \\tilde{\\pi}\\left(x_{i}\\right)&gt;C \\\\ &amp; =\\gamma, &amp; &amp; \\text { for } \\quad \\Pi_{1}^{n} \\tilde{\\pi}\\left(x_{i}\\right)=C \\\\ &amp; =0, &amp; &amp; \\text { for } \\quad \\Pi_{1}^{n} \\tilde{\\pi}\\left(x_{i}\\right)&lt;C \\end{aligned} \\] <p>where \\(C\\) and \\(\\gamma\\) are chosen such that \\(E_{Q_{0}} \\varphi=\\alpha\\), is a minimax test between \\(\\mathscr{P}_{0}\\) and \\(\\mathscr{P}_{1}\\), with the same level</p> \\[ \\sup _{\\mathscr{P}_{0}} E \\varphi=\\alpha \\] <p>and the same minimum power</p> \\[ \\inf _{\\mathscr{P}_{1}} E \\varphi=E_{Q_{1}} \\varphi \\] <p>Proof This is an immediate consequence of Lemma 3.1 and of the following well-known Lemma 3.3 [putting \\(U_{i}=\\log \\tilde{\\pi}\\left(X_{i}\\right), \\mathcal{E}\\left(X_{i}\\right)=Q\\), etc.].</p> <p>LEMMA 3.3 Let \\(\\left(U_{i}\\right)\\) and \\(\\left(V_{i}\\right), i=1,2, \\ldots\\), be two sequences of random variables, such that the \\(U_{i}\\) are independent among themselves, the \\(V_{i}\\) are independent among themselves, and \\(U_{i}\\) is stochastically larger than \\(V_{i}\\), for all \\(i\\). Then, for all \\(n, \\Sigma_{1}^{n} U_{i}\\) is stochastically larger than \\(\\Sigma_{1}^{n} V_{i}\\). Proof Let \\(\\left(Z_{i}\\right)\\) be a sequence of independent random variables with uniform distribution in \\((0,1)\\), and let \\(F_{i}&lt;G_{i}\\) be the distribution functions of \\(U_{i}\\) and \\(V_{i}\\), respectively. Then \\(F_{i}^{-1}\\left(Z_{i}\\right)\\) has the same distribution as \\(U_{i}\\), \\(G_{i}^{-1}\\left(Z_{i}\\right)\\) has the same distribution as \\(V_{i}\\), and the conclusion follows easily from \\(F_{i}^{-1}\\left(Z_{i}\\right) \\geqslant G_{i}^{-1}\\left(Z_{i}\\right)\\).</p> <p>For the above we have assumed that \\(c^{\\prime}&lt;1 / c^{\\prime \\prime}\\). We now show that this is equivalent to our initial assumption that \\(\\mathscr{P}_{0}\\) and \\(\\mathscr{P}_{1}\\) are disjoint. If \\(c^{\\prime}=1 / c^{\\prime \\prime}\\),</p> <p>then \\(Q_{0}=Q_{1}\\), and the sets \\(\\mathscr{G}_{0}\\) and \\(\\mathscr{G}_{1}\\) overlap. Since the solutions \\(c^{\\prime}\\) and \\(c^{\\prime \\prime}\\) of (3.11) and (3.12) are monotone increasing in the \\(\\varepsilon_{j}, \\delta_{j}\\), the overlap is even worse if \\(c^{\\prime}&gt;1 / c^{\\prime \\prime}\\). On the other hand if \\(c^{\\prime}&lt;1 / c^{\\prime \\prime}\\), then \\(Q_{0} \\neq Q_{1}\\), and \\(Q_{0}\\{\\tilde{\\pi}&lt;t\\} \\geqslant Q_{1}\\{\\tilde{\\pi}&lt;t\\}\\) with strict inequality for some \\(t=t_{0}\\) [the power of a Neyman-Pearson test exceeds its size, cf. Lehmann (1959), p. 67, Corollary 1]. In view of Lemma 3.1, then \\(Q_{0}^{\\prime}\\left\\{\\tilde{\\pi}&lt;t_{0}\\right\\}&gt;Q_{1}^{\\prime}\\left\\{\\tilde{\\pi}&lt;t_{0}\\right\\}\\); hence \\(\\mathscr{G}_{0}\\) and \\(\\mathscr{G}_{1}\\) do not overlap.</p> <p>The limiting test for the case \\(c^{\\prime}=1 / c^{\\prime \\prime}\\) is of some interest; it is a kind of sign test, based on the number of observations for which \\(p_{1}(x) / p_{0}(x) \\gtrless c^{\\prime}\\). Incidentally, if \\(\\varepsilon_{0}=\\varepsilon_{1}\\), the limiting value is \\(c^{\\prime}=1\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#particular-cases","title":"Particular Cases","text":"<p>In the following we assume that either \\(\\delta_{j}=0\\) or \\(\\varepsilon_{j}=0\\). Note that the set \\(\\mathscr{G}_{0}\\), defined in (3.1), contains each of the following five sets (1) to (5), and that \\(Q_{0}\\) is contained in each of them. It follows that the minimax tests of Theorem 3.2 are also minimax for testing between neighborhoods specified in terms of \\(\\varepsilon\\)-contamination, total variation, Prohorov distance, Kolmogorov distance, and L\u00e9vy distance, assuming only that \\(p_{1}(x) / p_{0}(x)\\) is monotone for the pair of idealized model distributions. (1) \\(\\varepsilon\\)-contamination With \\(\\delta_{0}=0\\),</p> \\[ \\left\\{Q \\in \\mathscr{M} \\mid Q=\\left(1-\\varepsilon_{0}\\right) P_{0}+\\varepsilon_{0} H, H \\in \\mathscr{M}\\right\\} \\] <p>(2) Total variation With \\(\\varepsilon_{0}=0\\),</p> \\[ \\left\\{Q \\in \\mathscr{M}|\\forall A| Q\\{A\\}-P_{0}\\{A\\}| \\leqslant \\delta_{0}\\right\\} \\] <p>(3) Prohorov With \\(\\varepsilon_{0}=0\\) and \\(P_{0, \\eta}(x)=P_{0}(x-\\eta)\\),</p> \\[ \\left\\{Q \\in \\mathscr{M} \\mid \\forall A Q\\{A\\} \\leqslant P_{0, \\eta}\\left\\{A^{\\eta}\\right\\}+\\delta_{0}\\right\\} \\] <p>(4) Kolmogorov With \\(\\varepsilon_{0}=0\\),</p> \\[ \\left\\{Q \\in \\mathscr{M}|\\forall x| Q(x)-P_{0}(x)| \\leqslant \\delta_{0}\\right\\} \\] <p>(5) L\u00e9vy With \\(\\varepsilon_{0}=0\\) and \\(P_{0, \\eta}(x)=P_{0}(x-\\eta)\\),</p> \\[ \\left\\{Q \\in \\mathscr{M} \\mid P_{0, \\eta}(x-\\eta)-\\delta_{0} \\leqslant Q(x) \\leqslant P_{0, \\eta}(x+\\eta)+\\delta_{0} \\text { for all } x\\right\\} \\] <p>Note that the gross error model (1) and the total variation model (2) make sense in arbitrary probability spaces; a closer look at the above proof</p> <p>shows that monotonicity of \\(p_{1}(x) / p_{0}(x)\\) then is not needed and that the proof carries through in arbitrary probability spaces.</p> <p>Furthermore note that the hypothesis \\(\\mathscr{P}_{0}\\) of (3.1) is such that it contains with every \\(Q\\) also all \\(Q^{\\prime}\\) stochastically smaller than \\(Q\\); similarly, \\(\\mathscr{P}_{1}\\) contains with every \\(Q\\) also all \\(Q^{\\prime}\\) stochastically larger than \\(Q\\). This has the important consequence that, if \\(\\left(P_{\\theta}\\right)_{\\theta \\in \\mathbf{R}}\\) is a monotone likelihood ratio family, that is, if \\(p_{\\theta_{1}}(x) / p_{\\theta_{0}}(x)\\) is monotone increasing in \\(x\\) if \\(\\theta_{0}&lt;\\theta_{1}\\), then the test of Theorem 3.2 constructed for neighborhoods \\(\\mathscr{P}_{j}\\) of \\(P_{\\theta_{j}}, j=0,1\\), is not only a minimax test for testing \\(\\theta_{0}\\) against \\(\\theta_{1}\\), but also for testing \\(\\theta&lt;\\theta_{0}\\) against \\(\\theta \\geqslant \\theta_{1}\\).</p> <p>Example 3.1 Normal Distribution Let \\(P_{0}\\) and \\(P_{1}\\) be normal distributions with variance 1 and mean \\(-a\\) and \\(+a\\), respectively. Then \\(g(x)=\\) \\(p_{1}(x) / p_{0}(x)=e^{2 a x}\\). Assume that \\(\\varepsilon_{0}=\\varepsilon_{1}=\\varepsilon\\), and \\(\\delta_{0}=\\delta_{1}=\\delta\\); then for reasons of symmetry \\(c^{\\prime}=c^{\\prime \\prime}\\). Write the common value in the form \\(c^{\\prime}=e^{-2 a k}\\); then (3.11) reduces to</p> \\[ e^{-2 a k} \\Phi(a-k)-\\Phi(-a-k)=\\frac{\\varepsilon+\\delta+\\delta e^{-2 a k}}{1-\\varepsilon} \\] <p>Assume that \\(k\\) has been determined from this equation. Then the logarithm of the test statistic in Theorem 3.2 is, apart from a constant factor,</p> \\[ h(\\mathbf{x})=\\sum_{1}^{n} \\psi\\left(x_{i}\\right) \\] <p>with</p> \\[ \\psi(x)=\\max (-k, \\min (k, x)) \\] <p>Exhibit 10.3.2 shows some numerical results. Note that the values of \\(k\\) are surprisingly small: if \\(\\delta \\geqslant 0.0005\\), then \\(k \\leqslant 2.5\\), and if \\(\\delta \\geqslant 0.01\\), then \\(k \\leqslant 1.5\\), for all choices of \\(a\\).</p> \\(a\\) \\(k=0\\) 0.5 1.0 1.5 2.0 2.5 0.05 0.020 0.010 0.004 0.0014 0.0004 0.00010 0.1 0.040 0.020 0.008 0.0029 0.0008 0.00019 0.2 0.079 0.039 0.016 0.0055 0.0015 0.00035 0.5 0.191 0.090 0.034 0.0103 0.0025 0.00048 1.0 0.341 0.162 0.040 0.0087 0.0016 0.00022 1.5 0.433 0.135 0.027 0.0042 0.0005 0.00006 2.0 0.477 0.111 0.014 0.0015 0.0001 0.00001 <p>Exhibit 10.3.2 Values of \\(\\delta\\) in function of \\(a\\) and \\(k(\\varepsilon=0)\\). From Huber (1968), with permission of the publisher.</p> <p>Example 3.2 Binomial Distributions Let \\(\\Omega=\\{0,1\\}\\), and let \\(b(x \\mid p)=\\) \\(p^{x}(1-p)^{1-x}, x=0,1\\). The problem is to test between \\(p=\\pi_{0}\\) and \\(p=\\pi_{1}\\), \\(0 \\leqslant \\pi_{0}&lt;\\pi_{1} \\leqslant 1\\), when there is uncertainty in terms of total variation. This means that</p> \\[ \\mathscr{G}_{j}=\\left\\{b(\\cdot \\mid p) \\mid 0 \\leqslant p \\leqslant 1, \\pi_{j}-\\delta_{j} \\leqslant p \\leqslant \\pi_{j}+\\delta_{j}\\right\\} \\] <p>It is evident that the minimax tests between \\(\\mathscr{G}_{0}\\) and \\(\\mathscr{G}_{1}\\) coincide with the Neyman-Pearson tests of the same level between \\(b\\left(\\cdot \\mid \\pi_{0}+\\delta_{0}\\right)\\) and \\(b\\left(\\cdot \\mid \\pi_{1}-\\right.\\) \\(\\delta_{1}\\) ), provided \\(\\pi_{0}+\\delta_{0}&lt;\\pi_{1}-\\delta_{1}\\). (This trivial example is used to construct a counterexample in the following section).</p> <p>In general, the level and power of these robust tests are not easy to determine. It is, however, possible to attack such problems asymptotically, assuming that, simultaneously, the hypotheses approach each other at a rate \\(\\theta_{1}-\\theta_{0} \\sim n^{-1 / 2}\\), while the neighborhood parameters \\(\\varepsilon\\) and \\(\\delta\\) shrink at the same rate. For details, see Section 11.2.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#104-sequential-tests","title":"10.4 SEQUENTIAL TESTS","text":"<p>Let \\(\\mathscr{G}_{0}\\) and \\(\\mathscr{G}_{1}\\) be two composite hypotheses as in the preceding section, and let \\(Q_{0}\\) and \\(Q_{1}\\) be a least favorable pair with probability ratio \\(\\pi(x)=\\) \\(q_{1}(x) / q_{0}(x)\\). We saw that this pair is least favorable for all fixed sample sizes. What happens if we use the sequential probability ratio test (SPRT) between \\(Q_{0}\\) and \\(Q_{1}\\) to discriminate between \\(\\mathscr{G}_{0}\\) and \\(\\mathscr{G}_{1}\\) ?</p> <p>Put \\(\\gamma(x)=\\log \\pi(x)\\) and let us agree that the SPRT terminates as soon as</p> \\[ K^{\\prime}&lt;\\sum_{i&lt;n} \\gamma\\left(x_{i}\\right)&lt;K^{\\prime \\prime} \\] <p>is violated for the first time \\(n=N(\\mathbf{x})\\), and that we decide in favor of \\(\\mathscr{G}_{0}\\) or \\(\\mathscr{G}_{1}\\), respectively, according as the left or right inequality in (4.1) is violated, respectively. Somewhat more generally, we may allow randomization on the boundary, but we leave this to the reader.</p> <p>Assume, for example, that \\(Q_{0}^{\\prime}\\) is true. We have to compare the stochastic behavior of the cumulative sums \\(\\Sigma \\gamma\\left(x_{i}\\right)\\) under \\(Q_{0}^{\\prime}\\) and \\(Q_{0}\\). According to the proof of Lemma 3.3, there are functions \\(f \\geqslant g\\) and independent random variables \\(Z_{i}\\) such that \\(f\\left(Z_{i}\\right)\\) and \\(g\\left(Z_{i}\\right)\\) have the same distribution as \\(\\gamma\\left(X_{i}\\right)\\) under \\(Q_{0}\\) and \\(Q_{0}^{\\prime}\\), respectively. Thus if the cumulative sum \\(\\Sigma g\\left(Z_{i}\\right)\\) leaves the interval ( \\(K^{\\prime}, K^{\\prime \\prime}\\) ) first at \\(K^{\\prime \\prime}, \\Sigma f\\left(Z_{i}\\right)\\) will do the same, but even earlier. Therefore the probability of falsely rejecting \\(\\mathscr{G}_{0}\\) is at least as large under \\(Q_{0}\\) as under \\(Q_{0}^{\\prime}\\). A similar argument applies to the other hypothesis \\(\\mathscr{G}_{1}\\), and we</p> <p>conclude that the pair \\(\\left(Q_{0}, Q_{1}\\right)\\) is also least favorable in the sequential case, as far as the probabilities of error are concerned.</p> <p>It need not be least favorable for the expected sample size, as the following example shows.</p> <p>Example 4.1 Assume that \\(X_{1}, X_{2}, \\ldots\\) are independent Bernoulli variables</p> \\[ P\\left\\{X_{i}=1\\right\\}=1-P\\left\\{X_{i}=0\\right\\}=p \\] <p>and that we are testing the hypothesis \\(\\mathscr{P}_{0}=\\{p \\leqslant \\alpha\\}\\) against the alternative \\(\\mathscr{P}_{1}=\\left\\{p \\geqslant \\frac{1}{2}\\right\\}\\), where \\(0&lt;\\alpha&lt;\\frac{1}{2}\\). There is a least favorable pair \\(Q_{0}, Q_{1}\\), corresponding to \\(p=\\alpha\\) and \\(p=\\frac{1}{2}\\), respectively (cf. Example 3.2). Then</p> \\[ \\begin{aligned} \\gamma(x)=\\log \\frac{q_{1}(x)}{q_{0}(x)} &amp; =-\\log 2(1-\\alpha), &amp; &amp; \\text { for } x=0 \\\\ &amp; =-\\log 2 \\alpha, &amp; &amp; \\text { for } x=1 \\end{aligned} \\] <p>Assume \\(\\alpha \\leqslant 2^{-m-1}\\), where \\(m\\) is a positive integer; then</p> \\[ \\frac{-\\log 2 \\alpha}{\\log 2(1-\\alpha)} \\geqslant \\frac{m \\log 2}{\\log 2+\\log (1-\\alpha)} \\geqslant m \\] <p>and we verify easily that the SPRT between \\(p=\\alpha\\) and \\(p=\\frac{1}{2}\\) with boundaries</p> \\[ \\begin{aligned} &amp; K^{\\prime}=-m \\log 2(1-\\alpha) \\\\ &amp; K^{\\prime \\prime}=-\\log 2 \\alpha-(m-1) \\log 2(1-\\alpha) \\end{aligned} \\] <p>can also be described by the simple rule: (1) Decide for \\(\\mathscr{P}_{1}\\) at the first appearance of a 1 . (2) But decide for \\(\\mathscr{P}_{0}\\) after \\(m\\) zeros in a row.</p> <p>The probability of deciding for \\(\\mathscr{P}_{0}\\) is \\((1-p)^{m}\\), the probability of deciding for \\(\\mathscr{P}_{1}\\) is \\(1-(1-p)^{m}\\), and the expected sample size is</p> \\[ E_{p}(N)=\\sum_{k=0}^{m-1}(1-p)^{k}=\\frac{1-(1-p)^{m}}{p} \\] <p>Note that the expected sample size reaches its maximum (namely m ) for \\(p=0\\), that is, outside of the interval \\(\\left[\\alpha, \\frac{1}{2}\\right]\\). The probabilities of error of the first and of the second kind are bounded from above by \\(1-(1-\\alpha)^{m} \\leqslant\\)</p> <p>\\(m \\alpha \\leqslant m 2^{-m-1}\\) and by \\(2^{-m}\\), respectively, and thus can be made arbitrarily small [this disproves conjecture 8(i) of Huber (1965)].</p> <p>However, if the boundaries \\(K^{\\prime}\\) and \\(K^{\\prime \\prime}\\) are so far away that the behavior of the cumulative sums is essentially determined by their nonrandom drift</p> \\[ \\sum \\gamma\\left(X_{i}\\right) \\sim n E_{Q^{\\prime}}[\\gamma(X)] \\] <p>then the expected sample size is asymptotically equal to</p> \\[ \\begin{array}{ll} E_{Q_{0}}(N) \\sim \\frac{K^{\\prime}}{E_{Q_{0}^{\\prime}}(\\gamma(X))}, &amp; \\text { for } Q_{0}^{\\prime} \\in \\mathscr{P}_{0} \\\\ E_{Q_{1}}(N) \\sim \\frac{K^{\\prime \\prime}}{E_{Q_{1}^{\\prime}}(\\gamma(X))}, &amp; \\text { for } Q_{1}^{\\prime} \\in \\mathscr{P}_{1} \\end{array} \\] <p>This heuristic argument can be made precise with the aid of the standard approximations for the expected sample sizes [cf., e.g., Lehmann (1959)]. In view of the inequalities of Theorem 3.2, it follows that the right-hand sides of (4.7) are indeed maximized for \\(Q_{0}\\) and \\(Q_{1}\\), respectively. So the pair \\(\\left(Q_{0}, Q_{1}\\right)\\) is in a certain sense, asymptotically least favorable also for the expected sample size if \\(K^{\\prime} \\rightarrow-\\infty\\) and \\(K^{\\prime \\prime} \\rightarrow+\\infty\\).</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#105-the-neyman-pearson-lemma-for-2-alternating-capacities","title":"10.5 THE NEYMAN-PEARSON LEMMA FOR 2-ALTERNATING CAPACITIES","text":"<p>Ordinarily, sample size \\(n\\) minimax tests between two composite alternatives \\(\\mathscr{P}_{0}\\) and \\(\\mathscr{P}_{1}\\) have a fairly complex structure. Setting aside all measure theoretic complications, they are Neyman-Pearson tests based on a likelihood ratio \\(\\bar{q}_{1}(\\mathbf{x}) / \\bar{q}_{0}(\\mathbf{x})\\), where each \\(\\bar{q}_{j}\\) is a mixture of product densities on \\(\\Omega^{n}:\\)</p> \\[ \\bar{q}_{j}(\\mathbf{x})=\\int \\prod_{i=1}^{n} q\\left(x_{i}\\right) \\lambda_{j}(d q) \\] <p>Here, \\(\\lambda_{j}\\) is a probability measure supported by the set \\(\\mathscr{P}_{j}\\); in general, \\(\\lambda_{j}\\) depends both on the level and on the sample size.</p> <p>The simple structure of the minimax tests found in Section 10.3 therefore was a surprise. On closer scrutiny it turned out that this had to do with the fact that all the \"usual\" neighborhoods \\(\\mathscr{P}\\) used in robustness theory could be characterized as \\(\\mathscr{P}=\\mathscr{P}_{v}\\) with \\(v=(\\underline{v}, \\bar{v})\\) being a pair of conjugated 2-monotone/2-alternating capacities (see Section 10.2).</p> <p>The following summarizes the main results of Huber and Strassen (1973). Let \\(\\Omega\\) be a Polish space (complete, separable, metrizable), equipped with its Borel \\(\\sigma\\)-algebra \\(\\mathscr{Q}\\), and let \\(\\mathscr{M}\\) be the set of all probability measures on \\((\\Omega, \\mathscr{Q})\\). Let \\(\\bar{v}\\) be a real valued set function defined on \\(\\mathscr{Q}\\), such that</p> \\[ \\begin{gathered} \\bar{v}(\\phi)=0, \\quad \\bar{v}(\\Omega)=1 \\\\ A \\subset B \\Rightarrow \\bar{v}(A) \\leqslant \\bar{v}(B) \\\\ A_{n} \\uparrow A \\Rightarrow \\bar{v}\\left(A_{n}\\right) \\uparrow \\bar{v}(A) \\\\ F_{n} \\downarrow F, F_{n} \\text { closed } \\Rightarrow \\bar{v}\\left(F_{n}\\right) \\downarrow \\bar{v}(F) \\\\ \\bar{v}(A \\cup B)+\\bar{v}(A \\cap B) \\leqslant \\bar{v}(A)+\\bar{v}(B) \\end{gathered} \\] <p>The conjugate set function \\(\\underline{v}\\) is defined by</p> \\[ \\underline{v}(A)=1-\\bar{v}\\left(A^{c}\\right) \\] <p>A set function \\(\\bar{v}\\) satisfying (5.1) to (5.5) is called a 2-alternating capacity, and the conjugate function \\(\\underline{v}\\) shall be called a 2 -monotone capacity.</p> <p>It can be shown that any such capacity is regular in the sense that, for every \\(A \\in \\mathscr{Q}\\),</p> \\[ \\bar{v}(A)=\\sup _{K} \\bar{v}(K)=\\inf _{G} \\bar{v}(G) \\] <p>where \\(K\\) ranges over the compact sets contained in \\(A\\) and \\(G\\) over the open sets containing \\(A\\).</p> <p>Among these requirements (5.4) is equivalent to</p> \\[ \\mathscr{P}_{v}=\\{P \\in \\mathscr{M} \\mid P \\leqslant \\bar{v}\\}=\\{P \\in \\mathscr{M} \\mid P \\geqslant \\underline{v}\\} \\] <p>being weakly compact, and (5.5) could be replaced by: for any monotone sequence of closed sets \\(F_{1} \\subset F_{2} \\subset \\cdots, F_{i} \\subset \\Omega\\), there is a \\(Q \\leqslant \\bar{v}\\) that simultaneously maximizes the probabilities of the \\(F_{i}\\), that is \\(Q\\left(F_{i}\\right)=\\bar{v}\\left(F_{i}\\right)\\), for all \\(i\\).</p> <p>Example 5.1 Let \\(\\Omega\\) be compact. Define \\(\\bar{v}(A)=(1-\\varepsilon) P_{0}(A)+\\varepsilon\\) for \\(A \\neq \\phi\\). Then \\(\\bar{v}\\) satisfies (5.1) to (5.5), and</p> \\[ \\mathscr{P}_{v}=\\left\\{P \\mid P=(1-\\varepsilon) P_{0}+\\varepsilon H, H \\in \\mathscr{M}\\right\\} \\] <p>is the \\(\\varepsilon\\)-contamination neighborhood of \\(P_{0}\\). Example 5.2 Let \\(\\Omega\\) be compact metric. Define \\(\\bar{v}(A)=\\min \\left[P_{0}\\left(A^{\\delta}\\right)+\\varepsilon, 1\\right]\\) for compact sets \\(A \\neq \\phi\\), and use (5.7) to extend \\(v\\) to \\(\\mathscr{Q}\\). Then \\(v\\) satisfies (5.1)</p> <p>to (5.5), and</p> \\[ \\mathscr{P}_{v}=\\left\\{P \\in \\mathscr{M} \\mid P(A) \\leqslant P_{0}\\left(A^{\\delta}\\right)+\\varepsilon \\text { for all } A \\in \\mathscr{R}\\right\\} \\] <p>is a Prohorov neighborhood of \\(P_{0}\\). Now let \\(\\bar{v}_{0}\\) and \\(\\bar{v}_{1}\\) be two 2 -alternating capacities on \\(\\Omega\\), and let \\(\\underline{v}_{0}\\) and \\(\\underline{v}_{1}\\) be their conjugates.</p> <p>Let \\(A\\) be a critical region for testing between \\(\\mathscr{P}_{0}=\\left\\{P \\in \\mathscr{M} \\mid P \\leqslant \\bar{v}_{0}\\right\\}\\) and \\(\\mathscr{P}_{1}=\\left\\{P \\in \\mathscr{M} \\mid P \\leqslant \\bar{v}_{1}\\right\\}\\), that is, reject \\(\\mathscr{P}_{0}\\) if \\(x \\in A\\) is observed. Then the upper probability of falsely rejecting \\(\\mathscr{P}_{0}\\) is \\(\\bar{v}_{0}(A)\\), and of falsely accepting \\(\\mathscr{P}_{0}\\) is \\(\\bar{v}_{1}\\left(A^{c}\\right)=1-\\underline{v}_{1}(A)\\).</p> <p>Assume that \\(\\mathscr{P}_{0}\\) is true with prior probability \\(t /(1+t), 0 \\leqslant t \\leqslant \\infty\\), then the upper Bayes risk of the critical region \\(A\\) is by definition</p> \\[ \\frac{t}{1+t} \\bar{v}_{0}(A)+\\frac{1}{1+t}\\left[1-\\underline{v}_{1}(A)\\right] \\] <p>This is minimized by minimizing the 2-alternating set function</p> \\[ w_{t}(A)=t \\bar{v}_{0}(A)-\\underline{v}_{1}(A) \\] <p>through a suitable choice of \\(A\\). It is not very difficult to show that, for each \\(t\\), there is a critical region \\(A_{t}\\) minimizing (5.8). Moreover, the sets \\(A_{t}\\) can be chosen decreasing, that is, \\(A_{t}=\\cup_{s&gt;t} A_{s}\\).</p> <p>Define</p> \\[ \\pi(x)=\\inf \\left\\{t \\mid x \\notin A_{t}\\right\\} \\] <p>If \\(\\bar{v}_{0}=\\underline{v}_{0}, \\bar{v}_{1}=\\underline{v}_{1}\\) are ordinary probability measures, then \\(\\pi\\) is a version of the Radon-Nikodym derivative \\(d v_{1} / d v_{0}\\), so the above constitutes a natural generalization of this notion to 2-alternating capacities.</p> <p>The crucial result now is given in the following theorem. THEOREM 5.1 (Neyman-Pearson lemma for capacities) There exist two probabilities \\(Q_{0} \\in \\mathscr{P}_{0}\\) and \\(Q_{1} \\in \\mathscr{P}_{1}\\) such that, for all \\(t\\),</p> \\[ \\begin{aligned} &amp; Q_{0}\\{\\pi&gt;t\\}=\\bar{v}_{0}\\{\\pi&gt;t\\} \\\\ &amp; Q_{1}\\{\\pi&gt;t\\}=\\underline{v}_{1}\\{\\pi&gt;t\\} \\end{aligned} \\] <p>and that \\(\\pi=d Q_{1} / d Q_{0}\\).</p> <p>Proof See Huber and Strassen (1973, with correction 1974). In other words among all distributions in \\(\\mathscr{G}_{0}, \\pi\\) is stochastically largest for \\(Q_{0}\\), and among all distributions in \\(\\mathscr{G}_{1}, \\pi\\) is stochastically smallest for \\(Q_{1}\\).</p> <p>The conclusion of Theorem 5.1 is essentially identical to that of Lemma 3.1, and we conclude, just as there, that the Neyman-Pearson tests between \\(Q_{0}\\) and \\(Q_{1}\\), based on the test statistic \\(\\Pi_{i=1}^{\\pi} \\pi\\left(x_{i}\\right)\\), are minimax tests between \\(\\mathscr{G}_{0}\\) and \\(\\mathscr{G}_{1}\\), and this for arbitrary levels and sample sizes.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#106-estimates-derived-from-tests","title":"10.6 ESTIMATES DERIVED FROM TESTS","text":"<p>In this section we derive a rigorous correspondence between tests and interval estimates of location.</p> <p>Let \\(X_{1}, \\ldots, X_{n}\\) be random variables whose joint distribution belongs to a location family, that is,</p> \\[ \\mathscr{E}_{\\theta}\\left(X_{1}, \\ldots, X_{n}\\right)=\\mathscr{E}_{0}\\left(X_{1}+\\theta, \\ldots, X_{n}+\\theta\\right) \\] <p>the \\(X_{i}\\) need not be independent. Let \\(\\theta_{1}&lt;\\theta_{2}\\), and let \\(\\varphi\\) be a (randomized) test of \\(\\theta_{1}\\) against \\(\\theta_{2}\\), of the form</p> \\[ \\begin{aligned} \\varphi(\\mathbf{x}) &amp; =0, &amp; &amp; \\text { for } h(\\mathbf{x})&lt;C \\\\ &amp; =\\gamma, &amp; &amp; \\text { for } h(\\mathbf{x})=C \\\\ &amp; =1, &amp; &amp; \\text { for } h(\\mathbf{x})&gt;C . \\end{aligned} \\] <p>The test statistic \\(h\\) is arbitrary, except that \\(h(\\mathbf{x}+\\theta)=h\\left(x_{1}+\\theta, \\ldots, x_{n}+\\theta\\right)\\) is assumed to be a monotone increasing function of \\(\\theta\\). Let</p> \\[ \\alpha=E_{\\theta, \\varphi} \\] <p>and</p> \\[ \\beta=E_{\\theta, \\varphi} \\] <p>be the level and the power of this test. As \\(\\alpha=E_{0} \\varphi\\left(\\mathbf{x}+\\theta_{1}\\right), \\beta=E_{0} \\varphi\\left(\\mathbf{x}+\\theta_{2}\\right)\\), and \\(\\varphi(\\mathbf{x}+\\theta)\\) is monotone increasing in \\(\\theta\\), we have \\(\\alpha \\leqslant \\beta\\).</p> <p>We define two random variables \\(T^{*}\\) and \\(T^{* *}\\) by</p> \\[ \\begin{aligned} T^{*} &amp; =\\sup \\{\\theta \\mid h(\\mathbf{x}-\\theta)&gt;C\\} \\\\ T^{* *} &amp; =\\inf \\{\\theta \\mid h(\\mathbf{x}-\\theta)&lt;C\\} \\end{aligned} \\] <p>and put</p> \\[ \\begin{aligned} T^{0} &amp; =T^{*} \\text { with probability } 1-\\gamma \\\\ &amp; =T^{* *} \\text { with probability } \\gamma \\end{aligned} \\] <p>This randomization should be independent of \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\); for example, take a uniform \\((0,1)\\) random variable \\(U\\) that is independent of \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) and let \\(T^{0}\\) be a deterministic function of \\(\\left(X_{1}, \\ldots, X_{n}, U\\right)\\), defined in the obvious way: \\(T^{0}(\\mathbf{X}, U)=T^{*}\\) or \\(T^{* *}\\) according as \\(U \\geqslant \\gamma\\) or \\(U&lt;\\gamma\\).</p> <p>Evidently all three statistics \\(T^{*}, T^{* *}\\), and \\(T^{0}\\) are translation invariant in the sense that \\(T(\\mathbf{x}+\\theta)=T(\\mathbf{x})+\\theta\\).</p> <p>We note that \\(T^{*} \\leqslant T^{* *}\\) and that</p> \\[ \\begin{aligned} &amp; \\left\\{\\mathbf{x} \\mid T^{*}&gt;\\theta\\right\\} \\subset\\{\\mathbf{x} \\mid h(\\mathbf{x}-\\theta)&gt;C\\} \\subset\\left\\{\\mathbf{x} \\mid T^{*} \\geqslant \\theta\\right\\} \\\\ &amp; \\left\\{\\mathbf{x} \\mid T^{* *}&gt;\\theta\\right\\} \\subset\\{\\mathbf{x} \\mid h(\\mathbf{x}-\\theta) \\geqslant C\\} \\subset\\left\\{\\mathbf{x} \\mid T^{* *} \\geqslant \\theta\\right\\} \\end{aligned} \\] <p>If \\(h(\\mathbf{x}-\\theta)\\) is continuous as a function of \\(\\theta\\), these relations simplify to</p> \\[ \\begin{aligned} \\left\\{T^{*}&gt;\\theta\\right\\} &amp; =\\{h(\\mathbf{x}-\\theta)&gt;C\\} \\\\ \\left\\{T^{* *} \\geqslant \\theta\\right\\} &amp; =\\{h(\\mathbf{x}-\\theta) \\geqslant C\\} \\end{aligned} \\] <p>In any case we have, for an arbitrary joint distribution of \\(X_{1}, \\ldots, X_{n}\\) and arbitrary \\(\\theta\\),</p> \\[ \\begin{aligned} P\\left(T^{0}&gt;\\theta\\right) &amp; =(1-\\gamma) P\\left\\{T^{*}&gt;\\theta\\right\\}+\\gamma P\\left\\{T^{* *}&gt;\\theta\\right\\} \\\\ &amp; \\leqslant(1-\\gamma) P\\{h(\\mathbf{X}-\\theta)&gt;C\\}+\\gamma P\\{h(\\mathbf{X}-\\theta) \\geqslant C\\} \\\\ &amp; =E \\varphi(\\mathbf{X}-\\theta) \\end{aligned} \\] <p>For \\(T^{0} \\geqslant \\theta\\) the inequality is reversed; thus</p> \\[ P\\left(T^{0}&gt;\\theta\\right) \\leqslant E \\varphi(\\mathbf{X}-\\theta) \\leqslant P\\left(T^{0} \\geqslant \\theta\\right) \\] <p>For the translation family (6.1) we have, in particular,</p> \\[ E_{\\theta \\mid \\varphi}(\\mathbf{X})=E_{\\theta} \\varphi\\left(\\mathbf{X}+\\theta_{1}\\right)=\\alpha \\] <p>Since \\(T^{0}\\) is translation invariant, this implies</p> \\[ P_{\\theta}\\left\\{T^{0}+\\theta_{1}&gt;\\theta\\right\\} \\leqslant \\alpha \\leqslant P_{\\theta}\\left\\{T^{0}+\\theta_{1} \\geqslant \\theta\\right\\} \\] <p>and similarly,</p> \\[ P_{\\theta}\\left\\{T^{0}+\\theta_{2}&gt;\\theta\\right\\} \\leqslant \\beta \\leqslant P_{\\theta}\\left\\{T^{0}+\\theta_{2} \\geqslant \\theta\\right\\} \\] <p>We conclude that \\(\\left[T^{0}+\\theta_{1}, T^{0}+\\theta_{2}\\right]\\) is a (fixed-length) confidence interval such that the true value \\(\\theta\\) lies to its left with probability \\(\\leqslant \\alpha\\), and to its right with probability \\(\\leqslant 1-\\beta\\). For the open interval \\(\\left(T^{0}+\\theta_{1}, T^{0}+\\theta_{2}\\right)\\) the inequalities are reversed, and the probabilities of error become \\(\\geqslant \\alpha\\) and \\(\\geqslant 1-\\beta\\) respectively.</p> <p>In particular, if the distribution of \\(T^{0}\\) is continuous, then \\(P_{\\theta}\\left\\{T^{0}+\\theta_{1}=\\theta\\right\\}\\) \\(=P_{\\theta}\\left\\{T^{0}+\\theta_{2}=\\theta\\right\\}=0\\); therefore we have equality in either case, and ( \\(T^{0}+\\) \\(\\theta_{1}, T^{0}+\\theta_{2}\\) ) catches the true value with probability \\(\\beta-\\alpha\\).</p> <p>The following lemma gives a sufficient condition for the absolute continuity of the distribution of \\(T^{0}\\).</p> <p>LEMMA 6.1 If the joint distribution of \\(\\mathbf{X}=\\left(X_{1}, \\ldots, X_{n}\\right)\\) is absolutely continuous with respect to Lebesgue measure in \\(\\mathbb{R}^{n}\\), then every translation invariant measurable estimate \\(T\\) has an absolutely continuous distribution with respect to Lebesgue measure in \\(\\mathbb{R}\\).</p> <p>Proof We prove the lemma by explicitly writing down the density of \\(T\\) : if the joint density of \\(\\mathbf{X}\\) is \\(f(\\mathbf{x})\\), then the density of \\(T\\) is</p> \\[ g(t)=\\int f\\left(y_{1}-T(\\mathbf{y})+t, \\ldots, y_{n-1}-T(\\mathbf{y})+t,-T(\\mathbf{y})+t\\right) d y_{1} \\ldots d y_{n-1} \\] <p>where \\(\\mathbf{y}\\) is short for \\(\\left(y_{1}, \\ldots, y_{n-1}, 0\\right)\\). In order to prove (6.9) it suffices to verify that, for every bounded measurable function \\(w\\),</p> \\[ \\int w(t) g(t) d t=\\int w(T(\\mathbf{x})) f(\\mathbf{x}) d x_{1} \\ldots d x_{n} \\] <p>By Fubini's theorem we can interchange the order of integrations on the left-hand side:</p> \\[ \\int w(t) g(t) d t=\\int\\left\\{\\int w(t) f(\\cdots) d t\\right\\} d y_{1} \\ldots d y_{n-1} \\] <p>where the argument list of \\(f(\\cdots)\\) is the same as in (6.9). We substitute \\(t=T(\\mathbf{y})+x_{n}=T\\left(\\mathbf{y}+x_{n}\\right)\\) in the inner integral and change the order of</p> <p>integrations again:</p> \\[ \\int w(t) g(t) d t=\\int\\left\\{\\int w\\left(T\\left(y+x_{n}\\right)\\right) f\\left(y+x_{n}\\right) d y_{1} \\ldots d y_{n-1}\\right\\} d x_{n} \\] <p>Finally, we substitute \\(x_{i}=y_{i}+x_{n}\\) for \\(i=1, \\ldots, n-1\\) and obtain the desired equivalence (6.10).</p> <p>NOTE 1 The assertion that the distribution of a translation invariant estimate \\(T\\) is continuous, provided the observations \\(X_{i}\\) are independent with identical continuous distributions, is plausible but false [cf. Torgersen (1971)].</p> <p>NOTE 2 It is possible to obtain confidence intervals with exact one-sided error probabilities \\(\\alpha\\) and \\(1-\\beta\\) also in the general discontinuous case if we are willing to choose a sometimes open, sometimes closed interval. More precisely, when \\(U \\geq \\gamma\\) and thus \\(T^{0}=T^{*}\\), and if the set \\(\\{\\theta \\mid h(\\mathbf{x}-\\theta)&gt;C\\}\\) is open, choose the interval \\(\\left[T^{0}+\\theta_{1}, T^{0}+\\theta_{2}\\right)\\); if it is closed, choose \\(\\left(T^{0}+\\theta_{1}, T^{0}+\\theta_{2}\\right]\\). When \\(T^{0}=T^{* *}\\) and \\(\\{\\theta \\mid h(\\mathbf{x}-\\theta) \\geqslant C\\}\\) is open, take \\(\\left[T^{0}+\\theta_{1}, T^{0}+\\theta_{2}\\right)\\); if it is closed, take \\(\\left(T^{0}+\\theta_{1}, T^{0}+\\theta_{2}\\right]\\).</p> <p>NOTE 3 The more traditional nonrandomized compromise \\(T^{(0)}=\\frac{1}{2}\\left(T^{*}+\\right.\\) \\(T^{* *}\\) ) between \\(T^{*}\\) and \\(T^{* *}\\) in general does not satisfy the crucial relation (6.6).</p> <p>NOTE 4 Starting from the translation invariant estimate \\(T^{0}\\), we can reconstruct a test between \\(\\theta_{1}\\) and \\(\\theta_{2}\\), having the original level \\(\\alpha\\) and power \\(\\beta\\), as follows. In view of (6.6)</p> \\[ \\begin{aligned} &amp; P_{\\theta_{1}}\\left(T^{0}&gt;0\\right) \\leqslant \\alpha \\leqslant P_{\\theta_{1}}\\left(T^{0} \\geqslant 0\\right) \\\\ &amp; P_{\\theta_{2}}\\left(T^{0}&gt;0\\right) \\leqslant \\beta \\leqslant P_{\\theta_{2}}\\left(T^{0} \\geqslant 0\\right) \\end{aligned} \\] <p>Hence if \\(T^{0}\\) has a continuous distribution so that \\(P_{\\theta}\\left(T^{0}=0\\right)=0\\) for all \\(\\theta\\), we simply take \\(\\left(T^{0}&gt;0\\right)\\) as the critical region. In the general case we would have to split the boundary \\(T^{0}=0\\) in the manner of Note 2 (for that, the mere value of \\(T^{0}\\) does not quite suffice-we also need to know on which side the confidence intervals are open and closed, respectively).</p> <p>Rank tests are particularly attractive to derive estimates from, since they are distribution free under the null hypothesis; the sign test is so generally, and the others at least for symmetric distributions. This leads to distribu-tion-free confidence intervals-the probabilities that the true value lies to the left or the right of the interval, respectively, do not depend on the underlying distribution.</p> <p>Example 6.1 Sign Test Assume that the \\(X_{1}, \\ldots, X_{n}\\) are independent, with common distribution \\(F_{\\theta}(x)=F(x-\\theta)\\), where \\(F\\) has median 0 and is continuous at 0 . We test \\(\\theta_{1}=0\\) against \\(\\theta_{2}&gt;0\\), using the test statistic</p> \\[ h(\\mathbf{x})=\\sum 1_{\\left\\{x_{i}&gt;0\\right\\}} \\] <p>assume that the level of the test is \\(\\alpha\\). Then there will be an integer \\(c\\), independent of the special \\(F\\), such that the test rejects the hypothesis if the \\(c\\) th order statistic \\(x_{(c)}&gt;0\\), accepts it if \\(x_{(c+1)} \\leqslant 0\\), and randomizes it if \\(x_{(c)} \\leqslant 0&lt;x_{(c+1)}\\). The corresponding estimate \\(T^{0}\\) randomizes between \\(x_{(c)}\\) and \\(x_{(c+1)}\\), and is a distribution-free lower confidence bound for the true median:</p> \\[ P_{\\theta}\\left\\{\\theta&lt;T^{0}\\right\\} \\leqslant \\alpha \\leqslant P_{\\theta}\\left\\{\\theta \\leqslant T^{0}\\right\\} \\] <p>As \\(F\\) is continuous at its median, \\(P_{\\theta}\\left(\\theta=T^{0}\\right)=P_{0}\\left(0=T^{0}\\right)=0\\), we have in fact equality in (6.12). (The upper confidence bound \\(T^{0}+\\theta_{2}\\) is uninteresting, since its level depends on \\(F\\).)</p> <p>Example 6.2 Wilcoxon and Similar Tests Assume that \\(X_{1}, \\ldots, X_{n}\\) are independent with common distribution \\(F_{\\theta}(x)=F(x-\\theta)\\), where \\(F\\) is continuous and symmetric. Rank the absolute values of the observations, and let \\(R_{i}\\) be the rank of \\(\\left|x_{i}\\right|\\). Define the test statistic</p> \\[ h(\\mathbf{x})=\\sum_{x_{i}&gt;0} a\\left(R_{i}\\right) \\] <p>If \\(a(\\cdot)\\) is an increasing function [as for the Wilcoxon test: \\(a(i)=i\\) ], then \\(h(\\mathbf{x}+\\theta)\\) is increasing in \\(\\theta\\). It is easy to see that it is piecewise constant, with jumps possible at the points \\(\\theta=-\\frac{1}{2}\\left(x_{i}+x_{j}\\right)\\). It follows that \\(T^{0}\\) randomizes between two (not necessarily adjacent) values of \\(\\frac{1}{2}\\left(x_{i}+x_{j}\\right)\\).</p> <p>It is evident from the foregoing results that there is a precise correspondence between optimality properties for tests and estimates. For instance, the theory of locally most powerful rank tests for location leads to locally most efficient \\(R\\)-estimates, that is to estimates \\(T\\) maximizing the probability that \\((T-\\Delta, T+\\Delta)\\) catches the true value of the location parameter (i.e., the center of symmetry of \\(F\\) ), provided \\(\\Delta\\) is chosen sufficiently small.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#107-minimax-interval-estimates","title":"10.7 MINIMAX INTERVAL ESTIMATES","text":"<p>The minimax robust tests of Section 10.3 can be translated in a straightforward fashion into location estimates possessing exact finite sample minimax properties.</p> <p>Let \\(G\\) be an absolutely continuous distribution on the real line, with a continuous density \\(g\\) such that \\(-\\log g\\) is strictly convex on its convex support (which need not be the whole real line).</p> <p>Let \\(\\mathscr{P}\\) be a \"blown-up\" version of \\(G\\) :</p> \\[ \\mathscr{P}=\\left\\{F \\in \\mathscr{M} \\mid\\left(1-\\varepsilon_{0}\\right) G(x)-\\delta_{0} \\leqslant F(x) \\leqslant\\left(1-\\varepsilon_{1}\\right) G(x)+\\varepsilon_{1}+\\delta_{1} \\text { for all } x\\right\\} \\] <p>Assume that the observations \\(X_{1}, \\ldots, X_{n}\\) of \\(\\theta\\) are independent, and that the distributions \\(F_{i}\\) of the observational errors \\(X_{i}-\\theta\\) lie in \\(\\mathscr{P}\\).</p> <p>We intend to find an estimate \\(T\\) that minimizes the probability of underor overshooting the true \\(\\theta\\) by more than \\(a\\), where \\(a&gt;0\\) is a constant fixed in advance. That is, we want to minimize</p> \\[ \\sup _{\\mathscr{P}, \\theta} \\max [P\\{T&lt;\\theta-a\\}, P\\{T&gt;\\theta+a\\}] \\] <p>We claim that this problem is essentially equivalent to finding minimax tests between \\(\\mathscr{P}_{-a}\\) and \\(\\mathscr{P}_{+a}\\), where \\(\\mathscr{P}_{ \\pm a}\\) are obtained by shifting the set \\(\\mathscr{P}\\) of distribution functions to the left and right by amounts \\(\\pm a\\).</p> <p>More precisely, define the two distribution functions \\(G_{-a}\\) and \\(G_{+a}\\) by their densities</p> \\[ \\begin{aligned} &amp; g_{-a}(x)=g(x+a) \\\\ &amp; g_{+a}(x)=g(x-a) \\end{aligned} \\] <p>Then</p> \\[ c(x)=\\frac{g(x-a)}{g(x+a)} \\] <p>is strictly monotone increasing wherever it is finite. Expand \\(P_{0}=G_{-a}\\) and \\(P_{1}=G_{+a}\\) to composite hypotheses \\(\\mathscr{P}_{0}\\) and \\(\\mathscr{P}_{1}\\) according to (3.1), and determine a least favorable pair \\(\\left(Q_{0}, Q_{1}\\right) \\in \\mathscr{P}_{0} \\times \\mathscr{P}_{1}\\). Determine the constants \\(C\\) and \\(\\gamma\\) of Theorem 3.2 such that errors of both kinds are equally probable under \\(Q_{0}\\) and \\(Q_{1}\\) :</p> \\[ E_{Q_{0} \\varphi}=E_{Q_{1}}(1-\\varphi)=\\alpha \\] <p>If \\(\\mathscr{P}_{-a}\\) and \\(\\mathscr{P}_{+a}\\) are the translates of \\(\\mathscr{P}\\) to the left and to the right by the amount \\(a&gt;0\\), then it is easy to verify that</p> \\[ \\begin{aligned} &amp; Q_{0} \\in \\mathscr{P}_{-a} \\subset \\mathscr{P}_{0} \\\\ &amp; Q_{1} \\in \\mathscr{P}_{+a} \\subset \\mathscr{P}_{1} \\end{aligned} \\] <p>If we now determine an estimate \\(T^{0}\\) according to (6.3) and (6.4) from the test statistic</p> \\[ h(\\mathbf{x})=\\Pi_{1}^{n} \\hat{\\pi}\\left(x_{i}\\right) \\] <p>of Theorem 3.2, then (6.6) shows that</p> \\[ \\begin{array}{ll} Q_{0}^{\\prime}\\left\\{T^{0}&gt;0\\right\\} \\leqslant E_{Q_{1}^{\\prime}} \\varphi(\\mathbf{X}) \\leqslant \\alpha, &amp; \\text { for } Q_{0}^{\\prime} \\in \\mathscr{P}_{0} \\\\ Q_{1}^{\\prime}\\left\\{T^{0}&lt;0\\right\\} \\leqslant E_{Q_{1}^{\\prime}}(1-\\varphi(\\mathbf{X})) \\leqslant \\alpha, &amp; \\text { for } Q_{1}^{\\prime} \\in \\mathscr{P}_{1} \\end{array} \\] <p>On the other hand for any statistic \\(T\\) satisfying</p> \\[ Q_{0}\\{T=0\\}=Q_{1}\\{T=0\\}=0 \\] <p>we must have</p> \\[ \\max \\left[Q_{0}\\{T&gt;0\\}, Q_{1}\\{T&lt;0\\}\\right] \\geqslant \\alpha \\] <p>This follows from the remark that we can view \\(T\\) as a test statistic for testing between \\(Q_{0}\\) and \\(Q_{1}\\), and the minimax risk is \\(\\alpha\\) according to (7.5). Since \\(Q_{0}\\) and \\(Q_{1}\\) have densities, any translation invariant estimate, in particular \\(T^{0}\\), satisfies (7.8) (Lemma 6.1). In view of (7.6) we have proved the following theorem.</p> <p>THEOREM 7.1 The estimate \\(T^{0}\\) minimizes (7.2); more precisely, if the distributions of the errors \\(X_{i}-\\theta\\) are contained in \\(\\mathscr{P}\\), then for all \\(\\theta\\),</p> \\[ \\begin{aligned} &amp; P\\left\\{T^{0}&lt;\\theta-a\\right\\} \\leqslant \\alpha \\\\ &amp; P\\left\\{T^{0}&gt;\\theta+a\\right\\} \\leqslant \\alpha \\end{aligned} \\] <p>and the bound \\(\\alpha\\) is the best possible for translation invariant estimates. NOTE The restriction to translation invariant estimates can be dropped in view of the Hunt-Stein theorem [Lehmann (1959), p. 335].</p> <p>It is useful to discuss particular cases of this theorem. Assume that \\(G\\) is symmetric, and that \\(\\varepsilon_{0}=\\varepsilon_{1}\\) and \\(\\delta_{0}=\\delta_{1}\\). Then for reasons of symmetry \\(C=1\\) and \\(\\gamma=\\frac{1}{2}\\). Put</p> \\[ \\psi(x)=\\log \\frac{q_{1}(x)}{q_{0}(x)} \\] <p>then</p> \\[ \\psi(x)=\\max \\left\\{-k, \\min \\left[k, \\log \\frac{g(x-a)}{g(x+a)}\\right]\\right\\} \\] <p>and \\(T^{*}\\) and \\(T^{* *}\\) are the smallest and the largest solutions of</p> \\[ \\sum \\psi\\left(x_{i}-T\\right)=0 \\] <p>respectively, and \\(T^{0}\\) randomizes between them with equal probability. Actually, \\(T^{*}=T^{* *}\\) with overwhelming probability; \\(T^{*}&lt;T^{* *}\\) occurs only if the sample size \\(n=2 m\\) is even and the sample has a large gap in the middle [so that all summands in (7.12) have values \\(\\pm k\\) ]. Although, ordinarily, the nonrandomized midpoint estimate \\(T^{00}=\\frac{1}{2}\\left(T^{*}+T^{* *}\\right)\\) seems to have slightly better properties than the randomized \\(T^{0}\\), it does not solve the minimax problem; see Huber (1968) for a counterexample.</p> <p>In the particular case where \\(G=\\Phi\\) is the normal distribution, \\(\\log g(x-\\) a) \\(/ g(x+a)=2 a x\\) is linear, and after dividing through \\(2 a\\), we obtain our old acquaintance</p> \\[ \\psi(x)=\\max \\left[-k^{\\prime}, \\min \\left(k^{\\prime}, x\\right)\\right] \\text { with } k^{\\prime}=k /(2 \\mathrm{a}) \\] <p>Thus the \\(M\\)-estimate \\(T^{0}\\), as defined by (7.12) and (7.13), has two quite different minimax robustness properties for approximately normal distributions: (1) It minimizes the maximal asymptotic variance, for symmetric \\(\\varepsilon\\) contamination. (2) It yields exact, finite sample minimax interval estimates, for not necessarily symmetric \\(\\varepsilon\\)-contamination (and for indeterminacy in terms of Kolmogorov distance, total variation and other models as well).</p> <p>In retrospect it strikes us as very remarkable that the \\(\\psi\\) defining the finite sample minimax estimate does not depend on the sample size (only on \\(\\varepsilon, \\delta\\), and \\(a\\) ), even though, as already mentioned, \\(1 \\%\\) contamination has conceptionally quite different effects for sample size 5 and for sample size 1000 .</p> <p>The above results assume the scale to be fixed. For the more realistic case, where scale is a nuisance parameter, no exact finite sample results are known.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#chapter-11","title":"CHAPTER 11","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#miscellaneous-topics","title":"Miscellaneous Topics","text":""},{"location":"research/note/lecture-note-automation/result/ocr_result/#111-hampels-extremal-problem","title":"11.1 HAMPEL'S EXTREMAL PROBLEM","text":"<p>The minimax approaches described in Chapters 4 and 10 do not generalize beyond problems possessing a high degree of symmetry. This symmetry (e.g., translation invariance) is essential in their case, since it allows us to extend the parametrization of the idealized model throughout a neighborhood.</p> <p>Hampel (1968, 1974b) proposed an alternative approach that avoids this problem by strictly staying at the idealized model: minimize the asymptotic variance of the estimate at the model, subject to a bound on the gross error sensitivity. This works for essentially arbitrary one-parameter families (and can even be extended to multiparameter problems). The main drawback is of a conceptual nature: only \"infinitesimal\" deviations from the model are allowed.</p> <p>For \\(L\\) - and \\(R\\)-estimates the concept of gross-error sensitivity is of questionable value (compare Examples 3.5.1 and 3.5.2). We therefore restrict our attention to \\(M\\)-estimates.</p> <p>Let \\(f_{\\theta}(x)=f(x ; \\theta)\\) be a family of probability densities, relative to some measure \\(\\mu\\), indexed by a real parameter \\(\\theta\\). We intend to estimate \\(\\theta\\) by an \\(M\\)-estimate \\(T=T(F)\\), where the functional \\(T\\) is defined through an implicit equation</p> \\[ \\int \\psi(x ; T(F)) F(d x)=0 \\] <p>The function \\(\\psi\\) is to be determined by the following extremal property. Subject to Fisher consistency</p> \\[ T\\left(F_{\\theta}\\right)=\\theta \\] <p>(where \\(d F_{\\theta}=f_{\\theta} d \\mu\\) ), and subject to a prescribed bound \\(k(\\theta)\\) on the gross</p> <p>error sensitivity</p> \\[ \\left|I C\\left(x ; F_{\\theta}, T\\right)\\right| \\leqslant k(\\theta), \\quad \\text { for all } x \\] <p>the resulting estimate should minimize the asymptotic variance</p> \\[ \\int I C\\left(x ; F_{\\theta}, T\\right)^{2} d F_{\\theta} \\] <p>Hampel showed that the solution is of the form</p> \\[ \\psi(x ; \\theta)=[g(x ; \\theta)-a(\\theta)]_{-b(\\theta)}^{+b(\\theta)} \\] <p>where</p> \\[ g(x ; \\theta)=\\frac{\\partial}{\\partial \\theta} \\log f(x ; \\theta) \\] <p>and where \\(a(\\theta)\\) and \\(b(\\theta)&gt;0\\) are some functions of \\(\\theta\\); we are using the notation \\([x]_{u}^{\\nu}=\\max [u, \\min (v, x)]\\).</p> <p>How should we choose \\(k(\\theta)\\) ? Hampel had left the choice open, noting that the problem fails to have a solution if \\(k(\\theta)\\) is too small, and pointing out that it might be preferable to start with a sensible choice for \\(b(\\theta)\\), and then to determine the corresponding values of \\(a(\\theta)\\) and \\(k(\\theta)\\). We now sketch a somewhat more systematic approach, by proposing that \\(k(\\theta)\\) should be an arbitrarily chosen, but fixed, multiple of the \"average error sensitivity\" [i.e., of the square root of the asymptotic variance (1.4)]. Thus we put</p> \\[ k(\\theta)^{2}=k^{2} \\int I C\\left(x ; F_{\\theta}, T\\right)^{2} d F_{\\theta} \\] <p>where the constant \\(k\\) clearly must satisfy \\(k \\geqslant 1\\), but otherwise can be chosen freely (we would tentatively recommend the range \\(1&lt;k \\leqslant 2.5\\) ).</p> <p>We now discuss existence and uniqueness of \\(a(\\theta)\\) and \\(b(\\theta)\\), when \\(k(\\theta)\\) is defined by (1.7).</p> <p>The influence function of an \\(M\\)-estimate (1.1) at \\(F_{\\theta}\\) can be written as</p> \\[ I C\\left(x ; F_{\\theta}, T\\right)=\\frac{\\psi(x ; \\theta)}{\\int \\psi(x ; \\theta) g(x ; \\theta) f(x ; \\theta) d \\mu} \\] <p>see (3.2.13). Here, we have used Fisher consistency and have transformed the denominator by an integration by parts.</p> <p>The side conditions (1.2) and (1.3) now may be rewritten as</p> \\[ \\int \\psi(x ; \\theta) f(x ; \\theta) d \\mu=0 \\] <p>and</p> \\[ \\psi(x ; \\theta)^{2}&lt;k^{2} \\int \\psi(x ; \\theta)^{2} f(x ; \\theta) d \\mu \\] <p>while the expression to be minimized is</p> \\[ \\frac{\\int \\psi(x ; \\theta)^{2} f(x ; \\theta) d \\mu}{\\left[\\int \\psi(x ; \\theta) g(x ; \\theta) f(x ; \\theta) d \\mu\\right]^{2}} \\] <p>This extremal problem can be solved separately for each value of \\(\\theta\\). Existence of a minimizing \\(\\psi\\) follows in a straightforward way from the fact that \\(\\psi\\) is bounded (1.10) and from weak compactness of the unit ball in \\(L_{\\infty}\\).</p> <p>The explicit form of the minimizing \\(\\psi\\) can be found by the standard methods of the calculus of variations.</p> <p>If we apply a small variation \\(\\delta \\psi\\) to the \\(\\psi\\) in (1.9) to (1.11), we obtain as a necessary condition for the extremum</p> \\[ \\int[\\psi-\\lambda g+\\nu] \\delta \\psi f d \\mu \\geqslant 0 \\] <p>where \\(\\lambda\\) and \\(\\nu\\) are Lagrange multipliers. Since \\(\\psi\\) is only determined up to a multiplicative constant, we may standardize \\(\\lambda=1\\), and it follows that \\(\\psi=g-\\nu\\) for those \\(x\\) where it can be freely varied [i.e., where we have strict inequality in (1.10)]. Hence the solution must be of the form (1.5), apart from an arbitrary multiplicative constant, and excepting a limiting case to be discussed later [corresponding to \\(b(\\theta)=0\\) ].</p> <p>We first show that \\(a(\\theta)\\) and \\(b(\\theta)\\) exist, and that under mild conditions they are uniquely determined by (1.9) and by the following relation derived from (1.10):</p> \\[ b(\\theta)^{2}=k^{2} \\int \\psi(x ; \\theta)^{2} f(x ; \\theta) d \\mu \\] <p>To simplify the writing we work at one fixed \\(\\theta\\) and drop both arguments \\(x\\) and \\(\\theta\\) from the notation.</p> <p>Existence and uniqueness of the solution \\((a, b)\\) of (1.9) and (1.12) can be established by a method we have used already in Chapter 7. Namely, put</p> \\[ \\begin{aligned} \\rho(z) &amp; =\\frac{1}{2}\\left(k^{-2}+z^{2}\\right), &amp; &amp; \\text { for }|z|&lt;1 \\\\ &amp; =\\frac{1}{2}\\left(k^{-2}-1\\right)+|z|, &amp; &amp; \\text { for }|z|&gt;1 \\end{aligned} \\] <p>and let</p> \\[ Q(a, b)=E\\left\\{\\rho\\left(\\frac{g-a}{b}\\right) b-|g|\\right\\} \\] <p>We note that \\(Q\\) is a convex function of \\((a, b)\\) [this is a special case of (7.7.9)ff], and that it is minimized by the solution \\((a, b)\\) of the two equations</p> \\[ \\begin{gathered} E\\left[\\rho^{\\prime}\\left(\\frac{g-a}{b}\\right)\\right]=0 \\\\ E\\left[\\frac{g-a}{b} \\rho^{\\prime}\\left(\\frac{g-a}{b}\\right)-\\rho\\left(\\frac{g-a}{b}\\right)\\right]=0 \\end{gathered} \\] <p>obtained from (1.14) by taking partial derivatives with respect to \\(a\\) and \\(b\\). But these two equations are equivalent to (1.9) and (1.12), respectively.</p> <p>Note that this amounts to estimating a location parameter \\(a\\) and a scale parameter \\(b\\) for the random variable \\(g\\) by the method of Huber (1964, \"Proposal 2\"); compare Example 6.4.1. In order to see this let \\(\\psi_{0}(z)=\\rho^{\\prime}(z)\\) \\(=\\max (-1, \\min (1, z))\\), and rewrite (1.15) and (1.16) as</p> \\[ \\begin{aligned} &amp; E\\left[\\psi_{0}\\left(\\frac{g-a}{b}\\right)\\right]=0 \\\\ &amp; E\\left[\\psi_{0}\\left(\\frac{g-a}{b}\\right)^{2}\\right]=\\frac{1}{k^{2}} \\end{aligned} \\] <p>As in Chapter 7 it is easy to show that there is always some pair ( \\(a_{0}, b_{0}\\) ) with \\(b_{0} \\geqslant 0\\) minimizing \\(Q(a, b)\\).</p> <p>We first take care of the limiting case \\(b_{0}=0\\). For this it is advisable to scale \\(\\psi\\) differently, namely to divide the right-hand side of (1.5) by \\(b(\\theta)\\). In the limit \\(b=0\\) this gives</p> \\[ \\psi(x ; \\theta)=\\operatorname{sign}(g(x ; \\theta)-a(\\theta)) \\] <p>The differential conditions for \\(\\left(a_{0}, 0\\right)\\) to be a minimum of \\(Q\\) now have a \\(&lt;\\) sign in (1.16), since we are on the boundary, and they can be written out as</p> \\[ \\begin{gathered} \\int \\operatorname{sign}(g(x ; \\theta)-a(\\theta)) f(x ; \\theta) d \\mu=0 \\\\ 1 \\geqslant k^{2} P\\{g(x ; \\theta) \\neq a(\\theta)\\} \\end{gathered} \\] <p>If \\(k&gt;1\\), and if the distribution of \\(g\\) under \\(F_{\\theta}\\) is such that</p> \\[ P\\{g(x ; \\theta)=a\\}&lt;1-k^{-2} \\] <p>for all real \\(a\\), then (1.19) clearly cannot be satisfied. It follows that (1.20) is a sufficient condition for \\(b_{0}&gt;0\\). Conversely, the choice \\(k=1\\) forces \\(b_{0}=0\\). In particular, if \\(g(x ; \\theta)\\) has a continuous distribution under \\(F_{\\theta}, k&gt;1\\) is a necessary and sufficient condition for \\(b_{0}&gt;0\\).</p> <p>Assume now that \\(b_{0}&gt;0\\). Then, in a way similar to that in Section 7.7, we find that \\(Q\\) is strictly convex at \\(\\left(a_{0}, b_{0}\\right)\\) provided the following two assumptions are true: (1) \\(\\left|g-a_{0}\\right|&lt;b_{0}\\) with nonzero probability. (2) Conditionally on \\(\\left|g-a_{0}\\right|&lt;b_{0}, g\\) is not constant.</p> <p>It follows that then \\(\\left(a_{0}, b_{0}\\right)\\) is unique. In other words we now have determined a \\(\\psi\\) that satisfies the side conditions (1.9) and (1.10), and for which (1.11) is stationary under infinitesimal variations of \\(\\psi\\), and it is the unique such \\(\\psi\\). Thus we have found the unique solution to the minimum problem.</p> <p>Unless \\(a(\\theta)\\) and \\(b(\\theta)\\) can be determined in closed form, the actual calculation of the estimate \\(T_{n}=T\\left(F_{n}\\right)\\) through solving (1.1) may still be quite difficult. Also we may encounter the usual problems of ML estimation caused by nonuniqueness of solutions.</p> <p>The limiting case \\(b=0\\) is of special interest, since it corresponds to a generalization of the median. In detail this estimate works as follows. We first determine the median \\(a(\\theta)\\) of \\(g(x ; \\theta)=(\\partial / \\partial \\theta) \\log f(x ; \\theta)\\) under the true distribution \\(F_{\\theta}\\). Then we estimate \\(\\hat{\\theta}_{n}\\) from a sample of size \\(n\\) such that one-half of the sample values of \\(g\\left(x_{i} ; \\hat{\\theta}_{n}\\right)-a\\left(\\hat{\\theta}_{n}\\right)\\) are positive, and the other half negative.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#112-shrinking-neighborhoods","title":"11.2 SHRINKING NEIGHBORHOODS","text":"<p>An interesting asymptotic approach to robust testing (and, through the methods of Section 10.6, to estimation) is obtained by letting both the</p> <p>alternative hypotheses and the distance between them shrink with increasing sample size. This approach was first utilized by Huber-Carol (1970); recently, it has been further exploited by Rieder (1978, 1979, 1980a, b). The issues involved here deserve some discussion.</p> <p>First, we note that the exact finite sample results of Chapter 10 are not easy to deal with; unless the sample size \\(n\\) is very small, the size and minimum power are hard to calculate. This suggests the use of asymptotic approximations. Indeed for large values of \\(n\\), the test statistics, or more precisely, their logarithms (10.3.16) are approximately normal. But for increasing \\(n\\), either the size or the power of these tests, or both, tend to 0 or 1 , respectively, exponentially fast, which corresponds to a limiting theory we are interested in only very rarely. In order to get limiting sizes and powers that are bounded away from 0 and 1 , the hypotheses must approach each other at the rate \\(n^{-1 / 2}\\) (at least in the nonpathological cases). If the diameters of the composite alternatives are kept constant, while they approach each other until they touch, we typically end up with a limiting sign-test. This may be a very sensible test for extremely large sample sizes (cf. Section 4.2 for a related discussion in an estimation context), but the underlying theory is relatively dull. So we shrink the hypotheses at the same rate \\(n^{-1 / 2}\\), and then we obtain nontrivial limiting tests.</p> <p>Now two related questions pose themselves: (1) Determine the asymptotic behavior of the sequence of exact, finite sample minimax tests. (2) Find the properties of the limiting test; is it asymptotically equivalent to the sequence of the exact minimax tests?</p> <p>The appeal of this approach lies in the fact that it does not make any assumptions about symmetry, and we therefore have good chances to obtain a workable theory of asymptotic robustness for tests and estimates without assuming symmetry.</p> <p>However, there are conceptual drawbacks connected with these shrinking neighborhoods; somewhat pointedly, we may say that these tests are robust with regard to zero contamination only!</p> <p>It appears that there is an intimate connection between limiting robust tests determined on the basis of shrinking neighborhoods and the robust estimates found through Hampel's extremal problem (Section 11.1), which share the same conceptual drawbacks.</p> <p>This connection is now sketched very briefly; details can be found in the references mentioned at the beginning of this section; compare, in particular, Theorem 3.7 of Rieder (1978).</p> <p>Assume that \\(\\left(P_{\\theta}\\right)_{\\theta}\\) is a sufficiently regular family of probability measures, with densities \\(p_{\\theta}\\), indexed by a real parameter \\(\\theta\\). To fix the idea consider total variation neighborhoods \\(\\mathscr{P}_{\\theta, \\delta}\\) of \\(P_{\\theta}\\), and assume that we are to test robustly between the two composite hypotheses</p> \\[ \\mathscr{P}_{\\theta \\pm n^{-1 / 2} \\tau, n^{-1 / 2} \\delta} \\] <p>According to Chapter 10 the minimax tests between these hypotheses will be based on test statistics of the form</p> \\[ \\sum \\psi_{n}\\left(X_{i}\\right) \\] <p>where \\(\\psi_{n}(X)\\) is a censored version of</p> \\[ \\log \\left(\\frac{p_{\\theta+n^{-1 / 2} \\tau}(X)}{p_{\\theta-n^{-1 / 2} \\tau}(X)}\\right) \\] <p>Clearly, the limiting test will be based on</p> \\[ \\sum \\psi\\left(X_{i}\\right) \\] <p>where \\(\\psi(X)\\) is a censored version of</p> \\[ \\frac{\\partial}{\\partial \\theta}\\left[\\log p_{\\theta}(X)\\right] \\] <p>It can be shown under quite mild regularity conditions that the limiting test is indeed asymptotically equivalent to the sequence of exact minimax tests.</p> <p>If we standardize \\(\\psi\\) by subtracting its expected value, so that</p> \\[ \\int \\psi d P_{\\theta}=0 \\] <p>it turns out that the censoring is symmetric:</p> \\[ \\psi(X)=\\left[\\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}-a_{\\theta}\\right]_{-b_{\\theta}}^{+b_{\\theta}} \\] <p>Note that this is formally identical to (1.5) and (1.6). In our case the</p> <p>constants \\(a_{\\theta}\\) and \\(b_{\\theta}\\) are determined by</p> \\[ \\int\\left(\\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}-a_{\\theta}-b_{\\theta}\\right)^{+} d P_{\\theta}=\\int\\left(\\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}-a_{\\theta}+b_{\\theta}\\right)^{-} d P_{\\theta}=\\frac{\\delta}{\\tau} \\] <p>In the above case the relations between the exact finite sample tests and the limiting test are straightforward, and the properties of the latter are easy to interpret. In particular, (2.8) shows that it will be very nearly minimax along a whole family of total variation neighborhood alternatives with a constant ratio \\(\\delta / \\tau\\).</p> <p>Trickier problems arise if such a shrinking sequence is used to describe and characterize the robustness properties of some given test. We noted earlier that some estimates get relatively less robust when the neighborhood shrinks, in the precise sense that the estimate is robust, but \\(\\lim b(\\varepsilon) / \\varepsilon\\) \\(=\\infty\\); compare Section 3.5. In particular, the normal scores estimate has this property. It is therefore not surprising that the robustness properties of the normal scores test do not show up in a naive shrinking neighborhood model [cf. Rieder (1979, 1980b)].</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#references","title":"References","text":"<p>D. F. Andrews et al. (1972), Robust Estimates of Location: Survey and Advances, Princeton University Press, Princeton N.J. F. J. Anscombe (1960), Rejection of outliers, Technometrics, 2, pp. 123-147. V. I. Averbukh and O. G. Smolyanov (1967), The theory of differentiation in linear topological spaces, Russian Math. Surveys, 22, pp. 201-258. \\(\\qquad\\) (1968), The various definitions of the derivative in linear topological spaces, Russian Math. Surveys, 23, pp. 67-113. R. Beran (1974), Asymptotically efficient adaptive rank estimates in location models, Ann. Statist., 2, pp. 63-74. \\(\\qquad\\) (1977a), Robust location estimates, Ann. Statist., 5, pp. 431-444. \\(\\qquad\\) (1977b), Minimum Hellinger distance estimates for parametric models, Ann. Statist., 5, pp. 445-463. \\(\\qquad\\) (1978), An efficient and robust adaptive estimator of location, Ann. Statist., 6, pp. 292-313. P. J. Bickel (1973), On some analogues to linear combinations of order statistics in the linear model, Ann. Statist., 1, pp. 597-616. \\(\\qquad\\) (1975), One-step Huber estimates in the linear model, J. Amer. Statist. Ass., 70, pp. \\(428-434\\). \\(\\qquad\\) (1976), Another look at robustness: A review of reviews and some new developments, Scand. J. Statist., 3, pp. 145-168. P. J. Bickel and A. M. Herzberg (1979), Robustness of design against autocorrelation in time I, Ann. Statist., 7, pp. 77-95. P. J. Bickel and J. L. Hodges (1967), The asymptotic theory of Galton's test and a related simple estimate of location, Ann. Math. Statist., 4, pp. 68-85. P. Billingsley (1968), Convergence of Probability Measures, John Wiley, New York. G. E. P. Box and N. R. Draper (1959), A basis for the selection of a response surface design, J. Amer. Statist. Ass., 54, pp. 622-654. N. Bourbaki (1952), Int\u00e9gration, Ch. III, Hermann, Paris. H. Chen, R. Gnanadesikan, and J. R. Kettenring (1974), Statistical methods for grouping corporations, Sankhya, B36, pp. 1-28. H. Chernoff, J. L. Gastwirth, and M. V. Johns (1967), Asymptotic distribution of linear combinations of functions of order statistics with applications to estimation, Ann. Math. Statist., 38, pp. 52-72.</p> <p>G. Choquet (1953/54), Theory of capacities, Ann. Inst. Fourier, 5, pp. 131-292. \\(\\qquad\\) (1959), Forme abstraite du th\u00e9or\u00e8me de capacitabilit\u00e9, Ann. Inst. Fourier, 9, pp. 83-89. J. R. Collins (1976), Robust estimation of a location parameter in the presence of asymmetry, Ann. Statist., 4, pp. 68-85. C. Daniel and F. S. Wood (1971), Fitting Equations to Data, John Wiley, New York. H. E. Daniels (1954), Saddle point approximations in statistics, Ann. Math. Statist., 25, pp. \\(631-650\\). \\(\\qquad\\) (1976), Paper presented at the Grenoble Statistics Meeting, 1976. A. P. Dempster (1967), Upper and lower probabilities induced by a multivalued mapping, Ann. Math. Statist., 38, pp. 325-339. \\(\\qquad\\) (1968), A generalization of Bayesian inference, J. Roy. Statist. Soc., B30, pp. 205-247. \\(\\qquad\\) (1975), A subjectivist look at robustness, Proc. 40th Session I. S. I., Warsaw, Bull. Int. Statist. Inst., 46, Book 1, pp. 349-374. L. Denby and C. L. Mallows (1977), Two diagnostic displays for robust regression analysis, Technometrics, 19, pp. 1-13. S. J. Devlin, R. Gnanadesikan, and J. R. Kettenring (1975), Robust estimation and outlier detection with correlation coefficients, Biometrika, 62, pp. 531-545. \\(\\qquad\\) (1979), Robust estimation of dispersion matrices and principal components, submitted to J. Amer. Statist. Ass. J. L. Doob (1953), Stochastic Processes, John Wiley, New York. R. M. Dudley (1969), The speed of mean Glivenko-Cantelli convergence, Ann. Math. Statist., 40, pp. 40-50. R. Dutter (1975), Robust regression: Different approaches to numerical solutions and algorithms, Res. Rep. no. 6, Fachgruppe f\u00fcr Statistik, Eidgen. Technische Hochschule, Zurich. \\(\\qquad\\) (1976), LINWDR: Computer linear robust curve fitting program, Res. Rep. no. 10, Fachgruppe f\u00fcr Statistik, Eidgen. Technische Hochschule, Zurich. \\(\\qquad\\) (1977a), Numerical solution of robust regression problems: Computational aspects, a comparison. J. Statist. Comput. Simul., 5, pp. 207-238. \\(\\qquad\\) (1977b), Algorithms for the Huber estimator in multiple regression, Computing, 18, pp. 167-176. \\(\\qquad\\) (1978), Robust regression: LINWDR and NLWDR, COMPSTAT 1978, Proc., in Computational Statistics, L. C. A. Corsten, Ed., Physica-Verlag, Vienna. A. S. Eddington (1914), Stellar Movements and the Structure of the Universe, Macmillan, London. W. Feller (1966), An Introduction to Probability Theory and its Applications, Vol. II, John Wiley, New York. C. A. Field and F. R. Hampel (1980), Small sample asymptotic distributions of \\(M\\)-estimates of location, submitted to Biometrika.</p> <p>A. A. Filippova (1962), Mises' theorem of the asymptotic behavior of functionals of empirical distribution functions and its statistical applications, Theor. Prob. Appl., 7, pp. \\(24-57\\). R. A. Fisher (1920), A mathematical examination of the methods of determining the accuracy of an observation by the mean error and the mean square error, Monthly Not. Roy. Astron. Soc., 80, pp. 758-770. D. Gale and H. Nikaid\u00f4 (1965), The Jacobian matrix and global univalence of mappings, Math. Ann., 159, pp. 81-93. R. Gnanadesikan and J. R. Kettenring (1972), Robust estimates, residuals and outlier detection with multiresponse data, Biometrics, 28, pp. 81-124. A. M. Gross (1977), Confidence intervals for bisquare regression estimates, \\(J\\). Amer. Statist. Ass., 72, pp. 341-354. J. H\u00e1jek (1968), Asymptotic normality of simple linear rank statistics under alternatives, Ann. Math. Statist., 39, pp. 325-346. J. H\u00e1jek (1972), Local asymptotic minimax and admissibility in estimation, in: Proc. Sixth Berkeley Symposium on Mathematical Statistics and Probability, Vol. 1. University of California Press, Berkeley. J. H\u00e1jek and V. Dupa\u010d (1969), Asymptotic normality of simple linear rank statistics under alternatives, II, Ann. Math. Statist., 40, pp. 1992-2017. J. H\u00e1jek and Z. \u0160id\u00e1k (1967), Theory of Rank Tests, Academic, New York. W. C. Hamilton (1970), The revolution in crystallography, Science, 169, pp. \\(133-141\\). F. R. Hampel (1968), Contributions to the theory of robust estimation, Ph. D. Thesis, University of California, Berkeley. \\(\\qquad\\) (1971), A general qualitative definition of robustness, Ann. Math. Statist., 42, pp. \\(1887-1896\\). (1973a), Robust estimation: A condensed partial survey, Z. Wahrscheinlichkeitstheorie Verw. Gebiete, 27, pp. 87-104. (1973b), Some small sample asymptotics, Proc. Prague Symposium on Asymptotic Statistics, Prague. (1974a), Rejection rules and robust estimates of location: An analysis of some Monte Carlo results, Proc. European Meeting of Statisticians and 7th Prague Conference on Information Theory, Statistical Decision Functions and Random Processes, Prague, 1974. (1974b), The influence curve and its role in robust estimation, J. Amer. Statist. Ass., 62, pp. 1179-1186. (1975), Beyond location parameters: Robust concepts and methods, Proc. 40th Session I. S. I., Warsaw 1975, Bull. Int. Statist. Inst., 46, Book 1, pp. 375-382. (1976), On the breakdown point of some rejection rules with mean, Res. Rep. no. 11, Fachgruppe f\u00fcr Statistik, Eidgen. Technische Hochschule, Zurich. E. F. Harding and D. G. Kendall (1974), Stochastic Geometry, Wiley, London. R. W. Hill (1977), Robust regression when there are outliers in the carriers, Ph. D. Thesis, Harvard University, Cambridge, Mass.</p> <p>R. V. Hogg (1967), Some observations on robust estimation, J. Amer. Statist. Ass., 62, pp. 1179-1186. (1972), More light on kurtosis and related statistics, J. Amer. Statist. Ass., 67, pp. 422-424. (1974), Adaptive robust procedures, J. Amer. Statist. Ass., 69, pp. 909-927. D. C. Hoaglin and R. E. Welsch (1978), The hat matrix in regression and ANOVA, Amer. Statist., 32, pp. 17-22. P. W. Holland and R. E. Welsch (1977), Robust regression using iteratively reweighted least squares, Comm. Statist., A6, pp. 813-827. P. J. Huber (1964), Robust estimation of a location parameter, Ann. Math. Statist., 35, pp. 73-101. (1965), A robust version of the probability ratio test, Ann. Math. Statist., 36, pp. 1753-1758. (1966), Strict efficiency excludes superefficiency, (Abstract), Ann. Math. Statist., 37, p. 1425. (1967), The behavior of maximum likelihood estimates under nonstandard conditions, in: Proc. Fifth Berkeley Symposium on Mathematical Statistics and Probability, Vol. 1, University of California Press, Berkeley. (1968), Robust confidence limits, Z. Wahrscheinlichkeitstheorie Verw. Gebiete, 10, pp. 269-278. (1969), Th\u00e9orie de l'Inf\u00e9rence Statistique Robuste, Presses de l'Universit\u00e9, Montreal. (1970), Studentizing robust estimates, in: Nonparametric Techniques in Statistical Inference, M. L. Puri, Ed., Cambridge University Press, Cambridge, England. (1972), Robust statistics: A review, Ann. Math. Statist., 43, pp. 1041-1067. (1973a), Robust regression: Asymptotics, conjectures and Monte Carlo, Ann. Statist., 1, pp. 799-821. (1973b), The use of Choquet capacities in statistics, Bull. Int. Statist. Inst., Proc. 39th Session, 45, pp. 181-191. (1975), Robustness and designs, in: A Survey of Statistical Design and Linear Models, J. N. Srivastava, Ed., North Holland, Amsterdam. (1976), Kapazit\u00e4ten statt Wahrscheinlichkeiten? Gedanken zur Grundlegung der Statistik, Jber. Deutsch. Math.-Verein., 78, H.2, pp. 81-92. (1977a), Robust covariances, in: Statistical Decision Theory and Related Topics, II, S. S. Gupta and D. S. Moore, Eds., Academic Press, New York. (1977b), Robust Statistical Procedures, Regional Conference Series in Applied Mathematics No. 27, Soc. Industr. Appl. Math, Philadelphia, Penn. (1979), Robust smoothing, Proc. ARO Workshop on Robustness in Statistics, April 11-12, 1978, R. L. Launer and G. N. Wilkinson, Eds., Academic Press, New York. P. J. Huber and R. Dutter (1974), Numerical solutions of robust regression problems, in: COMPSTAT 1974, Proc. in Computational Statistics, G. Bruckmann, Ed., Physika Verlag, Vienna.</p> <p>P. J. Huber and V. Strassen (1973), Minimax tests and the Neyman-Pearson lemma for capacities, Ann. Statist., 1, pp. 251-263; 2, pp. 223-224. C. Huber-Carol (1970), Etude asymptotique de tests robustes, Ph.D. Thesis, Eidgen. Technische Hochschule, Zurich. L. A. Jaeckel (1971a), Robust estimates of location: Symmetry and asymmetric contamination, Ann. Math. Statist., 42, pp. 1020-1034. (1971b), Some flexible estimates of location, Ann. Math. Statist., 42, pp. \\(1540-1552\\). (1972), Estimating regression coefficients by minimizing the dispersion of the residuals, Ann. Math. Statist., 43, pp. 1449-1458. J. Jure\u010dkov\u00e1 (1971), Nonparametric estimates of regression coefficients, Ann. Math. Statist., 42, pp. 1328-1338. L. Kantorovi\u010d and G. Rubinstein (1958), On a space of completely additive functions, Vestnik, Leningrad Univ., 13, no. 7 (Ser. Mat. Astr. 2), pp. 52-59, in Russian. J. L. Kelley (1955), General Topology, Van Nostrand, New York. G. D. Kersting (1978), Die Geschwindigkeit der Glivenko-Cantelli-Konvergenz gemessen in der Prohorov-Metrik, Habilitationsschrift, Georg-August-Universit\u00e4t, G\u00f6ttingen. B. Kleiner, R. D. Martin, and D. J. Thomson (1979), Robust estimation of power spectra, J. Roy. Statist. Soc., B41, No. 3, pp. 313-351. W. S. Krasker and R. E. Welsch (1980), Efficient bounded influence regression estimation using alternative definitions of sensitivity, unpublished. H. W. Kuhn and A. W. Tucker (1951), Nonlinear programming, in: Proc. Second Berkeley Symposium on Mathematical Statistics and Probability, University of California Press, Berkeley. C. L. Lawson and R. J. Hanson (1974), Solving Least Squares Problems, Prentice Hall, Englewood Cliffs, N.J. L. LeCam (1953), On some asymptotic properties of maximum likelihood estimates and related Bayes' estimates, Univ. Calif. Publ. Statist., 1, pp. 277-330. E. L. Lehmann (1959), Testing Statistical Hypotheses, John Wiley, New York. R. A. Maronna (1976), Robust \\(M\\)-estimators of multivariate location and scatter, Ann. Statist., 4, pp. 51-67. G. Matheron (1975), Random Sets and Integral Geometry, John Wiley, New York. R. Miller (1964), A trustworthy jackknife, Ann. Math. Statist., 35, pp. 1594-1605. (1974), The jackknife-A review, Biometrika, 61, pp. 1-15. F. Mosteller and J. W. Tukey (1977), Data Analysis and Regression, Addison-Wesley, Reading, Mass. J. Neveu (1964), Bases Math\u00e9matiques du Calcul des Probabilit\u00e9s, Masson, Paris; English translation by A. Feinstein, (1965), Mathematical Foundations of the Calculus of Probability, Holden-Day, San Francisco. Y. V. Prohorov (1956), Convergence of random processes and limit theorems in probability theory, Theor. Prob. Appl., 1, pp. 157-214.</p> <p>M. H. Quenouille (1956), Notes on bias in estimation, Biometrika, 43, pp. 353-360. J. A. Reeds (1976), On the definition of von Mises functionals, Ph. D. thesis, Dept. of Statistics, Harvard University, Cambridge, Mass. D. A. Relles and W. H. Rogers (1977), Statisticians are fairly robust estimators of location, J. Amer. Statist. Ass., 72, pp. 107-111. W. J. J. Rey (1978), Robust Statistical Methods, Lecture Notes in Mathematics, 690, Springer-Verlag, Berlin. H. Rieder (1978), A robust asymptotic testing model, Ann. Statist., 6, pp. 1080-1094. (1979), Robustness of one and two sample rank tests against gross errors, to appear in Ann. Statist., 9. (1980a), On local asymptotic minimaxity and admissibility in robust estimation, unpublished. (1980b), Qualitative robustness of rank tests, unpublished. M. Romanowski and E. Green (1965), Practical applications of the modified normal distribution, Bull. G\u00e9od\u00e9sique, 76, pp. 1-20. J. Sacks (1975), An asymptotically efficient sequence of estimators of a location parameter, Ann. Statist., 3, pp. 285-298. J. Sacks and D. Ylvisaker (1972), A note on Huber's robust estimation of a location parameter, Ann. Math. Statist., 43, pp. 1068-1075. (1978) Linear estimation for approximately linear models, Ann. Statist., 6, pp. 1122-1137. H. Sch\u00f6nholzer (1979), Robuste Kovarianz, Ph. D. Thesis, Eidgen. Technische Hochschule, Zurich. F. W. Scholz (1971), Comparison of optimal location estimators, Ph. D. Thesis, Dept. of Statistics, University of California, Berkeley. G. Shafer (1976), A Mathematical Theory of Evidence, Princeton University Press, Princeton, N.J. G. R. Shorack (1976), Robust studentization of location estimates, Statistica Neerlandica, 30, pp. 119-141. C. Stein (1956), Efficient nonparametric testing and estimation, in: Proc. Third Berkeley Symposium on Mathematical Statistics and Probability, Vol. 1, University of California Press, Berkeley. S. M. Stigler (1969), Linear functions of order statistics, Ann. Math. Statist., 40, pp. \\(770-788\\). (1973), Simon Newcomb, Percy Daniell and the history of robust estimation 1885-1920, J. Amer. Statist. Assoc., 68, pp. 872-879. C. J. Stone (1975), Adaptive maximum likelihood estimators of a location parameter., Ann. Statist., 3, pp. 267-284. V. Strassen (1964), Messfehler und Information, Z. Wahrscheinlichkeitstheorie Verw. Gebiete, 2, pp. 273-305. (1965), The existence of probability measures with given marginals, Ann. Math. Statist., 36, pp. 423-439.</p> <p>K. Takeuchi (1971), A uniformly asymptotically efficient estimator of a location parameter, J. Amer. Statist. Ass., 66, pp. 292-301. E. N. Torgerson (1970), Comparison of experiments when the parameter space is finite, Z. Wahrscheinlichkeitstheorie Verw. Gebiete, 16, pp. 219-249. (1971), A counterexample on translation invariant estimators, Ann. Math. Statist., 42, pp. 1450-1451. J. W. Tukey (1958), Bias and confidence in not-quite large samples (Abstract), Ann. Math. Statist., 29, p. 614. (1960), A survey of sampling from contaminated distributions, in: Contributions to Probability and Statistics, I. Olkin, Ed., Stanford University Press, Stanford, Calif. (1970), Exploratory Data Analysis, mimeographed preliminary edition. (1977), Exploratory Data Analysis, Addison-Wesley, Reading, Mass. R. von Mises (1937), Sur les fonctions statistiques, in: Conf\u00e9rence de la R\u00e9union Internationale des Math\u00e9maticiens, Gauthier-Villars, Paris; also in: Selecta R. von Mises, Vol. II, Amer. Math. Soc., Providence, R.I. 1964. (1947), On the asymptotic distribution of differentiable statistical functions, Ann. Math. Statist., 18, pp. 309-348. G. Wolf (1977), Obere und untere Wahrscheinlichkeiten, Ph. D. Thesis, Eidgen, Technische Hochschule, Zurich. V. J. Yohai and R. A. Maronna (1979), Asymptotic behavior of \\(M\\)-estimators for the linear model, Ann. Statist., 7, pp. 258-268.</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#index","title":"Index","text":"<p>Adaptive estimate, vi, 7 Analysis of variance, 195-198 Andrews, D. F., 54, 101, 103, 144, 175, 191 Ansari-Bradley-Siegel-Tukey test, 115 Anscombe, F. J., 73 Asymmetric contamination, 104-105 Asymptotic approximations, 47 Asymptotic efficiency, of M-, L-, and R- estimate, 68-72 of scale estimate, 116-118 Asymptotic expansion, 47, 170 Asymptotic minimax theory, for location, 73 Asymptotic normality, 11 of fitted value, 159 of L-estimate, 60 of M-estimate, 50 of multiparameter M-estimate, 132-135 of regression estimate, 158-159 of regression M-estimate, 170 of robust estimate of scatter matrix, 226227 via Fr\u00e9chet derivative, 39 Asymptotic properties, of M-estimate, 4551 Asymptotic relative efficiency, 2-3, 5 of covariance/correlation estimate, 210211 Asymptotics, of robust regression, 164-170 Averbukh, V. I., 40 Bayes formula, robust version, 253, 263264 Bayesian robustness, vi Beran, R., 7 Bias, 11 compared to statistical variability, 75-76 maximum, 11-12, 104 of L-estimate, 59 of M-estimate, 52 of R-estimate, 67 minimax, 74-76, 205 in regression, 243, 252 in robust regression, 171-172 of scale estimate, 107 Bickel, P. J., 164, 243 Billingsley, P., 20 Binomial distribution, minimax robust test, 273 Biweight, 101-103 Borel-sigma-algebra, 20 Bounded Lipschitz metric, 29-35, 39 Bourbaki, N., 77 Box, G. E. P., v, 252 Breakdown, by implosion, 141, 234 Breakdown point, 13, 104 of estimate of scatter matrix, 227-228 of Hodges-Lehmann estimate, 67 of joint M-estimate of location and scale, 141-144 of L-estimate, 60, 72 of median absolute deviation (MAD), 176 of M-estimate, 54 of M-estimate of scale, 110 of M-estimate with preliminary scale estimate, 144 of normal scores estimate, 67 of proposal, 2, 143-144 of R-estimate, 67 of symmetrized scale estimate, 114 of trimmed mean, 13, 104-106, 143-144 variance, 13,106 Capacity, 253-254 monotone and alternating of infinite order, 262-264 2-monotone and 2-alternating, 260-262, 276 Cauchy distribution, efficient estimate for, 71 Censoring, 264, 292</p> <p>Chen, H., 200 Chernoff, H., 60 Choquet, G., 261, 263 Collins, J. R., 101 Comparison function, 180, 182, 184-186, 239 Computation of M-estimate, 146 with modified residuals, 146 with modified weights, 146-147 by Newton-like method, 146-148 Computation of regression M-estimate, 1719, 179-192 convergence, 187-191 with modified residuals, 181 with modified weights, 183 Computation of robust covariance estimate, 237-242 Conjugate gradient method, 240-242 Consistency, 6, 8, 11 of fitted value, 157 of L-estimate, 60 of M-estimate, 48 of multiparameter M-estimate, 127-132 of R-estimate, 68 of robust estimate of scatter matrix, 226227 Consistent estimate, 41 Contaminated normal distribution, 2 minimax estimate for, 97,99 Contamination, 110, 271 asymmetric, 104-105 Contamination neighborhood, 11, 75, 85, 276 Continuity, of L-estimate, 60 of M-estimate, 54 of R-estimate, 68 of statistical functional, 41 of trimmed mean, 60 of Winsorized mean, 60 Correlation, robust, 203-204 Correlation matrix, 199 Covariance, estimation of matrix elements through robust correlation, 204-211 estimation of matrix elements through robust variances, 202-203 robust, 202 Covariance estimation in regression, 172-175 correction factors, 173-174 Covariance matrix, 17, 199 Cram\u00e9r-Rao bound, 4</p> <p>Daniell's theorem, 24 Daniels, H. E., 47 Dempster, A. P., 263 Derivative, Fr\u00e9chet, 34-40 G\u00e2teaux, 34-40 Descending M-estimate, 100-103 enforcing uniqueness, 54 minimax, 100-102 of regression, 191-192 sensitive to wrong scale, 102-103 Design, robustness, 243 Design matrix, conditions on, 165 errors in, 163, 195 Deviation, mean absolute and mean square, 2 Deviations, from linearity, 243 Devlin, S. J., 200, 203 Differentiability, Fr\u00e9chet, 35, 37 Discriminant analysis, 199 Distance, Bounded Lipschitz, see Bounded Lipschitz metric Kolmogorov, see Kolmogorov metric L\u00e9vy, see L\u00e9vy metric Prohorov, see Prohorov metric total variation, see Total variation metric Distributional robustness, 1,4 Distribution, limiting, of M-estimate, 48 Distribution-free, distinction between robust and, 6 Distribution function, empirical, 8 Doob, J. L., 128 Draper, N. R., 252 Dudley, R. M., 40 Dupac, V., 116 Dutter, R., 184, 187, 191 Eddington, A. S., v, 2 Edgeworth expansion, 47-48 Efficiency, absolute, 5 asymptotic, of M-, L-, and R-estimate, \\(68-72\\) of scale estimate, 116-118 asymptotic relative, 2-3, 5 Efficient estimate, for Cauchy distribution, 71 for least informative distribution, 71, 99 for logistic distribution, 71 for normal distribution, 70 Ellipsoid, to describe shape of pointcloud, 199</p> <p>Elliptic density, 211, 237 Empirical distribution function, or measure, 8 Error, gross, 3, 5, 9 Estimate, adaptive, vi, 7 consistent, 41 defined through implicit equations, 130 defined through a minimum property, \\(128-130\\) derived from rank test, see R-estimate derived from test, 278-282 Hodges-Lehmann, see Hodges-Lehmann estimate L-, see L-estimate of location and scale, 127 \\(\\mathrm{L}_{1}-\\), see \\(\\mathrm{L}_{1}\\)-estimate \\(\\mathrm{L}_{\\mathrm{p}^{-}}\\), see \\(\\mathrm{L}_{\\mathrm{p}}\\)-estimate M-, see M-estimate maximum likelihood, 8 Maximum likelihood type, see M-estimate minimax interval, 282-285 minimax of location and scale, 137 \\(\\mathrm{R}-\\), see R -estimate randomized, 279, 281, 285 of scale, 107 Expansion, asymptotic, 47, 170 Edgeworth, 47-48 Expectation, lower and upper, 254 Factor analysis, 199 Feller, W., 51, 159 Field, C. A., 48 Filippova, A. A., 40 Finite sample, 253 minimax robustness, 265 Fisher, R. A., 2 Fisher consistency, 6, 108, 149, 286 Fisher information, 68, 77 convexity, 79 distribution minimizing, 77-82, 208, 229237 equivalent expressions for, 82 minimization by variational methods, 82 90 minimized for \\(\\epsilon\\)-contamination, 85 for scale, 118-122 Fisher information matrix, 133 Fitted value, asymptotic normality, 159 consistency, 157 Fr\u00e9chet derivative, 34-40, 37, 39, 68</p> <p>Fr\u00e9chet differentiability, 35, 37, 68 Functional, statistical, 8,9 weakly continuous, 41 Gale, D., 139 G\u00e2teaux derivative, 34-40, 115 Global fit, minimax, 243-251 Gnanadesikan, R., 200, 202 Green, E., 91, 94 Gross error, 3, 5, 9 Gross error model, 11 generalized, 262-263 see also Contamination neighborhood Gross error sensitivity, 14, 17, 71, 74, 286287 of questionable value for L - and R estimate, 286 Grouping, 9 H\u00e1jek, J., 70, 116, 207 Hamilton, W. C., 164 Hampel, F. R., 10, 13-14, 17, 38, 41, 47, \\(48,74,101-102,194,286-287\\) Hampel estimate, 101-103, 146-147 Hampel's extremal problem, 286-290, 291 Hampel's theorem, 40-42 Harding, E. F., 263 Hat matrix, 156, 165 updating, 160-161 Herzberg, A. M., 243 Hodges-Lehmann estimate, 9, 63, 71, 145146 breakdown point, 67 influence function, 64 Hogg, R. V., 7 Huber-Carol, C., 291 Hunt-Stein theorem, 284 Influence curve, see Influence function Influence function, 13, 38 and asymptotic variance, 15 of Hodges-Lehmann estimate, 64 of interquantile distance, 111 and jackknife, 16, 150-152 of joint estimate of location and scale, 136-137 of L-estimate, 56-59 of median, 57 of median absolute deviation (MAD), 137138</p> <p>of M-estimate, 45, 287 of normal scores estimate, 65 of one-step M-estimate, 140-141 of quantile, 56 of R-estimate, 63-65 of robust estimate of scatter matrix, 223 of trimmed mean, 57-58 of trimmed standard deviation, 112 of Winsorized mean, 58 used for studentizing, 150-152 Interquantile distance, 126 influence function, 111 Interquartile distance, 12 compared to median absolute deviation (MAD), 107 Interval estimate, derived from rank test, 6 minimax, 282-285 Intuition, unreliability, 59 Iterativc reweighting, see Modified weights Jackknife, 15-16, 149-152 Jackknifed pseudovalue, 15 Jaeckel, L. A., 98, 163 Jeffreys, H., v Jure\u010dkov\u00e1, J., 163 Kantorovic, L., 30 Kelley, J. L., 22 Kendall, D. G., 263 Kersting, G. D., 40 Kettenring, J. R., 200, 202 Kleiner, B., 19 Klotz test, 115, 118 Kolmogorov metric, 34, 86, 110, 271 Krasker, W. S., 194 Kuhn-Tucker theorem, 30 Least favorable, pair of distributions, 266 see also Least informative distribution Least informative distribution, 71 discussion of its realism, 91 for \\(\\epsilon\\)-contamination, 85,87 for Kolmogorov metric, 86-90 for multivariate location, 229-231 for multivariate scatter, 231-234 for scale, 120-121 Least squares, 155 robustizing, 162-164 LeCam, L., 70 Lehmann, E. L., 52, 271, 275, 284</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#index_1","title":"INDEX","text":"<p>L-estimate, vi, 43, 55-61, 127 asymptotically efficient, 68-72 asymptotic normality, 60 breakdown point, 60, 72 consistency, 60 continuous as a functional, 60 gross error sensitivity, 286 influence function, 56-59 for logistic, 71 maximum bias, 59 minimax properties, 97-99 quantitative and qualitative robustness, 59-61 of regression, one-step, 164 of scale, 110-114, 117 \\(\\mathrm{L}_{1}\\)-estimate, 176 of regression, 164, 178 \\(\\mathrm{L}_{\\mathrm{p}}\\)-estimate, 133 Leverage point, 155, 160, 192-195, 243 L\u00e9vy metric, 25-29, 34-35, 39, 41, 110, 271 L\u00e9vy neighborhood, 11, 13, 75 Liggett, T., 79 Limiting distribution, of M-estimate, 48 Lindeberg condition, 49 Linear combinations of order statistics, see L-estimate Linearity, deviations from, 243 Lipschitz metric, bounded, see Bounded Lipschitz metric Location estimate, multivariate, 222 Location step, in computation of robust covariance matrix, 238 with modified residuals, 181 with modified weights, 183 Logistic distribution, efficient estimate for, 71 Lower expectation, 254 Lower probability, 254 MAD, see Median absolute deviation Mallows, C., 194 Maronna, R. A., 170, 215, 223, 226, 239 Matheron, G., 263 Maximum bias, 11-12, 104 Maximum likelihood estimate, 8 of scatter matrix, 211-215 Maximum likelihood type estimate, see M-estimate Maximum variance, 11-12</p> <p>under asymmetric contamination, 104 105 Mean absolute deviation, 2 Mean square deviation, 2 Measure, empirical, 8 regular, 20-21 substochastic, 76,81 Measure with finite support, 22 Median, 54, 97, 129, 137, 290 has minimax bias, 75 influence function, 57 Median absolute deviation (MAD), 107-\\(109,112,114,144-146,175,205\\) compared to halved interquartile distance, 107 influence function, 137 as the most robust estimate of scale, 122 Median absolute residual, 175-176 M-estimate, 43-55, 127 asymptotically efficient, 68-72 asymptotically minimax, 94-97, 177 asymptotically minimax for contaminated normal, 97 asymptotic normality, 50 asymptotic normality of multiparameter, 132-135 asymptotic properties, 45-51 breakdown point, 54 computation, 146 consistency, 48, 127-132 continuous as a functional, 54 exact distribution, 47 influence function, 45, 287 maximum bias, 52 nonnormal limiting distribution, 51, 96 one-step, 140 with preliminary scale estimate, 140-141 breakdown point, 144 quantitative and qualitative robustness, 52-54 studentizing, 150 M-estimate of location, 43, 285 M-estimate of location and scale, 135-139 breakdown, 141 existence, 138 uniqueness, 138 M-estimate of regression, 162 computation, 179-192 M-estimate of scale, 109-110, 117, 125 breakdown point, 110 minimax properties, 122-124 Metric, Bounded Lipschitz, see Bounded Lipschitz metric Kolmogorov, see Kolmogorov metric L\u00e9vy, see L\u00e9vy metric Prohorov, see Prohorov metric total variation, see Total variation metric Miller, R., 15 Minimax bias, 74-76, 205 Minimax descending M-estimate, 100-102 Minimax global fit, 243-251 Minimax interval estimate, 282-285 Minimax properties, of L-estimate, 97-99 of M-estimate, 94-97 of M-estimate of scale, 122-124 of M-estimate of scatter, 234 of R-estimate, 97-99 Minimax robustness, asymptotic, 17 finite sample, 17, 265 Minimax slope, 251 Minimax test, 264-273, 271 for binomial distribution, 273 for normal distribution, 272 Minimax theory, asymptotic, for location, 73 Minimax variance, 76 Modified residuals, in computing regression estimate, 181 Modified weights, in computing regression estimate, 183 Mood test, 115 Mosteller, F., 7 Multiparameter problems, 127 Multivariate location estimate, 222 Neighborhood, closed delta-, 26 contamination, see Contamination neighborhood Kolmogorov, see Kolmogorov neighborhood L\u00e9vy, see L\u00e9vy neighborhood Prohorov, see Prohorov neighborhood total variation, see Total variation neighborhood Neighborhoods, shrinking, 290 Neveu, J., 20-21, 24, 50 Newcomb, S., v Newton method, 146, 170, 239 Neyman-Pearson lemma, 270 for 2-alternating capacities, 275-278, 277</p> <p>test statistic, 8 Nikaid\u00fa, H., 139 Nonparametric, distinction between robust and, 6 Normal distribution, contaminated, 2 efficient estimate for, 70 minimax robust test, 272 Normal scores estimate, 145 breakdown point, 67 influence function, 65, 71 One-step estimate, of regression, 170 One-step L-estimate, 164 One-step M-estimate, 140 Optimality properties, correspondence between test and estimate, 282 Order statistics, linear combinations, see L-estimate Outlier, in regression, 4, 160 Outlier rejection, 4 comparison to robust procedures, 4-5 Outlier resistant, 4 Pointcloud, shape, 199 Polish space, 20, 25, 29 Principal component analysis, 199 Probability, lower and upper, 254 Prohorov, Y. V., 20, 24 Prohorov metric, 25-29, 27, 33-35, 34, 39, \\(41,42,110,271\\) Prohorov neighborhood, 26, 28, 277 Proposal, 2, 137, 144-145, 289 breakdown point, 143-144 Pseudo-covariance estimate, determined by implicit equations, 213 Pseudo-covariance matrix, 212 Pseudo-observations, 18, 197 Pseudo-variance, 12 Quadrant correlation, 205-206 Qualitative robustness, 7-10 of L-estimate, 59-61 of M-estimate, 52-54 of R-estimate, 65-68 and weak continuity, 8 Quantile, influence function, 56 Quantile distance, normalized, 12 Quantitative robustness, of L-estimate, 5961 of M-estimate, 52 of R-estimate, 65-68 Quenouille, M. H., 15 Rank correlation, Spearman, 205 Rank test, 281 estimate derived from, see R-estimate interval estimate derived from, 6 Redescending, see Descending Reeds, J. A., 37, 40 Regression, 17, 153 asymptotic normality of estimate, 158-159 L-estimate, 164 M-estimate, 162 R-estimate, 163 Regular measure, 20-21 Residuals, 160 and interpolated values, 162-164 Resistant, against outliers, 4 Resistant procedure, 7,9 R-estimate, vi, 43, 61-68, 127 asymptotically efficient, 68-72 breakdown point, 67 consistency, 68 continuity, 68 gross error sensitivity, 286 influence function, 63-65 of location, 62 minimax properties, 97-99 quantitative and qualitative robustness, 65-68 quantitative robustness, 65-68 of regression, 163 of scale, 114-116, 118 of shift, 62 Ridge regression, 155 Rieder, H., 291, 293 Robust, comparison to nonparametric and distribution-free, 6 Robust correlation, interpretation, 210 Robust covariance, affinely invariant estimate, 211-215 Robust covariance estimate, computation, 237-242 Robust estimate, computation, 17 construction, 72 standardization, 6 Robustness, 1 asymptotic minimax, 17 of design, 172, 243 distributional, 1,4</p> <p>finite sample minimax, 17 infinitesimal, 13-16 as an insurance problem, 73 optimal, 16-17 qualitative, \\(7-10\\) qualitative, definition, 10 and weak continuity, 8 quantitative, \\(10-13\\) of validity, 6 Robust procedure, desirable features, 5 Robust regression, asymptotic normality, 170 asymptotics, \\(164-170\\) bias, 171-172 Robust test, 253, 264-273, 290-293 Romanowski, M., 91, 94 Rounding, 9 Rubinstein, G., 30 Sacks, J., 7, 90, 98, 243 Saddlepoint technique, 47 Sample median, see Median Scale, L-estimate, 110-114 M-estimate, 109-110 R-estimate, 114-116 Scale estimate, 107 asymptotically efficient, 116-118 in regression, 163, 175-178 symmetrized version, 113 Scale functional, 202 Scale invariance, 127 Scale step, in computation of regression M-estimate, 179-180 Scatter matrix, breakdown point of estimate, 227-228 consistency and asymptotic normality, 226-227 existence of solution, 215-219 influence function of robust estimate, 223 maximum likelihood estimate, 211-215 uniqueness of solution, 219-221 Scatter step, in computation of robust covariance matrix, 238 Scholz, F. W., 138 Sch\u00f6nholzer, H., 215, 226 Schr\u00f6dinger equation, 83 Schweppe, F. C., 194 Scores generating function, 61, 63 Sensitivity, gross error, 14, 17, 71, 74, 286-287 of classical procedures to long tails, 4 Sensitivity curve, 15 Separability, in the sense of Doob, 128, 131 Sequential test, 273-275 Shafer, G., 263 Shorack, G. R., 150 Shrinking neighborhoods, 290 \u0160id\u00e1k, Z., 207 Sign test, 282, 291 Sine wave, of Andrews, 54, 103 Slope, minimax, 251 Smolyanov, O. G., 40 Space, Polish, 20, 25, 29 Spherical symmetry, 237 Stability principle, 1,10 Stahel, W., 228 Statistical functional, 8, 9 asymptotic normality, 11 consistency, 11 Stein, C., 7 Stein estimation, 155 Stigler, S. M., 61 Stone, C. J., 7 Strassen, V., 27, 263, 276, 278 Strassen's theorem, 27, 30, 41 Studentizing, 148-152, 197 comparison between jackknife and influence function, 150-152 an M-estimate of location, 150 a trimmed mean, 150-151 Subadditive, 255 Substochastic measure, 76, 81 Superadditive, 255 Symmetrized scale estimate, 113 breakdown point, 114 Symmetry, may be unrealistic assumption, 95</p> <p>Takeuchi, K., 7 Test, of independence, 199, 206 minimax robust, 265, 290-293 for binomial distribution, 273 for normal distribution, 272 sequential, 273-275 Tight, 23-24 Topology, vague, 76, 79 weak, 20, 25 Torgerson, E. N., 281 Total variation, 271</p> <p>Total variation metric, 27, 34 Trimmed mean, 9, 13, 71-72, 94, 104-106, \\(145-146\\) breakdown point, 143-144 continuity, 60 influence function, 57-58 studentizing, 150-151 Trimmed standard deviation, 94, 125 influence function, 112 Trimmed variance, 112, 122 Tukey, J. W., 1, 7, 15-16, 101 Upper expectation, 254 Upper probability, 254 Vague topology, 76, 79 Variance, estimation, 108 iteratively reweighted estimate is incon- sistent, 175, 198 jackknifed, 152 maximum, 11-12 minimax, 76 Variance breakdown point, 13</p>"},{"location":"research/note/lecture-note-automation/result/ocr_result/#index_2","title":"INDEX","text":"<p>Variance ratio, 249-251 Volterra derivative, see G\u00e2teaux derivative Von Mises, R., 40 Weak continuity, 8, 9-10, 21 Weak convergence, equivalence lemma, 22 on the real line, 23 Weak-star, see Weak continuity Weak topology, 20, 25 generated by Prohorov and Bounded Lipschitz metric, 33 Welsch, R. E., 194 Wilcoxon test, 63, 282 Winsorized mean, continuity, 60 influence function, 58 Winsorized residuals, metrically, 180, 182 Winsorized sample, 151 Winsorized variance, 113 Winsorizing, metrically, 18, 163 Wolf, G., 259 Ylvisaker, D., 90, 98, 243 Yohai, V. J., 170</p>"}]}